2025-01-06 22:23:30,019: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/ENTITY/', dataset='ENTITY', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106222313/ENTITY', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=1111, record=False, reply_loss_weight=0.1, save_path='./checkpoint/ENTITY', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[5000.0, 10000.0, 10000.0, 10000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:23:38,328: Snapshot:0	Epoch:0	Loss:13.235	translation_Loss:13.235	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:4.43	Hits@10:9.86	Best:4.43
2025-01-06 22:23:43,338: Snapshot:0	Epoch:1	Loss:11.044	translation_Loss:11.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.17	Hits@10:22.31	Best:9.17
2025-01-06 22:23:47,951: Snapshot:0	Epoch:2	Loss:9.167	translation_Loss:9.167	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.48	Hits@10:34.79	Best:13.48
2025-01-06 22:23:52,986: Snapshot:0	Epoch:3	Loss:7.176	translation_Loss:7.176	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.86	Hits@10:43.8	Best:18.86
2025-01-06 22:23:57,704: Snapshot:0	Epoch:4	Loss:5.303	translation_Loss:5.303	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.27	Hits@10:48.75	Best:24.27
2025-01-06 22:24:02,346: Snapshot:0	Epoch:5	Loss:3.795	translation_Loss:3.795	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:28.14	Hits@10:51.86	Best:28.14
2025-01-06 22:24:07,392: Snapshot:0	Epoch:6	Loss:2.666	translation_Loss:2.666	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:30.31	Hits@10:53.85	Best:30.31
2025-01-06 22:24:12,014: Snapshot:0	Epoch:7	Loss:1.876	translation_Loss:1.876	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:31.6	Hits@10:55.04	Best:31.6
2025-01-06 22:24:16,641: Snapshot:0	Epoch:8	Loss:1.352	translation_Loss:1.352	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.63	Hits@10:55.82	Best:32.63
2025-01-06 22:24:21,669: Snapshot:0	Epoch:9	Loss:0.994	translation_Loss:0.994	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.2	Hits@10:56.41	Best:33.2
2025-01-06 22:24:26,364: Snapshot:0	Epoch:10	Loss:0.761	translation_Loss:0.761	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.47	Hits@10:56.61	Best:33.47
2025-01-06 22:24:31,010: Snapshot:0	Epoch:11	Loss:0.612	translation_Loss:0.612	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.69	Hits@10:56.79	Best:33.69
2025-01-06 22:24:36,003: Snapshot:0	Epoch:12	Loss:0.508	translation_Loss:0.508	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.79	Hits@10:56.82	Best:33.79
2025-01-06 22:24:40,661: Snapshot:0	Epoch:13	Loss:0.431	translation_Loss:0.431	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.84	Hits@10:56.92	Best:33.84
2025-01-06 22:24:45,286: Snapshot:0	Epoch:14	Loss:0.375	translation_Loss:0.375	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.91	Hits@10:56.76	Best:33.91
2025-01-06 22:24:50,309: Snapshot:0	Epoch:15	Loss:0.333	translation_Loss:0.333	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:34.02	Hits@10:56.86	Best:34.02
2025-01-06 22:24:55,066: Snapshot:0	Epoch:16	Loss:0.299	translation_Loss:0.299	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.99	Hits@10:56.84	Best:34.02
2025-01-06 22:24:59,713: Snapshot:0	Epoch:17	Loss:0.275	translation_Loss:0.275	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.97	Hits@10:56.86	Best:34.02
2025-01-06 22:25:04,800: Early Stopping! Snapshot: 0 Epoch: 18 Best Results: 34.02
2025-01-06 22:25:04,801: Start to training tokens! Snapshot: 0 Epoch: 18 Loss:0.255 MRR:33.86 Best Results: 34.02
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:25:04,801: Snapshot:0	Epoch:18	Loss:0.255	translation_Loss:0.255	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.86	Hits@10:56.67	Best:34.02
2025-01-06 22:25:09,974: Snapshot:0	Epoch:19	Loss:21.472	translation_Loss:9.853	token_training_loss:11.62	distillation_Loss:0.0                                                   	MRR:33.86	Hits@10:56.67	Best:34.02
2025-01-06 22:25:14,944: End of token training: 0 Epoch: 20 Loss:10.319 MRR:33.86 Best Results: 34.02
2025-01-06 22:25:14,944: Snapshot:0	Epoch:20	Loss:10.319	translation_Loss:9.869	token_training_loss:0.45	distillation_Loss:0.0                                                           	MRR:33.86	Hits@10:56.67	Best:34.02
2025-01-06 22:25:15,196: => loading checkpoint './checkpoint/ENTITY/0model_best.tar'
2025-01-06 22:25:16,771: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3392 | 0.2163 | 0.3971 | 0.4721 |  0.5753 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (581,800)
├─Embedding: 1-2                         (93,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 676,200
Trainable params: 1,200
Non-trainable params: 675,000
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:25:31,348: Snapshot:1	Epoch:0	Loss:13.06	translation_Loss:12.577	token_training_loss:0.0	distillation_Loss:0.482                                                   	MRR:15.32	Hits@10:26.03	Best:15.32
2025-01-06 22:25:39,178: Snapshot:1	Epoch:1	Loss:5.293	translation_Loss:4.891	token_training_loss:0.0	distillation_Loss:0.402                                                   	MRR:19.9	Hits@10:35.11	Best:19.9
2025-01-06 22:25:47,373: Snapshot:1	Epoch:2	Loss:2.731	translation_Loss:2.486	token_training_loss:0.0	distillation_Loss:0.245                                                   	MRR:22.52	Hits@10:39.86	Best:22.52
2025-01-06 22:25:55,253: Snapshot:1	Epoch:3	Loss:1.893	translation_Loss:1.705	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:23.54	Hits@10:41.72	Best:23.54
2025-01-06 22:26:03,434: Snapshot:1	Epoch:4	Loss:1.535	translation_Loss:1.367	token_training_loss:0.0	distillation_Loss:0.168                                                   	MRR:24.07	Hits@10:42.51	Best:24.07
2025-01-06 22:26:11,263: Snapshot:1	Epoch:5	Loss:1.34	translation_Loss:1.187	token_training_loss:0.0	distillation_Loss:0.153                                                   	MRR:24.24	Hits@10:43.16	Best:24.24
2025-01-06 22:26:19,569: Snapshot:1	Epoch:6	Loss:1.223	translation_Loss:1.073	token_training_loss:0.0	distillation_Loss:0.15                                                   	MRR:24.46	Hits@10:43.47	Best:24.46
2025-01-06 22:26:27,556: Snapshot:1	Epoch:7	Loss:1.14	translation_Loss:0.996	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:24.7	Hits@10:43.79	Best:24.7
2025-01-06 22:26:35,844: Snapshot:1	Epoch:8	Loss:1.089	translation_Loss:0.947	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:24.72	Hits@10:43.56	Best:24.72
2025-01-06 22:26:43,676: Snapshot:1	Epoch:9	Loss:1.042	translation_Loss:0.902	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.02	Hits@10:43.94	Best:25.02
2025-01-06 22:26:51,872: Snapshot:1	Epoch:10	Loss:1.002	translation_Loss:0.864	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.01	Hits@10:44.08	Best:25.02
2025-01-06 22:27:00,160: Snapshot:1	Epoch:11	Loss:0.978	translation_Loss:0.841	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.05	Hits@10:44.29	Best:25.05
2025-01-06 22:27:08,003: Snapshot:1	Epoch:12	Loss:0.963	translation_Loss:0.824	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:25.1	Hits@10:44.42	Best:25.1
2025-01-06 22:27:16,221: Snapshot:1	Epoch:13	Loss:0.945	translation_Loss:0.808	token_training_loss:0.0	distillation_Loss:0.137                                                   	MRR:25.21	Hits@10:44.4	Best:25.21
2025-01-06 22:27:24,162: Snapshot:1	Epoch:14	Loss:0.919	translation_Loss:0.785	token_training_loss:0.0	distillation_Loss:0.134                                                   	MRR:25.17	Hits@10:44.59	Best:25.21
2025-01-06 22:27:32,494: Snapshot:1	Epoch:15	Loss:0.898	translation_Loss:0.763	token_training_loss:0.0	distillation_Loss:0.135                                                   	MRR:25.2	Hits@10:44.55	Best:25.21
2025-01-06 22:27:40,326: Early Stopping! Snapshot: 1 Epoch: 16 Best Results: 25.21
2025-01-06 22:27:40,327: Start to training tokens! Snapshot: 1 Epoch: 16 Loss:0.892 MRR:25.1 Best Results: 25.21
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:27:40,327: Snapshot:1	Epoch:16	Loss:0.892	translation_Loss:0.756	token_training_loss:0.0	distillation_Loss:0.136                                                   	MRR:25.1	Hits@10:44.36	Best:25.21
2025-01-06 22:27:48,553: Snapshot:1	Epoch:17	Loss:28.304	translation_Loss:16.022	token_training_loss:12.281	distillation_Loss:0.0                                                   	MRR:25.1	Hits@10:44.36	Best:25.21
2025-01-06 22:27:56,434: End of token training: 1 Epoch: 18 Loss:16.225 MRR:25.1 Best Results: 25.21
2025-01-06 22:27:56,434: Snapshot:1	Epoch:18	Loss:16.225	translation_Loss:16.002	token_training_loss:0.223	distillation_Loss:0.0                                                           	MRR:25.1	Hits@10:44.36	Best:25.21
2025-01-06 22:27:56,685: => loading checkpoint './checkpoint/ENTITY/1model_best.tar'
2025-01-06 22:28:01,100: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3396 | 0.2177 | 0.3972 | 0.4707 |  0.5735 |
|     1      | 0.2548 | 0.1571 | 0.2887 | 0.3539 |  0.4436 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,163,400)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,259,000
Trainable params: 1,200
Non-trainable params: 1,257,800
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:28:16,799: Snapshot:2	Epoch:0	Loss:11.762	translation_Loss:11.103	token_training_loss:0.0	distillation_Loss:0.659                                                   	MRR:14.83	Hits@10:25.5	Best:14.83
2025-01-06 22:28:25,460: Snapshot:2	Epoch:1	Loss:4.176	translation_Loss:3.699	token_training_loss:0.0	distillation_Loss:0.477                                                   	MRR:18.55	Hits@10:32.44	Best:18.55
2025-01-06 22:28:34,409: Snapshot:2	Epoch:2	Loss:2.277	translation_Loss:1.987	token_training_loss:0.0	distillation_Loss:0.291                                                   	MRR:20.43	Hits@10:36.19	Best:20.43
2025-01-06 22:28:43,017: Snapshot:2	Epoch:3	Loss:1.712	translation_Loss:1.477	token_training_loss:0.0	distillation_Loss:0.235                                                   	MRR:21.49	Hits@10:38.05	Best:21.49
2025-01-06 22:28:52,019: Snapshot:2	Epoch:4	Loss:1.465	translation_Loss:1.251	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:22.05	Hits@10:39.01	Best:22.05
2025-01-06 22:29:01,006: Snapshot:2	Epoch:5	Loss:1.333	translation_Loss:1.128	token_training_loss:0.0	distillation_Loss:0.206                                                   	MRR:22.53	Hits@10:39.78	Best:22.53
2025-01-06 22:29:09,589: Snapshot:2	Epoch:6	Loss:1.255	translation_Loss:1.053	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:22.78	Hits@10:40.24	Best:22.78
2025-01-06 22:29:18,486: Snapshot:2	Epoch:7	Loss:1.199	translation_Loss:1.0	token_training_loss:0.0	distillation_Loss:0.199                                                   	MRR:22.81	Hits@10:40.33	Best:22.81
2025-01-06 22:29:27,138: Snapshot:2	Epoch:8	Loss:1.141	translation_Loss:0.945	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:22.97	Hits@10:40.71	Best:22.97
2025-01-06 22:29:36,098: Snapshot:2	Epoch:9	Loss:1.11	translation_Loss:0.916	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:23.15	Hits@10:40.93	Best:23.15
2025-01-06 22:29:44,700: Snapshot:2	Epoch:10	Loss:1.089	translation_Loss:0.894	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:23.19	Hits@10:41.0	Best:23.19
2025-01-06 22:29:53,700: Snapshot:2	Epoch:11	Loss:1.067	translation_Loss:0.871	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:23.19	Hits@10:41.06	Best:23.19
2025-01-06 22:30:02,486: Snapshot:2	Epoch:12	Loss:1.044	translation_Loss:0.849	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:23.22	Hits@10:41.22	Best:23.22
2025-01-06 22:30:11,742: Snapshot:2	Epoch:13	Loss:1.029	translation_Loss:0.836	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:23.23	Hits@10:41.28	Best:23.23
2025-01-06 22:30:20,679: Snapshot:2	Epoch:14	Loss:1.013	translation_Loss:0.817	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:23.45	Hits@10:41.31	Best:23.45
2025-01-06 22:30:29,279: Snapshot:2	Epoch:15	Loss:1.005	translation_Loss:0.809	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:23.38	Hits@10:41.31	Best:23.45
2025-01-06 22:30:38,166: Snapshot:2	Epoch:16	Loss:1.009	translation_Loss:0.812	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:23.35	Hits@10:41.18	Best:23.45
2025-01-06 22:30:46,741: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 23.45
2025-01-06 22:30:46,742: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:0.991 MRR:23.35 Best Results: 23.45
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:30:46,742: Snapshot:2	Epoch:17	Loss:0.991	translation_Loss:0.795	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:23.35	Hits@10:41.21	Best:23.45
2025-01-06 22:30:55,722: Snapshot:2	Epoch:18	Loss:28.323	translation_Loss:15.857	token_training_loss:12.465	distillation_Loss:0.0                                                   	MRR:23.35	Hits@10:41.21	Best:23.45
2025-01-06 22:31:04,272: End of token training: 2 Epoch: 19 Loss:16.092 MRR:23.35 Best Results: 23.45
2025-01-06 22:31:04,272: Snapshot:2	Epoch:19	Loss:16.092	translation_Loss:15.856	token_training_loss:0.236	distillation_Loss:0.0                                                           	MRR:23.35	Hits@10:41.21	Best:23.45
2025-01-06 22:31:04,560: => loading checkpoint './checkpoint/ENTITY/2model_best.tar'
2025-01-06 22:31:12,196: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3378 | 0.215  | 0.3958 | 0.4713 |  0.5718 |
|     1      | 0.255  | 0.1568 | 0.2892 | 0.3533 |  0.4456 |
|     2      | 0.2371 | 0.1424 | 0.2738 | 0.3336 |  0.4158 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,745,000)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,840,600
Trainable params: 1,200
Non-trainable params: 1,839,400
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:31:28,481: Snapshot:3	Epoch:0	Loss:10.733	translation_Loss:10.022	token_training_loss:0.0	distillation_Loss:0.712                                                   	MRR:15.12	Hits@10:26.65	Best:15.12
2025-01-06 22:31:37,557: Snapshot:3	Epoch:1	Loss:3.337	translation_Loss:2.781	token_training_loss:0.0	distillation_Loss:0.556                                                   	MRR:18.58	Hits@10:33.19	Best:18.58
2025-01-06 22:31:46,313: Snapshot:3	Epoch:2	Loss:1.804	translation_Loss:1.491	token_training_loss:0.0	distillation_Loss:0.313                                                   	MRR:20.05	Hits@10:35.91	Best:20.05
2025-01-06 22:31:55,450: Snapshot:3	Epoch:3	Loss:1.371	translation_Loss:1.124	token_training_loss:0.0	distillation_Loss:0.246                                                   	MRR:21.16	Hits@10:37.63	Best:21.16
2025-01-06 22:32:04,171: Snapshot:3	Epoch:4	Loss:1.186	translation_Loss:0.962	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:21.6	Hits@10:38.37	Best:21.6
2025-01-06 22:32:13,288: Snapshot:3	Epoch:5	Loss:1.081	translation_Loss:0.868	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:21.86	Hits@10:39.03	Best:21.86
2025-01-06 22:32:22,023: Snapshot:3	Epoch:6	Loss:1.02	translation_Loss:0.811	token_training_loss:0.0	distillation_Loss:0.209                                                   	MRR:22.04	Hits@10:39.54	Best:22.04
2025-01-06 22:32:31,225: Snapshot:3	Epoch:7	Loss:0.964	translation_Loss:0.759	token_training_loss:0.0	distillation_Loss:0.204                                                   	MRR:22.16	Hits@10:39.8	Best:22.16
2025-01-06 22:32:39,927: Snapshot:3	Epoch:8	Loss:0.94	translation_Loss:0.736	token_training_loss:0.0	distillation_Loss:0.204                                                   	MRR:22.39	Hits@10:39.99	Best:22.39
2025-01-06 22:32:48,957: Snapshot:3	Epoch:9	Loss:0.913	translation_Loss:0.71	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:22.35	Hits@10:40.18	Best:22.39
2025-01-06 22:32:57,671: Snapshot:3	Epoch:10	Loss:0.894	translation_Loss:0.692	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:22.48	Hits@10:40.19	Best:22.48
2025-01-06 22:33:06,766: Snapshot:3	Epoch:11	Loss:0.88	translation_Loss:0.676	token_training_loss:0.0	distillation_Loss:0.204                                                   	MRR:22.61	Hits@10:40.54	Best:22.61
2025-01-06 22:33:15,454: Snapshot:3	Epoch:12	Loss:0.867	translation_Loss:0.665	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:22.5	Hits@10:40.68	Best:22.61
2025-01-06 22:33:24,637: Snapshot:3	Epoch:13	Loss:0.847	translation_Loss:0.646	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:22.78	Hits@10:40.59	Best:22.78
2025-01-06 22:33:33,324: Snapshot:3	Epoch:14	Loss:0.838	translation_Loss:0.635	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:22.7	Hits@10:40.78	Best:22.78
2025-01-06 22:33:42,339: Snapshot:3	Epoch:15	Loss:0.831	translation_Loss:0.628	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:22.63	Hits@10:40.87	Best:22.78
2025-01-06 22:33:50,998: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 22.78
2025-01-06 22:33:50,998: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:0.822 MRR:22.78 Best Results: 22.78
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:33:50,999: Snapshot:3	Epoch:16	Loss:0.822	translation_Loss:0.622	token_training_loss:0.0	distillation_Loss:0.2                                                   	MRR:22.78	Hits@10:40.9	Best:22.78
2025-01-06 22:34:00,109: Snapshot:3	Epoch:17	Loss:26.18	translation_Loss:14.309	token_training_loss:11.871	distillation_Loss:0.0                                                   	MRR:22.78	Hits@10:40.9	Best:22.78
2025-01-06 22:34:08,819: End of token training: 3 Epoch: 18 Loss:14.546 MRR:22.78 Best Results: 22.78
2025-01-06 22:34:08,819: Snapshot:3	Epoch:18	Loss:14.546	translation_Loss:14.305	token_training_loss:0.242	distillation_Loss:0.0                                                           	MRR:22.78	Hits@10:40.9	Best:22.78
2025-01-06 22:34:09,078: => loading checkpoint './checkpoint/ENTITY/3model_best.tar'
2025-01-06 22:34:20,264: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3371 | 0.2146 | 0.3934 | 0.471  |  0.5698 |
|     1      | 0.2558 | 0.1575 | 0.2907 | 0.3544 |  0.4464 |
|     2      | 0.2386 | 0.1435 | 0.2748 | 0.3357 |  0.4192 |
|     3      | 0.2305 | 0.1368 | 0.268  | 0.327  |  0.4064 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,326,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,422,600
Trainable params: 1,200
Non-trainable params: 2,421,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:34:33,536: Snapshot:4	Epoch:0	Loss:7.982	translation_Loss:7.522	token_training_loss:0.0	distillation_Loss:0.46                                                   	MRR:17.17	Hits@10:32.39	Best:17.17
2025-01-06 22:34:40,246: Snapshot:4	Epoch:1	Loss:2.479	translation_Loss:1.978	token_training_loss:0.0	distillation_Loss:0.501                                                   	MRR:22.75	Hits@10:41.9	Best:22.75
2025-01-06 22:34:46,651: Snapshot:4	Epoch:2	Loss:1.071	translation_Loss:0.775	token_training_loss:0.0	distillation_Loss:0.296                                                   	MRR:24.68	Hits@10:43.09	Best:24.68
2025-01-06 22:34:53,039: Snapshot:4	Epoch:3	Loss:0.674	translation_Loss:0.494	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:25.15	Hits@10:44.32	Best:25.15
2025-01-06 22:34:59,922: Snapshot:4	Epoch:4	Loss:0.526	translation_Loss:0.385	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.58	Hits@10:45.22	Best:25.58
2025-01-06 22:35:06,278: Snapshot:4	Epoch:5	Loss:0.468	translation_Loss:0.345	token_training_loss:0.0	distillation_Loss:0.123                                                   	MRR:26.16	Hits@10:45.96	Best:26.16
2025-01-06 22:35:12,994: Snapshot:4	Epoch:6	Loss:0.421	translation_Loss:0.304	token_training_loss:0.0	distillation_Loss:0.117                                                   	MRR:26.34	Hits@10:46.52	Best:26.34
2025-01-06 22:35:19,318: Snapshot:4	Epoch:7	Loss:0.392	translation_Loss:0.283	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:26.66	Hits@10:46.87	Best:26.66
2025-01-06 22:35:25,752: Snapshot:4	Epoch:8	Loss:0.375	translation_Loss:0.267	token_training_loss:0.0	distillation_Loss:0.107                                                   	MRR:26.79	Hits@10:47.04	Best:26.79
2025-01-06 22:35:32,465: Snapshot:4	Epoch:9	Loss:0.361	translation_Loss:0.256	token_training_loss:0.0	distillation_Loss:0.105                                                   	MRR:26.94	Hits@10:47.03	Best:26.94
2025-01-06 22:35:38,822: Snapshot:4	Epoch:10	Loss:0.351	translation_Loss:0.248	token_training_loss:0.0	distillation_Loss:0.104                                                   	MRR:27.02	Hits@10:47.59	Best:27.02
2025-01-06 22:35:45,126: Snapshot:4	Epoch:11	Loss:0.335	translation_Loss:0.233	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:27.02	Hits@10:47.53	Best:27.02
2025-01-06 22:35:51,871: Snapshot:4	Epoch:12	Loss:0.328	translation_Loss:0.228	token_training_loss:0.0	distillation_Loss:0.1                                                   	MRR:27.32	Hits@10:47.9	Best:27.32
2025-01-06 22:35:58,251: Snapshot:4	Epoch:13	Loss:0.325	translation_Loss:0.225	token_training_loss:0.0	distillation_Loss:0.1                                                   	MRR:27.24	Hits@10:47.84	Best:27.32
2025-01-06 22:36:04,575: Snapshot:4	Epoch:14	Loss:0.321	translation_Loss:0.22	token_training_loss:0.0	distillation_Loss:0.101                                                   	MRR:27.16	Hits@10:47.94	Best:27.32
2025-01-06 22:36:11,282: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 27.32
2025-01-06 22:36:11,282: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:0.32 MRR:27.26 Best Results: 27.32
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:36:11,282: Snapshot:4	Epoch:15	Loss:0.32	translation_Loss:0.218	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:27.26	Hits@10:48.08	Best:27.32
2025-01-06 22:36:17,510: Snapshot:4	Epoch:16	Loss:20.475	translation_Loss:7.816	token_training_loss:12.659	distillation_Loss:0.0                                                   	MRR:27.26	Hits@10:48.08	Best:27.32
2025-01-06 22:36:23,752: End of token training: 4 Epoch: 17 Loss:8.293 MRR:27.26 Best Results: 27.32
2025-01-06 22:36:23,753: Snapshot:4	Epoch:17	Loss:8.293	translation_Loss:7.809	token_training_loss:0.484	distillation_Loss:0.0                                                           	MRR:27.26	Hits@10:48.08	Best:27.32
2025-01-06 22:36:24,009: => loading checkpoint './checkpoint/ENTITY/4model_best.tar'
2025-01-06 22:36:38,769: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3335 | 0.2104 | 0.3891 | 0.4682 |  0.567  |
|     1      | 0.2545 | 0.1565 | 0.2901 | 0.352  |  0.4452 |
|     2      | 0.2392 | 0.1448 | 0.2739 | 0.3366 |  0.4221 |
|     3      | 0.2325 | 0.1385 | 0.2704 |  0.33  |  0.408  |
|     4      | 0.2742 | 0.1639 | 0.3268 | 0.4003 |  0.4851 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,004,200
Trainable params: 1,200
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:36:38,772: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3392 | 0.2163 | 0.3971 | 0.4721 |  0.5753 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3396 | 0.2177 | 0.3972 | 0.4707 |  0.5735 |
|     1      | 0.2548 | 0.1571 | 0.2887 | 0.3539 |  0.4436 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3378 | 0.215  | 0.3958 | 0.4713 |  0.5718 |
|     1      | 0.255  | 0.1568 | 0.2892 | 0.3533 |  0.4456 |
|     2      | 0.2371 | 0.1424 | 0.2738 | 0.3336 |  0.4158 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3371 | 0.2146 | 0.3934 | 0.471  |  0.5698 |
|     1      | 0.2558 | 0.1575 | 0.2907 | 0.3544 |  0.4464 |
|     2      | 0.2386 | 0.1435 | 0.2748 | 0.3357 |  0.4192 |
|     3      | 0.2305 | 0.1368 | 0.268  | 0.327  |  0.4064 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3335 | 0.2104 | 0.3891 | 0.4682 |  0.567  |
|     1      | 0.2545 | 0.1565 | 0.2901 | 0.352  |  0.4452 |
|     2      | 0.2392 | 0.1448 | 0.2739 | 0.3366 |  0.4221 |
|     3      | 0.2325 | 0.1385 | 0.2704 |  0.33  |  0.408  |
|     4      | 0.2742 | 0.1639 | 0.3268 | 0.4003 |  0.4851 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:36:38,773: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 104.92434239387512 |   0.339   |    0.216     |    0.397     |     0.575     |
|    1     | 157.01095151901245 |   0.288   |    0.181     |    0.331     |     0.494     |
|    2     | 179.9836869239807  |   0.268   |    0.165     |    0.309     |     0.465     |
|    3     | 173.0570991039276  |   0.259   |    0.158     |    0.298     |      0.45     |
|    4     | 120.2841227054596  |   0.261   |    0.159     |    0.302     |     0.456     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:36:38,773: Sum_Training_Time:735.2602026462555
2025-01-06 22:36:38,773: Every_Training_Time:[104.92434239387512, 157.01095151901245, 179.9836869239807, 173.0570991039276, 120.2841227054596]
2025-01-06 22:36:38,773: Forward transfer: 0.0465 Backward transfer: -0.0004750000000000032
2025-01-06 22:36:59,731: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/ENTITY/', dataset='ENTITY', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106223644/ENTITY', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=2222, record=False, reply_loss_weight=0.1, save_path='./checkpoint/ENTITY', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[5000.0, 10000.0, 10000.0, 10000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:37:07,999: Snapshot:0	Epoch:0	Loss:13.209	translation_Loss:13.209	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:4.4	Hits@10:9.58	Best:4.4
2025-01-06 22:37:12,953: Snapshot:0	Epoch:1	Loss:11.004	translation_Loss:11.004	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.35	Hits@10:22.9	Best:9.35
2025-01-06 22:37:17,562: Snapshot:0	Epoch:2	Loss:9.083	translation_Loss:9.083	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.77	Hits@10:35.34	Best:13.77
2025-01-06 22:37:22,722: Snapshot:0	Epoch:3	Loss:7.031	translation_Loss:7.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:19.2	Hits@10:43.54	Best:19.2
2025-01-06 22:37:27,359: Snapshot:0	Epoch:4	Loss:5.166	translation_Loss:5.166	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.72	Hits@10:48.71	Best:24.72
2025-01-06 22:37:31,935: Snapshot:0	Epoch:5	Loss:3.668	translation_Loss:3.668	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:28.34	Hits@10:51.95	Best:28.34
2025-01-06 22:37:36,912: Snapshot:0	Epoch:6	Loss:2.585	translation_Loss:2.585	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:30.29	Hits@10:54.05	Best:30.29
2025-01-06 22:37:41,508: Snapshot:0	Epoch:7	Loss:1.824	translation_Loss:1.824	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:31.47	Hits@10:55.28	Best:31.47
2025-01-06 22:37:46,072: Snapshot:0	Epoch:8	Loss:1.312	translation_Loss:1.312	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.31	Hits@10:55.84	Best:32.31
2025-01-06 22:37:51,047: Snapshot:0	Epoch:9	Loss:0.984	translation_Loss:0.984	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.83	Hits@10:56.28	Best:32.83
2025-01-06 22:37:55,701: Snapshot:0	Epoch:10	Loss:0.76	translation_Loss:0.76	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.06	Hits@10:56.38	Best:33.06
2025-01-06 22:38:00,294: Snapshot:0	Epoch:11	Loss:0.608	translation_Loss:0.608	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.4	Hits@10:56.43	Best:33.4
2025-01-06 22:38:05,232: Snapshot:0	Epoch:12	Loss:0.502	translation_Loss:0.502	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.45	Hits@10:56.35	Best:33.45
2025-01-06 22:38:09,840: Snapshot:0	Epoch:13	Loss:0.428	translation_Loss:0.428	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.38	Hits@10:56.39	Best:33.45
2025-01-06 22:38:14,457: Snapshot:0	Epoch:14	Loss:0.372	translation_Loss:0.372	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.52	Hits@10:56.24	Best:33.52
2025-01-06 22:38:19,428: Snapshot:0	Epoch:15	Loss:0.332	translation_Loss:0.332	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.5	Hits@10:56.26	Best:33.52
2025-01-06 22:38:24,052: Snapshot:0	Epoch:16	Loss:0.305	translation_Loss:0.305	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.42	Hits@10:56.22	Best:33.52
2025-01-06 22:38:28,661: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 33.52
2025-01-06 22:38:28,661: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.278 MRR:33.47 Best Results: 33.52
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:38:28,661: Snapshot:0	Epoch:17	Loss:0.278	translation_Loss:0.278	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.47	Hits@10:56.14	Best:33.52
2025-01-06 22:38:34,156: Snapshot:0	Epoch:18	Loss:21.106	translation_Loss:9.753	token_training_loss:11.353	distillation_Loss:0.0                                                   	MRR:33.47	Hits@10:56.14	Best:33.52
2025-01-06 22:38:38,744: End of token training: 0 Epoch: 19 Loss:10.196 MRR:33.47 Best Results: 33.52
2025-01-06 22:38:38,744: Snapshot:0	Epoch:19	Loss:10.196	translation_Loss:9.75	token_training_loss:0.445	distillation_Loss:0.0                                                           	MRR:33.47	Hits@10:56.14	Best:33.52
2025-01-06 22:38:38,992: => loading checkpoint './checkpoint/ENTITY/0model_best.tar'
2025-01-06 22:38:40,457: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3366 | 0.2163 | 0.3899 | 0.4629 |  0.565  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (581,800)
├─Embedding: 1-2                         (93,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 676,200
Trainable params: 1,200
Non-trainable params: 675,000
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:38:55,179: Snapshot:1	Epoch:0	Loss:13.137	translation_Loss:12.654	token_training_loss:0.0	distillation_Loss:0.484                                                   	MRR:15.2	Hits@10:25.73	Best:15.2
2025-01-06 22:39:03,020: Snapshot:1	Epoch:1	Loss:5.432	translation_Loss:5.029	token_training_loss:0.0	distillation_Loss:0.402                                                   	MRR:19.87	Hits@10:34.87	Best:19.87
2025-01-06 22:39:11,200: Snapshot:1	Epoch:2	Loss:2.846	translation_Loss:2.596	token_training_loss:0.0	distillation_Loss:0.25                                                   	MRR:22.43	Hits@10:39.43	Best:22.43
2025-01-06 22:39:18,999: Snapshot:1	Epoch:3	Loss:1.996	translation_Loss:1.802	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:23.61	Hits@10:41.52	Best:23.61
2025-01-06 22:39:27,205: Snapshot:1	Epoch:4	Loss:1.603	translation_Loss:1.431	token_training_loss:0.0	distillation_Loss:0.172                                                   	MRR:24.16	Hits@10:42.45	Best:24.16
2025-01-06 22:39:35,001: Snapshot:1	Epoch:5	Loss:1.404	translation_Loss:1.244	token_training_loss:0.0	distillation_Loss:0.16                                                   	MRR:24.39	Hits@10:43.03	Best:24.39
2025-01-06 22:39:43,146: Snapshot:1	Epoch:6	Loss:1.288	translation_Loss:1.134	token_training_loss:0.0	distillation_Loss:0.154                                                   	MRR:24.56	Hits@10:43.4	Best:24.56
2025-01-06 22:39:50,964: Snapshot:1	Epoch:7	Loss:1.197	translation_Loss:1.048	token_training_loss:0.0	distillation_Loss:0.149                                                   	MRR:24.71	Hits@10:43.67	Best:24.71
2025-01-06 22:39:59,405: Snapshot:1	Epoch:8	Loss:1.14	translation_Loss:0.992	token_training_loss:0.0	distillation_Loss:0.148                                                   	MRR:24.81	Hits@10:43.88	Best:24.81
2025-01-06 22:40:07,769: Snapshot:1	Epoch:9	Loss:1.091	translation_Loss:0.948	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:24.93	Hits@10:44.02	Best:24.93
2025-01-06 22:40:15,584: Snapshot:1	Epoch:10	Loss:1.057	translation_Loss:0.913	token_training_loss:0.0	distillation_Loss:0.145                                                   	MRR:25.02	Hits@10:44.18	Best:25.02
2025-01-06 22:40:23,719: Snapshot:1	Epoch:11	Loss:1.027	translation_Loss:0.884	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:24.97	Hits@10:44.0	Best:25.02
2025-01-06 22:40:31,523: Snapshot:1	Epoch:12	Loss:0.996	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.141                                                   	MRR:24.9	Hits@10:44.06	Best:25.02
2025-01-06 22:40:39,619: Early Stopping! Snapshot: 1 Epoch: 13 Best Results: 25.02
2025-01-06 22:40:39,620: Start to training tokens! Snapshot: 1 Epoch: 13 Loss:0.979 MRR:24.89 Best Results: 25.02
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:40:39,620: Snapshot:1	Epoch:13	Loss:0.979	translation_Loss:0.841	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:24.89	Hits@10:43.98	Best:25.02
2025-01-06 22:40:47,372: Snapshot:1	Epoch:14	Loss:27.885	translation_Loss:15.88	token_training_loss:12.005	distillation_Loss:0.0                                                   	MRR:24.89	Hits@10:43.98	Best:25.02
2025-01-06 22:40:55,551: End of token training: 1 Epoch: 15 Loss:16.088 MRR:24.89 Best Results: 25.02
2025-01-06 22:40:55,551: Snapshot:1	Epoch:15	Loss:16.088	translation_Loss:15.878	token_training_loss:0.21	distillation_Loss:0.0                                                           	MRR:24.89	Hits@10:43.98	Best:25.02
2025-01-06 22:40:55,823: => loading checkpoint './checkpoint/ENTITY/1model_best.tar'
2025-01-06 22:40:59,957: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3363 | 0.2152 | 0.3901 | 0.4622 |  0.5682 |
|     1      | 0.2501 | 0.1522 | 0.2862 | 0.3469 |  0.4378 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,163,400)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,259,000
Trainable params: 1,200
Non-trainable params: 1,257,800
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:41:15,557: Snapshot:2	Epoch:0	Loss:11.762	translation_Loss:11.096	token_training_loss:0.0	distillation_Loss:0.666                                                   	MRR:15.04	Hits@10:25.61	Best:15.04
2025-01-06 22:41:24,449: Snapshot:2	Epoch:1	Loss:4.247	translation_Loss:3.762	token_training_loss:0.0	distillation_Loss:0.484                                                   	MRR:18.48	Hits@10:32.58	Best:18.48
2025-01-06 22:41:32,998: Snapshot:2	Epoch:2	Loss:2.361	translation_Loss:2.062	token_training_loss:0.0	distillation_Loss:0.299                                                   	MRR:20.52	Hits@10:36.45	Best:20.52
2025-01-06 22:41:41,862: Snapshot:2	Epoch:3	Loss:1.795	translation_Loss:1.551	token_training_loss:0.0	distillation_Loss:0.243                                                   	MRR:21.59	Hits@10:37.94	Best:21.59
2025-01-06 22:41:50,715: Snapshot:2	Epoch:4	Loss:1.546	translation_Loss:1.321	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:22.19	Hits@10:38.93	Best:22.19
2025-01-06 22:41:59,351: Snapshot:2	Epoch:5	Loss:1.404	translation_Loss:1.189	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:22.47	Hits@10:39.65	Best:22.47
2025-01-06 22:42:08,456: Snapshot:2	Epoch:6	Loss:1.312	translation_Loss:1.103	token_training_loss:0.0	distillation_Loss:0.208                                                   	MRR:22.66	Hits@10:39.82	Best:22.66
2025-01-06 22:42:16,966: Snapshot:2	Epoch:7	Loss:1.246	translation_Loss:1.04	token_training_loss:0.0	distillation_Loss:0.206                                                   	MRR:22.84	Hits@10:40.22	Best:22.84
2025-01-06 22:42:25,947: Snapshot:2	Epoch:8	Loss:1.198	translation_Loss:0.994	token_training_loss:0.0	distillation_Loss:0.204                                                   	MRR:23.03	Hits@10:40.46	Best:23.03
2025-01-06 22:42:34,596: Snapshot:2	Epoch:9	Loss:1.169	translation_Loss:0.965	token_training_loss:0.0	distillation_Loss:0.204                                                   	MRR:23.18	Hits@10:40.67	Best:23.18
2025-01-06 22:42:43,488: Snapshot:2	Epoch:10	Loss:1.144	translation_Loss:0.942	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:23.04	Hits@10:40.53	Best:23.18
2025-01-06 22:42:51,928: Snapshot:2	Epoch:11	Loss:1.114	translation_Loss:0.911	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:23.17	Hits@10:40.9	Best:23.18
2025-01-06 22:43:00,875: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 23.18
2025-01-06 22:43:00,876: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:1.098 MRR:23.15 Best Results: 23.18
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:43:00,876: Snapshot:2	Epoch:12	Loss:1.098	translation_Loss:0.897	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:23.15	Hits@10:41.04	Best:23.18
2025-01-06 22:43:09,756: Snapshot:2	Epoch:13	Loss:28.928	translation_Loss:15.75	token_training_loss:13.178	distillation_Loss:0.0                                                   	MRR:23.15	Hits@10:41.04	Best:23.18
2025-01-06 22:43:18,274: End of token training: 2 Epoch: 14 Loss:15.987 MRR:23.15 Best Results: 23.18
2025-01-06 22:43:18,274: Snapshot:2	Epoch:14	Loss:15.987	translation_Loss:15.763	token_training_loss:0.224	distillation_Loss:0.0                                                           	MRR:23.15	Hits@10:41.04	Best:23.18
2025-01-06 22:43:18,595: => loading checkpoint './checkpoint/ENTITY/2model_best.tar'
2025-01-06 22:43:26,397: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.335  | 0.2146 | 0.3877 | 0.4637 |  0.5669 |
|     1      | 0.2501 | 0.1518 | 0.2852 | 0.3498 |  0.4383 |
|     2      | 0.2334 | 0.1404 | 0.2678 | 0.3276 |  0.4093 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,745,000)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,840,600
Trainable params: 1,200
Non-trainable params: 1,839,400
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:43:42,620: Snapshot:3	Epoch:0	Loss:10.786	translation_Loss:10.06	token_training_loss:0.0	distillation_Loss:0.726                                                   	MRR:15.12	Hits@10:26.86	Best:15.12
2025-01-06 22:43:51,247: Snapshot:3	Epoch:1	Loss:3.442	translation_Loss:2.87	token_training_loss:0.0	distillation_Loss:0.572                                                   	MRR:18.69	Hits@10:33.4	Best:18.69
2025-01-06 22:44:00,308: Snapshot:3	Epoch:2	Loss:1.883	translation_Loss:1.554	token_training_loss:0.0	distillation_Loss:0.329                                                   	MRR:20.06	Hits@10:36.1	Best:20.06
2025-01-06 22:44:08,922: Snapshot:3	Epoch:3	Loss:1.435	translation_Loss:1.175	token_training_loss:0.0	distillation_Loss:0.26                                                   	MRR:20.72	Hits@10:37.63	Best:20.72
2025-01-06 22:44:17,936: Snapshot:3	Epoch:4	Loss:1.244	translation_Loss:1.008	token_training_loss:0.0	distillation_Loss:0.236                                                   	MRR:21.37	Hits@10:38.9	Best:21.37
2025-01-06 22:44:26,660: Snapshot:3	Epoch:5	Loss:1.133	translation_Loss:0.907	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:21.65	Hits@10:39.07	Best:21.65
2025-01-06 22:44:35,746: Snapshot:3	Epoch:6	Loss:1.08	translation_Loss:0.859	token_training_loss:0.0	distillation_Loss:0.221                                                   	MRR:22.15	Hits@10:39.54	Best:22.15
2025-01-06 22:44:44,375: Snapshot:3	Epoch:7	Loss:1.028	translation_Loss:0.808	token_training_loss:0.0	distillation_Loss:0.22                                                   	MRR:22.28	Hits@10:40.03	Best:22.28
2025-01-06 22:44:53,411: Snapshot:3	Epoch:8	Loss:0.988	translation_Loss:0.771	token_training_loss:0.0	distillation_Loss:0.217                                                   	MRR:22.49	Hits@10:40.31	Best:22.49
2025-01-06 22:45:02,291: Snapshot:3	Epoch:9	Loss:0.956	translation_Loss:0.744	token_training_loss:0.0	distillation_Loss:0.212                                                   	MRR:22.59	Hits@10:40.36	Best:22.59
2025-01-06 22:45:11,323: Snapshot:3	Epoch:10	Loss:0.949	translation_Loss:0.735	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:22.73	Hits@10:40.73	Best:22.73
2025-01-06 22:45:19,908: Snapshot:3	Epoch:11	Loss:0.932	translation_Loss:0.717	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:22.72	Hits@10:40.79	Best:22.73
2025-01-06 22:45:28,916: Snapshot:3	Epoch:12	Loss:0.914	translation_Loss:0.698	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:22.67	Hits@10:40.8	Best:22.73
2025-01-06 22:45:37,898: Early Stopping! Snapshot: 3 Epoch: 13 Best Results: 22.73
2025-01-06 22:45:37,899: Start to training tokens! Snapshot: 3 Epoch: 13 Loss:0.904 MRR:22.68 Best Results: 22.73
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:45:37,899: Snapshot:3	Epoch:13	Loss:0.904	translation_Loss:0.689	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:22.68	Hits@10:40.95	Best:22.73
2025-01-06 22:45:46,494: Snapshot:3	Epoch:14	Loss:26.601	translation_Loss:14.138	token_training_loss:12.462	distillation_Loss:0.0                                                   	MRR:22.68	Hits@10:40.95	Best:22.73
2025-01-06 22:45:55,545: End of token training: 3 Epoch: 15 Loss:14.386 MRR:22.68 Best Results: 22.73
2025-01-06 22:45:55,546: Snapshot:3	Epoch:15	Loss:14.386	translation_Loss:14.126	token_training_loss:0.26	distillation_Loss:0.0                                                           	MRR:22.68	Hits@10:40.95	Best:22.73
2025-01-06 22:45:55,804: => loading checkpoint './checkpoint/ENTITY/3model_best.tar'
2025-01-06 22:46:06,666: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3338 | 0.2127 | 0.3856 | 0.4627 |  0.5654 |
|     1      | 0.2508 | 0.1518 | 0.2874 |  0.35  |  0.4391 |
|     2      | 0.2359 | 0.1421 | 0.2718 | 0.3312 |  0.4148 |
|     3      | 0.2292 | 0.1354 | 0.2662 | 0.3273 |  0.4059 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,326,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,422,600
Trainable params: 1,200
Non-trainable params: 2,421,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:46:19,762: Snapshot:4	Epoch:0	Loss:8.01	translation_Loss:7.539	token_training_loss:0.0	distillation_Loss:0.47                                                   	MRR:17.05	Hits@10:32.27	Best:17.05
2025-01-06 22:46:26,486: Snapshot:4	Epoch:1	Loss:2.512	translation_Loss:1.995	token_training_loss:0.0	distillation_Loss:0.516                                                   	MRR:22.5	Hits@10:41.69	Best:22.5
2025-01-06 22:46:32,847: Snapshot:4	Epoch:2	Loss:1.105	translation_Loss:0.796	token_training_loss:0.0	distillation_Loss:0.309                                                   	MRR:24.63	Hits@10:43.46	Best:24.63
2025-01-06 22:46:39,128: Snapshot:4	Epoch:3	Loss:0.695	translation_Loss:0.505	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:25.41	Hits@10:44.64	Best:25.41
2025-01-06 22:46:45,787: Snapshot:4	Epoch:4	Loss:0.553	translation_Loss:0.404	token_training_loss:0.0	distillation_Loss:0.149                                                   	MRR:25.86	Hits@10:45.61	Best:25.86
2025-01-06 22:46:52,081: Snapshot:4	Epoch:5	Loss:0.483	translation_Loss:0.352	token_training_loss:0.0	distillation_Loss:0.131                                                   	MRR:26.32	Hits@10:46.11	Best:26.32
2025-01-06 22:46:58,429: Snapshot:4	Epoch:6	Loss:0.439	translation_Loss:0.318	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:26.37	Hits@10:46.73	Best:26.37
2025-01-06 22:47:05,162: Snapshot:4	Epoch:7	Loss:0.406	translation_Loss:0.292	token_training_loss:0.0	distillation_Loss:0.114                                                   	MRR:26.49	Hits@10:47.02	Best:26.49
2025-01-06 22:47:11,479: Snapshot:4	Epoch:8	Loss:0.393	translation_Loss:0.28	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:26.71	Hits@10:47.32	Best:26.71
2025-01-06 22:47:17,812: Snapshot:4	Epoch:9	Loss:0.378	translation_Loss:0.267	token_training_loss:0.0	distillation_Loss:0.111                                                   	MRR:26.83	Hits@10:47.61	Best:26.83
2025-01-06 22:47:24,591: Snapshot:4	Epoch:10	Loss:0.374	translation_Loss:0.261	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:26.97	Hits@10:47.71	Best:26.97
2025-01-06 22:47:30,855: Snapshot:4	Epoch:11	Loss:0.363	translation_Loss:0.252	token_training_loss:0.0	distillation_Loss:0.111                                                   	MRR:26.92	Hits@10:47.77	Best:26.97
2025-01-06 22:47:37,133: Snapshot:4	Epoch:12	Loss:0.35	translation_Loss:0.242	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:26.99	Hits@10:47.91	Best:26.99
2025-01-06 22:47:43,758: Snapshot:4	Epoch:13	Loss:0.348	translation_Loss:0.239	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:27.14	Hits@10:48.14	Best:27.14
2025-01-06 22:47:50,046: Snapshot:4	Epoch:14	Loss:0.34	translation_Loss:0.231	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:27.29	Hits@10:48.27	Best:27.29
2025-01-06 22:47:56,377: Snapshot:4	Epoch:15	Loss:0.339	translation_Loss:0.231	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:27.17	Hits@10:48.52	Best:27.29
2025-01-06 22:48:03,203: Snapshot:4	Epoch:16	Loss:0.331	translation_Loss:0.223	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:27.51	Hits@10:48.6	Best:27.51
2025-01-06 22:48:09,483: Snapshot:4	Epoch:17	Loss:0.323	translation_Loss:0.217	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:27.4	Hits@10:48.74	Best:27.51
2025-01-06 22:48:15,777: Snapshot:4	Epoch:18	Loss:0.32	translation_Loss:0.213	token_training_loss:0.0	distillation_Loss:0.107                                                   	MRR:27.55	Hits@10:48.52	Best:27.55
2025-01-06 22:48:22,506: Snapshot:4	Epoch:19	Loss:0.318	translation_Loss:0.213	token_training_loss:0.0	distillation_Loss:0.105                                                   	MRR:27.58	Hits@10:48.6	Best:27.58
2025-01-06 22:48:28,851: Snapshot:4	Epoch:20	Loss:0.317	translation_Loss:0.212	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:27.24	Hits@10:48.46	Best:27.58
2025-01-06 22:48:35,088: Snapshot:4	Epoch:21	Loss:0.317	translation_Loss:0.209	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:27.57	Hits@10:48.68	Best:27.58
2025-01-06 22:48:41,803: Snapshot:4	Epoch:22	Loss:0.316	translation_Loss:0.208	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:27.64	Hits@10:48.9	Best:27.64
2025-01-06 22:48:48,022: Snapshot:4	Epoch:23	Loss:0.313	translation_Loss:0.205	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:27.35	Hits@10:49.09	Best:27.64
2025-01-06 22:48:54,603: Snapshot:4	Epoch:24	Loss:0.308	translation_Loss:0.201	token_training_loss:0.0	distillation_Loss:0.107                                                   	MRR:27.47	Hits@10:49.07	Best:27.64
2025-01-06 22:49:00,911: Early Stopping! Snapshot: 4 Epoch: 25 Best Results: 27.64
2025-01-06 22:49:00,911: Start to training tokens! Snapshot: 4 Epoch: 25 Loss:0.308 MRR:27.53 Best Results: 27.64
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:49:00,911: Snapshot:4	Epoch:25	Loss:0.308	translation_Loss:0.202	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:27.53	Hits@10:49.18	Best:27.64
2025-01-06 22:49:07,037: Snapshot:4	Epoch:26	Loss:21.276	translation_Loss:7.648	token_training_loss:13.628	distillation_Loss:0.0                                                   	MRR:27.53	Hits@10:49.18	Best:27.64
2025-01-06 22:49:13,590: End of token training: 4 Epoch: 27 Loss:8.143 MRR:27.53 Best Results: 27.64
2025-01-06 22:49:13,590: Snapshot:4	Epoch:27	Loss:8.143	translation_Loss:7.648	token_training_loss:0.495	distillation_Loss:0.0                                                           	MRR:27.53	Hits@10:49.18	Best:27.64
2025-01-06 22:49:13,846: => loading checkpoint './checkpoint/ENTITY/4model_best.tar'
2025-01-06 22:49:27,945: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3298 | 0.209  | 0.3834 | 0.4563 |  0.5613 |
|     1      | 0.2482 | 0.1492 | 0.2844 | 0.3469 |  0.439  |
|     2      | 0.2341 | 0.1397 | 0.271  | 0.3306 |  0.4142 |
|     3      | 0.2303 | 0.1363 | 0.2671 | 0.3286 |  0.4083 |
|     4      | 0.2783 | 0.1652 | 0.3326 | 0.4057 |  0.4953 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,004,200
Trainable params: 1,200
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:49:27,947: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3366 | 0.2163 | 0.3899 | 0.4629 |  0.565  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3363 | 0.2152 | 0.3901 | 0.4622 |  0.5682 |
|     1      | 0.2501 | 0.1522 | 0.2862 | 0.3469 |  0.4378 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.335  | 0.2146 | 0.3877 | 0.4637 |  0.5669 |
|     1      | 0.2501 | 0.1518 | 0.2852 | 0.3498 |  0.4383 |
|     2      | 0.2334 | 0.1404 | 0.2678 | 0.3276 |  0.4093 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3338 | 0.2127 | 0.3856 | 0.4627 |  0.5654 |
|     1      | 0.2508 | 0.1518 | 0.2874 |  0.35  |  0.4391 |
|     2      | 0.2359 | 0.1421 | 0.2718 | 0.3312 |  0.4148 |
|     3      | 0.2292 | 0.1354 | 0.2662 | 0.3273 |  0.4059 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3298 | 0.209  | 0.3834 | 0.4563 |  0.5613 |
|     1      | 0.2482 | 0.1492 | 0.2844 | 0.3469 |  0.439  |
|     2      | 0.2341 | 0.1397 | 0.271  | 0.3306 |  0.4142 |
|     3      | 0.2303 | 0.1363 | 0.2671 | 0.3286 |  0.4083 |
|     4      | 0.2783 | 0.1652 | 0.3326 | 0.4057 |  0.4953 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:49:27,948: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 99.01210856437683  |   0.337   |    0.216     |     0.39     |     0.565     |
|    1     | 132.20618891716003 |   0.284   |    0.177     |    0.327     |     0.489     |
|    2     | 134.87590551376343 |   0.264   |    0.163     |    0.303     |     0.458     |
|    3     | 145.57746481895447 |   0.255   |    0.155     |    0.295     |     0.446     |
|    4     | 183.74510145187378 |   0.258   |    0.155     |    0.299     |     0.453     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:49:27,948: Sum_Training_Time:695.4167692661285
2025-01-06 22:49:27,948: Every_Training_Time:[99.01210856437683, 132.20618891716003, 134.87590551376343, 145.57746481895447, 183.74510145187378]
2025-01-06 22:49:27,948: Forward transfer: 0.0456 Backward transfer: -0.0017249999999999974
2025-01-06 22:49:48,356: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/ENTITY/', dataset='ENTITY', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106224932/ENTITY', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3333, record=False, reply_loss_weight=0.1, save_path='./checkpoint/ENTITY', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[5000.0, 10000.0, 10000.0, 10000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:49:56,828: Snapshot:0	Epoch:0	Loss:13.218	translation_Loss:13.218	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:4.62	Hits@10:10.04	Best:4.62
2025-01-06 22:50:01,848: Snapshot:0	Epoch:1	Loss:11.011	translation_Loss:11.011	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.21	Hits@10:22.27	Best:9.21
2025-01-06 22:50:06,524: Snapshot:0	Epoch:2	Loss:9.121	translation_Loss:9.121	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.7	Hits@10:35.09	Best:13.7
2025-01-06 22:50:11,630: Snapshot:0	Epoch:3	Loss:7.104	translation_Loss:7.104	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:19.08	Hits@10:43.42	Best:19.08
2025-01-06 22:50:16,260: Snapshot:0	Epoch:4	Loss:5.195	translation_Loss:5.195	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.56	Hits@10:48.78	Best:24.56
2025-01-06 22:50:20,920: Snapshot:0	Epoch:5	Loss:3.698	translation_Loss:3.698	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:28.01	Hits@10:51.96	Best:28.01
2025-01-06 22:50:26,043: Snapshot:0	Epoch:6	Loss:2.598	translation_Loss:2.598	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:30.08	Hits@10:53.86	Best:30.08
2025-01-06 22:50:30,756: Snapshot:0	Epoch:7	Loss:1.836	translation_Loss:1.836	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:31.32	Hits@10:55.1	Best:31.32
2025-01-06 22:50:35,370: Snapshot:0	Epoch:8	Loss:1.324	translation_Loss:1.324	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.23	Hits@10:56.05	Best:32.23
2025-01-06 22:50:40,363: Snapshot:0	Epoch:9	Loss:0.976	translation_Loss:0.976	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.75	Hits@10:56.52	Best:32.75
2025-01-06 22:50:44,979: Snapshot:0	Epoch:10	Loss:0.763	translation_Loss:0.763	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.15	Hits@10:56.54	Best:33.15
2025-01-06 22:50:49,599: Snapshot:0	Epoch:11	Loss:0.607	translation_Loss:0.607	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.23	Hits@10:56.67	Best:33.23
2025-01-06 22:50:54,582: Snapshot:0	Epoch:12	Loss:0.497	translation_Loss:0.497	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.38	Hits@10:56.69	Best:33.38
2025-01-06 22:50:59,280: Snapshot:0	Epoch:13	Loss:0.429	translation_Loss:0.429	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.61	Hits@10:56.88	Best:33.61
2025-01-06 22:51:03,938: Snapshot:0	Epoch:14	Loss:0.377	translation_Loss:0.377	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.53	Hits@10:56.67	Best:33.61
2025-01-06 22:51:08,959: Snapshot:0	Epoch:15	Loss:0.329	translation_Loss:0.329	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.37	Hits@10:56.84	Best:33.61
2025-01-06 22:51:13,576: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 33.61
2025-01-06 22:51:13,577: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.294 MRR:33.39 Best Results: 33.61
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:51:13,577: Snapshot:0	Epoch:16	Loss:0.294	translation_Loss:0.294	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.39	Hits@10:56.9	Best:33.61
2025-01-06 22:51:18,712: Snapshot:0	Epoch:17	Loss:22.713	translation_Loss:9.838	token_training_loss:12.875	distillation_Loss:0.0                                                   	MRR:33.39	Hits@10:56.9	Best:33.61
2025-01-06 22:51:23,798: End of token training: 0 Epoch: 18 Loss:10.285 MRR:33.39 Best Results: 33.61
2025-01-06 22:51:23,799: Snapshot:0	Epoch:18	Loss:10.285	translation_Loss:9.83	token_training_loss:0.455	distillation_Loss:0.0                                                           	MRR:33.39	Hits@10:56.9	Best:33.61
2025-01-06 22:51:24,046: => loading checkpoint './checkpoint/ENTITY/0model_best.tar'
2025-01-06 22:51:25,538: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3385 | 0.2187 | 0.3915 | 0.4639 |  0.5701 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (581,800)
├─Embedding: 1-2                         (93,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 676,200
Trainable params: 1,200
Non-trainable params: 675,000
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:51:40,006: Snapshot:1	Epoch:0	Loss:13.066	translation_Loss:12.588	token_training_loss:0.0	distillation_Loss:0.478                                                   	MRR:15.37	Hits@10:26.17	Best:15.37
2025-01-06 22:51:48,221: Snapshot:1	Epoch:1	Loss:5.422	translation_Loss:5.023	token_training_loss:0.0	distillation_Loss:0.399                                                   	MRR:20.31	Hits@10:35.64	Best:20.31
2025-01-06 22:51:56,120: Snapshot:1	Epoch:2	Loss:2.917	translation_Loss:2.665	token_training_loss:0.0	distillation_Loss:0.252                                                   	MRR:22.79	Hits@10:40.45	Best:22.79
2025-01-06 22:52:04,390: Snapshot:1	Epoch:3	Loss:2.058	translation_Loss:1.858	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:23.69	Hits@10:41.87	Best:23.69
2025-01-06 22:52:12,376: Snapshot:1	Epoch:4	Loss:1.681	translation_Loss:1.503	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:24.23	Hits@10:42.56	Best:24.23
2025-01-06 22:52:20,657: Snapshot:1	Epoch:5	Loss:1.468	translation_Loss:1.302	token_training_loss:0.0	distillation_Loss:0.166                                                   	MRR:24.5	Hits@10:43.1	Best:24.5
2025-01-06 22:52:28,767: Snapshot:1	Epoch:6	Loss:1.346	translation_Loss:1.186	token_training_loss:0.0	distillation_Loss:0.16                                                   	MRR:24.59	Hits@10:43.23	Best:24.59
2025-01-06 22:52:37,062: Snapshot:1	Epoch:7	Loss:1.267	translation_Loss:1.11	token_training_loss:0.0	distillation_Loss:0.157                                                   	MRR:24.67	Hits@10:43.62	Best:24.67
2025-01-06 22:52:45,377: Snapshot:1	Epoch:8	Loss:1.193	translation_Loss:1.04	token_training_loss:0.0	distillation_Loss:0.153                                                   	MRR:24.83	Hits@10:43.93	Best:24.83
2025-01-06 22:52:53,290: Snapshot:1	Epoch:9	Loss:1.155	translation_Loss:1.003	token_training_loss:0.0	distillation_Loss:0.152                                                   	MRR:24.95	Hits@10:44.2	Best:24.95
2025-01-06 22:53:01,620: Snapshot:1	Epoch:10	Loss:1.12	translation_Loss:0.972	token_training_loss:0.0	distillation_Loss:0.149                                                   	MRR:25.07	Hits@10:43.95	Best:25.07
2025-01-06 22:53:09,565: Snapshot:1	Epoch:11	Loss:1.09	translation_Loss:0.941	token_training_loss:0.0	distillation_Loss:0.149                                                   	MRR:25.11	Hits@10:43.98	Best:25.11
2025-01-06 22:53:17,828: Snapshot:1	Epoch:12	Loss:1.057	translation_Loss:0.91	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:25.22	Hits@10:44.19	Best:25.22
2025-01-06 22:53:25,778: Snapshot:1	Epoch:13	Loss:1.032	translation_Loss:0.885	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:25.26	Hits@10:44.31	Best:25.26
2025-01-06 22:53:34,042: Snapshot:1	Epoch:14	Loss:1.018	translation_Loss:0.874	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:25.17	Hits@10:44.27	Best:25.26
2025-01-06 22:53:41,904: Snapshot:1	Epoch:15	Loss:1.01	translation_Loss:0.863	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:25.12	Hits@10:44.33	Best:25.26
2025-01-06 22:53:50,154: Snapshot:1	Epoch:16	Loss:0.989	translation_Loss:0.845	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:25.31	Hits@10:44.54	Best:25.31
2025-01-06 22:53:58,077: Snapshot:1	Epoch:17	Loss:0.976	translation_Loss:0.83	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:25.28	Hits@10:44.22	Best:25.31
2025-01-06 22:54:06,356: Snapshot:1	Epoch:18	Loss:0.971	translation_Loss:0.825	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:25.26	Hits@10:44.32	Best:25.31
2025-01-06 22:54:14,546: Early Stopping! Snapshot: 1 Epoch: 19 Best Results: 25.31
2025-01-06 22:54:14,546: Start to training tokens! Snapshot: 1 Epoch: 19 Loss:0.962 MRR:25.28 Best Results: 25.31
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:54:14,547: Snapshot:1	Epoch:19	Loss:0.962	translation_Loss:0.816	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:25.28	Hits@10:44.31	Best:25.31
2025-01-06 22:54:22,450: Snapshot:1	Epoch:20	Loss:28.165	translation_Loss:15.894	token_training_loss:12.271	distillation_Loss:0.0                                                   	MRR:25.28	Hits@10:44.31	Best:25.31
2025-01-06 22:54:30,738: End of token training: 1 Epoch: 21 Loss:16.142 MRR:25.28 Best Results: 25.31
2025-01-06 22:54:30,738: Snapshot:1	Epoch:21	Loss:16.142	translation_Loss:15.91	token_training_loss:0.232	distillation_Loss:0.0                                                           	MRR:25.28	Hits@10:44.31	Best:25.31
2025-01-06 22:54:30,984: => loading checkpoint './checkpoint/ENTITY/1model_best.tar'
2025-01-06 22:54:35,108: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3365 | 0.2162 | 0.3901 | 0.4623 |  0.568  |
|     1      | 0.2571 | 0.1587 | 0.2926 | 0.3558 |  0.4479 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,163,400)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,259,000
Trainable params: 1,200
Non-trainable params: 1,257,800
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:54:50,773: Snapshot:2	Epoch:0	Loss:11.704	translation_Loss:11.058	token_training_loss:0.0	distillation_Loss:0.646                                                   	MRR:15.06	Hits@10:25.55	Best:15.06
2025-01-06 22:55:00,011: Snapshot:2	Epoch:1	Loss:4.312	translation_Loss:3.842	token_training_loss:0.0	distillation_Loss:0.47                                                   	MRR:18.71	Hits@10:32.48	Best:18.71
2025-01-06 22:55:08,818: Snapshot:2	Epoch:2	Loss:2.424	translation_Loss:2.127	token_training_loss:0.0	distillation_Loss:0.297                                                   	MRR:20.61	Hits@10:36.2	Best:20.61
2025-01-06 22:55:17,855: Snapshot:2	Epoch:3	Loss:1.842	translation_Loss:1.598	token_training_loss:0.0	distillation_Loss:0.243                                                   	MRR:21.69	Hits@10:38.02	Best:21.69
2025-01-06 22:55:26,861: Snapshot:2	Epoch:4	Loss:1.594	translation_Loss:1.369	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:22.07	Hits@10:38.86	Best:22.07
2025-01-06 22:55:35,520: Snapshot:2	Epoch:5	Loss:1.454	translation_Loss:1.239	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:22.43	Hits@10:39.62	Best:22.43
2025-01-06 22:55:44,482: Snapshot:2	Epoch:6	Loss:1.346	translation_Loss:1.139	token_training_loss:0.0	distillation_Loss:0.208                                                   	MRR:22.61	Hits@10:39.8	Best:22.61
2025-01-06 22:55:53,095: Snapshot:2	Epoch:7	Loss:1.294	translation_Loss:1.087	token_training_loss:0.0	distillation_Loss:0.207                                                   	MRR:22.82	Hits@10:40.09	Best:22.82
2025-01-06 22:56:02,150: Snapshot:2	Epoch:8	Loss:1.245	translation_Loss:1.041	token_training_loss:0.0	distillation_Loss:0.204                                                   	MRR:22.99	Hits@10:40.36	Best:22.99
2025-01-06 22:56:10,837: Snapshot:2	Epoch:9	Loss:1.204	translation_Loss:1.0	token_training_loss:0.0	distillation_Loss:0.204                                                   	MRR:23.17	Hits@10:40.45	Best:23.17
2025-01-06 22:56:19,934: Snapshot:2	Epoch:10	Loss:1.183	translation_Loss:0.978	token_training_loss:0.0	distillation_Loss:0.205                                                   	MRR:23.3	Hits@10:40.6	Best:23.3
2025-01-06 22:56:28,574: Snapshot:2	Epoch:11	Loss:1.155	translation_Loss:0.952	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:23.29	Hits@10:40.82	Best:23.3
2025-01-06 22:56:37,511: Snapshot:2	Epoch:12	Loss:1.132	translation_Loss:0.931	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:23.25	Hits@10:40.8	Best:23.3
2025-01-06 22:56:46,453: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 23.3
2025-01-06 22:56:46,453: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:1.118 MRR:23.29 Best Results: 23.3
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:56:46,454: Snapshot:2	Epoch:13	Loss:1.118	translation_Loss:0.915	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:23.29	Hits@10:40.87	Best:23.3
2025-01-06 22:56:55,022: Snapshot:2	Epoch:14	Loss:28.825	translation_Loss:15.765	token_training_loss:13.06	distillation_Loss:0.0                                                   	MRR:23.29	Hits@10:40.87	Best:23.3
2025-01-06 22:57:04,028: End of token training: 2 Epoch: 15 Loss:15.994 MRR:23.29 Best Results: 23.3
2025-01-06 22:57:04,028: Snapshot:2	Epoch:15	Loss:15.994	translation_Loss:15.775	token_training_loss:0.219	distillation_Loss:0.0                                                           	MRR:23.29	Hits@10:40.87	Best:23.3
2025-01-06 22:57:04,277: => loading checkpoint './checkpoint/ENTITY/2model_best.tar'
2025-01-06 22:57:11,587: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3361 | 0.2167 | 0.3881 | 0.4614 |  0.5665 |
|     1      | 0.258  | 0.1594 | 0.2932 | 0.3573 |  0.4489 |
|     2      | 0.2353 | 0.1422 | 0.2713 | 0.3259 |  0.4127 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,745,000)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,840,600
Trainable params: 1,200
Non-trainable params: 1,839,400
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:57:27,924: Snapshot:3	Epoch:0	Loss:10.74	translation_Loss:10.028	token_training_loss:0.0	distillation_Loss:0.711                                                   	MRR:15.15	Hits@10:26.67	Best:15.15
2025-01-06 22:57:37,065: Snapshot:3	Epoch:1	Loss:3.483	translation_Loss:2.924	token_training_loss:0.0	distillation_Loss:0.56                                                   	MRR:18.58	Hits@10:33.12	Best:18.58
2025-01-06 22:57:46,117: Snapshot:3	Epoch:2	Loss:1.924	translation_Loss:1.597	token_training_loss:0.0	distillation_Loss:0.327                                                   	MRR:20.17	Hits@10:36.12	Best:20.17
2025-01-06 22:57:54,831: Snapshot:3	Epoch:3	Loss:1.472	translation_Loss:1.211	token_training_loss:0.0	distillation_Loss:0.261                                                   	MRR:20.91	Hits@10:37.75	Best:20.91
2025-01-06 22:58:03,951: Snapshot:3	Epoch:4	Loss:1.278	translation_Loss:1.041	token_training_loss:0.0	distillation_Loss:0.237                                                   	MRR:21.5	Hits@10:38.54	Best:21.5
2025-01-06 22:58:12,837: Snapshot:3	Epoch:5	Loss:1.172	translation_Loss:0.945	token_training_loss:0.0	distillation_Loss:0.226                                                   	MRR:21.83	Hits@10:39.33	Best:21.83
2025-01-06 22:58:21,971: Snapshot:3	Epoch:6	Loss:1.097	translation_Loss:0.875	token_training_loss:0.0	distillation_Loss:0.222                                                   	MRR:22.15	Hits@10:39.78	Best:22.15
2025-01-06 22:58:30,806: Snapshot:3	Epoch:7	Loss:1.043	translation_Loss:0.826	token_training_loss:0.0	distillation_Loss:0.217                                                   	MRR:22.34	Hits@10:40.21	Best:22.34
2025-01-06 22:58:39,970: Snapshot:3	Epoch:8	Loss:1.019	translation_Loss:0.801	token_training_loss:0.0	distillation_Loss:0.218                                                   	MRR:22.41	Hits@10:40.13	Best:22.41
2025-01-06 22:58:48,710: Snapshot:3	Epoch:9	Loss:0.986	translation_Loss:0.772	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:22.64	Hits@10:40.5	Best:22.64
2025-01-06 22:58:57,867: Snapshot:3	Epoch:10	Loss:0.966	translation_Loss:0.752	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:22.78	Hits@10:40.54	Best:22.78
2025-01-06 22:59:06,597: Snapshot:3	Epoch:11	Loss:0.95	translation_Loss:0.736	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:22.88	Hits@10:40.86	Best:22.88
2025-01-06 22:59:15,649: Snapshot:3	Epoch:12	Loss:0.938	translation_Loss:0.724	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:22.86	Hits@10:40.89	Best:22.88
2025-01-06 22:59:24,355: Snapshot:3	Epoch:13	Loss:0.922	translation_Loss:0.708	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:22.84	Hits@10:41.1	Best:22.88
2025-01-06 22:59:33,448: Snapshot:3	Epoch:14	Loss:0.911	translation_Loss:0.696	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:23.11	Hits@10:41.37	Best:23.11
2025-01-06 22:59:42,190: Snapshot:3	Epoch:15	Loss:0.909	translation_Loss:0.694	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:23.2	Hits@10:41.29	Best:23.2
2025-01-06 22:59:51,292: Snapshot:3	Epoch:16	Loss:0.892	translation_Loss:0.678	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:23.02	Hits@10:41.04	Best:23.2
2025-01-06 23:00:00,126: Snapshot:3	Epoch:17	Loss:0.881	translation_Loss:0.668	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:23.16	Hits@10:41.47	Best:23.2
2025-01-06 23:00:09,287: Snapshot:3	Epoch:18	Loss:0.88	translation_Loss:0.666	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:23.23	Hits@10:41.36	Best:23.23
2025-01-06 23:00:18,343: Snapshot:3	Epoch:19	Loss:0.88	translation_Loss:0.665	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:22.99	Hits@10:41.41	Best:23.23
2025-01-06 23:00:27,112: Snapshot:3	Epoch:20	Loss:0.875	translation_Loss:0.659	token_training_loss:0.0	distillation_Loss:0.216                                                   	MRR:23.2	Hits@10:41.5	Best:23.23
2025-01-06 23:00:35,856: Snapshot:3	Epoch:21	Loss:0.863	translation_Loss:0.648	token_training_loss:0.0	distillation_Loss:0.216                                                   	MRR:23.29	Hits@10:41.61	Best:23.29
2025-01-06 23:00:44,984: Snapshot:3	Epoch:22	Loss:0.858	translation_Loss:0.644	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:23.38	Hits@10:41.59	Best:23.38
2025-01-06 23:00:54,095: Snapshot:3	Epoch:23	Loss:0.849	translation_Loss:0.635	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:23.16	Hits@10:41.5	Best:23.38
2025-01-06 23:01:02,872: Snapshot:3	Epoch:24	Loss:0.854	translation_Loss:0.638	token_training_loss:0.0	distillation_Loss:0.216                                                   	MRR:23.08	Hits@10:41.64	Best:23.38
2025-01-06 23:01:12,063: Early Stopping! Snapshot: 3 Epoch: 25 Best Results: 23.38
2025-01-06 23:01:12,063: Start to training tokens! Snapshot: 3 Epoch: 25 Loss:0.851 MRR:23.27 Best Results: 23.38
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:01:12,063: Snapshot:3	Epoch:25	Loss:0.851	translation_Loss:0.636	token_training_loss:0.0	distillation_Loss:0.215                                                   	MRR:23.27	Hits@10:41.59	Best:23.38
2025-01-06 23:01:20,749: Snapshot:3	Epoch:26	Loss:26.577	translation_Loss:14.211	token_training_loss:12.366	distillation_Loss:0.0                                                   	MRR:23.27	Hits@10:41.59	Best:23.38
2025-01-06 23:01:29,857: End of token training: 3 Epoch: 27 Loss:14.444 MRR:23.27 Best Results: 23.38
2025-01-06 23:01:29,858: Snapshot:3	Epoch:27	Loss:14.444	translation_Loss:14.197	token_training_loss:0.247	distillation_Loss:0.0                                                           	MRR:23.27	Hits@10:41.59	Best:23.38
2025-01-06 23:01:30,106: => loading checkpoint './checkpoint/ENTITY/3model_best.tar'
2025-01-06 23:01:41,023: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3358 | 0.2168 | 0.3884 | 0.4599 |  0.5642 |
|     1      | 0.2582 | 0.1592 | 0.2948 | 0.358  |  0.448  |
|     2      | 0.2372 | 0.1446 | 0.2717 | 0.3291 |  0.4142 |
|     3      | 0.2352 | 0.1414 | 0.2706 | 0.3318 |  0.4158 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,326,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,422,600
Trainable params: 1,200
Non-trainable params: 2,421,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:01:54,505: Snapshot:4	Epoch:0	Loss:8.005	translation_Loss:7.551	token_training_loss:0.0	distillation_Loss:0.453                                                   	MRR:17.16	Hits@10:32.24	Best:17.16
2025-01-06 23:02:00,901: Snapshot:4	Epoch:1	Loss:2.555	translation_Loss:2.063	token_training_loss:0.0	distillation_Loss:0.492                                                   	MRR:23.05	Hits@10:41.69	Best:23.05
2025-01-06 23:02:07,268: Snapshot:4	Epoch:2	Loss:1.12	translation_Loss:0.825	token_training_loss:0.0	distillation_Loss:0.295                                                   	MRR:24.72	Hits@10:43.48	Best:24.72
2025-01-06 23:02:14,118: Snapshot:4	Epoch:3	Loss:0.717	translation_Loss:0.533	token_training_loss:0.0	distillation_Loss:0.183                                                   	MRR:25.45	Hits@10:44.66	Best:25.45
2025-01-06 23:02:20,463: Snapshot:4	Epoch:4	Loss:0.569	translation_Loss:0.424	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:25.89	Hits@10:45.34	Best:25.89
2025-01-06 23:02:26,841: Snapshot:4	Epoch:5	Loss:0.499	translation_Loss:0.369	token_training_loss:0.0	distillation_Loss:0.13                                                   	MRR:26.39	Hits@10:46.26	Best:26.39
2025-01-06 23:02:33,654: Snapshot:4	Epoch:6	Loss:0.453	translation_Loss:0.334	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:26.75	Hits@10:46.58	Best:26.75
2025-01-06 23:02:40,014: Snapshot:4	Epoch:7	Loss:0.425	translation_Loss:0.31	token_training_loss:0.0	distillation_Loss:0.115                                                   	MRR:26.98	Hits@10:46.76	Best:26.98
2025-01-06 23:02:46,294: Snapshot:4	Epoch:8	Loss:0.398	translation_Loss:0.287	token_training_loss:0.0	distillation_Loss:0.111                                                   	MRR:26.92	Hits@10:46.82	Best:26.98
2025-01-06 23:02:52,932: Snapshot:4	Epoch:9	Loss:0.381	translation_Loss:0.272	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:26.86	Hits@10:47.25	Best:26.98
2025-01-06 23:02:59,350: Snapshot:4	Epoch:10	Loss:0.371	translation_Loss:0.265	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:27.1	Hits@10:47.27	Best:27.1
2025-01-06 23:03:05,717: Snapshot:4	Epoch:11	Loss:0.357	translation_Loss:0.253	token_training_loss:0.0	distillation_Loss:0.104                                                   	MRR:27.24	Hits@10:47.65	Best:27.24
2025-01-06 23:03:12,452: Snapshot:4	Epoch:12	Loss:0.35	translation_Loss:0.246	token_training_loss:0.0	distillation_Loss:0.104                                                   	MRR:27.47	Hits@10:47.72	Best:27.47
2025-01-06 23:03:18,806: Snapshot:4	Epoch:13	Loss:0.345	translation_Loss:0.242	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:27.42	Hits@10:47.77	Best:27.47
2025-01-06 23:03:25,202: Snapshot:4	Epoch:14	Loss:0.343	translation_Loss:0.239	token_training_loss:0.0	distillation_Loss:0.104                                                   	MRR:27.51	Hits@10:48.24	Best:27.51
2025-01-06 23:03:31,965: Snapshot:4	Epoch:15	Loss:0.337	translation_Loss:0.233	token_training_loss:0.0	distillation_Loss:0.104                                                   	MRR:27.34	Hits@10:48.23	Best:27.51
2025-01-06 23:03:38,355: Snapshot:4	Epoch:16	Loss:0.333	translation_Loss:0.232	token_training_loss:0.0	distillation_Loss:0.101                                                   	MRR:27.56	Hits@10:48.59	Best:27.56
2025-01-06 23:03:44,626: Snapshot:4	Epoch:17	Loss:0.328	translation_Loss:0.225	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:27.53	Hits@10:48.35	Best:27.56
2025-01-06 23:03:51,343: Snapshot:4	Epoch:18	Loss:0.327	translation_Loss:0.224	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:27.79	Hits@10:48.58	Best:27.79
2025-01-06 23:03:57,728: Snapshot:4	Epoch:19	Loss:0.322	translation_Loss:0.219	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:27.61	Hits@10:48.73	Best:27.79
2025-01-06 23:04:04,075: Snapshot:4	Epoch:20	Loss:0.322	translation_Loss:0.218	token_training_loss:0.0	distillation_Loss:0.104                                                   	MRR:27.62	Hits@10:48.68	Best:27.79
2025-01-06 23:04:10,709: Early Stopping! Snapshot: 4 Epoch: 21 Best Results: 27.79
2025-01-06 23:04:10,709: Start to training tokens! Snapshot: 4 Epoch: 21 Loss:0.316 MRR:27.66 Best Results: 27.79
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:04:10,709: Snapshot:4	Epoch:21	Loss:0.316	translation_Loss:0.214	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:27.66	Hits@10:48.81	Best:27.79
2025-01-06 23:04:16,899: Snapshot:4	Epoch:22	Loss:20.268	translation_Loss:7.732	token_training_loss:12.536	distillation_Loss:0.0                                                   	MRR:27.66	Hits@10:48.81	Best:27.79
2025-01-06 23:04:23,130: End of token training: 4 Epoch: 23 Loss:8.246 MRR:27.66 Best Results: 27.79
2025-01-06 23:04:23,130: Snapshot:4	Epoch:23	Loss:8.246	translation_Loss:7.748	token_training_loss:0.498	distillation_Loss:0.0                                                           	MRR:27.66	Hits@10:48.81	Best:27.79
2025-01-06 23:04:23,401: => loading checkpoint './checkpoint/ENTITY/4model_best.tar'
2025-01-06 23:04:37,793: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3315 | 0.2108 | 0.3861 | 0.4575 |  0.5612 |
|     1      | 0.2561 | 0.1575 | 0.2912 | 0.3558 |  0.4475 |
|     2      | 0.2361 | 0.1427 | 0.2712 | 0.3293 |  0.4147 |
|     3      | 0.2379 | 0.1436 | 0.2733 | 0.336  |  0.4198 |
|     4      | 0.281  | 0.1677 | 0.3384 | 0.4115 |  0.4931 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,004,200
Trainable params: 1,200
Non-trainable params: 3,003,000
=================================================================
2025-01-06 23:04:37,796: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3385 | 0.2187 | 0.3915 | 0.4639 |  0.5701 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3365 | 0.2162 | 0.3901 | 0.4623 |  0.568  |
|     1      | 0.2571 | 0.1587 | 0.2926 | 0.3558 |  0.4479 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3361 | 0.2167 | 0.3881 | 0.4614 |  0.5665 |
|     1      | 0.258  | 0.1594 | 0.2932 | 0.3573 |  0.4489 |
|     2      | 0.2353 | 0.1422 | 0.2713 | 0.3259 |  0.4127 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3358 | 0.2168 | 0.3884 | 0.4599 |  0.5642 |
|     1      | 0.2582 | 0.1592 | 0.2948 | 0.358  |  0.448  |
|     2      | 0.2372 | 0.1446 | 0.2717 | 0.3291 |  0.4142 |
|     3      | 0.2352 | 0.1414 | 0.2706 | 0.3318 |  0.4158 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3315 | 0.2108 | 0.3861 | 0.4575 |  0.5612 |
|     1      | 0.2561 | 0.1575 | 0.2912 | 0.3558 |  0.4475 |
|     2      | 0.2361 | 0.1427 | 0.2712 | 0.3293 |  0.4147 |
|     3      | 0.2379 | 0.1436 | 0.2733 | 0.336  |  0.4198 |
|     4      | 0.281  | 0.1677 | 0.3384 | 0.4115 |  0.4931 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 23:04:37,797: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 95.44170498847961  |   0.339   |    0.219     |    0.392     |      0.57     |
|    1     | 182.55789971351624 |   0.288   |    0.181     |    0.331     |     0.495     |
|    2     | 145.76396536827087 |   0.268   |    0.167     |    0.308     |     0.463     |
|    3     | 254.72065234184265 |    0.26   |     0.16     |    0.298     |      0.45     |
|    4     | 158.95408129692078 |   0.262   |     0.16     |    0.304     |     0.457     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 23:04:37,797: Sum_Training_Time:837.4383037090302
2025-01-06 23:04:37,797: Every_Training_Time:[95.44170498847961, 182.55789971351624, 145.76396536827087, 254.72065234184265, 158.95408129692078]
2025-01-06 23:04:37,797: Forward transfer: 0.047575 Backward transfer: -0.001125000000000001
2025-01-06 23:04:59,072: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/ENTITY/', dataset='ENTITY', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106230443/ENTITY', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=4444, record=False, reply_loss_weight=0.1, save_path='./checkpoint/ENTITY', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[5000.0, 10000.0, 10000.0, 10000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 23:05:07,426: Snapshot:0	Epoch:0	Loss:13.181	translation_Loss:13.181	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:4.63	Hits@10:10.29	Best:4.63
2025-01-06 23:05:12,387: Snapshot:0	Epoch:1	Loss:10.94	translation_Loss:10.94	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.42	Hits@10:23.44	Best:9.42
2025-01-06 23:05:16,982: Snapshot:0	Epoch:2	Loss:9.018	translation_Loss:9.018	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.64	Hits@10:35.18	Best:13.64
2025-01-06 23:05:21,997: Snapshot:0	Epoch:3	Loss:7.019	translation_Loss:7.019	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.94	Hits@10:43.32	Best:18.94
2025-01-06 23:05:26,591: Snapshot:0	Epoch:4	Loss:5.181	translation_Loss:5.181	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.39	Hits@10:48.69	Best:24.39
2025-01-06 23:05:31,204: Snapshot:0	Epoch:5	Loss:3.72	translation_Loss:3.72	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:28.04	Hits@10:51.96	Best:28.04
2025-01-06 23:05:36,205: Snapshot:0	Epoch:6	Loss:2.633	translation_Loss:2.633	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:29.92	Hits@10:54.01	Best:29.92
2025-01-06 23:05:40,789: Snapshot:0	Epoch:7	Loss:1.87	translation_Loss:1.87	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:31.39	Hits@10:55.2	Best:31.39
2025-01-06 23:05:45,368: Snapshot:0	Epoch:8	Loss:1.344	translation_Loss:1.344	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.27	Hits@10:56.31	Best:32.27
2025-01-06 23:05:50,316: Snapshot:0	Epoch:9	Loss:1.003	translation_Loss:1.003	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.84	Hits@10:56.72	Best:32.84
2025-01-06 23:05:54,931: Snapshot:0	Epoch:10	Loss:0.774	translation_Loss:0.774	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.31	Hits@10:57.03	Best:33.31
2025-01-06 23:05:59,600: Snapshot:0	Epoch:11	Loss:0.613	translation_Loss:0.613	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.53	Hits@10:57.23	Best:33.53
2025-01-06 23:06:04,567: Snapshot:0	Epoch:12	Loss:0.508	translation_Loss:0.508	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.67	Hits@10:57.23	Best:33.67
2025-01-06 23:06:09,197: Snapshot:0	Epoch:13	Loss:0.429	translation_Loss:0.429	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.71	Hits@10:57.23	Best:33.71
2025-01-06 23:06:13,796: Snapshot:0	Epoch:14	Loss:0.375	translation_Loss:0.375	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.77	Hits@10:57.33	Best:33.77
2025-01-06 23:06:18,808: Snapshot:0	Epoch:15	Loss:0.334	translation_Loss:0.334	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.66	Hits@10:57.05	Best:33.77
2025-01-06 23:06:23,464: Snapshot:0	Epoch:16	Loss:0.294	translation_Loss:0.294	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.68	Hits@10:57.14	Best:33.77
2025-01-06 23:06:28,110: Snapshot:0	Epoch:17	Loss:0.27	translation_Loss:0.27	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.79	Hits@10:56.98	Best:33.79
2025-01-06 23:06:33,107: Snapshot:0	Epoch:18	Loss:0.254	translation_Loss:0.254	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.79	Hits@10:56.96	Best:33.79
2025-01-06 23:06:37,686: Snapshot:0	Epoch:19	Loss:0.233	translation_Loss:0.233	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.87	Hits@10:56.8	Best:33.87
2025-01-06 23:06:42,624: Snapshot:0	Epoch:20	Loss:0.217	translation_Loss:0.217	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.6	Hits@10:56.71	Best:33.87
2025-01-06 23:06:47,193: Snapshot:0	Epoch:21	Loss:0.207	translation_Loss:0.207	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.67	Hits@10:56.68	Best:33.87
2025-01-06 23:06:51,755: Early Stopping! Snapshot: 0 Epoch: 22 Best Results: 33.87
2025-01-06 23:06:51,755: Start to training tokens! Snapshot: 0 Epoch: 22 Loss:0.197 MRR:33.61 Best Results: 33.87
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:06:51,755: Snapshot:0	Epoch:22	Loss:0.197	translation_Loss:0.197	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.61	Hits@10:56.66	Best:33.87
2025-01-06 23:06:57,232: Snapshot:0	Epoch:23	Loss:21.386	translation_Loss:9.772	token_training_loss:11.613	distillation_Loss:0.0                                                   	MRR:33.61	Hits@10:56.66	Best:33.87
2025-01-06 23:07:01,813: End of token training: 0 Epoch: 24 Loss:10.229 MRR:33.61 Best Results: 33.87
2025-01-06 23:07:01,813: Snapshot:0	Epoch:24	Loss:10.229	translation_Loss:9.766	token_training_loss:0.464	distillation_Loss:0.0                                                           	MRR:33.61	Hits@10:56.66	Best:33.87
2025-01-06 23:07:02,061: => loading checkpoint './checkpoint/ENTITY/0model_best.tar'
2025-01-06 23:07:03,555: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3398 | 0.2186 | 0.3947 | 0.4695 |  0.5722 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (581,800)
├─Embedding: 1-2                         (93,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 676,200
Trainable params: 1,200
Non-trainable params: 675,000
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:07:17,979: Snapshot:1	Epoch:0	Loss:12.965	translation_Loss:12.479	token_training_loss:0.0	distillation_Loss:0.486                                                   	MRR:15.15	Hits@10:25.91	Best:15.15
2025-01-06 23:07:26,457: Snapshot:1	Epoch:1	Loss:5.084	translation_Loss:4.688	token_training_loss:0.0	distillation_Loss:0.396                                                   	MRR:19.72	Hits@10:34.7	Best:19.72
2025-01-06 23:07:34,371: Snapshot:1	Epoch:2	Loss:2.513	translation_Loss:2.281	token_training_loss:0.0	distillation_Loss:0.233                                                   	MRR:22.29	Hits@10:39.55	Best:22.29
2025-01-06 23:07:42,554: Snapshot:1	Epoch:3	Loss:1.704	translation_Loss:1.53	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:23.41	Hits@10:41.44	Best:23.41
2025-01-06 23:07:50,377: Snapshot:1	Epoch:4	Loss:1.352	translation_Loss:1.201	token_training_loss:0.0	distillation_Loss:0.151                                                   	MRR:23.83	Hits@10:42.2	Best:23.83
2025-01-06 23:07:58,718: Snapshot:1	Epoch:5	Loss:1.174	translation_Loss:1.035	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:24.25	Hits@10:42.87	Best:24.25
2025-01-06 23:08:06,876: Snapshot:1	Epoch:6	Loss:1.076	translation_Loss:0.943	token_training_loss:0.0	distillation_Loss:0.133                                                   	MRR:24.2	Hits@10:42.74	Best:24.25
2025-01-06 23:08:14,720: Snapshot:1	Epoch:7	Loss:0.993	translation_Loss:0.863	token_training_loss:0.0	distillation_Loss:0.13                                                   	MRR:24.26	Hits@10:43.05	Best:24.26
2025-01-06 23:08:22,947: Snapshot:1	Epoch:8	Loss:0.942	translation_Loss:0.816	token_training_loss:0.0	distillation_Loss:0.127                                                   	MRR:24.5	Hits@10:43.17	Best:24.5
2025-01-06 23:08:30,794: Snapshot:1	Epoch:9	Loss:0.905	translation_Loss:0.78	token_training_loss:0.0	distillation_Loss:0.125                                                   	MRR:24.56	Hits@10:43.45	Best:24.56
2025-01-06 23:08:38,958: Snapshot:1	Epoch:10	Loss:0.87	translation_Loss:0.748	token_training_loss:0.0	distillation_Loss:0.123                                                   	MRR:24.73	Hits@10:43.68	Best:24.73
2025-01-06 23:08:46,742: Snapshot:1	Epoch:11	Loss:0.843	translation_Loss:0.721	token_training_loss:0.0	distillation_Loss:0.123                                                   	MRR:24.71	Hits@10:43.51	Best:24.73
2025-01-06 23:08:54,908: Snapshot:1	Epoch:12	Loss:0.83	translation_Loss:0.71	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:24.77	Hits@10:43.81	Best:24.77
2025-01-06 23:09:02,812: Snapshot:1	Epoch:13	Loss:0.805	translation_Loss:0.684	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:24.66	Hits@10:43.69	Best:24.77
2025-01-06 23:09:11,037: Snapshot:1	Epoch:14	Loss:0.794	translation_Loss:0.674	token_training_loss:0.0	distillation_Loss:0.12                                                   	MRR:24.89	Hits@10:43.9	Best:24.89
2025-01-06 23:09:18,795: Snapshot:1	Epoch:15	Loss:0.776	translation_Loss:0.656	token_training_loss:0.0	distillation_Loss:0.12                                                   	MRR:24.82	Hits@10:44.13	Best:24.89
2025-01-06 23:09:26,997: Snapshot:1	Epoch:16	Loss:0.765	translation_Loss:0.646	token_training_loss:0.0	distillation_Loss:0.118                                                   	MRR:25.07	Hits@10:44.16	Best:25.07
2025-01-06 23:09:35,189: Snapshot:1	Epoch:17	Loss:0.752	translation_Loss:0.631	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:24.87	Hits@10:44.27	Best:25.07
2025-01-06 23:09:42,954: Snapshot:1	Epoch:18	Loss:0.76	translation_Loss:0.639	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:24.88	Hits@10:44.17	Best:25.07
2025-01-06 23:09:51,062: Early Stopping! Snapshot: 1 Epoch: 19 Best Results: 25.07
2025-01-06 23:09:51,063: Start to training tokens! Snapshot: 1 Epoch: 19 Loss:0.74 MRR:24.97 Best Results: 25.07
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:09:51,063: Snapshot:1	Epoch:19	Loss:0.74	translation_Loss:0.619	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:24.97	Hits@10:44.29	Best:25.07
2025-01-06 23:09:59,040: Snapshot:1	Epoch:20	Loss:28.673	translation_Loss:15.909	token_training_loss:12.764	distillation_Loss:0.0                                                   	MRR:24.97	Hits@10:44.29	Best:25.07
2025-01-06 23:10:07,416: End of token training: 1 Epoch: 21 Loss:16.143 MRR:24.97 Best Results: 25.07
2025-01-06 23:10:07,416: Snapshot:1	Epoch:21	Loss:16.143	translation_Loss:15.913	token_training_loss:0.23	distillation_Loss:0.0                                                           	MRR:24.97	Hits@10:44.29	Best:25.07
2025-01-06 23:10:07,698: => loading checkpoint './checkpoint/ENTITY/1model_best.tar'
2025-01-06 23:10:11,874: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3394 | 0.2171 | 0.397  | 0.4703 |  0.5754 |
|     1      | 0.2521 | 0.1553 | 0.2845 | 0.3494 |  0.4408 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,163,400)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,259,000
Trainable params: 1,200
Non-trainable params: 1,257,800
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:10:27,540: Snapshot:2	Epoch:0	Loss:11.64	translation_Loss:10.983	token_training_loss:0.0	distillation_Loss:0.656                                                   	MRR:14.94	Hits@10:25.28	Best:14.94
2025-01-06 23:10:36,430: Snapshot:2	Epoch:1	Loss:3.96	translation_Loss:3.497	token_training_loss:0.0	distillation_Loss:0.463                                                   	MRR:18.3	Hits@10:31.71	Best:18.3
2025-01-06 23:10:44,928: Snapshot:2	Epoch:2	Loss:2.075	translation_Loss:1.801	token_training_loss:0.0	distillation_Loss:0.273                                                   	MRR:20.19	Hits@10:35.62	Best:20.19
2025-01-06 23:10:53,787: Snapshot:2	Epoch:3	Loss:1.536	translation_Loss:1.318	token_training_loss:0.0	distillation_Loss:0.218                                                   	MRR:21.37	Hits@10:37.54	Best:21.37
2025-01-06 23:11:02,773: Snapshot:2	Epoch:4	Loss:1.308	translation_Loss:1.11	token_training_loss:0.0	distillation_Loss:0.198                                                   	MRR:21.95	Hits@10:38.36	Best:21.95
2025-01-06 23:11:11,279: Snapshot:2	Epoch:5	Loss:1.173	translation_Loss:0.986	token_training_loss:0.0	distillation_Loss:0.187                                                   	MRR:22.22	Hits@10:39.05	Best:22.22
2025-01-06 23:11:20,180: Snapshot:2	Epoch:6	Loss:1.099	translation_Loss:0.917	token_training_loss:0.0	distillation_Loss:0.182                                                   	MRR:22.5	Hits@10:39.45	Best:22.5
2025-01-06 23:11:28,759: Snapshot:2	Epoch:7	Loss:1.043	translation_Loss:0.865	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:22.62	Hits@10:39.76	Best:22.62
2025-01-06 23:11:37,668: Snapshot:2	Epoch:8	Loss:1.006	translation_Loss:0.829	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:22.83	Hits@10:40.02	Best:22.83
2025-01-06 23:11:46,199: Snapshot:2	Epoch:9	Loss:0.976	translation_Loss:0.798	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:22.91	Hits@10:40.13	Best:22.91
2025-01-06 23:11:55,047: Snapshot:2	Epoch:10	Loss:0.951	translation_Loss:0.774	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:23.03	Hits@10:40.22	Best:23.03
2025-01-06 23:12:03,599: Snapshot:2	Epoch:11	Loss:0.933	translation_Loss:0.757	token_training_loss:0.0	distillation_Loss:0.176                                                   	MRR:23.05	Hits@10:40.53	Best:23.05
2025-01-06 23:12:12,422: Snapshot:2	Epoch:12	Loss:0.917	translation_Loss:0.74	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:23.0	Hits@10:40.6	Best:23.05
2025-01-06 23:12:21,277: Snapshot:2	Epoch:13	Loss:0.906	translation_Loss:0.728	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:23.09	Hits@10:40.45	Best:23.09
2025-01-06 23:12:29,904: Snapshot:2	Epoch:14	Loss:0.893	translation_Loss:0.716	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:23.11	Hits@10:40.55	Best:23.11
2025-01-06 23:12:38,852: Snapshot:2	Epoch:15	Loss:0.888	translation_Loss:0.712	token_training_loss:0.0	distillation_Loss:0.176                                                   	MRR:23.13	Hits@10:40.61	Best:23.13
2025-01-06 23:12:47,358: Snapshot:2	Epoch:16	Loss:0.863	translation_Loss:0.686	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:23.13	Hits@10:40.99	Best:23.13
2025-01-06 23:12:56,189: Snapshot:2	Epoch:17	Loss:0.86	translation_Loss:0.683	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:23.27	Hits@10:40.74	Best:23.27
2025-01-06 23:13:04,709: Snapshot:2	Epoch:18	Loss:0.858	translation_Loss:0.681	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:23.12	Hits@10:40.68	Best:23.27
2025-01-06 23:13:13,532: Snapshot:2	Epoch:19	Loss:0.849	translation_Loss:0.669	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:23.33	Hits@10:41.0	Best:23.33
2025-01-06 23:13:22,058: Snapshot:2	Epoch:20	Loss:0.846	translation_Loss:0.667	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:23.3	Hits@10:40.89	Best:23.33
2025-01-06 23:13:31,021: Snapshot:2	Epoch:21	Loss:0.841	translation_Loss:0.662	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:23.31	Hits@10:41.02	Best:23.33
2025-01-06 23:13:39,916: Snapshot:2	Epoch:22	Loss:0.831	translation_Loss:0.651	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:23.35	Hits@10:40.97	Best:23.35
2025-01-06 23:13:48,430: Snapshot:2	Epoch:23	Loss:0.832	translation_Loss:0.652	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:23.28	Hits@10:40.74	Best:23.35
2025-01-06 23:13:57,314: Snapshot:2	Epoch:24	Loss:0.827	translation_Loss:0.649	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:23.35	Hits@10:40.78	Best:23.35
2025-01-06 23:14:05,842: Snapshot:2	Epoch:25	Loss:0.823	translation_Loss:0.644	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:23.43	Hits@10:40.9	Best:23.43
2025-01-06 23:14:14,713: Snapshot:2	Epoch:26	Loss:0.806	translation_Loss:0.628	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:23.24	Hits@10:40.92	Best:23.43
2025-01-06 23:14:23,202: Snapshot:2	Epoch:27	Loss:0.819	translation_Loss:0.638	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:23.39	Hits@10:40.77	Best:23.43
2025-01-06 23:14:32,147: Snapshot:2	Epoch:28	Loss:0.811	translation_Loss:0.63	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:23.48	Hits@10:41.01	Best:23.48
2025-01-06 23:14:40,596: Snapshot:2	Epoch:29	Loss:0.806	translation_Loss:0.625	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:23.45	Hits@10:41.05	Best:23.48
2025-01-06 23:14:49,422: Snapshot:2	Epoch:30	Loss:0.799	translation_Loss:0.619	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:23.4	Hits@10:41.15	Best:23.48
2025-01-06 23:14:58,418: Snapshot:2	Epoch:31	Loss:0.798	translation_Loss:0.617	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:23.54	Hits@10:41.07	Best:23.54
2025-01-06 23:15:07,030: Snapshot:2	Epoch:32	Loss:0.8	translation_Loss:0.62	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:23.46	Hits@10:41.42	Best:23.54
2025-01-06 23:15:15,919: Snapshot:2	Epoch:33	Loss:0.797	translation_Loss:0.617	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:23.55	Hits@10:41.02	Best:23.55
2025-01-06 23:15:24,426: Snapshot:2	Epoch:34	Loss:0.79	translation_Loss:0.609	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:23.4	Hits@10:40.92	Best:23.55
2025-01-06 23:15:33,371: Snapshot:2	Epoch:35	Loss:0.779	translation_Loss:0.599	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:23.34	Hits@10:40.83	Best:23.55
2025-01-06 23:15:41,991: Early Stopping! Snapshot: 2 Epoch: 36 Best Results: 23.55
2025-01-06 23:15:41,991: Start to training tokens! Snapshot: 2 Epoch: 36 Loss:0.796 MRR:23.43 Best Results: 23.55
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:15:41,992: Snapshot:2	Epoch:36	Loss:0.796	translation_Loss:0.613	token_training_loss:0.0	distillation_Loss:0.182                                                   	MRR:23.43	Hits@10:40.99	Best:23.55
2025-01-06 23:15:50,811: Snapshot:2	Epoch:37	Loss:28.557	translation_Loss:15.767	token_training_loss:12.79	distillation_Loss:0.0                                                   	MRR:23.43	Hits@10:40.99	Best:23.55
2025-01-06 23:15:59,339: End of token training: 2 Epoch: 38 Loss:15.982 MRR:23.43 Best Results: 23.55
2025-01-06 23:15:59,339: Snapshot:2	Epoch:38	Loss:15.982	translation_Loss:15.75	token_training_loss:0.232	distillation_Loss:0.0                                                           	MRR:23.43	Hits@10:40.99	Best:23.55
2025-01-06 23:15:59,576: => loading checkpoint './checkpoint/ENTITY/2model_best.tar'
2025-01-06 23:16:07,114: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3378 | 0.2151 | 0.3926 | 0.4703 |  0.5734 |
|     1      | 0.2504 | 0.153  | 0.2832 | 0.3491 |  0.4395 |
|     2      | 0.2349 | 0.141  |  0.27  | 0.3301 |  0.4147 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,745,000)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,840,600
Trainable params: 1,200
Non-trainable params: 1,839,400
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:16:23,188: Snapshot:3	Epoch:0	Loss:10.586	translation_Loss:9.909	token_training_loss:0.0	distillation_Loss:0.678                                                   	MRR:14.94	Hits@10:26.28	Best:14.94
2025-01-06 23:16:32,242: Snapshot:3	Epoch:1	Loss:3.153	translation_Loss:2.641	token_training_loss:0.0	distillation_Loss:0.513                                                   	MRR:18.17	Hits@10:32.29	Best:18.17
2025-01-06 23:16:40,870: Snapshot:3	Epoch:2	Loss:1.656	translation_Loss:1.376	token_training_loss:0.0	distillation_Loss:0.28                                                   	MRR:19.45	Hits@10:34.98	Best:19.45
2025-01-06 23:16:49,824: Snapshot:3	Epoch:3	Loss:1.25	translation_Loss:1.031	token_training_loss:0.0	distillation_Loss:0.219                                                   	MRR:20.33	Hits@10:36.46	Best:20.33
2025-01-06 23:16:58,508: Snapshot:3	Epoch:4	Loss:1.054	translation_Loss:0.861	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:20.88	Hits@10:37.59	Best:20.88
2025-01-06 23:17:07,523: Snapshot:3	Epoch:5	Loss:0.972	translation_Loss:0.785	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:21.19	Hits@10:38.14	Best:21.19
2025-01-06 23:17:16,150: Snapshot:3	Epoch:6	Loss:0.898	translation_Loss:0.719	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:21.28	Hits@10:38.19	Best:21.28
2025-01-06 23:17:25,287: Snapshot:3	Epoch:7	Loss:0.862	translation_Loss:0.685	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:21.47	Hits@10:38.82	Best:21.47
2025-01-06 23:17:33,923: Snapshot:3	Epoch:8	Loss:0.827	translation_Loss:0.649	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:21.57	Hits@10:38.81	Best:21.57
2025-01-06 23:17:42,944: Snapshot:3	Epoch:9	Loss:0.81	translation_Loss:0.635	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:21.77	Hits@10:39.15	Best:21.77
2025-01-06 23:17:51,837: Snapshot:3	Epoch:10	Loss:0.796	translation_Loss:0.619	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:21.6	Hits@10:39.05	Best:21.77
2025-01-06 23:18:00,522: Snapshot:3	Epoch:11	Loss:0.78	translation_Loss:0.605	token_training_loss:0.0	distillation_Loss:0.176                                                   	MRR:21.89	Hits@10:39.35	Best:21.89
2025-01-06 23:18:09,088: Snapshot:3	Epoch:12	Loss:0.758	translation_Loss:0.583	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:21.88	Hits@10:39.56	Best:21.89
2025-01-06 23:18:17,953: Snapshot:3	Epoch:13	Loss:0.752	translation_Loss:0.577	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:21.88	Hits@10:39.75	Best:21.89
2025-01-06 23:18:26,967: Snapshot:3	Epoch:14	Loss:0.75	translation_Loss:0.574	token_training_loss:0.0	distillation_Loss:0.176                                                   	MRR:22.01	Hits@10:39.69	Best:22.01
2025-01-06 23:18:35,636: Snapshot:3	Epoch:15	Loss:0.74	translation_Loss:0.563	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:22.13	Hits@10:39.82	Best:22.13
2025-01-06 23:18:44,601: Snapshot:3	Epoch:16	Loss:0.731	translation_Loss:0.556	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:22.16	Hits@10:39.88	Best:22.16
2025-01-06 23:18:53,198: Snapshot:3	Epoch:17	Loss:0.721	translation_Loss:0.546	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:22.2	Hits@10:39.9	Best:22.2
2025-01-06 23:19:02,253: Snapshot:3	Epoch:18	Loss:0.718	translation_Loss:0.543	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:22.23	Hits@10:40.07	Best:22.23
2025-01-06 23:19:10,834: Snapshot:3	Epoch:19	Loss:0.717	translation_Loss:0.539	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:22.07	Hits@10:39.87	Best:22.23
2025-01-06 23:19:19,746: Snapshot:3	Epoch:20	Loss:0.71	translation_Loss:0.532	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:22.14	Hits@10:40.0	Best:22.23
2025-01-06 23:19:28,351: Early Stopping! Snapshot: 3 Epoch: 21 Best Results: 22.23
2025-01-06 23:19:28,351: Start to training tokens! Snapshot: 3 Epoch: 21 Loss:0.701 MRR:22.08 Best Results: 22.23
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:19:28,352: Snapshot:3	Epoch:21	Loss:0.701	translation_Loss:0.523	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:22.08	Hits@10:40.15	Best:22.23
2025-01-06 23:19:37,333: Snapshot:3	Epoch:22	Loss:27.576	translation_Loss:14.26	token_training_loss:13.316	distillation_Loss:0.0                                                   	MRR:22.08	Hits@10:40.15	Best:22.23
2025-01-06 23:19:45,910: End of token training: 3 Epoch: 23 Loss:14.539 MRR:22.08 Best Results: 22.23
2025-01-06 23:19:45,910: Snapshot:3	Epoch:23	Loss:14.539	translation_Loss:14.268	token_training_loss:0.271	distillation_Loss:0.0                                                           	MRR:22.08	Hits@10:40.15	Best:22.23
2025-01-06 23:19:46,146: => loading checkpoint './checkpoint/ENTITY/3model_best.tar'
2025-01-06 23:19:57,514: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3371 | 0.2149 | 0.3921 | 0.4662 |  0.5702 |
|     1      | 0.2515 | 0.154  | 0.2844 | 0.3492 |  0.4413 |
|     2      | 0.2364 | 0.1411 | 0.2716 | 0.3324 |  0.4168 |
|     3      | 0.2253 | 0.1319 | 0.2627 | 0.3196 |  0.4007 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,326,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,422,600
Trainable params: 1,200
Non-trainable params: 2,421,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:20:10,794: Snapshot:4	Epoch:0	Loss:7.941	translation_Loss:7.498	token_training_loss:0.0	distillation_Loss:0.442                                                   	MRR:16.76	Hits@10:31.43	Best:16.76
2025-01-06 23:20:17,095: Snapshot:4	Epoch:1	Loss:2.37	translation_Loss:1.902	token_training_loss:0.0	distillation_Loss:0.468                                                   	MRR:22.35	Hits@10:40.76	Best:22.35
2025-01-06 23:20:23,725: Snapshot:4	Epoch:2	Loss:0.995	translation_Loss:0.728	token_training_loss:0.0	distillation_Loss:0.267                                                   	MRR:24.25	Hits@10:42.81	Best:24.25
2025-01-06 23:20:30,081: Snapshot:4	Epoch:3	Loss:0.61	translation_Loss:0.449	token_training_loss:0.0	distillation_Loss:0.161                                                   	MRR:25.0	Hits@10:43.46	Best:25.0
2025-01-06 23:20:36,381: Snapshot:4	Epoch:4	Loss:0.484	translation_Loss:0.362	token_training_loss:0.0	distillation_Loss:0.122                                                   	MRR:25.44	Hits@10:44.5	Best:25.44
2025-01-06 23:20:43,008: Snapshot:4	Epoch:5	Loss:0.423	translation_Loss:0.314	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:25.62	Hits@10:45.29	Best:25.62
2025-01-06 23:20:49,259: Snapshot:4	Epoch:6	Loss:0.377	translation_Loss:0.277	token_training_loss:0.0	distillation_Loss:0.1                                                   	MRR:25.78	Hits@10:45.81	Best:25.78
2025-01-06 23:20:55,574: Snapshot:4	Epoch:7	Loss:0.357	translation_Loss:0.263	token_training_loss:0.0	distillation_Loss:0.094                                                   	MRR:26.23	Hits@10:46.44	Best:26.23
2025-01-06 23:21:02,248: Snapshot:4	Epoch:8	Loss:0.334	translation_Loss:0.242	token_training_loss:0.0	distillation_Loss:0.092                                                   	MRR:26.27	Hits@10:46.28	Best:26.27
2025-01-06 23:21:08,653: Snapshot:4	Epoch:9	Loss:0.326	translation_Loss:0.236	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:26.33	Hits@10:46.45	Best:26.33
2025-01-06 23:21:14,987: Snapshot:4	Epoch:10	Loss:0.31	translation_Loss:0.221	token_training_loss:0.0	distillation_Loss:0.089                                                   	MRR:26.51	Hits@10:46.98	Best:26.51
2025-01-06 23:21:21,619: Snapshot:4	Epoch:11	Loss:0.307	translation_Loss:0.219	token_training_loss:0.0	distillation_Loss:0.088                                                   	MRR:26.69	Hits@10:47.12	Best:26.69
2025-01-06 23:21:27,838: Snapshot:4	Epoch:12	Loss:0.297	translation_Loss:0.211	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:26.69	Hits@10:47.03	Best:26.69
2025-01-06 23:21:34,136: Snapshot:4	Epoch:13	Loss:0.29	translation_Loss:0.203	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:26.71	Hits@10:47.08	Best:26.71
2025-01-06 23:21:40,837: Snapshot:4	Epoch:14	Loss:0.285	translation_Loss:0.198	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:27.02	Hits@10:47.35	Best:27.02
2025-01-06 23:21:47,070: Snapshot:4	Epoch:15	Loss:0.283	translation_Loss:0.196	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:26.9	Hits@10:47.64	Best:27.02
2025-01-06 23:21:53,272: Snapshot:4	Epoch:16	Loss:0.276	translation_Loss:0.189	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:26.98	Hits@10:47.5	Best:27.02
2025-01-06 23:21:59,957: Snapshot:4	Epoch:17	Loss:0.273	translation_Loss:0.188	token_training_loss:0.0	distillation_Loss:0.085                                                   	MRR:27.11	Hits@10:47.46	Best:27.11
2025-01-06 23:22:06,287: Snapshot:4	Epoch:18	Loss:0.273	translation_Loss:0.186	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:27.15	Hits@10:47.95	Best:27.15
2025-01-06 23:22:12,831: Snapshot:4	Epoch:19	Loss:0.276	translation_Loss:0.187	token_training_loss:0.0	distillation_Loss:0.089                                                   	MRR:27.06	Hits@10:47.67	Best:27.15
2025-01-06 23:22:19,022: Snapshot:4	Epoch:20	Loss:0.266	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:27.0	Hits@10:47.67	Best:27.15
2025-01-06 23:22:25,286: Early Stopping! Snapshot: 4 Epoch: 21 Best Results: 27.15
2025-01-06 23:22:25,286: Start to training tokens! Snapshot: 4 Epoch: 21 Loss:0.264 MRR:26.94 Best Results: 27.15
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:22:25,286: Snapshot:4	Epoch:21	Loss:0.264	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.085                                                   	MRR:26.94	Hits@10:47.71	Best:27.15
2025-01-06 23:22:31,491: Snapshot:4	Epoch:22	Loss:19.801	translation_Loss:7.752	token_training_loss:12.049	distillation_Loss:0.0                                                   	MRR:26.94	Hits@10:47.71	Best:27.15
2025-01-06 23:22:37,935: End of token training: 4 Epoch: 23 Loss:8.188 MRR:26.94 Best Results: 27.15
2025-01-06 23:22:37,936: Snapshot:4	Epoch:23	Loss:8.188	translation_Loss:7.743	token_training_loss:0.445	distillation_Loss:0.0                                                           	MRR:26.94	Hits@10:47.71	Best:27.15
2025-01-06 23:22:38,184: => loading checkpoint './checkpoint/ENTITY/4model_best.tar'
2025-01-06 23:22:52,196: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3341 | 0.2111 | 0.3893 | 0.4652 |  0.5698 |
|     1      | 0.2503 | 0.1517 | 0.2842 | 0.3501 |  0.4421 |
|     2      | 0.2361 | 0.1408 | 0.2717 | 0.3327 |  0.418  |
|     3      | 0.2274 | 0.1341 | 0.265  | 0.3217 |  0.4039 |
|     4      | 0.2742 | 0.1651 | 0.3249 | 0.3979 |  0.4818 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,004,200
Trainable params: 1,200
Non-trainable params: 3,003,000
=================================================================
2025-01-06 23:22:52,199: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3398 | 0.2186 | 0.3947 | 0.4695 |  0.5722 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3394 | 0.2171 | 0.397  | 0.4703 |  0.5754 |
|     1      | 0.2521 | 0.1553 | 0.2845 | 0.3494 |  0.4408 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3378 | 0.2151 | 0.3926 | 0.4703 |  0.5734 |
|     1      | 0.2504 | 0.153  | 0.2832 | 0.3491 |  0.4395 |
|     2      | 0.2349 | 0.141  |  0.27  | 0.3301 |  0.4147 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3371 | 0.2149 | 0.3921 | 0.4662 |  0.5702 |
|     1      | 0.2515 | 0.154  | 0.2844 | 0.3492 |  0.4413 |
|     2      | 0.2364 | 0.1411 | 0.2716 | 0.3324 |  0.4168 |
|     3      | 0.2253 | 0.1319 | 0.2627 | 0.3196 |  0.4007 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3341 | 0.2111 | 0.3893 | 0.4652 |  0.5698 |
|     1      | 0.2503 | 0.1517 | 0.2842 | 0.3501 |  0.4421 |
|     2      | 0.2361 | 0.1408 | 0.2717 | 0.3327 |  0.418  |
|     3      | 0.2274 | 0.1341 | 0.265  | 0.3217 |  0.4039 |
|     4      | 0.2742 | 0.1651 | 0.3249 | 0.3979 |  0.4818 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 23:22:52,200: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 122.74005484580994 |    0.34   |    0.219     |    0.395     |     0.572     |
|    1     | 181.21922945976257 |   0.286   |    0.179     |    0.329     |     0.493     |
|    2     | 344.2926878929138  |   0.266   |    0.163     |    0.305     |     0.462     |
|    3     | 214.97500133514404 |   0.255   |    0.155     |    0.294     |     0.446     |
|    4     | 157.45998620986938 |   0.258   |    0.156     |    0.299     |     0.453     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 23:22:52,200: Sum_Training_Time:1020.6869597434998
2025-01-06 23:22:52,200: Every_Training_Time:[122.74005484580994, 181.21922945976257, 344.2926878929138, 214.97500133514404, 157.45998620986938]
2025-01-06 23:22:52,200: Forward transfer: 0.043725 Backward transfer: -0.0010499999999999884
2025-01-06 23:23:13,143: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/ENTITY/', dataset='ENTITY', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106232257/ENTITY', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=5555, record=False, reply_loss_weight=0.1, save_path='./checkpoint/ENTITY', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[5000.0, 10000.0, 10000.0, 10000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 23:23:21,422: Snapshot:0	Epoch:0	Loss:13.189	translation_Loss:13.189	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:4.82	Hits@10:10.79	Best:4.82
2025-01-06 23:23:26,432: Snapshot:0	Epoch:1	Loss:10.945	translation_Loss:10.945	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.76	Hits@10:23.87	Best:9.76
2025-01-06 23:23:31,090: Snapshot:0	Epoch:2	Loss:8.979	translation_Loss:8.979	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.22	Hits@10:36.15	Best:14.22
2025-01-06 23:23:36,112: Snapshot:0	Epoch:3	Loss:6.924	translation_Loss:6.924	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:20.1	Hits@10:44.17	Best:20.1
2025-01-06 23:23:40,692: Snapshot:0	Epoch:4	Loss:5.082	translation_Loss:5.082	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.05	Hits@10:49.08	Best:25.05
2025-01-06 23:23:45,292: Snapshot:0	Epoch:5	Loss:3.622	translation_Loss:3.622	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:28.26	Hits@10:52.04	Best:28.26
2025-01-06 23:23:50,414: Snapshot:0	Epoch:6	Loss:2.554	translation_Loss:2.554	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:30.4	Hits@10:53.75	Best:30.4
2025-01-06 23:23:55,110: Snapshot:0	Epoch:7	Loss:1.798	translation_Loss:1.798	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:31.78	Hits@10:55.15	Best:31.78
2025-01-06 23:23:59,719: Snapshot:0	Epoch:8	Loss:1.311	translation_Loss:1.311	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:32.91	Hits@10:55.94	Best:32.91
2025-01-06 23:24:04,661: Snapshot:0	Epoch:9	Loss:0.988	translation_Loss:0.988	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.35	Hits@10:56.29	Best:33.35
2025-01-06 23:24:09,262: Snapshot:0	Epoch:10	Loss:0.761	translation_Loss:0.761	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.5	Hits@10:56.6	Best:33.5
2025-01-06 23:24:13,824: Snapshot:0	Epoch:11	Loss:0.606	translation_Loss:0.606	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.65	Hits@10:56.74	Best:33.65
2025-01-06 23:24:18,758: Snapshot:0	Epoch:12	Loss:0.509	translation_Loss:0.509	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.64	Hits@10:56.91	Best:33.65
2025-01-06 23:24:23,388: Snapshot:0	Epoch:13	Loss:0.431	translation_Loss:0.431	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.83	Hits@10:56.84	Best:33.83
2025-01-06 23:24:27,983: Snapshot:0	Epoch:14	Loss:0.375	translation_Loss:0.375	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.92	Hits@10:56.82	Best:33.92
2025-01-06 23:24:33,135: Snapshot:0	Epoch:15	Loss:0.337	translation_Loss:0.337	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.84	Hits@10:56.9	Best:33.92
2025-01-06 23:24:37,808: Snapshot:0	Epoch:16	Loss:0.306	translation_Loss:0.306	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.8	Hits@10:56.92	Best:33.92
2025-01-06 23:24:42,389: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 33.92
2025-01-06 23:24:42,389: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.272 MRR:33.61 Best Results: 33.92
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:24:42,390: Snapshot:0	Epoch:17	Loss:0.272	translation_Loss:0.272	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:33.61	Hits@10:56.76	Best:33.92
2025-01-06 23:24:47,872: Snapshot:0	Epoch:18	Loss:20.982	translation_Loss:9.809	token_training_loss:11.174	distillation_Loss:0.0                                                   	MRR:33.61	Hits@10:56.76	Best:33.92
2025-01-06 23:24:52,427: End of token training: 0 Epoch: 19 Loss:10.264 MRR:33.61 Best Results: 33.92
2025-01-06 23:24:52,427: Snapshot:0	Epoch:19	Loss:10.264	translation_Loss:9.816	token_training_loss:0.448	distillation_Loss:0.0                                                           	MRR:33.61	Hits@10:56.76	Best:33.92
2025-01-06 23:24:52,675: => loading checkpoint './checkpoint/ENTITY/0model_best.tar'
2025-01-06 23:24:54,129: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3414 | 0.2217 | 0.3949 | 0.4683 |  0.5699 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (581,800)
├─Embedding: 1-2                         (93,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 676,200
Trainable params: 1,200
Non-trainable params: 675,000
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:25:08,973: Snapshot:1	Epoch:0	Loss:13.036	translation_Loss:12.556	token_training_loss:0.0	distillation_Loss:0.479                                                   	MRR:15.37	Hits@10:26.08	Best:15.37
2025-01-06 23:25:16,709: Snapshot:1	Epoch:1	Loss:5.308	translation_Loss:4.906	token_training_loss:0.0	distillation_Loss:0.402                                                   	MRR:20.09	Hits@10:35.56	Best:20.09
2025-01-06 23:25:24,851: Snapshot:1	Epoch:2	Loss:2.793	translation_Loss:2.544	token_training_loss:0.0	distillation_Loss:0.249                                                   	MRR:22.62	Hits@10:40.1	Best:22.62
2025-01-06 23:25:32,770: Snapshot:1	Epoch:3	Loss:1.96	translation_Loss:1.767	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:23.78	Hits@10:41.88	Best:23.78
2025-01-06 23:25:40,904: Snapshot:1	Epoch:4	Loss:1.592	translation_Loss:1.421	token_training_loss:0.0	distillation_Loss:0.171                                                   	MRR:24.27	Hits@10:42.9	Best:24.27
2025-01-06 23:25:48,740: Snapshot:1	Epoch:5	Loss:1.39	translation_Loss:1.23	token_training_loss:0.0	distillation_Loss:0.16                                                   	MRR:24.62	Hits@10:43.36	Best:24.62
2025-01-06 23:25:56,941: Snapshot:1	Epoch:6	Loss:1.274	translation_Loss:1.12	token_training_loss:0.0	distillation_Loss:0.154                                                   	MRR:24.67	Hits@10:43.62	Best:24.67
2025-01-06 23:26:04,826: Snapshot:1	Epoch:7	Loss:1.185	translation_Loss:1.035	token_training_loss:0.0	distillation_Loss:0.15                                                   	MRR:24.88	Hits@10:43.85	Best:24.88
2025-01-06 23:26:13,031: Snapshot:1	Epoch:8	Loss:1.137	translation_Loss:0.99	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:24.97	Hits@10:44.13	Best:24.97
2025-01-06 23:26:21,168: Snapshot:1	Epoch:9	Loss:1.084	translation_Loss:0.942	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:25.1	Hits@10:44.17	Best:25.1
2025-01-06 23:26:29,001: Snapshot:1	Epoch:10	Loss:1.054	translation_Loss:0.911	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:25.17	Hits@10:44.22	Best:25.17
2025-01-06 23:26:37,162: Snapshot:1	Epoch:11	Loss:1.03	translation_Loss:0.888	token_training_loss:0.0	distillation_Loss:0.141                                                   	MRR:25.14	Hits@10:44.12	Best:25.17
2025-01-06 23:26:44,954: Snapshot:1	Epoch:12	Loss:0.995	translation_Loss:0.855	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.19	Hits@10:44.13	Best:25.19
2025-01-06 23:26:53,132: Snapshot:1	Epoch:13	Loss:0.985	translation_Loss:0.844	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.21	Hits@10:44.3	Best:25.21
2025-01-06 23:27:01,031: Snapshot:1	Epoch:14	Loss:0.968	translation_Loss:0.828	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.2	Hits@10:44.34	Best:25.21
2025-01-06 23:27:09,163: Snapshot:1	Epoch:15	Loss:0.952	translation_Loss:0.812	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.17	Hits@10:44.36	Best:25.21
2025-01-06 23:27:16,941: Snapshot:1	Epoch:16	Loss:0.94	translation_Loss:0.8	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.26	Hits@10:44.37	Best:25.26
2025-01-06 23:27:25,303: Snapshot:1	Epoch:17	Loss:0.922	translation_Loss:0.783	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:25.36	Hits@10:44.76	Best:25.36
2025-01-06 23:27:33,512: Snapshot:1	Epoch:18	Loss:0.914	translation_Loss:0.774	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.35	Hits@10:44.73	Best:25.36
2025-01-06 23:27:41,277: Snapshot:1	Epoch:19	Loss:0.906	translation_Loss:0.767	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.37	Hits@10:44.7	Best:25.37
2025-01-06 23:27:49,376: Snapshot:1	Epoch:20	Loss:0.9	translation_Loss:0.76	token_training_loss:0.0	distillation_Loss:0.141                                                   	MRR:25.23	Hits@10:44.62	Best:25.37
2025-01-06 23:27:57,143: Snapshot:1	Epoch:21	Loss:0.898	translation_Loss:0.759	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:25.32	Hits@10:44.51	Best:25.37
2025-01-06 23:28:05,318: Snapshot:1	Epoch:22	Loss:0.883	translation_Loss:0.745	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.43	Hits@10:44.81	Best:25.43
2025-01-06 23:28:13,096: Snapshot:1	Epoch:23	Loss:0.875	translation_Loss:0.736	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:25.37	Hits@10:44.75	Best:25.43
2025-01-06 23:28:21,282: Snapshot:1	Epoch:24	Loss:0.866	translation_Loss:0.728	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.49	Hits@10:44.71	Best:25.49
2025-01-06 23:28:29,077: Snapshot:1	Epoch:25	Loss:0.865	translation_Loss:0.727	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.43	Hits@10:44.68	Best:25.49
2025-01-06 23:28:37,243: Snapshot:1	Epoch:26	Loss:0.861	translation_Loss:0.723	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.47	Hits@10:44.77	Best:25.49
2025-01-06 23:28:45,007: Early Stopping! Snapshot: 1 Epoch: 27 Best Results: 25.49
2025-01-06 23:28:45,007: Start to training tokens! Snapshot: 1 Epoch: 27 Loss:0.852 MRR:25.42 Best Results: 25.49
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:28:45,008: Snapshot:1	Epoch:27	Loss:0.852	translation_Loss:0.713	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:25.42	Hits@10:44.71	Best:25.49
2025-01-06 23:28:53,156: Snapshot:1	Epoch:28	Loss:29.54	translation_Loss:15.918	token_training_loss:13.621	distillation_Loss:0.0                                                   	MRR:25.42	Hits@10:44.71	Best:25.49
2025-01-06 23:29:01,336: End of token training: 1 Epoch: 29 Loss:16.184 MRR:25.42 Best Results: 25.49
2025-01-06 23:29:01,336: Snapshot:1	Epoch:29	Loss:16.184	translation_Loss:15.94	token_training_loss:0.244	distillation_Loss:0.0                                                           	MRR:25.42	Hits@10:44.71	Best:25.49
2025-01-06 23:29:01,579: => loading checkpoint './checkpoint/ENTITY/1model_best.tar'
2025-01-06 23:29:05,728: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3404 | 0.2201 | 0.3952 | 0.4676 |  0.5702 |
|     1      | 0.2571 | 0.1574 | 0.2925 | 0.358  |  0.4472 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,163,400)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,259,000
Trainable params: 1,200
Non-trainable params: 1,257,800
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:29:21,288: Snapshot:2	Epoch:0	Loss:11.7	translation_Loss:11.072	token_training_loss:0.0	distillation_Loss:0.629                                                   	MRR:15.04	Hits@10:25.52	Best:15.04
2025-01-06 23:29:29,813: Snapshot:2	Epoch:1	Loss:4.214	translation_Loss:3.758	token_training_loss:0.0	distillation_Loss:0.455                                                   	MRR:18.56	Hits@10:32.23	Best:18.56
2025-01-06 23:29:38,731: Snapshot:2	Epoch:2	Loss:2.328	translation_Loss:2.046	token_training_loss:0.0	distillation_Loss:0.282                                                   	MRR:20.47	Hits@10:35.76	Best:20.47
2025-01-06 23:29:47,675: Snapshot:2	Epoch:3	Loss:1.747	translation_Loss:1.518	token_training_loss:0.0	distillation_Loss:0.229                                                   	MRR:21.36	Hits@10:37.59	Best:21.36
2025-01-06 23:29:56,205: Snapshot:2	Epoch:4	Loss:1.511	translation_Loss:1.301	token_training_loss:0.0	distillation_Loss:0.21                                                   	MRR:21.9	Hits@10:38.33	Best:21.9
2025-01-06 23:30:05,327: Snapshot:2	Epoch:5	Loss:1.365	translation_Loss:1.165	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:22.17	Hits@10:39.17	Best:22.17
2025-01-06 23:30:13,848: Snapshot:2	Epoch:6	Loss:1.276	translation_Loss:1.081	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:22.47	Hits@10:39.72	Best:22.47
2025-01-06 23:30:22,714: Snapshot:2	Epoch:7	Loss:1.224	translation_Loss:1.03	token_training_loss:0.0	distillation_Loss:0.194                                                   	MRR:22.49	Hits@10:39.92	Best:22.49
2025-01-06 23:30:31,212: Snapshot:2	Epoch:8	Loss:1.172	translation_Loss:0.981	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:22.64	Hits@10:40.14	Best:22.64
2025-01-06 23:30:40,067: Snapshot:2	Epoch:9	Loss:1.143	translation_Loss:0.953	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:22.83	Hits@10:40.3	Best:22.83
2025-01-06 23:30:48,551: Snapshot:2	Epoch:10	Loss:1.111	translation_Loss:0.921	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:22.89	Hits@10:40.47	Best:22.89
2025-01-06 23:30:57,387: Snapshot:2	Epoch:11	Loss:1.083	translation_Loss:0.895	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:22.86	Hits@10:40.5	Best:22.89
2025-01-06 23:31:06,257: Snapshot:2	Epoch:12	Loss:1.071	translation_Loss:0.882	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:22.96	Hits@10:40.77	Best:22.96
2025-01-06 23:31:14,704: Snapshot:2	Epoch:13	Loss:1.055	translation_Loss:0.865	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:23.08	Hits@10:40.8	Best:23.08
2025-01-06 23:31:23,513: Snapshot:2	Epoch:14	Loss:1.047	translation_Loss:0.858	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:23.07	Hits@10:40.61	Best:23.08
2025-01-06 23:31:32,006: Snapshot:2	Epoch:15	Loss:1.026	translation_Loss:0.836	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:23.03	Hits@10:40.87	Best:23.08
2025-01-06 23:31:40,836: Snapshot:2	Epoch:16	Loss:1.024	translation_Loss:0.835	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:23.17	Hits@10:41.1	Best:23.17
2025-01-06 23:31:49,279: Snapshot:2	Epoch:17	Loss:1.006	translation_Loss:0.816	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:23.18	Hits@10:41.32	Best:23.18
2025-01-06 23:31:58,163: Snapshot:2	Epoch:18	Loss:1.005	translation_Loss:0.812	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:23.29	Hits@10:41.26	Best:23.29
2025-01-06 23:32:06,683: Snapshot:2	Epoch:19	Loss:0.992	translation_Loss:0.801	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:23.38	Hits@10:41.21	Best:23.38
2025-01-06 23:32:15,541: Snapshot:2	Epoch:20	Loss:0.987	translation_Loss:0.797	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:23.27	Hits@10:41.21	Best:23.38
2025-01-06 23:32:24,386: Snapshot:2	Epoch:21	Loss:0.984	translation_Loss:0.792	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:23.43	Hits@10:41.32	Best:23.43
2025-01-06 23:32:32,849: Snapshot:2	Epoch:22	Loss:0.975	translation_Loss:0.783	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:23.31	Hits@10:41.23	Best:23.43
2025-01-06 23:32:41,623: Snapshot:2	Epoch:23	Loss:0.972	translation_Loss:0.782	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:23.3	Hits@10:41.15	Best:23.43
2025-01-06 23:32:50,107: Early Stopping! Snapshot: 2 Epoch: 24 Best Results: 23.43
2025-01-06 23:32:50,107: Start to training tokens! Snapshot: 2 Epoch: 24 Loss:0.956 MRR:23.28 Best Results: 23.43
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:32:50,107: Snapshot:2	Epoch:24	Loss:0.956	translation_Loss:0.765	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:23.28	Hits@10:41.16	Best:23.43
2025-01-06 23:32:58,919: Snapshot:2	Epoch:25	Loss:29.182	translation_Loss:15.811	token_training_loss:13.371	distillation_Loss:0.0                                                   	MRR:23.28	Hits@10:41.16	Best:23.43
2025-01-06 23:33:07,382: End of token training: 2 Epoch: 26 Loss:16.068 MRR:23.28 Best Results: 23.43
2025-01-06 23:33:07,382: Snapshot:2	Epoch:26	Loss:16.068	translation_Loss:15.827	token_training_loss:0.24	distillation_Loss:0.0                                                           	MRR:23.28	Hits@10:41.16	Best:23.43
2025-01-06 23:33:07,630: => loading checkpoint './checkpoint/ENTITY/2model_best.tar'
2025-01-06 23:33:15,130: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3393 | 0.219  | 0.3925 | 0.4668 |  0.569  |
|     1      | 0.2556 | 0.1556 | 0.2913 | 0.3571 |  0.4488 |
|     2      | 0.2367 | 0.1414 | 0.2727 | 0.3339 |  0.4166 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (1,745,000)
├─Embedding: 1-2                         (94,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,840,600
Trainable params: 1,200
Non-trainable params: 1,839,400
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:33:31,135: Snapshot:3	Epoch:0	Loss:10.687	translation_Loss:10.01	token_training_loss:0.0	distillation_Loss:0.677                                                   	MRR:14.93	Hits@10:26.43	Best:14.93
2025-01-06 23:33:40,119: Snapshot:3	Epoch:1	Loss:3.374	translation_Loss:2.849	token_training_loss:0.0	distillation_Loss:0.525                                                   	MRR:18.41	Hits@10:32.78	Best:18.41
2025-01-06 23:33:48,740: Snapshot:3	Epoch:2	Loss:1.851	translation_Loss:1.55	token_training_loss:0.0	distillation_Loss:0.301                                                   	MRR:19.91	Hits@10:35.69	Best:19.91
2025-01-06 23:33:57,703: Snapshot:3	Epoch:3	Loss:1.402	translation_Loss:1.164	token_training_loss:0.0	distillation_Loss:0.237                                                   	MRR:20.55	Hits@10:36.93	Best:20.55
2025-01-06 23:34:06,318: Snapshot:3	Epoch:4	Loss:1.214	translation_Loss:1.0	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:21.11	Hits@10:37.75	Best:21.11
2025-01-06 23:34:15,231: Snapshot:3	Epoch:5	Loss:1.118	translation_Loss:0.912	token_training_loss:0.0	distillation_Loss:0.206                                                   	MRR:21.52	Hits@10:38.81	Best:21.52
2025-01-06 23:34:23,858: Snapshot:3	Epoch:6	Loss:1.048	translation_Loss:0.848	token_training_loss:0.0	distillation_Loss:0.2                                                   	MRR:21.73	Hits@10:39.16	Best:21.73
2025-01-06 23:34:32,781: Snapshot:3	Epoch:7	Loss:0.993	translation_Loss:0.796	token_training_loss:0.0	distillation_Loss:0.197                                                   	MRR:21.73	Hits@10:39.37	Best:21.73
2025-01-06 23:34:41,347: Snapshot:3	Epoch:8	Loss:0.964	translation_Loss:0.766	token_training_loss:0.0	distillation_Loss:0.198                                                   	MRR:22.04	Hits@10:39.6	Best:22.04
2025-01-06 23:34:50,264: Snapshot:3	Epoch:9	Loss:0.935	translation_Loss:0.739	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:22.12	Hits@10:39.76	Best:22.12
2025-01-06 23:34:58,854: Snapshot:3	Epoch:10	Loss:0.917	translation_Loss:0.723	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:22.07	Hits@10:39.96	Best:22.12
2025-01-06 23:35:07,874: Snapshot:3	Epoch:11	Loss:0.896	translation_Loss:0.705	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:22.22	Hits@10:40.21	Best:22.22
2025-01-06 23:35:16,494: Snapshot:3	Epoch:12	Loss:0.892	translation_Loss:0.697	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:22.39	Hits@10:40.31	Best:22.39
2025-01-06 23:35:25,396: Snapshot:3	Epoch:13	Loss:0.875	translation_Loss:0.683	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:22.35	Hits@10:40.55	Best:22.39
2025-01-06 23:35:33,996: Snapshot:3	Epoch:14	Loss:0.86	translation_Loss:0.666	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:22.39	Hits@10:40.69	Best:22.39
2025-01-06 23:35:43,032: Snapshot:3	Epoch:15	Loss:0.844	translation_Loss:0.652	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:22.55	Hits@10:40.86	Best:22.55
2025-01-06 23:35:51,641: Snapshot:3	Epoch:16	Loss:0.852	translation_Loss:0.659	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:22.57	Hits@10:40.51	Best:22.57
2025-01-06 23:36:00,539: Snapshot:3	Epoch:17	Loss:0.833	translation_Loss:0.64	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:22.55	Hits@10:40.7	Best:22.57
2025-01-06 23:36:09,040: Snapshot:3	Epoch:18	Loss:0.839	translation_Loss:0.645	token_training_loss:0.0	distillation_Loss:0.194                                                   	MRR:22.49	Hits@10:40.62	Best:22.57
2025-01-06 23:36:17,991: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 22.57
2025-01-06 23:36:17,991: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:0.824 MRR:22.44 Best Results: 22.57
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:36:17,991: Snapshot:3	Epoch:19	Loss:0.824	translation_Loss:0.63	token_training_loss:0.0	distillation_Loss:0.194                                                   	MRR:22.44	Hits@10:40.57	Best:22.57
2025-01-06 23:36:26,566: Snapshot:3	Epoch:20	Loss:25.516	translation_Loss:14.242	token_training_loss:11.274	distillation_Loss:0.0                                                   	MRR:22.44	Hits@10:40.57	Best:22.57
2025-01-06 23:36:35,513: End of token training: 3 Epoch: 21 Loss:14.506 MRR:22.44 Best Results: 22.57
2025-01-06 23:36:35,513: Snapshot:3	Epoch:21	Loss:14.506	translation_Loss:14.267	token_training_loss:0.239	distillation_Loss:0.0                                                           	MRR:22.44	Hits@10:40.57	Best:22.57
2025-01-06 23:36:35,762: => loading checkpoint './checkpoint/ENTITY/3model_best.tar'
2025-01-06 23:36:46,718: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3365 | 0.2153 | 0.3925 | 0.4659 |  0.5676 |
|     1      | 0.256  | 0.1555 | 0.293  | 0.3574 |  0.4487 |
|     2      | 0.2384 | 0.1431 | 0.2754 | 0.3363 |  0.4173 |
|     3      | 0.2262 | 0.1313 | 0.2646 | 0.3259 |  0.4079 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,326,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,422,600
Trainable params: 1,200
Non-trainable params: 2,421,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:36:59,646: Snapshot:4	Epoch:0	Loss:7.977	translation_Loss:7.534	token_training_loss:0.0	distillation_Loss:0.442                                                   	MRR:17.03	Hits@10:32.31	Best:17.03
2025-01-06 23:37:05,890: Snapshot:4	Epoch:1	Loss:2.473	translation_Loss:1.998	token_training_loss:0.0	distillation_Loss:0.476                                                   	MRR:22.86	Hits@10:41.98	Best:22.86
2025-01-06 23:37:12,490: Snapshot:4	Epoch:2	Loss:1.065	translation_Loss:0.783	token_training_loss:0.0	distillation_Loss:0.282                                                   	MRR:24.41	Hits@10:44.04	Best:24.41
2025-01-06 23:37:18,740: Snapshot:4	Epoch:3	Loss:0.679	translation_Loss:0.507	token_training_loss:0.0	distillation_Loss:0.172                                                   	MRR:25.13	Hits@10:44.67	Best:25.13
2025-01-06 23:37:25,070: Snapshot:4	Epoch:4	Loss:0.539	translation_Loss:0.404	token_training_loss:0.0	distillation_Loss:0.134                                                   	MRR:25.63	Hits@10:45.65	Best:25.63
2025-01-06 23:37:31,703: Snapshot:4	Epoch:5	Loss:0.468	translation_Loss:0.349	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:25.94	Hits@10:46.09	Best:25.94
2025-01-06 23:37:37,939: Snapshot:4	Epoch:6	Loss:0.426	translation_Loss:0.317	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:26.11	Hits@10:46.78	Best:26.11
2025-01-06 23:37:44,220: Snapshot:4	Epoch:7	Loss:0.404	translation_Loss:0.299	token_training_loss:0.0	distillation_Loss:0.105                                                   	MRR:26.36	Hits@10:46.91	Best:26.36
2025-01-06 23:37:50,898: Snapshot:4	Epoch:8	Loss:0.383	translation_Loss:0.28	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:26.38	Hits@10:47.21	Best:26.38
2025-01-06 23:37:57,210: Snapshot:4	Epoch:9	Loss:0.369	translation_Loss:0.267	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:26.74	Hits@10:47.55	Best:26.74
2025-01-06 23:38:03,889: Snapshot:4	Epoch:10	Loss:0.354	translation_Loss:0.255	token_training_loss:0.0	distillation_Loss:0.099                                                   	MRR:26.85	Hits@10:47.91	Best:26.85
2025-01-06 23:38:10,076: Snapshot:4	Epoch:11	Loss:0.346	translation_Loss:0.248	token_training_loss:0.0	distillation_Loss:0.099                                                   	MRR:26.83	Hits@10:48.01	Best:26.85
2025-01-06 23:38:16,329: Snapshot:4	Epoch:12	Loss:0.34	translation_Loss:0.243	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:26.88	Hits@10:47.95	Best:26.88
2025-01-06 23:38:22,603: Snapshot:4	Epoch:13	Loss:0.335	translation_Loss:0.238	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:26.96	Hits@10:48.09	Best:26.96
2025-01-06 23:38:29,228: Snapshot:4	Epoch:14	Loss:0.324	translation_Loss:0.227	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:27.08	Hits@10:48.03	Best:27.08
2025-01-06 23:38:35,439: Snapshot:4	Epoch:15	Loss:0.32	translation_Loss:0.224	token_training_loss:0.0	distillation_Loss:0.096                                                   	MRR:26.98	Hits@10:48.15	Best:27.08
2025-01-06 23:38:42,040: Snapshot:4	Epoch:16	Loss:0.322	translation_Loss:0.224	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:27.23	Hits@10:48.09	Best:27.23
2025-01-06 23:38:48,235: Snapshot:4	Epoch:17	Loss:0.315	translation_Loss:0.219	token_training_loss:0.0	distillation_Loss:0.096                                                   	MRR:27.22	Hits@10:48.3	Best:27.23
2025-01-06 23:38:54,479: Snapshot:4	Epoch:18	Loss:0.317	translation_Loss:0.22	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:27.37	Hits@10:48.8	Best:27.37
2025-01-06 23:39:01,058: Snapshot:4	Epoch:19	Loss:0.309	translation_Loss:0.213	token_training_loss:0.0	distillation_Loss:0.096                                                   	MRR:27.25	Hits@10:48.33	Best:27.37
2025-01-06 23:39:07,251: Snapshot:4	Epoch:20	Loss:0.305	translation_Loss:0.209	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:27.25	Hits@10:48.46	Best:27.37
2025-01-06 23:39:13,432: Early Stopping! Snapshot: 4 Epoch: 21 Best Results: 27.37
2025-01-06 23:39:13,432: Start to training tokens! Snapshot: 4 Epoch: 21 Loss:0.31 MRR:27.22 Best Results: 27.37
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:39:13,432: Snapshot:4	Epoch:21	Loss:0.31	translation_Loss:0.213	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:27.22	Hits@10:48.54	Best:27.37
2025-01-06 23:39:19,864: Snapshot:4	Epoch:22	Loss:19.044	translation_Loss:7.774	token_training_loss:11.27	distillation_Loss:0.0                                                   	MRR:27.22	Hits@10:48.54	Best:27.37
2025-01-06 23:39:26,004: End of token training: 4 Epoch: 23 Loss:8.204 MRR:27.22 Best Results: 27.37
2025-01-06 23:39:26,005: Snapshot:4	Epoch:23	Loss:8.204	translation_Loss:7.78	token_training_loss:0.424	distillation_Loss:0.0                                                           	MRR:27.22	Hits@10:48.54	Best:27.37
2025-01-06 23:39:26,242: => loading checkpoint './checkpoint/ENTITY/4model_best.tar'
2025-01-06 23:39:40,101: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3345 | 0.2141 | 0.3887 | 0.4626 |  0.5664 |
|     1      | 0.2552 | 0.1544 | 0.2919 | 0.3567 |  0.4501 |
|     2      | 0.2389 | 0.1433 | 0.2757 | 0.3376 |  0.4177 |
|     3      | 0.2272 | 0.1308 | 0.2662 | 0.3278 |  0.412  |
|     4      | 0.2721 | 0.1585 | 0.3273 | 0.4016 |  0.4879 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,004,200
Trainable params: 1,200
Non-trainable params: 3,003,000
=================================================================
2025-01-06 23:39:40,103: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3414 | 0.2217 | 0.3949 | 0.4683 |  0.5699 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3404 | 0.2201 | 0.3952 | 0.4676 |  0.5702 |
|     1      | 0.2571 | 0.1574 | 0.2925 | 0.358  |  0.4472 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3393 | 0.219  | 0.3925 | 0.4668 |  0.569  |
|     1      | 0.2556 | 0.1556 | 0.2913 | 0.3571 |  0.4488 |
|     2      | 0.2367 | 0.1414 | 0.2727 | 0.3339 |  0.4166 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3365 | 0.2153 | 0.3925 | 0.4659 |  0.5676 |
|     1      | 0.256  | 0.1555 | 0.293  | 0.3574 |  0.4487 |
|     2      | 0.2384 | 0.1431 | 0.2754 | 0.3363 |  0.4173 |
|     3      | 0.2262 | 0.1313 | 0.2646 | 0.3259 |  0.4079 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3345 | 0.2141 | 0.3887 | 0.4626 |  0.5664 |
|     1      | 0.2552 | 0.1544 | 0.2919 | 0.3567 |  0.4501 |
|     2      | 0.2389 | 0.1433 | 0.2757 | 0.3376 |  0.4177 |
|     3      | 0.2272 | 0.1308 | 0.2662 | 0.3278 |  0.412  |
|     4      | 0.2721 | 0.1585 | 0.3273 | 0.4016 |  0.4879 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 23:39:40,104: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 99.28312397003174  |   0.341   |    0.222     |    0.395     |      0.57     |
|    1     | 244.33386611938477 |    0.29   |    0.182     |    0.333     |     0.495     |
|    2     | 238.49624872207642 |   0.269   |    0.165     |    0.309     |     0.465     |
|    3     | 196.84684681892395 |   0.257   |    0.156     |    0.298     |      0.45     |
|    4     | 156.43205165863037 |   0.259   |    0.156     |    0.302     |     0.457     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 23:39:40,104: Sum_Training_Time:935.3921372890472
2025-01-06 23:39:40,104: Every_Training_Time:[99.28312397003174, 244.33386611938477, 238.49624872207642, 196.84684681892395, 156.43205165863037]
2025-01-06 23:39:40,104: Forward transfer: 0.047375 Backward transfer: -0.0013999999999999915
