2025-01-07 18:01:10,626: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107180057/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=1, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:01:19,642: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:01:27,331: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:01:35,140: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-07 18:01:42,965: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.19	Best:13.18
2025-01-07 18:01:50,508: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.02	Hits@10:35.49	Best:14.02
2025-01-07 18:01:58,257: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.62	Hits@10:36.7	Best:14.62
2025-01-07 18:02:05,800: Snapshot:0	Epoch:6	Loss:0.349	translation_Loss:0.349	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.96	Hits@10:37.4	Best:14.96
2025-01-07 18:02:13,553: Snapshot:0	Epoch:7	Loss:0.243	translation_Loss:0.243	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.17	Hits@10:37.94	Best:15.17
2025-01-07 18:02:21,087: Snapshot:0	Epoch:8	Loss:0.179	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.34	Hits@10:38.25	Best:15.34
2025-01-07 18:02:28,881: Snapshot:0	Epoch:9	Loss:0.14	translation_Loss:0.14	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.49	Hits@10:38.38	Best:15.49
2025-01-07 18:02:36,681: Snapshot:0	Epoch:10	Loss:0.112	translation_Loss:0.112	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.58	Hits@10:38.8	Best:15.58
2025-01-07 18:02:44,194: Snapshot:0	Epoch:11	Loss:0.093	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.67	Hits@10:39.0	Best:15.67
2025-01-07 18:02:51,947: Snapshot:0	Epoch:12	Loss:0.079	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.69	Hits@10:39.1	Best:15.69
2025-01-07 18:02:59,478: Snapshot:0	Epoch:13	Loss:0.071	translation_Loss:0.071	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.78	Hits@10:39.27	Best:15.78
2025-01-07 18:03:07,208: Snapshot:0	Epoch:14	Loss:0.059	translation_Loss:0.059	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.84	Hits@10:39.2	Best:15.84
2025-01-07 18:03:14,944: Snapshot:0	Epoch:15	Loss:0.051	translation_Loss:0.051	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.85	Hits@10:39.4	Best:15.85
2025-01-07 18:03:22,498: Snapshot:0	Epoch:16	Loss:0.047	translation_Loss:0.047	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.89	Hits@10:39.66	Best:15.89
2025-01-07 18:03:30,269: Snapshot:0	Epoch:17	Loss:0.044	translation_Loss:0.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.95	Hits@10:39.73	Best:15.95
2025-01-07 18:03:37,795: Snapshot:0	Epoch:18	Loss:0.04	translation_Loss:0.04	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.75	Best:15.96
2025-01-07 18:03:45,577: Snapshot:0	Epoch:19	Loss:0.039	translation_Loss:0.039	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.02	Hits@10:39.82	Best:16.02
2025-01-07 18:03:53,076: Snapshot:0	Epoch:20	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.99	Hits@10:39.84	Best:16.02
2025-01-07 18:04:00,828: Snapshot:0	Epoch:21	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:39.91	Best:16.03
2025-01-07 18:04:08,363: Snapshot:0	Epoch:22	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:39.98	Best:16.05
2025-01-07 18:04:16,057: Snapshot:0	Epoch:23	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.17	Best:16.05
2025-01-07 18:04:23,812: Snapshot:0	Epoch:24	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.07	Hits@10:40.24	Best:16.07
2025-01-07 18:04:31,298: Snapshot:0	Epoch:25	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.24	Best:16.07
2025-01-07 18:04:39,049: Snapshot:0	Epoch:26	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.09	Hits@10:40.3	Best:16.09
2025-01-07 18:04:46,576: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.11	Hits@10:40.34	Best:16.11
2025-01-07 18:04:54,439: Snapshot:0	Epoch:28	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.41	Best:16.13
2025-01-07 18:05:02,167: Snapshot:0	Epoch:29	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.55	Best:16.13
2025-01-07 18:05:09,721: Snapshot:0	Epoch:30	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.56	Best:16.13
2025-01-07 18:05:17,478: Snapshot:0	Epoch:31	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.56	Best:16.16
2025-01-07 18:05:25,025: Snapshot:0	Epoch:32	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.51	Best:16.19
2025-01-07 18:05:32,838: Snapshot:0	Epoch:33	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.2	Hits@10:40.58	Best:16.2
2025-01-07 18:05:40,393: Snapshot:0	Epoch:34	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.73	Best:16.21
2025-01-07 18:05:48,173: Snapshot:0	Epoch:35	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.79	Best:16.23
2025-01-07 18:05:55,685: Snapshot:0	Epoch:36	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.88	Best:16.23
2025-01-07 18:06:03,429: Snapshot:0	Epoch:37	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.93	Best:16.23
2025-01-07 18:06:10,920: Early Stopping! Snapshot: 0 Epoch: 38 Best Results: 16.23
2025-01-07 18:06:10,921: Start to training tokens! Snapshot: 0 Epoch: 38 Loss:0.026 MRR:16.19 Best Results: 16.23
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:06:10,921: Snapshot:0	Epoch:38	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.95	Best:16.23
2025-01-07 18:06:19,256: Snapshot:0	Epoch:39	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.95	Best:16.23
2025-01-07 18:06:27,091: End of token training: 0 Epoch: 40 Loss:nan MRR:16.19 Best Results: 16.23
2025-01-07 18:06:27,091: Snapshot:0	Epoch:40	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:16.19	Hits@10:40.95	Best:16.23
2025-01-07 18:06:27,188: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:06:31,177: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1638 | 0.0065 | 0.2929 | 0.3549 |   0.41  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,918,200
Trainable params: 400
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:06:34,878: Snapshot:1	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.03	Best:0.03
2025-01-07 18:06:36,310: Snapshot:1	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.03	Best:0.03
2025-01-07 18:06:37,740: Snapshot:1	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.03	Best:0.03
2025-01-07 18:06:39,174: Early Stopping! Snapshot: 1 Epoch: 3 Best Results: 0.03
2025-01-07 18:06:39,174: Start to training tokens! Snapshot: 1 Epoch: 3 Loss:nan MRR:0.03 Best Results: 0.03
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:06:39,174: Snapshot:1	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.03	Best:0.03
2025-01-07 18:06:40,607: Snapshot:1	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.03	Hits@10:0.03	Best:0.03
2025-01-07 18:06:42,050: End of token training: 1 Epoch: 5 Loss:nan MRR:0.03 Best Results: 0.03
2025-01-07 18:06:42,050: Snapshot:1	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.03	Hits@10:0.03	Best:0.03
2025-01-07 18:06:42,129: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:06:46,810: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,736,800
Trainable params: 400
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:06:50,733: Snapshot:2	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:06:52,269: Snapshot:2	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:06:53,804: Snapshot:2	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:06:55,339: Early Stopping! Snapshot: 2 Epoch: 3 Best Results: 0.03
2025-01-07 18:06:55,339: Start to training tokens! Snapshot: 2 Epoch: 3 Loss:nan MRR:0.03 Best Results: 0.03
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:06:55,339: Snapshot:2	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:06:56,891: Snapshot:2	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:06:58,417: End of token training: 2 Epoch: 5 Loss:nan MRR:0.03 Best Results: 0.03
2025-01-07 18:06:58,417: Snapshot:2	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:06:58,495: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:07:04,264: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
|     2      | 0.0004 |  0.0   |  0.0   | 0.0003 |  0.0003 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,555,600
Trainable params: 400
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:07:08,481: Snapshot:3	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:10,187: Snapshot:3	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:11,903: Snapshot:3	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:13,842: Early Stopping! Snapshot: 3 Epoch: 3 Best Results: 0.03
2025-01-07 18:07:13,842: Start to training tokens! Snapshot: 3 Epoch: 3 Loss:nan MRR:0.03 Best Results: 0.03
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:07:13,842: Snapshot:3	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:15,493: Snapshot:3	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:17,150: End of token training: 3 Epoch: 5 Loss:nan MRR:0.03 Best Results: 0.03
2025-01-07 18:07:17,150: Snapshot:3	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:17,263: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:07:23,821: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
|     2      | 0.0004 |  0.0   |  0.0   | 0.0003 |  0.0003 |
|     3      | 0.0003 |  0.0   |  0.0   |  0.0   |   0.0   |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,374,400
Trainable params: 400
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:07:28,147: Snapshot:4	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:29,941: Snapshot:4	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:31,704: Snapshot:4	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:33,477: Early Stopping! Snapshot: 4 Epoch: 3 Best Results: 0.03
2025-01-07 18:07:33,477: Start to training tokens! Snapshot: 4 Epoch: 3 Loss:nan MRR:0.03 Best Results: 0.03
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:07:33,477: Snapshot:4	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:35,215: Snapshot:4	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:37,189: End of token training: 4 Epoch: 5 Loss:nan MRR:0.03 Best Results: 0.03
2025-01-07 18:07:37,189: Snapshot:4	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.03	Hits@10:0.0	Best:0.03
2025-01-07 18:07:37,267: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:07:45,006: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
|     2      | 0.0004 |  0.0   |  0.0   | 0.0003 |  0.0003 |
|     3      | 0.0003 |  0.0   |  0.0   |  0.0   |   0.0   |
|     4      | 0.0006 |  0.0   | 0.0003 | 0.0003 |  0.0011 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,193,400
Trainable params: 400
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:07:45,008: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1638 | 0.0065 | 0.2929 | 0.3549 |   0.41  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
|     2      | 0.0004 |  0.0   |  0.0   | 0.0003 |  0.0003 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
|     2      | 0.0004 |  0.0   |  0.0   | 0.0003 |  0.0003 |
|     3      | 0.0003 |  0.0   |  0.0   |  0.0   |   0.0   |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0006 |  0.0   |  0.0   | 0.0002 |  0.0004 |
|     1      | 0.0004 |  0.0   |  0.0   |  0.0   |   0.0   |
|     2      | 0.0004 |  0.0   |  0.0   | 0.0003 |  0.0003 |
|     3      | 0.0003 |  0.0   |  0.0   |  0.0   |   0.0   |
|     4      | 0.0006 |  0.0   | 0.0003 | 0.0003 |  0.0011 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:07:45,009: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 316.4644408226013  |   0.164   |    0.006     |    0.293     |      0.41     |
|    1     | 10.052923679351807 |   0.001   |     0.0      |     0.0      |      0.0      |
|    2     | 10.486949443817139 |   0.001   |     0.0      |     0.0      |      0.0      |
|    3     | 11.609436511993408 |   0.001   |     0.0      |     0.0      |      0.0      |
|    4     | 12.071959018707275 |   0.001   |     0.0      |     0.0      |      0.0      |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:07:45,009: Sum_Training_Time:360.68570947647095
2025-01-07 18:07:45,009: Every_Training_Time:[316.4644408226013, 10.052923679351807, 10.486949443817139, 11.609436511993408, 12.071959018707275]
2025-01-07 18:07:45,009: Forward transfer: 0.0012749999999999999 Backward transfer: -0.0408
2025-01-07 18:07:57,407: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107180748/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:08:06,457: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:08:14,045: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:08:21,900: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-07 18:08:29,794: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.19	Best:13.18
2025-01-07 18:08:37,389: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.03	Hits@10:35.5	Best:14.03
2025-01-07 18:08:45,220: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.62	Hits@10:36.69	Best:14.62
2025-01-07 18:08:52,856: Snapshot:0	Epoch:6	Loss:0.347	translation_Loss:0.347	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.99	Hits@10:37.45	Best:14.99
2025-01-07 18:09:00,748: Snapshot:0	Epoch:7	Loss:0.244	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.2	Hits@10:37.96	Best:15.2
2025-01-07 18:09:08,343: Snapshot:0	Epoch:8	Loss:0.179	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.35	Hits@10:38.25	Best:15.35
2025-01-07 18:09:16,163: Snapshot:0	Epoch:9	Loss:0.138	translation_Loss:0.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.46	Hits@10:38.48	Best:15.46
2025-01-07 18:09:24,002: Snapshot:0	Epoch:10	Loss:0.113	translation_Loss:0.113	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.61	Hits@10:38.71	Best:15.61
2025-01-07 18:09:31,600: Snapshot:0	Epoch:11	Loss:0.093	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.7	Hits@10:39.03	Best:15.7
2025-01-07 18:09:39,409: Snapshot:0	Epoch:12	Loss:0.077	translation_Loss:0.077	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.69	Hits@10:38.97	Best:15.7
2025-01-07 18:09:47,000: Snapshot:0	Epoch:13	Loss:0.07	translation_Loss:0.07	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.76	Hits@10:39.24	Best:15.76
2025-01-07 18:09:54,884: Snapshot:0	Epoch:14	Loss:0.061	translation_Loss:0.061	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.78	Hits@10:39.37	Best:15.78
2025-01-07 18:10:02,718: Snapshot:0	Epoch:15	Loss:0.052	translation_Loss:0.052	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.82	Hits@10:39.53	Best:15.82
2025-01-07 18:10:10,430: Snapshot:0	Epoch:16	Loss:0.047	translation_Loss:0.047	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.7	Best:15.92
2025-01-07 18:10:18,254: Snapshot:0	Epoch:17	Loss:0.044	translation_Loss:0.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.79	Best:15.98
2025-01-07 18:10:25,790: Snapshot:0	Epoch:18	Loss:0.041	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.89	Best:15.98
2025-01-07 18:10:33,627: Snapshot:0	Epoch:19	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:39.93	Best:16.05
2025-01-07 18:10:41,190: Snapshot:0	Epoch:20	Loss:0.036	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:40.0	Best:16.05
2025-01-07 18:10:48,989: Snapshot:0	Epoch:21	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.02	Hits@10:40.0	Best:16.05
2025-01-07 18:10:56,532: Early Stopping! Snapshot: 0 Epoch: 22 Best Results: 16.05
2025-01-07 18:10:56,532: Start to training tokens! Snapshot: 0 Epoch: 22 Loss:0.031 MRR:16.0 Best Results: 16.05
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:10:56,532: Snapshot:0	Epoch:22	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:40.06	Best:16.05
2025-01-07 18:11:04,860: Snapshot:0	Epoch:23	Loss:15.681	translation_Loss:5.764	token_training_loss:9.918	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:40.06	Best:16.05
2025-01-07 18:11:12,644: End of token training: 0 Epoch: 24 Loss:6.117 MRR:16.0 Best Results: 16.05
2025-01-07 18:11:12,645: Snapshot:0	Epoch:24	Loss:6.117	translation_Loss:5.765	token_training_loss:0.351	distillation_Loss:0.0                                                           	MRR:16.0	Hits@10:40.06	Best:16.05
2025-01-07 18:11:12,745: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:11:16,867: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1625 | 0.0065 | 0.2936 | 0.3529 |  0.4027 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,918,600
Trainable params: 800
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:11:20,556: Snapshot:1	Epoch:0	Loss:2.663	translation_Loss:2.632	token_training_loss:0.0	distillation_Loss:0.032                                                   	MRR:2.37	Hits@10:6.4	Best:2.37
2025-01-07 18:11:22,111: Snapshot:1	Epoch:1	Loss:1.676	translation_Loss:1.579	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:7.48	Hits@10:19.73	Best:7.48
2025-01-07 18:11:23,666: Snapshot:1	Epoch:2	Loss:0.888	translation_Loss:0.757	token_training_loss:0.0	distillation_Loss:0.131                                                   	MRR:10.8	Hits@10:26.83	Best:10.8
2025-01-07 18:11:25,253: Snapshot:1	Epoch:3	Loss:0.417	translation_Loss:0.271	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:12.64	Hits@10:30.03	Best:12.64
2025-01-07 18:11:26,808: Snapshot:1	Epoch:4	Loss:0.249	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.156                                                   	MRR:13.46	Hits@10:31.34	Best:13.46
2025-01-07 18:11:28,387: Snapshot:1	Epoch:5	Loss:0.196	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.159                                                   	MRR:13.88	Hits@10:32.2	Best:13.88
2025-01-07 18:11:29,926: Snapshot:1	Epoch:6	Loss:0.173	translation_Loss:0.018	token_training_loss:0.0	distillation_Loss:0.155                                                   	MRR:14.04	Hits@10:32.55	Best:14.04
2025-01-07 18:11:31,479: Snapshot:1	Epoch:7	Loss:0.157	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.145                                                   	MRR:14.12	Hits@10:32.85	Best:14.12
2025-01-07 18:11:32,951: Snapshot:1	Epoch:8	Loss:0.141	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.133                                                   	MRR:14.12	Hits@10:32.98	Best:14.12
2025-01-07 18:11:34,733: Snapshot:1	Epoch:9	Loss:0.124	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.118                                                   	MRR:14.15	Hits@10:33.09	Best:14.15
2025-01-07 18:11:36,191: Snapshot:1	Epoch:10	Loss:0.109	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.104                                                   	MRR:14.09	Hits@10:33.2	Best:14.15
2025-01-07 18:11:37,664: Snapshot:1	Epoch:11	Loss:0.095	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:14.04	Hits@10:33.23	Best:14.15
2025-01-07 18:11:39,118: Early Stopping! Snapshot: 1 Epoch: 12 Best Results: 14.15
2025-01-07 18:11:39,119: Start to training tokens! Snapshot: 1 Epoch: 12 Loss:0.082 MRR:13.98 Best Results: 14.15
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:11:39,119: Snapshot:1	Epoch:12	Loss:0.082	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.078                                                   	MRR:13.98	Hits@10:33.15	Best:14.15
2025-01-07 18:11:40,551: Snapshot:1	Epoch:13	Loss:8.215	translation_Loss:1.169	token_training_loss:7.046	distillation_Loss:0.0                                                   	MRR:13.98	Hits@10:33.15	Best:14.15
2025-01-07 18:11:41,984: End of token training: 1 Epoch: 14 Loss:4.056 MRR:13.98 Best Results: 14.15
2025-01-07 18:11:41,984: Snapshot:1	Epoch:14	Loss:4.056	translation_Loss:1.168	token_training_loss:2.888	distillation_Loss:0.0                                                           	MRR:13.98	Hits@10:33.15	Best:14.15
2025-01-07 18:11:42,063: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:11:46,754: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1588 | 0.0068 | 0.2825 | 0.3479 |  0.4036 |
|     1      | 0.1421 | 0.0067 | 0.2581 | 0.2935 |  0.3328 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,737,200
Trainable params: 800
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:11:50,758: Snapshot:2	Epoch:0	Loss:2.588	translation_Loss:2.572	token_training_loss:0.0	distillation_Loss:0.017                                                   	MRR:2.74	Hits@10:7.26	Best:2.74
2025-01-07 18:11:52,452: Snapshot:2	Epoch:1	Loss:1.546	translation_Loss:1.484	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:7.82	Hits@10:19.84	Best:7.82
2025-01-07 18:11:54,173: Snapshot:2	Epoch:2	Loss:0.756	translation_Loss:0.656	token_training_loss:0.0	distillation_Loss:0.101                                                   	MRR:10.86	Hits@10:25.7	Best:10.86
2025-01-07 18:11:55,884: Snapshot:2	Epoch:3	Loss:0.326	translation_Loss:0.204	token_training_loss:0.0	distillation_Loss:0.122                                                   	MRR:12.48	Hits@10:28.01	Best:12.48
2025-01-07 18:11:57,574: Snapshot:2	Epoch:4	Loss:0.199	translation_Loss:0.071	token_training_loss:0.0	distillation_Loss:0.128                                                   	MRR:13.12	Hits@10:28.79	Best:13.12
2025-01-07 18:11:59,470: Snapshot:2	Epoch:5	Loss:0.155	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.124                                                   	MRR:13.31	Hits@10:29.35	Best:13.31
2025-01-07 18:12:01,189: Snapshot:2	Epoch:6	Loss:0.133	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.114                                                   	MRR:13.47	Hits@10:29.73	Best:13.47
2025-01-07 18:12:02,882: Snapshot:2	Epoch:7	Loss:0.115	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:13.58	Hits@10:29.95	Best:13.58
2025-01-07 18:12:04,576: Snapshot:2	Epoch:8	Loss:0.1	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:13.66	Hits@10:30.27	Best:13.66
2025-01-07 18:12:06,530: Snapshot:2	Epoch:9	Loss:0.088	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:13.7	Hits@10:30.56	Best:13.7
2025-01-07 18:12:08,221: Snapshot:2	Epoch:10	Loss:0.079	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:13.73	Hits@10:30.67	Best:13.73
2025-01-07 18:12:09,806: Snapshot:2	Epoch:11	Loss:0.071	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:13.73	Hits@10:30.81	Best:13.73
2025-01-07 18:12:11,505: Snapshot:2	Epoch:12	Loss:0.065	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:13.74	Hits@10:30.86	Best:13.74
2025-01-07 18:12:13,202: Snapshot:2	Epoch:13	Loss:0.059	translation_Loss:0.003	token_training_loss:0.0	distillation_Loss:0.056                                                   	MRR:13.78	Hits@10:30.94	Best:13.78
2025-01-07 18:12:14,926: Snapshot:2	Epoch:14	Loss:0.054	translation_Loss:0.003	token_training_loss:0.0	distillation_Loss:0.051                                                   	MRR:13.79	Hits@10:30.97	Best:13.79
2025-01-07 18:12:16,502: Snapshot:2	Epoch:15	Loss:0.049	translation_Loss:0.002	token_training_loss:0.0	distillation_Loss:0.047                                                   	MRR:13.79	Hits@10:30.97	Best:13.79
2025-01-07 18:12:18,089: Snapshot:2	Epoch:16	Loss:0.045	translation_Loss:0.002	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:13.74	Hits@10:30.94	Best:13.79
2025-01-07 18:12:19,674: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 13.79
2025-01-07 18:12:19,674: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:0.041 MRR:13.76 Best Results: 13.79
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:12:19,675: Snapshot:2	Epoch:17	Loss:0.041	translation_Loss:0.002	token_training_loss:0.0	distillation_Loss:0.039                                                   	MRR:13.76	Hits@10:30.97	Best:13.79
2025-01-07 18:12:21,216: Snapshot:2	Epoch:18	Loss:6.951	translation_Loss:1.158	token_training_loss:5.793	distillation_Loss:0.0                                                   	MRR:13.76	Hits@10:30.97	Best:13.79
2025-01-07 18:12:22,746: End of token training: 2 Epoch: 19 Loss:3.329 MRR:13.76 Best Results: 13.79
2025-01-07 18:12:22,747: Snapshot:2	Epoch:19	Loss:3.329	translation_Loss:1.156	token_training_loss:2.173	distillation_Loss:0.0                                                           	MRR:13.76	Hits@10:30.97	Best:13.79
2025-01-07 18:12:22,864: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:12:28,662: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1535 | 0.0061 | 0.2706 | 0.3381 |  0.3955 |
|     1      | 0.1369 | 0.0059 | 0.2446 | 0.289  |  0.3285 |
|     2      | 0.1394 | 0.0067 | 0.2516 | 0.2801 |  0.3156 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,556,000
Trainable params: 800
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:12:33,150: Snapshot:3	Epoch:0	Loss:2.564	translation_Loss:2.535	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:2.82	Hits@10:7.07	Best:2.82
2025-01-07 18:12:34,953: Snapshot:3	Epoch:1	Loss:1.518	translation_Loss:1.416	token_training_loss:0.0	distillation_Loss:0.101                                                   	MRR:7.59	Hits@10:19.09	Best:7.59
2025-01-07 18:12:36,786: Snapshot:3	Epoch:2	Loss:0.732	translation_Loss:0.585	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:10.68	Hits@10:24.81	Best:10.68
2025-01-07 18:12:38,593: Snapshot:3	Epoch:3	Loss:0.34	translation_Loss:0.178	token_training_loss:0.0	distillation_Loss:0.162                                                   	MRR:12.1	Hits@10:26.91	Best:12.1
2025-01-07 18:12:40,411: Snapshot:3	Epoch:4	Loss:0.222	translation_Loss:0.061	token_training_loss:0.0	distillation_Loss:0.161                                                   	MRR:12.82	Hits@10:28.15	Best:12.82
2025-01-07 18:12:42,245: Snapshot:3	Epoch:5	Loss:0.182	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.151                                                   	MRR:13.04	Hits@10:28.79	Best:13.04
2025-01-07 18:12:44,074: Snapshot:3	Epoch:6	Loss:0.158	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.137                                                   	MRR:13.13	Hits@10:29.25	Best:13.13
2025-01-07 18:12:45,908: Snapshot:3	Epoch:7	Loss:0.138	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.125                                                   	MRR:13.27	Hits@10:29.35	Best:13.27
2025-01-07 18:12:47,721: Snapshot:3	Epoch:8	Loss:0.123	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:13.37	Hits@10:29.52	Best:13.37
2025-01-07 18:12:49,567: Snapshot:3	Epoch:9	Loss:0.109	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:13.47	Hits@10:29.76	Best:13.47
2025-01-07 18:12:51,408: Snapshot:3	Epoch:10	Loss:0.098	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.091                                                   	MRR:13.5	Hits@10:30.0	Best:13.5
2025-01-07 18:12:53,447: Snapshot:3	Epoch:11	Loss:0.086	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:13.55	Hits@10:30.05	Best:13.55
2025-01-07 18:12:55,258: Snapshot:3	Epoch:12	Loss:0.075	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:13.56	Hits@10:30.08	Best:13.56
2025-01-07 18:12:57,072: Snapshot:3	Epoch:13	Loss:0.067	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:13.57	Hits@10:30.08	Best:13.57
2025-01-07 18:12:58,740: Snapshot:3	Epoch:14	Loss:0.059	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.055                                                   	MRR:13.56	Hits@10:30.05	Best:13.57
2025-01-07 18:13:00,457: Snapshot:3	Epoch:15	Loss:0.052	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:13.52	Hits@10:30.16	Best:13.57
2025-01-07 18:13:02,123: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 13.57
2025-01-07 18:13:02,123: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:0.047 MRR:13.48 Best Results: 13.57
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:13:02,124: Snapshot:3	Epoch:16	Loss:0.047	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:13.48	Hits@10:30.16	Best:13.57
2025-01-07 18:13:03,769: Snapshot:3	Epoch:17	Loss:7.69	translation_Loss:1.153	token_training_loss:6.537	distillation_Loss:0.0                                                   	MRR:13.48	Hits@10:30.16	Best:13.57
2025-01-07 18:13:05,450: End of token training: 3 Epoch: 18 Loss:3.634 MRR:13.48 Best Results: 13.57
2025-01-07 18:13:05,450: Snapshot:3	Epoch:18	Loss:3.634	translation_Loss:1.155	token_training_loss:2.479	distillation_Loss:0.0                                                           	MRR:13.48	Hits@10:30.16	Best:13.57
2025-01-07 18:13:05,530: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:13:12,307: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1508 | 0.006  | 0.2641 | 0.3335 |  0.3915 |
|     1      | 0.1339 | 0.0062 | 0.236  | 0.2823 |  0.3255 |
|     2      | 0.1355 | 0.0067 | 0.2444 | 0.2766 |  0.314  |
|     3      | 0.1377 | 0.0099 | 0.2473 | 0.2806 |  0.3167 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,374,800
Trainable params: 800
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:13:16,683: Snapshot:4	Epoch:0	Loss:2.492	translation_Loss:2.464	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:9.81	Hits@10:23.41	Best:9.81
2025-01-07 18:13:18,619: Snapshot:4	Epoch:1	Loss:1.442	translation_Loss:1.34	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:11.43	Hits@10:27.34	Best:11.43
2025-01-07 18:13:20,537: Snapshot:4	Epoch:2	Loss:0.668	translation_Loss:0.517	token_training_loss:0.0	distillation_Loss:0.151                                                   	MRR:12.54	Hits@10:29.22	Best:12.54
2025-01-07 18:13:22,508: Snapshot:4	Epoch:3	Loss:0.315	translation_Loss:0.149	token_training_loss:0.0	distillation_Loss:0.166                                                   	MRR:13.1	Hits@10:29.73	Best:13.1
2025-01-07 18:13:24,435: Snapshot:4	Epoch:4	Loss:0.214	translation_Loss:0.053	token_training_loss:0.0	distillation_Loss:0.161                                                   	MRR:13.29	Hits@10:30.19	Best:13.29
2025-01-07 18:13:26,361: Snapshot:4	Epoch:5	Loss:0.177	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:13.41	Hits@10:30.62	Best:13.41
2025-01-07 18:13:28,297: Snapshot:4	Epoch:6	Loss:0.151	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.128                                                   	MRR:13.6	Hits@10:31.1	Best:13.6
2025-01-07 18:13:30,261: Snapshot:4	Epoch:7	Loss:0.13	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.114                                                   	MRR:13.69	Hits@10:31.37	Best:13.69
2025-01-07 18:13:32,211: Snapshot:4	Epoch:8	Loss:0.114	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:13.78	Hits@10:31.67	Best:13.78
2025-01-07 18:13:34,420: Snapshot:4	Epoch:9	Loss:0.101	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.092                                                   	MRR:13.89	Hits@10:31.94	Best:13.89
2025-01-07 18:13:36,350: Snapshot:4	Epoch:10	Loss:0.089	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:13.94	Hits@10:32.15	Best:13.94
2025-01-07 18:13:38,295: Snapshot:4	Epoch:11	Loss:0.08	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:14.02	Hits@10:32.12	Best:14.02
2025-01-07 18:13:40,237: Snapshot:4	Epoch:12	Loss:0.07	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.066                                                   	MRR:14.05	Hits@10:32.2	Best:14.05
2025-01-07 18:13:42,031: Snapshot:4	Epoch:13	Loss:0.063	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.058                                                   	MRR:14.02	Hits@10:32.26	Best:14.05
2025-01-07 18:13:43,985: Snapshot:4	Epoch:14	Loss:0.055	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.052                                                   	MRR:14.06	Hits@10:32.45	Best:14.06
2025-01-07 18:13:45,899: Snapshot:4	Epoch:15	Loss:0.05	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.046                                                   	MRR:14.13	Hits@10:32.47	Best:14.13
2025-01-07 18:13:47,681: Snapshot:4	Epoch:16	Loss:0.046	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.042                                                   	MRR:14.11	Hits@10:32.5	Best:14.13
2025-01-07 18:13:49,632: Snapshot:4	Epoch:17	Loss:0.041	translation_Loss:0.003	token_training_loss:0.0	distillation_Loss:0.038                                                   	MRR:14.17	Hits@10:32.53	Best:14.17
2025-01-07 18:13:51,575: Snapshot:4	Epoch:18	Loss:0.04	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.035                                                   	MRR:14.22	Hits@10:32.63	Best:14.22
2025-01-07 18:13:53,376: Snapshot:4	Epoch:19	Loss:0.036	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.032                                                   	MRR:14.19	Hits@10:32.61	Best:14.22
2025-01-07 18:13:55,394: Snapshot:4	Epoch:20	Loss:0.035	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.03                                                   	MRR:14.22	Hits@10:32.69	Best:14.22
2025-01-07 18:13:57,194: Early Stopping! Snapshot: 4 Epoch: 21 Best Results: 14.22
2025-01-07 18:13:57,194: Start to training tokens! Snapshot: 4 Epoch: 21 Loss:0.033 MRR:14.19 Best Results: 14.22
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:13:57,194: Snapshot:4	Epoch:21	Loss:0.033	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:14.19	Hits@10:32.77	Best:14.22
2025-01-07 18:13:58,944: Snapshot:4	Epoch:22	Loss:6.748	translation_Loss:1.142	token_training_loss:5.606	distillation_Loss:0.0                                                   	MRR:14.19	Hits@10:32.77	Best:14.22
2025-01-07 18:14:00,692: End of token training: 4 Epoch: 23 Loss:3.098 MRR:14.19 Best Results: 14.22
2025-01-07 18:14:00,692: Snapshot:4	Epoch:23	Loss:3.098	translation_Loss:1.144	token_training_loss:1.954	distillation_Loss:0.0                                                           	MRR:14.19	Hits@10:32.77	Best:14.22
2025-01-07 18:14:00,794: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:14:08,626: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1496 | 0.0058 | 0.261  | 0.3303 |  0.3902 |
|     1      | 0.1319 | 0.0051 | 0.2336 | 0.2801 |  0.3207 |
|     2      | 0.1342 | 0.0054 | 0.2422 | 0.2788 |  0.3129 |
|     3      | 0.1367 | 0.0078 | 0.2441 | 0.2831 |  0.3188 |
|     4      | 0.129  | 0.0081 | 0.2294 | 0.2738 |  0.3178 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,193,800
Trainable params: 800
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:14:08,630: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1625 | 0.0065 | 0.2936 | 0.3529 |  0.4027 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1588 | 0.0068 | 0.2825 | 0.3479 |  0.4036 |
|     1      | 0.1421 | 0.0067 | 0.2581 | 0.2935 |  0.3328 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1535 | 0.0061 | 0.2706 | 0.3381 |  0.3955 |
|     1      | 0.1369 | 0.0059 | 0.2446 | 0.289  |  0.3285 |
|     2      | 0.1394 | 0.0067 | 0.2516 | 0.2801 |  0.3156 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1508 | 0.006  | 0.2641 | 0.3335 |  0.3915 |
|     1      | 0.1339 | 0.0062 | 0.236  | 0.2823 |  0.3255 |
|     2      | 0.1355 | 0.0067 | 0.2444 | 0.2766 |  0.314  |
|     3      | 0.1377 | 0.0099 | 0.2473 | 0.2806 |  0.3167 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1496 | 0.0058 | 0.261  | 0.3303 |  0.3902 |
|     1      | 0.1319 | 0.0051 | 0.2336 | 0.2801 |  0.3207 |
|     2      | 0.1342 | 0.0054 | 0.2422 | 0.2788 |  0.3129 |
|     3      | 0.1367 | 0.0078 | 0.2441 | 0.2831 |  0.3188 |
|     4      | 0.129  | 0.0081 | 0.2294 | 0.2738 |  0.3178 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:14:08,631: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 195.23715901374817 |   0.163   |    0.006     |    0.294     |     0.403     |
|    1     | 24.291060209274292 |   0.156   |    0.007     |    0.279     |     0.393     |
|    2     | 35.07227802276611  |    0.15   |    0.006     |    0.265     |     0.377     |
|    3     |  35.5114324092865  |   0.146   |    0.007     |    0.257     |     0.367     |
|    4     | 47.274988412857056 |   0.143   |    0.006     |    0.252     |     0.361     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:14:08,631: Sum_Training_Time:337.38691806793213
2025-01-07 18:14:08,631: Every_Training_Time:[195.23715901374817, 24.291060209274292, 35.07227802276611, 35.5114324092865, 47.274988412857056]
2025-01-07 18:14:08,631: Forward transfer: 0.058374999999999996 Backward transfer: -0.007324999999999998
2025-01-07 18:14:21,242: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107181412/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=4, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:14:30,224: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:14:37,757: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:14:45,512: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-07 18:14:53,357: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.19	Best:13.18
2025-01-07 18:15:00,873: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.04	Hits@10:35.49	Best:14.04
2025-01-07 18:15:08,685: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.62	Hits@10:36.65	Best:14.62
2025-01-07 18:15:16,213: Snapshot:0	Epoch:6	Loss:0.349	translation_Loss:0.349	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.99	Hits@10:37.39	Best:14.99
2025-01-07 18:15:24,044: Snapshot:0	Epoch:7	Loss:0.244	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.18	Hits@10:37.93	Best:15.18
2025-01-07 18:15:31,555: Snapshot:0	Epoch:8	Loss:0.179	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.37	Hits@10:38.24	Best:15.37
2025-01-07 18:15:39,310: Snapshot:0	Epoch:9	Loss:0.139	translation_Loss:0.139	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.51	Hits@10:38.48	Best:15.51
2025-01-07 18:15:47,047: Snapshot:0	Epoch:10	Loss:0.111	translation_Loss:0.111	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.62	Hits@10:38.76	Best:15.62
2025-01-07 18:15:54,582: Snapshot:0	Epoch:11	Loss:0.093	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.68	Hits@10:39.05	Best:15.68
2025-01-07 18:16:02,318: Snapshot:0	Epoch:12	Loss:0.078	translation_Loss:0.078	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.7	Hits@10:39.11	Best:15.7
2025-01-07 18:16:09,847: Snapshot:0	Epoch:13	Loss:0.069	translation_Loss:0.069	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.76	Hits@10:39.27	Best:15.76
2025-01-07 18:16:17,631: Snapshot:0	Epoch:14	Loss:0.06	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.44	Best:15.79
2025-01-07 18:16:25,392: Snapshot:0	Epoch:15	Loss:0.052	translation_Loss:0.052	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.83	Hits@10:39.57	Best:15.83
2025-01-07 18:16:32,907: Snapshot:0	Epoch:16	Loss:0.047	translation_Loss:0.047	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.88	Hits@10:39.61	Best:15.88
2025-01-07 18:16:40,657: Snapshot:0	Epoch:17	Loss:0.044	translation_Loss:0.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.72	Best:15.98
2025-01-07 18:16:48,167: Snapshot:0	Epoch:18	Loss:0.041	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:39.8	Best:16.0
2025-01-07 18:16:55,917: Snapshot:0	Epoch:19	Loss:0.038	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.02	Hits@10:39.83	Best:16.02
2025-01-07 18:17:03,381: Snapshot:0	Epoch:20	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.02	Hits@10:39.85	Best:16.02
2025-01-07 18:17:11,105: Snapshot:0	Epoch:21	Loss:0.035	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.08	Best:16.05
2025-01-07 18:17:18,611: Snapshot:0	Epoch:22	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.1	Hits@10:40.18	Best:16.1
2025-01-07 18:17:26,377: Snapshot:0	Epoch:23	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.07	Hits@10:40.26	Best:16.1
2025-01-07 18:17:34,113: Snapshot:0	Epoch:24	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.39	Best:16.12
2025-01-07 18:17:41,623: Snapshot:0	Epoch:25	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.37	Best:16.13
2025-01-07 18:17:49,365: Snapshot:0	Epoch:26	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.43	Best:16.18
2025-01-07 18:17:56,834: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.48	Best:16.18
2025-01-07 18:18:04,556: Snapshot:0	Epoch:28	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.2	Hits@10:40.51	Best:16.2
2025-01-07 18:18:12,304: Snapshot:0	Epoch:29	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.51	Best:16.21
2025-01-07 18:18:19,823: Snapshot:0	Epoch:30	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.26	Hits@10:40.54	Best:16.26
2025-01-07 18:18:27,536: Snapshot:0	Epoch:31	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.54	Best:16.26
2025-01-07 18:18:35,005: Snapshot:0	Epoch:32	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.56	Best:16.26
2025-01-07 18:18:42,720: Early Stopping! Snapshot: 0 Epoch: 33 Best Results: 16.26
2025-01-07 18:18:42,721: Start to training tokens! Snapshot: 0 Epoch: 33 Loss:0.027 MRR:16.18 Best Results: 16.26
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:18:42,721: Snapshot:0	Epoch:33	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.52	Best:16.26
2025-01-07 18:18:50,755: Snapshot:0	Epoch:34	Loss:19.133	translation_Loss:5.475	token_training_loss:13.659	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.52	Best:16.26
2025-01-07 18:18:58,515: End of token training: 0 Epoch: 35 Loss:5.845 MRR:16.18 Best Results: 16.26
2025-01-07 18:18:58,515: Snapshot:0	Epoch:35	Loss:5.845	translation_Loss:5.478	token_training_loss:0.368	distillation_Loss:0.0                                                           	MRR:16.18	Hits@10:40.52	Best:16.26
2025-01-07 18:18:58,613: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:19:02,305: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1648 | 0.0072 | 0.2956 | 0.3559 |  0.409  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,919,400
Trainable params: 1,600
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:19:06,002: Snapshot:1	Epoch:0	Loss:2.705	translation_Loss:2.65	token_training_loss:0.0	distillation_Loss:0.055                                                   	MRR:2.53	Hits@10:6.72	Best:2.53
2025-01-07 18:19:07,533: Snapshot:1	Epoch:1	Loss:1.735	translation_Loss:1.609	token_training_loss:0.0	distillation_Loss:0.126                                                   	MRR:7.58	Hits@10:19.84	Best:7.58
2025-01-07 18:19:09,109: Snapshot:1	Epoch:2	Loss:0.946	translation_Loss:0.794	token_training_loss:0.0	distillation_Loss:0.152                                                   	MRR:10.68	Hits@10:26.83	Best:10.68
2025-01-07 18:19:10,649: Snapshot:1	Epoch:3	Loss:0.477	translation_Loss:0.295	token_training_loss:0.0	distillation_Loss:0.182                                                   	MRR:12.62	Hits@10:30.3	Best:12.62
2025-01-07 18:19:12,209: Snapshot:1	Epoch:4	Loss:0.319	translation_Loss:0.11	token_training_loss:0.0	distillation_Loss:0.21                                                   	MRR:13.49	Hits@10:31.77	Best:13.49
2025-01-07 18:19:14,004: Snapshot:1	Epoch:5	Loss:0.26	translation_Loss:0.042	token_training_loss:0.0	distillation_Loss:0.218                                                   	MRR:13.77	Hits@10:32.47	Best:13.77
2025-01-07 18:19:15,584: Snapshot:1	Epoch:6	Loss:0.227	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.206                                                   	MRR:13.89	Hits@10:32.93	Best:13.89
2025-01-07 18:19:17,140: Snapshot:1	Epoch:7	Loss:0.196	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.183                                                   	MRR:13.92	Hits@10:33.09	Best:13.92
2025-01-07 18:19:18,618: Snapshot:1	Epoch:8	Loss:0.165	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.156                                                   	MRR:13.88	Hits@10:33.23	Best:13.92
2025-01-07 18:19:20,100: Snapshot:1	Epoch:9	Loss:0.136	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.128                                                   	MRR:13.83	Hits@10:33.25	Best:13.92
2025-01-07 18:19:21,574: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.92
2025-01-07 18:19:21,575: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.112 MRR:13.77 Best Results: 13.92
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:19:21,575: Snapshot:1	Epoch:10	Loss:0.112	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:13.77	Hits@10:33.41	Best:13.92
2025-01-07 18:19:23,043: Snapshot:1	Epoch:11	Loss:8.124	translation_Loss:1.163	token_training_loss:6.961	distillation_Loss:0.0                                                   	MRR:13.77	Hits@10:33.41	Best:13.92
2025-01-07 18:19:24,489: End of token training: 1 Epoch: 12 Loss:5.14 MRR:13.77 Best Results: 13.92
2025-01-07 18:19:24,489: Snapshot:1	Epoch:12	Loss:5.14	translation_Loss:1.165	token_training_loss:3.975	distillation_Loss:0.0                                                           	MRR:13.77	Hits@10:33.41	Best:13.92
2025-01-07 18:19:24,570: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:19:29,227: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.008  | 0.2903 | 0.3575 |  0.4123 |
|     1      | 0.1401 | 0.0054 | 0.2527 | 0.2895 |  0.3336 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,738,000
Trainable params: 1,600
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:19:33,182: Snapshot:2	Epoch:0	Loss:2.623	translation_Loss:2.592	token_training_loss:0.0	distillation_Loss:0.03                                                   	MRR:2.98	Hits@10:8.06	Best:2.98
2025-01-07 18:19:34,889: Snapshot:2	Epoch:1	Loss:1.611	translation_Loss:1.513	token_training_loss:0.0	distillation_Loss:0.098                                                   	MRR:7.78	Hits@10:19.95	Best:7.78
2025-01-07 18:19:36,587: Snapshot:2	Epoch:2	Loss:0.818	translation_Loss:0.683	token_training_loss:0.0	distillation_Loss:0.135                                                   	MRR:10.83	Hits@10:26.16	Best:10.83
2025-01-07 18:19:38,280: Snapshot:2	Epoch:3	Loss:0.369	translation_Loss:0.22	token_training_loss:0.0	distillation_Loss:0.149                                                   	MRR:12.38	Hits@10:28.33	Best:12.38
2025-01-07 18:19:39,979: Snapshot:2	Epoch:4	Loss:0.229	translation_Loss:0.075	token_training_loss:0.0	distillation_Loss:0.153                                                   	MRR:13.19	Hits@10:29.76	Best:13.19
2025-01-07 18:19:41,682: Snapshot:2	Epoch:5	Loss:0.184	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.15                                                   	MRR:13.56	Hits@10:30.38	Best:13.56
2025-01-07 18:19:43,429: Snapshot:2	Epoch:6	Loss:0.161	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.142                                                   	MRR:13.78	Hits@10:30.73	Best:13.78
2025-01-07 18:19:45,144: Snapshot:2	Epoch:7	Loss:0.143	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.131                                                   	MRR:13.79	Hits@10:31.24	Best:13.79
2025-01-07 18:19:47,087: Snapshot:2	Epoch:8	Loss:0.128	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.81	Hits@10:31.61	Best:13.81
2025-01-07 18:19:48,680: Snapshot:2	Epoch:9	Loss:0.112	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:13.81	Hits@10:31.88	Best:13.81
2025-01-07 18:19:50,315: Snapshot:2	Epoch:10	Loss:0.099	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.094                                                   	MRR:13.77	Hits@10:31.99	Best:13.81
2025-01-07 18:19:51,892: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 13.81
2025-01-07 18:19:51,892: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:0.086 MRR:13.77 Best Results: 13.81
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:19:51,893: Snapshot:2	Epoch:11	Loss:0.086	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:13.77	Hits@10:31.96	Best:13.81
2025-01-07 18:19:53,427: Snapshot:2	Epoch:12	Loss:7.916	translation_Loss:1.135	token_training_loss:6.781	distillation_Loss:0.0                                                   	MRR:13.77	Hits@10:31.96	Best:13.81
2025-01-07 18:19:54,962: End of token training: 2 Epoch: 13 Loss:4.929 MRR:13.77 Best Results: 13.81
2025-01-07 18:19:54,962: Snapshot:2	Epoch:13	Loss:4.929	translation_Loss:1.136	token_training_loss:3.794	distillation_Loss:0.0                                                           	MRR:13.77	Hits@10:31.96	Best:13.81
2025-01-07 18:19:55,060: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:20:00,669: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.159  | 0.0069 | 0.2779 | 0.3492 |  0.4102 |
|     1      | 0.1367 | 0.0078 | 0.2414 | 0.2788 |  0.3296 |
|     2      | 0.1403 | 0.0073 | 0.254  | 0.2887 |  0.3199 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,556,800
Trainable params: 1,600
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:20:05,130: Snapshot:3	Epoch:0	Loss:2.603	translation_Loss:2.549	token_training_loss:0.0	distillation_Loss:0.053                                                   	MRR:3.06	Hits@10:7.61	Best:3.06
2025-01-07 18:20:07,031: Snapshot:3	Epoch:1	Loss:1.586	translation_Loss:1.443	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:7.5	Hits@10:18.9	Best:7.5
2025-01-07 18:20:08,931: Snapshot:3	Epoch:2	Loss:0.79	translation_Loss:0.614	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:10.67	Hits@10:25.13	Best:10.67
2025-01-07 18:20:10,966: Snapshot:3	Epoch:3	Loss:0.383	translation_Loss:0.194	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:12.22	Hits@10:27.66	Best:12.22
2025-01-07 18:20:12,819: Snapshot:3	Epoch:4	Loss:0.267	translation_Loss:0.07	token_training_loss:0.0	distillation_Loss:0.198                                                   	MRR:12.98	Hits@10:29.01	Best:12.98
2025-01-07 18:20:14,637: Snapshot:3	Epoch:5	Loss:0.229	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:13.2	Hits@10:29.46	Best:13.2
2025-01-07 18:20:16,462: Snapshot:3	Epoch:6	Loss:0.203	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:13.33	Hits@10:29.65	Best:13.33
2025-01-07 18:20:18,296: Snapshot:3	Epoch:7	Loss:0.177	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.162                                                   	MRR:13.52	Hits@10:30.08	Best:13.52
2025-01-07 18:20:20,109: Snapshot:3	Epoch:8	Loss:0.151	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.141                                                   	MRR:13.59	Hits@10:30.27	Best:13.59
2025-01-07 18:20:21,796: Snapshot:3	Epoch:9	Loss:0.127	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.58	Hits@10:30.43	Best:13.59
2025-01-07 18:20:23,489: Snapshot:3	Epoch:10	Loss:0.105	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.099                                                   	MRR:13.55	Hits@10:30.48	Best:13.59
2025-01-07 18:20:25,309: Snapshot:3	Epoch:11	Loss:0.088	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:13.6	Hits@10:30.59	Best:13.6
2025-01-07 18:20:26,988: Snapshot:3	Epoch:12	Loss:0.075	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:13.58	Hits@10:30.62	Best:13.6
2025-01-07 18:20:28,683: Snapshot:3	Epoch:13	Loss:0.064	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.057                                                   	MRR:13.53	Hits@10:30.59	Best:13.6
2025-01-07 18:20:30,602: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 13.6
2025-01-07 18:20:30,602: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:0.057 MRR:13.51 Best Results: 13.6
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:20:30,603: Snapshot:3	Epoch:14	Loss:0.057	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:13.51	Hits@10:30.65	Best:13.6
2025-01-07 18:20:32,243: Snapshot:3	Epoch:15	Loss:7.871	translation_Loss:1.138	token_training_loss:6.734	distillation_Loss:0.0                                                   	MRR:13.51	Hits@10:30.65	Best:13.6
2025-01-07 18:20:33,879: End of token training: 3 Epoch: 16 Loss:4.847 MRR:13.51 Best Results: 13.6
2025-01-07 18:20:33,880: Snapshot:3	Epoch:16	Loss:4.847	translation_Loss:1.136	token_training_loss:3.711	distillation_Loss:0.0                                                           	MRR:13.51	Hits@10:30.65	Best:13.6
2025-01-07 18:20:33,960: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:20:40,727: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1579 | 0.0079 | 0.2727 | 0.3462 |  0.4077 |
|     1      | 0.1347 | 0.0062 | 0.2371 | 0.279  |  0.3288 |
|     2      | 0.1382 | 0.0078 | 0.247  | 0.2831 |  0.3199 |
|     3      | 0.1362 | 0.0086 | 0.2395 | 0.2833 |  0.3167 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,375,600
Trainable params: 1,600
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:20:44,896: Snapshot:4	Epoch:0	Loss:2.541	translation_Loss:2.489	token_training_loss:0.0	distillation_Loss:0.052                                                   	MRR:10.01	Hits@10:23.98	Best:10.01
2025-01-07 18:20:46,849: Snapshot:4	Epoch:1	Loss:1.513	translation_Loss:1.366	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:11.65	Hits@10:28.06	Best:11.65
2025-01-07 18:20:49,026: Snapshot:4	Epoch:2	Loss:0.732	translation_Loss:0.549	token_training_loss:0.0	distillation_Loss:0.183                                                   	MRR:12.76	Hits@10:30.24	Best:12.76
2025-01-07 18:20:50,975: Snapshot:4	Epoch:3	Loss:0.343	translation_Loss:0.153	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:13.3	Hits@10:31.02	Best:13.3
2025-01-07 18:20:52,932: Snapshot:4	Epoch:4	Loss:0.246	translation_Loss:0.058	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:13.62	Hits@10:31.48	Best:13.62
2025-01-07 18:20:54,878: Snapshot:4	Epoch:5	Loss:0.212	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:13.8	Hits@10:31.83	Best:13.8
2025-01-07 18:20:56,847: Snapshot:4	Epoch:6	Loss:0.186	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.165                                                   	MRR:13.88	Hits@10:31.99	Best:13.88
2025-01-07 18:20:58,755: Snapshot:4	Epoch:7	Loss:0.162	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:13.95	Hits@10:32.23	Best:13.95
2025-01-07 18:21:00,716: Snapshot:4	Epoch:8	Loss:0.14	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.129                                                   	MRR:14.01	Hits@10:32.34	Best:14.01
2025-01-07 18:21:02,511: Snapshot:4	Epoch:9	Loss:0.118	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.11                                                   	MRR:14.01	Hits@10:32.5	Best:14.01
2025-01-07 18:21:04,455: Snapshot:4	Epoch:10	Loss:0.099	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.093                                                   	MRR:14.1	Hits@10:32.88	Best:14.1
2025-01-07 18:21:06,413: Snapshot:4	Epoch:11	Loss:0.085	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.078                                                   	MRR:14.15	Hits@10:33.09	Best:14.15
2025-01-07 18:21:08,364: Snapshot:4	Epoch:12	Loss:0.072	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.066                                                   	MRR:14.23	Hits@10:33.23	Best:14.23
2025-01-07 18:21:10,184: Snapshot:4	Epoch:13	Loss:0.063	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.056                                                   	MRR:14.2	Hits@10:33.23	Best:14.23
2025-01-07 18:21:12,274: Snapshot:4	Epoch:14	Loss:0.055	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.048                                                   	MRR:14.22	Hits@10:33.15	Best:14.23
2025-01-07 18:21:14,071: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 14.23
2025-01-07 18:21:14,071: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:0.049 MRR:14.23 Best Results: 14.23
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:21:14,072: Snapshot:4	Epoch:15	Loss:0.049	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:14.23	Hits@10:33.06	Best:14.23
2025-01-07 18:21:15,813: Snapshot:4	Epoch:16	Loss:7.668	translation_Loss:1.136	token_training_loss:6.532	distillation_Loss:0.0                                                   	MRR:14.23	Hits@10:33.06	Best:14.23
2025-01-07 18:21:17,558: End of token training: 4 Epoch: 17 Loss:4.705 MRR:14.23 Best Results: 14.23
2025-01-07 18:21:17,559: Snapshot:4	Epoch:17	Loss:4.705	translation_Loss:1.135	token_training_loss:3.57	distillation_Loss:0.0                                                           	MRR:14.23	Hits@10:33.06	Best:14.23
2025-01-07 18:21:17,660: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:21:25,496: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1564 | 0.007  | 0.2716 | 0.3445 |  0.4075 |
|     1      | 0.133  | 0.0046 | 0.2352 | 0.2766 |  0.3239 |
|     2      | 0.137  | 0.0062 | 0.247  | 0.2823 |  0.3204 |
|     3      | 0.1363 | 0.0089 | 0.2425 | 0.2817 |  0.3218 |
|     4      | 0.1327 | 0.0075 | 0.2321 | 0.2851 |  0.328  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,194,600
Trainable params: 1,600
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:21:25,499: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1648 | 0.0072 | 0.2956 | 0.3559 |  0.409  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.008  | 0.2903 | 0.3575 |  0.4123 |
|     1      | 0.1401 | 0.0054 | 0.2527 | 0.2895 |  0.3336 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.159  | 0.0069 | 0.2779 | 0.3492 |  0.4102 |
|     1      | 0.1367 | 0.0078 | 0.2414 | 0.2788 |  0.3296 |
|     2      | 0.1403 | 0.0073 | 0.254  | 0.2887 |  0.3199 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1579 | 0.0079 | 0.2727 | 0.3462 |  0.4077 |
|     1      | 0.1347 | 0.0062 | 0.2371 | 0.279  |  0.3288 |
|     2      | 0.1382 | 0.0078 | 0.247  | 0.2831 |  0.3199 |
|     3      | 0.1362 | 0.0086 | 0.2395 | 0.2833 |  0.3167 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1564 | 0.007  | 0.2716 | 0.3445 |  0.4075 |
|     1      | 0.133  | 0.0046 | 0.2352 | 0.2766 |  0.3239 |
|     2      | 0.137  | 0.0062 | 0.247  | 0.2823 |  0.3204 |
|     3      | 0.1363 | 0.0089 | 0.2425 | 0.2817 |  0.3218 |
|     4      | 0.1327 | 0.0075 | 0.2321 | 0.2851 |  0.328  |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:21:25,500: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 277.27220582962036 |   0.165   |    0.007     |    0.296     |     0.409     |
|    1     | 21.175025463104248 |   0.161   |    0.008     |    0.285     |     0.401     |
|    2     | 24.817758321762085 |   0.154   |    0.007     |     0.27     |     0.389     |
|    3     | 31.752151012420654 |   0.151   |    0.008     |    0.262     |     0.379     |
|    4     | 35.718183517456055 |   0.148   |    0.007     |    0.259     |     0.374     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:21:25,500: Sum_Training_Time:390.7353241443634
2025-01-07 18:21:25,500: Every_Training_Time:[277.27220582962036, 21.175025463104248, 24.817758321762085, 31.752151012420654, 35.718183517456055]
2025-01-07 18:21:25,500: Forward transfer: 0.058774999999999994 Backward transfer: -0.004674999999999992
2025-01-07 18:21:37,967: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107182129/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:21:46,945: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:21:54,478: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:22:02,236: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-07 18:22:10,034: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.19	Best:13.18
2025-01-07 18:22:17,537: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.02	Hits@10:35.45	Best:14.02
2025-01-07 18:22:25,278: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.62	Hits@10:36.6	Best:14.62
2025-01-07 18:22:32,791: Snapshot:0	Epoch:6	Loss:0.349	translation_Loss:0.349	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.97	Hits@10:37.37	Best:14.97
2025-01-07 18:22:40,547: Snapshot:0	Epoch:7	Loss:0.245	translation_Loss:0.245	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.18	Hits@10:37.83	Best:15.18
2025-01-07 18:22:48,003: Snapshot:0	Epoch:8	Loss:0.179	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.37	Hits@10:38.19	Best:15.37
2025-01-07 18:22:55,763: Snapshot:0	Epoch:9	Loss:0.138	translation_Loss:0.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.51	Hits@10:38.46	Best:15.51
2025-01-07 18:23:03,491: Snapshot:0	Epoch:10	Loss:0.112	translation_Loss:0.112	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.6	Hits@10:38.71	Best:15.6
2025-01-07 18:23:11,002: Snapshot:0	Epoch:11	Loss:0.092	translation_Loss:0.092	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.69	Hits@10:38.95	Best:15.69
2025-01-07 18:23:18,762: Snapshot:0	Epoch:12	Loss:0.077	translation_Loss:0.077	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.74	Hits@10:39.11	Best:15.74
2025-01-07 18:23:26,302: Snapshot:0	Epoch:13	Loss:0.072	translation_Loss:0.072	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.85	Hits@10:39.21	Best:15.85
2025-01-07 18:23:34,107: Snapshot:0	Epoch:14	Loss:0.06	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.87	Hits@10:39.38	Best:15.87
2025-01-07 18:23:41,806: Snapshot:0	Epoch:15	Loss:0.053	translation_Loss:0.053	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.86	Hits@10:39.47	Best:15.87
2025-01-07 18:23:49,257: Snapshot:0	Epoch:16	Loss:0.048	translation_Loss:0.048	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.86	Hits@10:39.51	Best:15.87
2025-01-07 18:23:57,004: Snapshot:0	Epoch:17	Loss:0.044	translation_Loss:0.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.93	Hits@10:39.57	Best:15.93
2025-01-07 18:24:04,503: Snapshot:0	Epoch:18	Loss:0.041	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.94	Hits@10:39.62	Best:15.94
2025-01-07 18:24:12,197: Snapshot:0	Epoch:19	Loss:0.04	translation_Loss:0.04	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.94	Hits@10:39.7	Best:15.94
2025-01-07 18:24:19,673: Snapshot:0	Epoch:20	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.77	Best:15.94
2025-01-07 18:24:27,433: Snapshot:0	Epoch:21	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.97	Hits@10:39.94	Best:15.97
2025-01-07 18:24:34,976: Snapshot:0	Epoch:22	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:40.09	Best:16.0
2025-01-07 18:24:42,744: Snapshot:0	Epoch:23	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.27	Best:16.05
2025-01-07 18:24:50,493: Snapshot:0	Epoch:24	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.07	Hits@10:40.32	Best:16.07
2025-01-07 18:24:57,955: Snapshot:0	Epoch:25	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.29	Best:16.07
2025-01-07 18:25:05,738: Snapshot:0	Epoch:26	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.08	Hits@10:40.29	Best:16.08
2025-01-07 18:25:13,277: Snapshot:0	Epoch:27	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.47	Best:16.12
2025-01-07 18:25:21,033: Snapshot:0	Epoch:28	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.52	Best:16.15
2025-01-07 18:25:28,876: Snapshot:0	Epoch:29	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.2	Hits@10:40.65	Best:16.2
2025-01-07 18:25:36,409: Snapshot:0	Epoch:30	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.22	Hits@10:40.64	Best:16.22
2025-01-07 18:25:44,134: Snapshot:0	Epoch:31	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.17	Hits@10:40.6	Best:16.22
2025-01-07 18:25:51,618: Snapshot:0	Epoch:32	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.17	Hits@10:40.7	Best:16.22
2025-01-07 18:25:59,323: Early Stopping! Snapshot: 0 Epoch: 33 Best Results: 16.22
2025-01-07 18:25:59,323: Start to training tokens! Snapshot: 0 Epoch: 33 Loss:0.025 MRR:16.19 Best Results: 16.22
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:25:59,324: Snapshot:0	Epoch:33	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.73	Best:16.22
2025-01-07 18:26:07,327: Snapshot:0	Epoch:34	Loss:20.32	translation_Loss:5.461	token_training_loss:14.859	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.73	Best:16.22
2025-01-07 18:26:15,103: End of token training: 0 Epoch: 35 Loss:5.861 MRR:16.19 Best Results: 16.22
2025-01-07 18:26:15,103: Snapshot:0	Epoch:35	Loss:5.861	translation_Loss:5.464	token_training_loss:0.397	distillation_Loss:0.0                                                           	MRR:16.19	Hits@10:40.73	Best:16.22
2025-01-07 18:26:15,201: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:26:19,064: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.0069 | 0.2937 | 0.3543 |  0.4083 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,919,800
Trainable params: 2,000
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:26:22,794: Snapshot:1	Epoch:0	Loss:2.717	translation_Loss:2.651	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:2.51	Hits@10:6.69	Best:2.51
2025-01-07 18:26:24,351: Snapshot:1	Epoch:1	Loss:1.747	translation_Loss:1.613	token_training_loss:0.0	distillation_Loss:0.135                                                   	MRR:7.55	Hits@10:19.54	Best:7.55
2025-01-07 18:26:25,915: Snapshot:1	Epoch:2	Loss:0.964	translation_Loss:0.8	token_training_loss:0.0	distillation_Loss:0.163                                                   	MRR:10.73	Hits@10:26.77	Best:10.73
2025-01-07 18:26:27,469: Snapshot:1	Epoch:3	Loss:0.503	translation_Loss:0.301	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:12.53	Hits@10:30.03	Best:12.53
2025-01-07 18:26:29,036: Snapshot:1	Epoch:4	Loss:0.346	translation_Loss:0.113	token_training_loss:0.0	distillation_Loss:0.233                                                   	MRR:13.35	Hits@10:31.37	Best:13.35
2025-01-07 18:26:30,866: Snapshot:1	Epoch:5	Loss:0.283	translation_Loss:0.045	token_training_loss:0.0	distillation_Loss:0.237                                                   	MRR:13.61	Hits@10:32.31	Best:13.61
2025-01-07 18:26:32,410: Snapshot:1	Epoch:6	Loss:0.242	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.219                                                   	MRR:13.84	Hits@10:32.77	Best:13.84
2025-01-07 18:26:33,987: Snapshot:1	Epoch:7	Loss:0.204	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:13.85	Hits@10:32.88	Best:13.85
2025-01-07 18:26:35,458: Snapshot:1	Epoch:8	Loss:0.168	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.157                                                   	MRR:13.79	Hits@10:33.01	Best:13.85
2025-01-07 18:26:36,930: Snapshot:1	Epoch:9	Loss:0.136	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.125                                                   	MRR:13.74	Hits@10:33.15	Best:13.85
2025-01-07 18:26:38,407: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.85
2025-01-07 18:26:38,407: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.11 MRR:13.72 Best Results: 13.85
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:26:38,408: Snapshot:1	Epoch:10	Loss:0.11	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.099                                                   	MRR:13.72	Hits@10:33.2	Best:13.85
2025-01-07 18:26:39,854: Snapshot:1	Epoch:11	Loss:8.244	translation_Loss:1.177	token_training_loss:7.067	distillation_Loss:0.0                                                   	MRR:13.72	Hits@10:33.2	Best:13.85
2025-01-07 18:26:41,275: End of token training: 1 Epoch: 12 Loss:5.46 MRR:13.72 Best Results: 13.85
2025-01-07 18:26:41,275: Snapshot:1	Epoch:12	Loss:5.46	translation_Loss:1.178	token_training_loss:4.281	distillation_Loss:0.0                                                           	MRR:13.72	Hits@10:33.2	Best:13.85
2025-01-07 18:26:41,354: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:26:46,021: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1641 | 0.0079 | 0.2902 | 0.3576 |  0.4147 |
|     1      | 0.1404 | 0.0067 | 0.253  | 0.2914 |  0.3269 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,738,400
Trainable params: 2,000
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:26:49,986: Snapshot:2	Epoch:0	Loss:2.633	translation_Loss:2.596	token_training_loss:0.0	distillation_Loss:0.037                                                   	MRR:2.88	Hits@10:7.77	Best:2.88
2025-01-07 18:26:51,717: Snapshot:2	Epoch:1	Loss:1.628	translation_Loss:1.518	token_training_loss:0.0	distillation_Loss:0.11                                                   	MRR:7.8	Hits@10:19.52	Best:7.8
2025-01-07 18:26:53,465: Snapshot:2	Epoch:2	Loss:0.832	translation_Loss:0.687	token_training_loss:0.0	distillation_Loss:0.145                                                   	MRR:10.71	Hits@10:25.46	Best:10.71
2025-01-07 18:26:55,177: Snapshot:2	Epoch:3	Loss:0.382	translation_Loss:0.223	token_training_loss:0.0	distillation_Loss:0.158                                                   	MRR:12.17	Hits@10:28.15	Best:12.17
2025-01-07 18:26:56,873: Snapshot:2	Epoch:4	Loss:0.242	translation_Loss:0.077	token_training_loss:0.0	distillation_Loss:0.165                                                   	MRR:12.91	Hits@10:29.27	Best:12.91
2025-01-07 18:26:58,580: Snapshot:2	Epoch:5	Loss:0.199	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.165                                                   	MRR:13.31	Hits@10:29.95	Best:13.31
2025-01-07 18:27:00,280: Snapshot:2	Epoch:6	Loss:0.175	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.156                                                   	MRR:13.47	Hits@10:30.43	Best:13.47
2025-01-07 18:27:01,967: Snapshot:2	Epoch:7	Loss:0.155	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:13.58	Hits@10:30.75	Best:13.58
2025-01-07 18:27:03,791: Snapshot:2	Epoch:8	Loss:0.138	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.129                                                   	MRR:13.58	Hits@10:31.05	Best:13.58
2025-01-07 18:27:05,370: Snapshot:2	Epoch:9	Loss:0.119	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:13.52	Hits@10:31.18	Best:13.58
2025-01-07 18:27:06,922: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 13.58
2025-01-07 18:27:06,922: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.103 MRR:13.51 Best Results: 13.58
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:27:06,922: Snapshot:2	Epoch:10	Loss:0.103	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.098                                                   	MRR:13.51	Hits@10:31.16	Best:13.58
2025-01-07 18:27:08,446: Snapshot:2	Epoch:11	Loss:8.117	translation_Loss:1.157	token_training_loss:6.959	distillation_Loss:0.0                                                   	MRR:13.51	Hits@10:31.16	Best:13.58
2025-01-07 18:27:09,977: End of token training: 2 Epoch: 12 Loss:5.293 MRR:13.51 Best Results: 13.58
2025-01-07 18:27:09,977: Snapshot:2	Epoch:12	Loss:5.293	translation_Loss:1.158	token_training_loss:4.136	distillation_Loss:0.0                                                           	MRR:13.51	Hits@10:31.16	Best:13.58
2025-01-07 18:27:10,076: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:27:15,640: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1596 | 0.0073 | 0.2799 | 0.3503 |  0.4106 |
|     1      | 0.137  | 0.0078 | 0.2403 | 0.2836 |  0.3298 |
|     2      | 0.1393 | 0.0075 | 0.2513 | 0.2852 |  0.3185 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,557,200
Trainable params: 2,000
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:27:20,054: Snapshot:3	Epoch:0	Loss:2.613	translation_Loss:2.55	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:3.12	Hits@10:7.66	Best:3.12
2025-01-07 18:27:21,864: Snapshot:3	Epoch:1	Loss:1.604	translation_Loss:1.448	token_training_loss:0.0	distillation_Loss:0.156                                                   	MRR:7.61	Hits@10:18.92	Best:7.61
2025-01-07 18:27:23,680: Snapshot:3	Epoch:2	Loss:0.805	translation_Loss:0.62	token_training_loss:0.0	distillation_Loss:0.185                                                   	MRR:10.7	Hits@10:25.4	Best:10.7
2025-01-07 18:27:25,488: Snapshot:3	Epoch:3	Loss:0.398	translation_Loss:0.195	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:12.16	Hits@10:27.77	Best:12.16
2025-01-07 18:27:27,310: Snapshot:3	Epoch:4	Loss:0.29	translation_Loss:0.073	token_training_loss:0.0	distillation_Loss:0.217                                                   	MRR:12.86	Hits@10:28.82	Best:12.86
2025-01-07 18:27:29,347: Snapshot:3	Epoch:5	Loss:0.25	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:13.2	Hits@10:29.46	Best:13.2
2025-01-07 18:27:31,188: Snapshot:3	Epoch:6	Loss:0.218	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:13.32	Hits@10:29.95	Best:13.32
2025-01-07 18:27:33,016: Snapshot:3	Epoch:7	Loss:0.186	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.172                                                   	MRR:13.41	Hits@10:30.11	Best:13.41
2025-01-07 18:27:34,833: Snapshot:3	Epoch:8	Loss:0.155	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.145                                                   	MRR:13.45	Hits@10:30.38	Best:13.45
2025-01-07 18:27:36,624: Snapshot:3	Epoch:9	Loss:0.128	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.46	Hits@10:30.4	Best:13.46
2025-01-07 18:27:38,314: Snapshot:3	Epoch:10	Loss:0.104	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.096                                                   	MRR:13.43	Hits@10:30.56	Best:13.46
2025-01-07 18:27:40,014: Snapshot:3	Epoch:11	Loss:0.087	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.078                                                   	MRR:13.44	Hits@10:30.65	Best:13.46
2025-01-07 18:27:41,854: Snapshot:3	Epoch:12	Loss:0.073	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:13.51	Hits@10:30.67	Best:13.51
2025-01-07 18:27:43,541: Snapshot:3	Epoch:13	Loss:0.063	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.054                                                   	MRR:13.49	Hits@10:30.78	Best:13.51
2025-01-07 18:27:45,230: Snapshot:3	Epoch:14	Loss:0.057	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.048                                                   	MRR:13.5	Hits@10:30.86	Best:13.51
2025-01-07 18:27:46,921: Early Stopping! Snapshot: 3 Epoch: 15 Best Results: 13.51
2025-01-07 18:27:46,921: Start to training tokens! Snapshot: 3 Epoch: 15 Loss:0.051 MRR:13.49 Best Results: 13.51
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:27:46,921: Snapshot:3	Epoch:15	Loss:0.051	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:13.49	Hits@10:30.91	Best:13.51
2025-01-07 18:27:48,556: Snapshot:3	Epoch:16	Loss:8.147	translation_Loss:1.158	token_training_loss:6.99	distillation_Loss:0.0                                                   	MRR:13.49	Hits@10:30.91	Best:13.51
2025-01-07 18:27:50,420: End of token training: 3 Epoch: 17 Loss:5.31 MRR:13.49 Best Results: 13.51
2025-01-07 18:27:50,421: Snapshot:3	Epoch:17	Loss:5.31	translation_Loss:1.155	token_training_loss:4.155	distillation_Loss:0.0                                                           	MRR:13.49	Hits@10:30.91	Best:13.51
2025-01-07 18:27:50,499: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:27:57,038: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1594 | 0.0082 | 0.2786 | 0.3496 |  0.4092 |
|     1      | 0.1359 | 0.007  | 0.239  | 0.2817 |  0.3266 |
|     2      | 0.1367 | 0.0067 | 0.2487 | 0.2804 |  0.3191 |
|     3      | 0.1368 | 0.0091 | 0.2425 | 0.2823 |  0.3215 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,000
Trainable params: 2,000
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:28:01,389: Snapshot:4	Epoch:0	Loss:2.561	translation_Loss:2.499	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:10.14	Hits@10:24.27	Best:10.14
2025-01-07 18:28:03,308: Snapshot:4	Epoch:1	Loss:1.542	translation_Loss:1.38	token_training_loss:0.0	distillation_Loss:0.162                                                   	MRR:11.68	Hits@10:28.17	Best:11.68
2025-01-07 18:28:05,479: Snapshot:4	Epoch:2	Loss:0.749	translation_Loss:0.557	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:12.71	Hits@10:30.11	Best:12.71
2025-01-07 18:28:07,420: Snapshot:4	Epoch:3	Loss:0.359	translation_Loss:0.158	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:13.38	Hits@10:31.21	Best:13.38
2025-01-07 18:28:09,360: Snapshot:4	Epoch:4	Loss:0.265	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.205                                                   	MRR:13.78	Hits@10:31.56	Best:13.78
2025-01-07 18:28:11,299: Snapshot:4	Epoch:5	Loss:0.228	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.197                                                   	MRR:13.97	Hits@10:31.99	Best:13.97
2025-01-07 18:28:13,251: Snapshot:4	Epoch:6	Loss:0.199	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:14.04	Hits@10:32.39	Best:14.04
2025-01-07 18:28:15,194: Snapshot:4	Epoch:7	Loss:0.172	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.158                                                   	MRR:14.09	Hits@10:32.58	Best:14.09
2025-01-07 18:28:17,120: Snapshot:4	Epoch:8	Loss:0.145	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.135                                                   	MRR:14.19	Hits@10:32.74	Best:14.19
2025-01-07 18:28:19,067: Snapshot:4	Epoch:9	Loss:0.121	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.112                                                   	MRR:14.2	Hits@10:32.98	Best:14.2
2025-01-07 18:28:20,866: Snapshot:4	Epoch:10	Loss:0.101	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.092                                                   	MRR:14.12	Hits@10:32.96	Best:14.2
2025-01-07 18:28:22,668: Snapshot:4	Epoch:11	Loss:0.084	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.076                                                   	MRR:14.15	Hits@10:32.85	Best:14.2
2025-01-07 18:28:24,466: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 14.2
2025-01-07 18:28:24,466: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:0.07 MRR:14.18 Best Results: 14.2
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:28:24,467: Snapshot:4	Epoch:12	Loss:0.07	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:14.18	Hits@10:32.9	Best:14.2
2025-01-07 18:28:26,203: Snapshot:4	Epoch:13	Loss:7.857	translation_Loss:1.139	token_training_loss:6.718	distillation_Loss:0.0                                                   	MRR:14.18	Hits@10:32.9	Best:14.2
2025-01-07 18:28:28,182: End of token training: 4 Epoch: 14 Loss:5.098 MRR:14.18 Best Results: 14.2
2025-01-07 18:28:28,182: Snapshot:4	Epoch:14	Loss:5.098	translation_Loss:1.138	token_training_loss:3.96	distillation_Loss:0.0                                                           	MRR:14.18	Hits@10:32.9	Best:14.2
2025-01-07 18:28:28,281: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:28:36,051: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1578 | 0.0078 | 0.2749 | 0.3453 |  0.4074 |
|     1      | 0.1335 | 0.0059 | 0.2341 | 0.2769 |  0.3277 |
|     2      | 0.136  | 0.0062 | 0.2481 | 0.282  |  0.3191 |
|     3      | 0.1369 | 0.0091 | 0.2438 | 0.2841 |  0.3239 |
|     4      | 0.1359 | 0.0089 | 0.2399 | 0.2896 |  0.3315 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,000
Trainable params: 2,000
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:28:36,054: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.0069 | 0.2937 | 0.3543 |  0.4083 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1641 | 0.0079 | 0.2902 | 0.3576 |  0.4147 |
|     1      | 0.1404 | 0.0067 | 0.253  | 0.2914 |  0.3269 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1596 | 0.0073 | 0.2799 | 0.3503 |  0.4106 |
|     1      | 0.137  | 0.0078 | 0.2403 | 0.2836 |  0.3298 |
|     2      | 0.1393 | 0.0075 | 0.2513 | 0.2852 |  0.3185 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1594 | 0.0082 | 0.2786 | 0.3496 |  0.4092 |
|     1      | 0.1359 | 0.007  | 0.239  | 0.2817 |  0.3266 |
|     2      | 0.1367 | 0.0067 | 0.2487 | 0.2804 |  0.3191 |
|     3      | 0.1368 | 0.0091 | 0.2425 | 0.2823 |  0.3215 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1578 | 0.0078 | 0.2749 | 0.3453 |  0.4074 |
|     1      | 0.1335 | 0.0059 | 0.2341 | 0.2769 |  0.3277 |
|     2      | 0.136  | 0.0062 | 0.2481 | 0.282  |  0.3191 |
|     3      | 0.1369 | 0.0091 | 0.2438 | 0.2841 |  0.3239 |
|     4      | 0.1359 | 0.0089 | 0.2399 | 0.2896 |  0.3315 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:28:36,055: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 277.1354286670685  |   0.164   |    0.007     |    0.294     |     0.408     |
|    1     | 21.20598554611206  |   0.161   |    0.008     |    0.285     |     0.402     |
|    2     | 23.045063734054565 |   0.154   |    0.007     |    0.271     |     0.389     |
|    3     | 33.318886518478394 |   0.152   |    0.008     |    0.267     |      0.38     |
|    4     | 29.847769021987915 |   0.149   |    0.008     |    0.262     |     0.375     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:28:36,055: Sum_Training_Time:384.5531334877014
2025-01-07 18:28:36,055: Every_Training_Time:[277.1354286670685, 21.20598554611206, 23.045063734054565, 33.318886518478394, 29.847769021987915]
2025-01-07 18:28:36,055: Forward transfer: 0.059825 Backward transfer: -0.004049999999999998
2025-01-07 18:28:48,541: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107182840/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=6, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:28:57,645: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:29:05,253: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:29:13,142: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.92	Best:10.92
2025-01-07 18:29:21,085: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.2	Hits@10:33.22	Best:13.2
2025-01-07 18:29:28,749: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.06	Hits@10:35.45	Best:14.06
2025-01-07 18:29:36,607: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.62	Hits@10:36.51	Best:14.62
2025-01-07 18:29:44,211: Snapshot:0	Epoch:6	Loss:0.35	translation_Loss:0.35	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.99	Hits@10:37.51	Best:14.99
2025-01-07 18:29:52,168: Snapshot:0	Epoch:7	Loss:0.245	translation_Loss:0.245	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.23	Hits@10:38.04	Best:15.23
2025-01-07 18:29:59,815: Snapshot:0	Epoch:8	Loss:0.177	translation_Loss:0.177	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.36	Hits@10:38.24	Best:15.36
2025-01-07 18:30:07,800: Snapshot:0	Epoch:9	Loss:0.14	translation_Loss:0.14	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.51	Hits@10:38.54	Best:15.51
2025-01-07 18:30:15,670: Snapshot:0	Epoch:10	Loss:0.111	translation_Loss:0.111	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.61	Hits@10:38.85	Best:15.61
2025-01-07 18:30:23,313: Snapshot:0	Epoch:11	Loss:0.095	translation_Loss:0.095	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.69	Hits@10:39.1	Best:15.69
2025-01-07 18:30:31,167: Snapshot:0	Epoch:12	Loss:0.079	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.76	Hits@10:39.12	Best:15.76
2025-01-07 18:30:38,797: Snapshot:0	Epoch:13	Loss:0.068	translation_Loss:0.068	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.26	Best:15.79
2025-01-07 18:30:46,662: Snapshot:0	Epoch:14	Loss:0.059	translation_Loss:0.059	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.44	Best:15.79
2025-01-07 18:30:54,548: Snapshot:0	Epoch:15	Loss:0.052	translation_Loss:0.052	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.85	Hits@10:39.48	Best:15.85
2025-01-07 18:31:02,183: Snapshot:0	Epoch:16	Loss:0.049	translation_Loss:0.049	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.9	Hits@10:39.61	Best:15.9
2025-01-07 18:31:10,027: Snapshot:0	Epoch:17	Loss:0.043	translation_Loss:0.043	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.95	Hits@10:39.75	Best:15.95
2025-01-07 18:31:17,664: Snapshot:0	Epoch:18	Loss:0.041	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.82	Best:15.96
2025-01-07 18:31:25,522: Snapshot:0	Epoch:19	Loss:0.038	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:39.92	Best:16.0
2025-01-07 18:31:33,154: Snapshot:0	Epoch:20	Loss:0.038	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:39.88	Best:16.03
2025-01-07 18:31:40,941: Snapshot:0	Epoch:21	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:40.13	Best:16.03
2025-01-07 18:31:48,505: Snapshot:0	Epoch:22	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:40.17	Best:16.04
2025-01-07 18:31:56,417: Snapshot:0	Epoch:23	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.22	Best:16.05
2025-01-07 18:32:04,272: Snapshot:0	Epoch:24	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.27	Best:16.06
2025-01-07 18:32:11,867: Snapshot:0	Epoch:25	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.3	Best:16.06
2025-01-07 18:32:19,738: Snapshot:0	Epoch:26	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.33	Best:16.13
2025-01-07 18:32:27,314: Snapshot:0	Epoch:27	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.35	Best:16.13
2025-01-07 18:32:35,114: Snapshot:0	Epoch:28	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.08	Hits@10:40.4	Best:16.13
2025-01-07 18:32:42,887: Early Stopping! Snapshot: 0 Epoch: 29 Best Results: 16.13
2025-01-07 18:32:42,888: Start to training tokens! Snapshot: 0 Epoch: 29 Loss:0.027 MRR:16.09 Best Results: 16.13
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:32:42,888: Snapshot:0	Epoch:29	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.09	Hits@10:40.42	Best:16.13
2025-01-07 18:32:51,016: Snapshot:0	Epoch:30	Loss:21.227	translation_Loss:5.588	token_training_loss:15.639	distillation_Loss:0.0                                                   	MRR:16.09	Hits@10:40.42	Best:16.13
2025-01-07 18:32:58,954: End of token training: 0 Epoch: 31 Loss:6.044 MRR:16.09 Best Results: 16.13
2025-01-07 18:32:58,954: Snapshot:0	Epoch:31	Loss:6.044	translation_Loss:5.589	token_training_loss:0.456	distillation_Loss:0.0                                                           	MRR:16.09	Hits@10:40.42	Best:16.13
2025-01-07 18:32:59,062: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:33:02,777: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1629 | 0.0065 | 0.2914 | 0.3546 |  0.4055 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,920,200
Trainable params: 2,400
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:33:06,530: Snapshot:1	Epoch:0	Loss:2.719	translation_Loss:2.644	token_training_loss:0.0	distillation_Loss:0.075                                                   	MRR:2.39	Hits@10:6.34	Best:2.39
2025-01-07 18:33:08,112: Snapshot:1	Epoch:1	Loss:1.749	translation_Loss:1.604	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:7.35	Hits@10:19.22	Best:7.35
2025-01-07 18:33:09,694: Snapshot:1	Epoch:2	Loss:0.972	translation_Loss:0.794	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:10.59	Hits@10:27.1	Best:10.59
2025-01-07 18:33:11,270: Snapshot:1	Epoch:3	Loss:0.527	translation_Loss:0.303	token_training_loss:0.0	distillation_Loss:0.224                                                   	MRR:12.47	Hits@10:30.32	Best:12.47
2025-01-07 18:33:12,863: Snapshot:1	Epoch:4	Loss:0.367	translation_Loss:0.113	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:13.35	Hits@10:31.88	Best:13.35
2025-01-07 18:33:14,453: Snapshot:1	Epoch:5	Loss:0.302	translation_Loss:0.048	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:13.71	Hits@10:32.53	Best:13.71
2025-01-07 18:33:16,053: Snapshot:1	Epoch:6	Loss:0.254	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.229                                                   	MRR:13.79	Hits@10:32.9	Best:13.79
2025-01-07 18:33:17,776: Snapshot:1	Epoch:7	Loss:0.211	translation_Loss:0.016	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:13.77	Hits@10:33.25	Best:13.79
2025-01-07 18:33:19,280: Snapshot:1	Epoch:8	Loss:0.171	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.157                                                   	MRR:13.77	Hits@10:33.28	Best:13.79
2025-01-07 18:33:20,779: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 13.79
2025-01-07 18:33:20,779: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:0.136 MRR:13.75 Best Results: 13.79
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:33:20,780: Snapshot:1	Epoch:9	Loss:0.136	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.123                                                   	MRR:13.75	Hits@10:33.36	Best:13.79
2025-01-07 18:33:22,271: Snapshot:1	Epoch:10	Loss:8.369	translation_Loss:1.208	token_training_loss:7.161	distillation_Loss:0.0                                                   	MRR:13.75	Hits@10:33.36	Best:13.79
2025-01-07 18:33:23,748: End of token training: 1 Epoch: 11 Loss:5.76 MRR:13.75 Best Results: 13.79
2025-01-07 18:33:23,748: Snapshot:1	Epoch:11	Loss:5.76	translation_Loss:1.206	token_training_loss:4.554	distillation_Loss:0.0                                                           	MRR:13.75	Hits@10:33.36	Best:13.79
2025-01-07 18:33:23,829: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:33:28,515: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1631 | 0.0075 | 0.2879 | 0.3574 |  0.4108 |
|     1      | 0.1388 | 0.0062 | 0.2489 | 0.2866 |  0.3272 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,738,800
Trainable params: 2,400
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:33:32,288: Snapshot:2	Epoch:0	Loss:2.628	translation_Loss:2.585	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:2.89	Hits@10:7.72	Best:2.89
2025-01-07 18:33:34,022: Snapshot:2	Epoch:1	Loss:1.63	translation_Loss:1.507	token_training_loss:0.0	distillation_Loss:0.123                                                   	MRR:7.99	Hits@10:19.87	Best:7.99
2025-01-07 18:33:35,966: Snapshot:2	Epoch:2	Loss:0.836	translation_Loss:0.68	token_training_loss:0.0	distillation_Loss:0.155                                                   	MRR:10.78	Hits@10:26.16	Best:10.78
2025-01-07 18:33:37,690: Snapshot:2	Epoch:3	Loss:0.391	translation_Loss:0.221	token_training_loss:0.0	distillation_Loss:0.169                                                   	MRR:12.31	Hits@10:28.44	Best:12.31
2025-01-07 18:33:39,422: Snapshot:2	Epoch:4	Loss:0.258	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:13.12	Hits@10:29.46	Best:13.12
2025-01-07 18:33:41,144: Snapshot:2	Epoch:5	Loss:0.215	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:13.48	Hits@10:30.27	Best:13.48
2025-01-07 18:33:42,890: Snapshot:2	Epoch:6	Loss:0.19	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.17                                                   	MRR:13.65	Hits@10:30.78	Best:13.65
2025-01-07 18:33:44,609: Snapshot:2	Epoch:7	Loss:0.166	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.156                                                   	MRR:13.71	Hits@10:30.83	Best:13.71
2025-01-07 18:33:46,215: Snapshot:2	Epoch:8	Loss:0.148	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:13.7	Hits@10:31.05	Best:13.71
2025-01-07 18:33:47,844: Snapshot:2	Epoch:9	Loss:0.127	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.7	Hits@10:31.26	Best:13.71
2025-01-07 18:33:49,453: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 13.71
2025-01-07 18:33:49,453: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.107 MRR:13.69 Best Results: 13.71
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:33:49,453: Snapshot:2	Epoch:10	Loss:0.107	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.101                                                   	MRR:13.69	Hits@10:31.32	Best:13.71
2025-01-07 18:33:51,008: Snapshot:2	Epoch:11	Loss:8.283	translation_Loss:1.186	token_training_loss:7.097	distillation_Loss:0.0                                                   	MRR:13.69	Hits@10:31.32	Best:13.71
2025-01-07 18:33:52,567: End of token training: 2 Epoch: 12 Loss:5.632 MRR:13.69 Best Results: 13.71
2025-01-07 18:33:52,567: Snapshot:2	Epoch:12	Loss:5.632	translation_Loss:1.185	token_training_loss:4.447	distillation_Loss:0.0                                                           	MRR:13.69	Hits@10:31.32	Best:13.71
2025-01-07 18:33:52,645: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:33:58,429: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1582 | 0.0076 | 0.2769 | 0.3468 |  0.4067 |
|     1      | 0.1337 | 0.0073 | 0.2344 | 0.2766 |  0.3228 |
|     2      | 0.1403 | 0.0086 | 0.2519 | 0.2804 |  0.3167 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,557,600
Trainable params: 2,400
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:34:02,869: Snapshot:3	Epoch:0	Loss:2.617	translation_Loss:2.543	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:3.03	Hits@10:7.34	Best:3.03
2025-01-07 18:34:04,702: Snapshot:3	Epoch:1	Loss:1.603	translation_Loss:1.434	token_training_loss:0.0	distillation_Loss:0.168                                                   	MRR:7.48	Hits@10:18.6	Best:7.48
2025-01-07 18:34:06,567: Snapshot:3	Epoch:2	Loss:0.817	translation_Loss:0.62	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:10.64	Hits@10:24.78	Best:10.64
2025-01-07 18:34:08,612: Snapshot:3	Epoch:3	Loss:0.418	translation_Loss:0.198	token_training_loss:0.0	distillation_Loss:0.221                                                   	MRR:12.11	Hits@10:27.34	Best:12.11
2025-01-07 18:34:10,488: Snapshot:3	Epoch:4	Loss:0.311	translation_Loss:0.074	token_training_loss:0.0	distillation_Loss:0.237                                                   	MRR:12.74	Hits@10:28.79	Best:12.74
2025-01-07 18:34:12,326: Snapshot:3	Epoch:5	Loss:0.268	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.232                                                   	MRR:12.99	Hits@10:29.14	Best:12.99
2025-01-07 18:34:14,178: Snapshot:3	Epoch:6	Loss:0.232	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.209                                                   	MRR:13.08	Hits@10:29.57	Best:13.08
2025-01-07 18:34:16,273: Snapshot:3	Epoch:7	Loss:0.194	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:13.21	Hits@10:29.7	Best:13.21
2025-01-07 18:34:18,002: Snapshot:3	Epoch:8	Loss:0.161	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.148                                                   	MRR:13.2	Hits@10:30.0	Best:13.21
2025-01-07 18:34:19,852: Snapshot:3	Epoch:9	Loss:0.129	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.22	Hits@10:29.92	Best:13.22
2025-01-07 18:34:21,572: Snapshot:3	Epoch:10	Loss:0.106	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.095                                                   	MRR:13.19	Hits@10:29.87	Best:13.22
2025-01-07 18:34:23,289: Snapshot:3	Epoch:11	Loss:0.088	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.077                                                   	MRR:13.21	Hits@10:30.03	Best:13.22
2025-01-07 18:34:25,146: Snapshot:3	Epoch:12	Loss:0.074	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:13.23	Hits@10:30.24	Best:13.23
2025-01-07 18:34:26,877: Snapshot:3	Epoch:13	Loss:0.064	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.054                                                   	MRR:13.18	Hits@10:30.24	Best:13.23
2025-01-07 18:34:28,592: Snapshot:3	Epoch:14	Loss:0.06	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.048                                                   	MRR:13.23	Hits@10:30.38	Best:13.23
2025-01-07 18:34:30,423: Snapshot:3	Epoch:15	Loss:0.054	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.044                                                   	MRR:13.29	Hits@10:30.32	Best:13.29
2025-01-07 18:34:32,276: Snapshot:3	Epoch:16	Loss:0.051	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.041                                                   	MRR:13.38	Hits@10:30.3	Best:13.38
2025-01-07 18:34:33,994: Snapshot:3	Epoch:17	Loss:0.047	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.039                                                   	MRR:13.31	Hits@10:30.48	Best:13.38
2025-01-07 18:34:35,974: Snapshot:3	Epoch:18	Loss:0.045	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.037                                                   	MRR:13.35	Hits@10:30.56	Best:13.38
2025-01-07 18:34:37,691: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 13.38
2025-01-07 18:34:37,691: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:0.043 MRR:13.37 Best Results: 13.38
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:34:37,691: Snapshot:3	Epoch:19	Loss:0.043	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.035                                                   	MRR:13.37	Hits@10:30.75	Best:13.38
2025-01-07 18:34:39,366: Snapshot:3	Epoch:20	Loss:8.055	translation_Loss:1.172	token_training_loss:6.884	distillation_Loss:0.0                                                   	MRR:13.37	Hits@10:30.75	Best:13.38
2025-01-07 18:34:41,047: End of token training: 3 Epoch: 21 Loss:5.364 MRR:13.37 Best Results: 13.38
2025-01-07 18:34:41,047: Snapshot:3	Epoch:21	Loss:5.364	translation_Loss:1.169	token_training_loss:4.195	distillation_Loss:0.0                                                           	MRR:13.37	Hits@10:30.75	Best:13.38
2025-01-07 18:34:41,146: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:34:47,897: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1583 | 0.008  | 0.276  | 0.3477 |  0.4054 |
|     1      | 0.1328 | 0.007  | 0.2328 | 0.2761 |  0.3212 |
|     2      | 0.1395 | 0.0083 | 0.2505 | 0.2796 |  0.3175 |
|     3      | 0.1359 | 0.0078 | 0.2403 | 0.2825 |  0.318  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,400
Trainable params: 2,400
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:34:52,184: Snapshot:4	Epoch:0	Loss:2.572	translation_Loss:2.499	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:9.88	Hits@10:23.76	Best:9.88
2025-01-07 18:34:54,357: Snapshot:4	Epoch:1	Loss:1.546	translation_Loss:1.372	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:11.51	Hits@10:27.5	Best:11.51
2025-01-07 18:34:56,331: Snapshot:4	Epoch:2	Loss:0.749	translation_Loss:0.548	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:12.6	Hits@10:29.81	Best:12.6
2025-01-07 18:34:58,307: Snapshot:4	Epoch:3	Loss:0.369	translation_Loss:0.155	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:13.2	Hits@10:30.59	Best:13.2
2025-01-07 18:35:00,247: Snapshot:4	Epoch:4	Loss:0.281	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.221                                                   	MRR:13.52	Hits@10:30.99	Best:13.52
2025-01-07 18:35:02,232: Snapshot:4	Epoch:5	Loss:0.246	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:13.67	Hits@10:31.42	Best:13.67
2025-01-07 18:35:04,227: Snapshot:4	Epoch:6	Loss:0.212	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:13.72	Hits@10:31.61	Best:13.72
2025-01-07 18:35:06,268: Snapshot:4	Epoch:7	Loss:0.18	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.165                                                   	MRR:13.8	Hits@10:32.07	Best:13.8
2025-01-07 18:35:08,239: Snapshot:4	Epoch:8	Loss:0.149	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:13.85	Hits@10:32.45	Best:13.85
2025-01-07 18:35:10,182: Snapshot:4	Epoch:9	Loss:0.122	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:13.89	Hits@10:32.53	Best:13.89
2025-01-07 18:35:12,135: Snapshot:4	Epoch:10	Loss:0.099	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.091                                                   	MRR:13.9	Hits@10:32.53	Best:13.9
2025-01-07 18:35:13,968: Snapshot:4	Epoch:11	Loss:0.084	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:13.9	Hits@10:32.58	Best:13.9
2025-01-07 18:35:15,953: Snapshot:4	Epoch:12	Loss:0.07	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:13.92	Hits@10:32.74	Best:13.92
2025-01-07 18:35:18,000: Snapshot:4	Epoch:13	Loss:0.061	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.052                                                   	MRR:13.89	Hits@10:32.72	Best:13.92
2025-01-07 18:35:19,830: Snapshot:4	Epoch:14	Loss:0.054	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.046                                                   	MRR:13.92	Hits@10:32.72	Best:13.92
2025-01-07 18:35:21,654: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 13.92
2025-01-07 18:35:21,654: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:0.051 MRR:13.89 Best Results: 13.92
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:35:21,655: Snapshot:4	Epoch:15	Loss:0.051	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.042                                                   	MRR:13.89	Hits@10:32.72	Best:13.92
2025-01-07 18:35:23,456: Snapshot:4	Epoch:16	Loss:8.1	translation_Loss:1.165	token_training_loss:6.935	distillation_Loss:0.0                                                   	MRR:13.89	Hits@10:32.72	Best:13.92
2025-01-07 18:35:25,233: End of token training: 4 Epoch: 17 Loss:5.435 MRR:13.89 Best Results: 13.92
2025-01-07 18:35:25,233: Snapshot:4	Epoch:17	Loss:5.435	translation_Loss:1.163	token_training_loss:4.273	distillation_Loss:0.0                                                           	MRR:13.89	Hits@10:32.72	Best:13.92
2025-01-07 18:35:25,311: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:35:33,165: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1579 | 0.0077 | 0.2759 | 0.3472 |  0.4066 |
|     1      | 0.1319 | 0.0054 | 0.2301 | 0.2755 |  0.3223 |
|     2      | 0.1391 | 0.0073 | 0.2508 | 0.2855 |  0.3148 |
|     3      | 0.1359 | 0.0081 | 0.2384 | 0.2823 |  0.3188 |
|     4      | 0.1366 | 0.0089 | 0.2431 | 0.2923 |  0.3297 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,400
Trainable params: 2,400
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:35:33,167: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1629 | 0.0065 | 0.2914 | 0.3546 |  0.4055 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1631 | 0.0075 | 0.2879 | 0.3574 |  0.4108 |
|     1      | 0.1388 | 0.0062 | 0.2489 | 0.2866 |  0.3272 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1582 | 0.0076 | 0.2769 | 0.3468 |  0.4067 |
|     1      | 0.1337 | 0.0073 | 0.2344 | 0.2766 |  0.3228 |
|     2      | 0.1403 | 0.0086 | 0.2519 | 0.2804 |  0.3167 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1583 | 0.008  | 0.276  | 0.3477 |  0.4054 |
|     1      | 0.1328 | 0.007  | 0.2328 | 0.2761 |  0.3212 |
|     2      | 0.1395 | 0.0083 | 0.2505 | 0.2796 |  0.3175 |
|     3      | 0.1359 | 0.0078 | 0.2403 | 0.2825 |  0.318  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1579 | 0.0077 | 0.2759 | 0.3472 |  0.4066 |
|     1      | 0.1319 | 0.0054 | 0.2301 | 0.2755 |  0.3223 |
|     2      | 0.1391 | 0.0073 | 0.2508 | 0.2855 |  0.3148 |
|     3      | 0.1359 | 0.0081 | 0.2384 | 0.2823 |  0.3188 |
|     4      | 0.1366 | 0.0089 | 0.2431 | 0.2923 |  0.3297 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:35:33,168: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 250.4125897884369  |   0.163   |    0.006     |    0.291     |     0.405     |
|    1     | 20.151472806930542 |    0.16   |    0.007     |    0.282     |     0.399     |
|    2     | 23.135254859924316 |   0.153   |    0.008     |    0.268     |     0.385     |
|    3     | 41.344159841537476 |   0.151   |    0.008     |    0.264     |     0.377     |
|    4     | 36.22196865081787  |   0.149   |    0.008     |    0.262     |     0.373     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:35:33,168: Sum_Training_Time:371.2654459476471
2025-01-07 18:35:33,168: Every_Training_Time:[250.4125897884369, 20.151472806930542, 23.135254859924316, 41.344159841537476, 36.22196865081787]
2025-01-07 18:35:33,168: Forward transfer: 0.059625 Backward transfer: -0.003275
2025-01-07 18:35:45,618: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107183537/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:35:54,706: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:36:02,315: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:36:10,156: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-07 18:36:18,049: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.18	Best:13.18
2025-01-07 18:36:25,640: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.02	Hits@10:35.49	Best:14.02
2025-01-07 18:36:33,464: Snapshot:0	Epoch:5	Loss:0.534	translation_Loss:0.534	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.61	Hits@10:36.68	Best:14.61
2025-01-07 18:36:41,059: Snapshot:0	Epoch:6	Loss:0.348	translation_Loss:0.348	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.97	Hits@10:37.41	Best:14.97
2025-01-07 18:36:48,921: Snapshot:0	Epoch:7	Loss:0.244	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.18	Hits@10:37.95	Best:15.18
2025-01-07 18:36:56,517: Snapshot:0	Epoch:8	Loss:0.178	translation_Loss:0.178	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.38	Hits@10:38.14	Best:15.38
2025-01-07 18:37:04,356: Snapshot:0	Epoch:9	Loss:0.139	translation_Loss:0.139	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.5	Hits@10:38.45	Best:15.5
2025-01-07 18:37:12,165: Snapshot:0	Epoch:10	Loss:0.113	translation_Loss:0.113	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.62	Hits@10:38.66	Best:15.62
2025-01-07 18:37:19,787: Snapshot:0	Epoch:11	Loss:0.094	translation_Loss:0.094	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.74	Hits@10:38.91	Best:15.74
2025-01-07 18:37:27,671: Snapshot:0	Epoch:12	Loss:0.078	translation_Loss:0.078	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.77	Hits@10:38.97	Best:15.77
2025-01-07 18:37:35,290: Snapshot:0	Epoch:13	Loss:0.07	translation_Loss:0.07	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.14	Best:15.79
2025-01-07 18:37:43,146: Snapshot:0	Epoch:14	Loss:0.06	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.81	Hits@10:39.29	Best:15.81
2025-01-07 18:37:50,943: Snapshot:0	Epoch:15	Loss:0.052	translation_Loss:0.052	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.88	Hits@10:39.43	Best:15.88
2025-01-07 18:37:58,527: Snapshot:0	Epoch:16	Loss:0.047	translation_Loss:0.047	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.49	Best:15.92
2025-01-07 18:38:06,340: Snapshot:0	Epoch:17	Loss:0.045	translation_Loss:0.045	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.93	Hits@10:39.77	Best:15.93
2025-01-07 18:38:13,952: Snapshot:0	Epoch:18	Loss:0.041	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.94	Hits@10:39.83	Best:15.94
2025-01-07 18:38:21,779: Snapshot:0	Epoch:19	Loss:0.039	translation_Loss:0.039	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.99	Hits@10:39.88	Best:15.99
2025-01-07 18:38:29,372: Snapshot:0	Epoch:20	Loss:0.035	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:40.01	Best:16.0
2025-01-07 18:38:37,240: Snapshot:0	Epoch:21	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.01	Hits@10:40.02	Best:16.01
2025-01-07 18:38:44,827: Snapshot:0	Epoch:22	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:40.22	Best:16.04
2025-01-07 18:38:52,622: Snapshot:0	Epoch:23	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:40.28	Best:16.04
2025-01-07 18:39:00,430: Snapshot:0	Epoch:24	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.31	Best:16.05
2025-01-07 18:39:07,980: Snapshot:0	Epoch:25	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.29	Best:16.05
2025-01-07 18:39:15,823: Snapshot:0	Epoch:26	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.07	Hits@10:40.2	Best:16.07
2025-01-07 18:39:23,474: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.08	Hits@10:40.32	Best:16.08
2025-01-07 18:39:31,337: Snapshot:0	Epoch:28	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.1	Hits@10:40.46	Best:16.1
2025-01-07 18:39:39,170: Snapshot:0	Epoch:29	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.05	Hits@10:40.41	Best:16.1
2025-01-07 18:39:46,762: Snapshot:0	Epoch:30	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.11	Hits@10:40.5	Best:16.11
2025-01-07 18:39:54,649: Snapshot:0	Epoch:31	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.17	Hits@10:40.5	Best:16.17
2025-01-07 18:40:02,210: Snapshot:0	Epoch:32	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.6	Best:16.17
2025-01-07 18:40:10,087: Snapshot:0	Epoch:33	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.7	Best:16.17
2025-01-07 18:40:17,645: Early Stopping! Snapshot: 0 Epoch: 34 Best Results: 16.17
2025-01-07 18:40:17,646: Start to training tokens! Snapshot: 0 Epoch: 34 Loss:0.026 MRR:16.16 Best Results: 16.17
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:40:17,646: Snapshot:0	Epoch:34	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.75	Best:16.17
2025-01-07 18:40:26,042: Snapshot:0	Epoch:35	Loss:21.438	translation_Loss:5.507	token_training_loss:15.931	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.75	Best:16.17
2025-01-07 18:40:33,663: End of token training: 0 Epoch: 36 Loss:5.984 MRR:16.16 Best Results: 16.17
2025-01-07 18:40:33,663: Snapshot:0	Epoch:36	Loss:5.984	translation_Loss:5.511	token_training_loss:0.473	distillation_Loss:0.0                                                           	MRR:16.16	Hits@10:40.75	Best:16.17
2025-01-07 18:40:33,775: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:40:38,205: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1646 | 0.0071 | 0.2943 | 0.3544 |  0.4081 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,920,600
Trainable params: 2,800
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:40:41,949: Snapshot:1	Epoch:0	Loss:2.735	translation_Loss:2.652	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:2.51	Hits@10:6.56	Best:2.51
2025-01-07 18:40:43,527: Snapshot:1	Epoch:1	Loss:1.767	translation_Loss:1.62	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:7.51	Hits@10:19.49	Best:7.51
2025-01-07 18:40:45,139: Snapshot:1	Epoch:2	Loss:1.004	translation_Loss:0.816	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:10.54	Hits@10:26.53	Best:10.54
2025-01-07 18:40:46,728: Snapshot:1	Epoch:3	Loss:0.553	translation_Loss:0.315	token_training_loss:0.0	distillation_Loss:0.238                                                   	MRR:12.37	Hits@10:29.84	Best:12.37
2025-01-07 18:40:48,305: Snapshot:1	Epoch:4	Loss:0.386	translation_Loss:0.121	token_training_loss:0.0	distillation_Loss:0.265                                                   	MRR:13.23	Hits@10:31.59	Best:13.23
2025-01-07 18:40:49,921: Snapshot:1	Epoch:5	Loss:0.315	translation_Loss:0.054	token_training_loss:0.0	distillation_Loss:0.261                                                   	MRR:13.59	Hits@10:32.28	Best:13.59
2025-01-07 18:40:51,546: Snapshot:1	Epoch:6	Loss:0.258	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.231                                                   	MRR:13.69	Hits@10:32.85	Best:13.69
2025-01-07 18:40:53,129: Snapshot:1	Epoch:7	Loss:0.209	translation_Loss:0.018	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:13.7	Hits@10:33.31	Best:13.7
2025-01-07 18:40:54,633: Snapshot:1	Epoch:8	Loss:0.166	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.151                                                   	MRR:13.69	Hits@10:33.33	Best:13.7
2025-01-07 18:40:56,133: Snapshot:1	Epoch:9	Loss:0.131	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.116                                                   	MRR:13.61	Hits@10:33.39	Best:13.7
2025-01-07 18:40:57,842: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.7
2025-01-07 18:40:57,842: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.105 MRR:13.57 Best Results: 13.7
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:40:57,842: Snapshot:1	Epoch:10	Loss:0.105	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:13.57	Hits@10:33.44	Best:13.7
2025-01-07 18:40:59,298: Snapshot:1	Epoch:11	Loss:8.322	translation_Loss:1.184	token_training_loss:7.138	distillation_Loss:0.0                                                   	MRR:13.57	Hits@10:33.44	Best:13.7
2025-01-07 18:41:00,757: End of token training: 1 Epoch: 12 Loss:5.792 MRR:13.57 Best Results: 13.7
2025-01-07 18:41:00,757: Snapshot:1	Epoch:12	Loss:5.792	translation_Loss:1.179	token_training_loss:4.613	distillation_Loss:0.0                                                           	MRR:13.57	Hits@10:33.44	Best:13.7
2025-01-07 18:41:00,833: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:41:05,549: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1659 | 0.0082 | 0.2946 | 0.3591 |  0.4143 |
|     1      | 0.1392 | 0.0056 | 0.2497 | 0.2876 |  0.3306 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,739,200
Trainable params: 2,800
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:41:09,311: Snapshot:2	Epoch:0	Loss:2.646	translation_Loss:2.597	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:2.86	Hits@10:7.8	Best:2.86
2025-01-07 18:41:11,035: Snapshot:2	Epoch:1	Loss:1.651	translation_Loss:1.523	token_training_loss:0.0	distillation_Loss:0.128                                                   	MRR:7.69	Hits@10:19.27	Best:7.69
2025-01-07 18:41:13,011: Snapshot:2	Epoch:2	Loss:0.852	translation_Loss:0.694	token_training_loss:0.0	distillation_Loss:0.157                                                   	MRR:10.57	Hits@10:25.91	Best:10.57
2025-01-07 18:41:14,736: Snapshot:2	Epoch:3	Loss:0.4	translation_Loss:0.226	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:12.14	Hits@10:28.68	Best:12.14
2025-01-07 18:41:16,458: Snapshot:2	Epoch:4	Loss:0.267	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:13.0	Hits@10:29.65	Best:13.0
2025-01-07 18:41:18,168: Snapshot:2	Epoch:5	Loss:0.227	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:13.33	Hits@10:30.38	Best:13.33
2025-01-07 18:41:19,909: Snapshot:2	Epoch:6	Loss:0.198	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:13.45	Hits@10:30.75	Best:13.45
2025-01-07 18:41:21,626: Snapshot:2	Epoch:7	Loss:0.174	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.162                                                   	MRR:13.5	Hits@10:30.97	Best:13.5
2025-01-07 18:41:23,293: Snapshot:2	Epoch:8	Loss:0.149	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.141                                                   	MRR:13.48	Hits@10:31.16	Best:13.5
2025-01-07 18:41:24,901: Snapshot:2	Epoch:9	Loss:0.126	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.5	Hits@10:31.21	Best:13.5
2025-01-07 18:41:26,497: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 13.5
2025-01-07 18:41:26,497: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.104 MRR:13.5 Best Results: 13.5
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:41:26,497: Snapshot:2	Epoch:10	Loss:0.104	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.099                                                   	MRR:13.5	Hits@10:31.37	Best:13.5
2025-01-07 18:41:28,050: Snapshot:2	Epoch:11	Loss:8.385	translation_Loss:1.16	token_training_loss:7.225	distillation_Loss:0.0                                                   	MRR:13.5	Hits@10:31.37	Best:13.5
2025-01-07 18:41:29,599: End of token training: 2 Epoch: 12 Loss:5.829 MRR:13.5 Best Results: 13.5
2025-01-07 18:41:29,599: Snapshot:2	Epoch:12	Loss:5.829	translation_Loss:1.163	token_training_loss:4.665	distillation_Loss:0.0                                                           	MRR:13.5	Hits@10:31.37	Best:13.5
2025-01-07 18:41:29,676: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:41:35,447: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1625 | 0.0076 | 0.286  | 0.3537 |  0.4126 |
|     1      | 0.1371 | 0.0073 | 0.2387 | 0.2858 |  0.3317 |
|     2      | 0.1398 | 0.0086 | 0.2508 | 0.2831 |  0.3212 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,000
Trainable params: 2,800
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:41:39,903: Snapshot:3	Epoch:0	Loss:2.64	translation_Loss:2.557	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:3.07	Hits@10:7.58	Best:3.07
2025-01-07 18:41:41,727: Snapshot:3	Epoch:1	Loss:1.631	translation_Loss:1.458	token_training_loss:0.0	distillation_Loss:0.173                                                   	MRR:7.61	Hits@10:18.79	Best:7.61
2025-01-07 18:41:43,551: Snapshot:3	Epoch:2	Loss:0.836	translation_Loss:0.634	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:10.6	Hits@10:25.13	Best:10.6
2025-01-07 18:41:45,405: Snapshot:3	Epoch:3	Loss:0.435	translation_Loss:0.203	token_training_loss:0.0	distillation_Loss:0.233                                                   	MRR:12.09	Hits@10:27.53	Best:12.09
2025-01-07 18:41:47,282: Snapshot:3	Epoch:4	Loss:0.325	translation_Loss:0.075	token_training_loss:0.0	distillation_Loss:0.25                                                   	MRR:12.76	Hits@10:28.71	Best:12.76
2025-01-07 18:41:49,122: Snapshot:3	Epoch:5	Loss:0.278	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.242                                                   	MRR:13.06	Hits@10:29.52	Best:13.06
2025-01-07 18:41:50,974: Snapshot:3	Epoch:6	Loss:0.235	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:13.11	Hits@10:29.84	Best:13.11
2025-01-07 18:41:52,827: Snapshot:3	Epoch:7	Loss:0.194	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:13.13	Hits@10:29.97	Best:13.13
2025-01-07 18:41:54,669: Snapshot:3	Epoch:8	Loss:0.156	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:13.14	Hits@10:29.97	Best:13.14
2025-01-07 18:41:56,513: Snapshot:3	Epoch:9	Loss:0.124	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:13.2	Hits@10:30.13	Best:13.2
2025-01-07 18:41:58,483: Snapshot:3	Epoch:10	Loss:0.1	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.088                                                   	MRR:13.2	Hits@10:30.27	Best:13.2
2025-01-07 18:42:00,212: Snapshot:3	Epoch:11	Loss:0.082	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:13.19	Hits@10:30.35	Best:13.2
2025-01-07 18:42:02,067: Snapshot:3	Epoch:12	Loss:0.07	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:13.22	Hits@10:30.48	Best:13.22
2025-01-07 18:42:03,912: Snapshot:3	Epoch:13	Loss:0.062	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.05                                                   	MRR:13.28	Hits@10:30.62	Best:13.28
2025-01-07 18:42:05,631: Snapshot:3	Epoch:14	Loss:0.056	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.045                                                   	MRR:13.28	Hits@10:30.7	Best:13.28
2025-01-07 18:42:07,467: Snapshot:3	Epoch:15	Loss:0.051	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.042                                                   	MRR:13.35	Hits@10:30.7	Best:13.35
2025-01-07 18:42:09,300: Snapshot:3	Epoch:16	Loss:0.048	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.039                                                   	MRR:13.4	Hits@10:30.78	Best:13.4
2025-01-07 18:42:11,021: Snapshot:3	Epoch:17	Loss:0.045	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.037                                                   	MRR:13.38	Hits@10:30.7	Best:13.4
2025-01-07 18:42:12,740: Snapshot:3	Epoch:18	Loss:0.043	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.035                                                   	MRR:13.36	Hits@10:30.78	Best:13.4
2025-01-07 18:42:14,459: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 13.4
2025-01-07 18:42:14,460: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:0.041 MRR:13.38 Best Results: 13.4
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:42:14,460: Snapshot:3	Epoch:19	Loss:0.041	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.033                                                   	MRR:13.38	Hits@10:30.94	Best:13.4
2025-01-07 18:42:16,128: Snapshot:3	Epoch:20	Loss:8.137	translation_Loss:1.159	token_training_loss:6.978	distillation_Loss:0.0                                                   	MRR:13.38	Hits@10:30.94	Best:13.4
2025-01-07 18:42:17,782: End of token training: 3 Epoch: 21 Loss:5.526 MRR:13.38 Best Results: 13.4
2025-01-07 18:42:17,783: Snapshot:3	Epoch:21	Loss:5.526	translation_Loss:1.154	token_training_loss:4.372	distillation_Loss:0.0                                                           	MRR:13.38	Hits@10:30.94	Best:13.4
2025-01-07 18:42:17,898: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:42:24,642: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.163  | 0.0078 | 0.2868 | 0.3539 |  0.4134 |
|     1      | 0.1364 | 0.0067 | 0.2379 | 0.2831 |  0.332  |
|     2      | 0.1394 | 0.0089 | 0.2487 | 0.2841 |  0.3228 |
|     3      | 0.1381 | 0.0086 | 0.2403 | 0.2874 |  0.3247 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,800
Trainable params: 2,800
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:42:29,025: Snapshot:4	Epoch:0	Loss:2.595	translation_Loss:2.514	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:10.01	Hits@10:24.41	Best:10.01
2025-01-07 18:42:30,967: Snapshot:4	Epoch:1	Loss:1.578	translation_Loss:1.397	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:11.74	Hits@10:28.17	Best:11.74
2025-01-07 18:42:33,132: Snapshot:4	Epoch:2	Loss:0.772	translation_Loss:0.567	token_training_loss:0.0	distillation_Loss:0.205                                                   	MRR:12.91	Hits@10:30.59	Best:12.91
2025-01-07 18:42:35,097: Snapshot:4	Epoch:3	Loss:0.388	translation_Loss:0.163	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:13.49	Hits@10:31.8	Best:13.49
2025-01-07 18:42:37,041: Snapshot:4	Epoch:4	Loss:0.295	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.235                                                   	MRR:13.78	Hits@10:32.2	Best:13.78
2025-01-07 18:42:38,996: Snapshot:4	Epoch:5	Loss:0.257	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:13.98	Hits@10:32.72	Best:13.98
2025-01-07 18:42:40,943: Snapshot:4	Epoch:6	Loss:0.218	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.198                                                   	MRR:13.99	Hits@10:33.01	Best:13.99
2025-01-07 18:42:42,890: Snapshot:4	Epoch:7	Loss:0.183	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.168                                                   	MRR:14.01	Hits@10:32.98	Best:14.01
2025-01-07 18:42:44,864: Snapshot:4	Epoch:8	Loss:0.147	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:14.03	Hits@10:33.09	Best:14.03
2025-01-07 18:42:46,807: Snapshot:4	Epoch:9	Loss:0.119	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:14.11	Hits@10:33.23	Best:14.11
2025-01-07 18:42:48,631: Snapshot:4	Epoch:10	Loss:0.095	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.086                                                   	MRR:14.06	Hits@10:33.25	Best:14.11
2025-01-07 18:42:50,446: Snapshot:4	Epoch:11	Loss:0.079	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:14.07	Hits@10:33.15	Best:14.11
2025-01-07 18:42:52,270: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 14.11
2025-01-07 18:42:52,270: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:0.066 MRR:14.04 Best Results: 14.11
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:42:52,270: Snapshot:4	Epoch:12	Loss:0.066	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.057                                                   	MRR:14.04	Hits@10:33.12	Best:14.11
2025-01-07 18:42:54,034: Snapshot:4	Epoch:13	Loss:8.224	translation_Loss:1.145	token_training_loss:7.079	distillation_Loss:0.0                                                   	MRR:14.04	Hits@10:33.12	Best:14.11
2025-01-07 18:42:56,036: End of token training: 4 Epoch: 14 Loss:5.643 MRR:14.04 Best Results: 14.11
2025-01-07 18:42:56,036: Snapshot:4	Epoch:14	Loss:5.643	translation_Loss:1.144	token_training_loss:4.499	distillation_Loss:0.0                                                           	MRR:14.04	Hits@10:33.12	Best:14.11
2025-01-07 18:42:56,110: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:43:03,900: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1622 | 0.008  | 0.2849 | 0.3529 |  0.413  |
|     1      | 0.1351 | 0.0056 | 0.2341 | 0.2823 |  0.3312 |
|     2      | 0.1381 | 0.0067 |  0.25  | 0.2858 |  0.3242 |
|     3      | 0.1386 | 0.0094 | 0.2406 | 0.2855 |  0.3261 |
|     4      | 0.1371 | 0.0078 | 0.2429 | 0.2904 |  0.3356 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,800
Trainable params: 2,800
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:43:03,903: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1646 | 0.0071 | 0.2943 | 0.3544 |  0.4081 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1659 | 0.0082 | 0.2946 | 0.3591 |  0.4143 |
|     1      | 0.1392 | 0.0056 | 0.2497 | 0.2876 |  0.3306 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1625 | 0.0076 | 0.286  | 0.3537 |  0.4126 |
|     1      | 0.1371 | 0.0073 | 0.2387 | 0.2858 |  0.3317 |
|     2      | 0.1398 | 0.0086 | 0.2508 | 0.2831 |  0.3212 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.163  | 0.0078 | 0.2868 | 0.3539 |  0.4134 |
|     1      | 0.1364 | 0.0067 | 0.2379 | 0.2831 |  0.332  |
|     2      | 0.1394 | 0.0089 | 0.2487 | 0.2841 |  0.3228 |
|     3      | 0.1381 | 0.0086 | 0.2403 | 0.2874 |  0.3247 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1622 | 0.008  | 0.2849 | 0.3529 |  0.413  |
|     1      | 0.1351 | 0.0056 | 0.2341 | 0.2823 |  0.3312 |
|     2      | 0.1381 | 0.0067 |  0.25  | 0.2858 |  0.3242 |
|     3      | 0.1386 | 0.0094 | 0.2406 | 0.2855 |  0.3261 |
|     4      | 0.1371 | 0.0078 | 0.2429 | 0.2904 |  0.3356 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:43:03,904: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 288.04373145103455 |   0.165   |    0.007     |    0.294     |     0.408     |
|    1     | 21.732524394989014 |   0.162   |    0.008     |    0.288     |     0.402     |
|    2     | 23.114407300949097 |   0.156   |    0.008     |    0.276     |     0.391     |
|    3     | 41.049434423446655 |   0.155   |    0.008     |    0.272     |     0.384     |
|    4     | 30.094303846359253 |   0.152   |    0.008     |    0.268     |      0.38     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:43:03,904: Sum_Training_Time:404.03440141677856
2025-01-07 18:43:03,904: Every_Training_Time:[288.04373145103455, 21.732524394989014, 23.114407300949097, 41.049434423446655, 30.094303846359253]
2025-01-07 18:43:03,904: Forward transfer: 0.060050000000000006 Backward transfer: -0.0019249999999999962
2025-01-07 18:43:16,459: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107184307/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=8, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:43:25,669: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:43:33,381: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:43:41,367: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-07 18:43:49,363: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.19	Best:13.18
2025-01-07 18:43:57,110: Snapshot:0	Epoch:4	Loss:0.855	translation_Loss:0.855	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.02	Hits@10:35.48	Best:14.02
2025-01-07 18:44:05,055: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.62	Hits@10:36.63	Best:14.62
2025-01-07 18:44:12,762: Snapshot:0	Epoch:6	Loss:0.349	translation_Loss:0.349	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.98	Hits@10:37.36	Best:14.98
2025-01-07 18:44:20,790: Snapshot:0	Epoch:7	Loss:0.244	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.18	Hits@10:37.86	Best:15.18
2025-01-07 18:44:28,497: Snapshot:0	Epoch:8	Loss:0.178	translation_Loss:0.178	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.37	Hits@10:38.2	Best:15.37
2025-01-07 18:44:36,405: Snapshot:0	Epoch:9	Loss:0.138	translation_Loss:0.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.51	Hits@10:38.32	Best:15.51
2025-01-07 18:44:44,258: Snapshot:0	Epoch:10	Loss:0.111	translation_Loss:0.111	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.61	Hits@10:38.78	Best:15.61
2025-01-07 18:44:51,979: Snapshot:0	Epoch:11	Loss:0.093	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.66	Hits@10:38.98	Best:15.66
2025-01-07 18:44:59,973: Snapshot:0	Epoch:12	Loss:0.078	translation_Loss:0.078	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.72	Hits@10:39.15	Best:15.72
2025-01-07 18:45:07,731: Snapshot:0	Epoch:13	Loss:0.069	translation_Loss:0.069	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.26	Best:15.79
2025-01-07 18:45:15,715: Snapshot:0	Epoch:14	Loss:0.059	translation_Loss:0.059	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.83	Hits@10:39.44	Best:15.83
2025-01-07 18:45:23,676: Snapshot:0	Epoch:15	Loss:0.051	translation_Loss:0.051	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.63	Best:15.92
2025-01-07 18:45:31,384: Snapshot:0	Epoch:16	Loss:0.047	translation_Loss:0.047	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.93	Hits@10:39.66	Best:15.93
2025-01-07 18:45:39,322: Snapshot:0	Epoch:17	Loss:0.045	translation_Loss:0.045	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.81	Best:15.98
2025-01-07 18:45:46,982: Snapshot:0	Epoch:18	Loss:0.041	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.97	Hits@10:39.76	Best:15.98
2025-01-07 18:45:54,948: Snapshot:0	Epoch:19	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.01	Hits@10:39.87	Best:16.01
2025-01-07 18:46:02,616: Snapshot:0	Epoch:20	Loss:0.035	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.96	Best:16.01
2025-01-07 18:46:10,539: Snapshot:0	Epoch:21	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:39.97	Best:16.01
2025-01-07 18:46:18,254: Snapshot:0	Epoch:22	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:40.04	Best:16.03
2025-01-07 18:46:26,152: Snapshot:0	Epoch:23	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:40.22	Best:16.03
2025-01-07 18:46:34,074: Snapshot:0	Epoch:24	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.1	Hits@10:40.39	Best:16.1
2025-01-07 18:46:41,736: Snapshot:0	Epoch:25	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.09	Hits@10:40.34	Best:16.1
2025-01-07 18:46:49,668: Snapshot:0	Epoch:26	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.38	Best:16.13
2025-01-07 18:46:57,404: Snapshot:0	Epoch:27	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.46	Best:16.14
2025-01-07 18:47:05,383: Snapshot:0	Epoch:28	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.43	Best:16.15
2025-01-07 18:47:13,297: Snapshot:0	Epoch:29	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.48	Best:16.15
2025-01-07 18:47:20,993: Snapshot:0	Epoch:30	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.5	Best:16.16
2025-01-07 18:47:28,980: Snapshot:0	Epoch:31	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.2	Hits@10:40.57	Best:16.2
2025-01-07 18:47:36,642: Snapshot:0	Epoch:32	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.6	Best:16.2
2025-01-07 18:47:44,532: Snapshot:0	Epoch:33	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.64	Best:16.2
2025-01-07 18:47:52,267: Snapshot:0	Epoch:34	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.27	Hits@10:40.76	Best:16.27
2025-01-07 18:48:00,193: Snapshot:0	Epoch:35	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.24	Hits@10:40.72	Best:16.27
2025-01-07 18:48:07,852: Snapshot:0	Epoch:36	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.27	Hits@10:40.73	Best:16.27
2025-01-07 18:48:15,786: Early Stopping! Snapshot: 0 Epoch: 37 Best Results: 16.27
2025-01-07 18:48:15,786: Start to training tokens! Snapshot: 0 Epoch: 37 Loss:0.026 MRR:16.25 Best Results: 16.27
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:48:15,787: Snapshot:0	Epoch:37	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.77	Best:16.27
2025-01-07 18:48:24,065: Snapshot:0	Epoch:38	Loss:22.199	translation_Loss:5.578	token_training_loss:16.621	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.77	Best:16.27
2025-01-07 18:48:32,113: End of token training: 0 Epoch: 39 Loss:6.112 MRR:16.25 Best Results: 16.27
2025-01-07 18:48:32,113: Snapshot:0	Epoch:39	Loss:6.112	translation_Loss:5.59	token_training_loss:0.521	distillation_Loss:0.0                                                           	MRR:16.25	Hits@10:40.77	Best:16.27
2025-01-07 18:48:32,211: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:48:36,075: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1641 | 0.0068 | 0.2942 | 0.3556 |  0.4088 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,921,000
Trainable params: 3,200
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:48:39,820: Snapshot:1	Epoch:0	Loss:2.746	translation_Loss:2.655	token_training_loss:0.0	distillation_Loss:0.091                                                   	MRR:2.55	Hits@10:6.69	Best:2.55
2025-01-07 18:48:41,416: Snapshot:1	Epoch:1	Loss:1.774	translation_Loss:1.624	token_training_loss:0.0	distillation_Loss:0.15                                                   	MRR:7.55	Hits@10:19.7	Best:7.55
2025-01-07 18:48:43,035: Snapshot:1	Epoch:2	Loss:1.022	translation_Loss:0.823	token_training_loss:0.0	distillation_Loss:0.2                                                   	MRR:10.6	Hits@10:26.96	Best:10.6
2025-01-07 18:48:44,849: Snapshot:1	Epoch:3	Loss:0.574	translation_Loss:0.324	token_training_loss:0.0	distillation_Loss:0.25                                                   	MRR:12.3	Hits@10:30.4	Best:12.3
2025-01-07 18:48:46,430: Snapshot:1	Epoch:4	Loss:0.401	translation_Loss:0.127	token_training_loss:0.0	distillation_Loss:0.274                                                   	MRR:13.19	Hits@10:31.99	Best:13.19
2025-01-07 18:48:48,033: Snapshot:1	Epoch:5	Loss:0.323	translation_Loss:0.058	token_training_loss:0.0	distillation_Loss:0.265                                                   	MRR:13.57	Hits@10:32.45	Best:13.57
2025-01-07 18:48:49,633: Snapshot:1	Epoch:6	Loss:0.26	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.23                                                   	MRR:13.71	Hits@10:32.98	Best:13.71
2025-01-07 18:48:51,174: Snapshot:1	Epoch:7	Loss:0.206	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.186                                                   	MRR:13.71	Hits@10:33.15	Best:13.71
2025-01-07 18:48:52,693: Snapshot:1	Epoch:8	Loss:0.162	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.145                                                   	MRR:13.69	Hits@10:33.39	Best:13.71
2025-01-07 18:48:54,219: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 13.71
2025-01-07 18:48:54,219: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:0.128 MRR:13.53 Best Results: 13.71
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:48:54,220: Snapshot:1	Epoch:9	Loss:0.128	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.111                                                   	MRR:13.53	Hits@10:33.68	Best:13.71
2025-01-07 18:48:55,681: Snapshot:1	Epoch:10	Loss:8.347	translation_Loss:1.188	token_training_loss:7.159	distillation_Loss:0.0                                                   	MRR:13.53	Hits@10:33.68	Best:13.71
2025-01-07 18:48:57,139: End of token training: 1 Epoch: 11 Loss:5.884 MRR:13.53 Best Results: 13.71
2025-01-07 18:48:57,139: Snapshot:1	Epoch:11	Loss:5.884	translation_Loss:1.187	token_training_loss:4.697	distillation_Loss:0.0                                                           	MRR:13.53	Hits@10:33.68	Best:13.71
2025-01-07 18:48:57,221: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:49:01,885: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1654 | 0.0079 | 0.2932 | 0.3603 |  0.4164 |
|     1      | 0.1402 | 0.0062 | 0.2522 | 0.2876 |  0.3317 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,739,600
Trainable params: 3,200
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:49:05,890: Snapshot:2	Epoch:0	Loss:2.656	translation_Loss:2.601	token_training_loss:0.0	distillation_Loss:0.055                                                   	MRR:3.07	Hits@10:8.04	Best:3.07
2025-01-07 18:49:07,632: Snapshot:2	Epoch:1	Loss:1.663	translation_Loss:1.529	token_training_loss:0.0	distillation_Loss:0.134                                                   	MRR:7.91	Hits@10:20.16	Best:7.91
2025-01-07 18:49:09,379: Snapshot:2	Epoch:2	Loss:0.861	translation_Loss:0.699	token_training_loss:0.0	distillation_Loss:0.162                                                   	MRR:10.78	Hits@10:25.91	Best:10.78
2025-01-07 18:49:11,131: Snapshot:2	Epoch:3	Loss:0.416	translation_Loss:0.233	token_training_loss:0.0	distillation_Loss:0.182                                                   	MRR:12.24	Hits@10:28.28	Best:12.24
2025-01-07 18:49:13,098: Snapshot:2	Epoch:4	Loss:0.281	translation_Loss:0.081	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:12.93	Hits@10:29.65	Best:12.93
2025-01-07 18:49:14,834: Snapshot:2	Epoch:5	Loss:0.239	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:13.2	Hits@10:30.3	Best:13.2
2025-01-07 18:49:16,571: Snapshot:2	Epoch:6	Loss:0.209	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:13.36	Hits@10:30.89	Best:13.36
2025-01-07 18:49:18,310: Snapshot:2	Epoch:7	Loss:0.181	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.168                                                   	MRR:13.43	Hits@10:31.05	Best:13.43
2025-01-07 18:49:19,942: Snapshot:2	Epoch:8	Loss:0.154	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:13.4	Hits@10:30.99	Best:13.43
2025-01-07 18:49:21,575: Snapshot:2	Epoch:9	Loss:0.127	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.4	Hits@10:31.24	Best:13.43
2025-01-07 18:49:23,246: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 13.43
2025-01-07 18:49:23,246: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.104 MRR:13.37 Best Results: 13.43
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:49:23,246: Snapshot:2	Epoch:10	Loss:0.104	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:13.37	Hits@10:31.4	Best:13.43
2025-01-07 18:49:24,822: Snapshot:2	Epoch:11	Loss:8.307	translation_Loss:1.155	token_training_loss:7.153	distillation_Loss:0.0                                                   	MRR:13.37	Hits@10:31.4	Best:13.43
2025-01-07 18:49:26,403: End of token training: 2 Epoch: 12 Loss:5.799 MRR:13.37 Best Results: 13.43
2025-01-07 18:49:26,403: Snapshot:2	Epoch:12	Loss:5.799	translation_Loss:1.155	token_training_loss:4.644	distillation_Loss:0.0                                                           	MRR:13.37	Hits@10:31.4	Best:13.43
2025-01-07 18:49:26,510: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:49:32,323: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1627 | 0.0081 | 0.2851 | 0.356  |  0.4143 |
|     1      | 0.1377 | 0.0073 | 0.2419 | 0.2844 |  0.3306 |
|     2      | 0.1395 | 0.0081 | 0.247  | 0.2852 |  0.3253 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,400
Trainable params: 3,200
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:49:36,607: Snapshot:3	Epoch:0	Loss:2.654	translation_Loss:2.563	token_training_loss:0.0	distillation_Loss:0.091                                                   	MRR:3.09	Hits@10:7.45	Best:3.09
2025-01-07 18:49:38,444: Snapshot:3	Epoch:1	Loss:1.642	translation_Loss:1.463	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:7.48	Hits@10:18.95	Best:7.48
2025-01-07 18:49:40,316: Snapshot:3	Epoch:2	Loss:0.851	translation_Loss:0.641	token_training_loss:0.0	distillation_Loss:0.21                                                   	MRR:10.54	Hits@10:25.38	Best:10.54
2025-01-07 18:49:42,403: Snapshot:3	Epoch:3	Loss:0.452	translation_Loss:0.207	token_training_loss:0.0	distillation_Loss:0.246                                                   	MRR:12.09	Hits@10:27.72	Best:12.09
2025-01-07 18:49:44,279: Snapshot:3	Epoch:4	Loss:0.341	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.262                                                   	MRR:12.79	Hits@10:29.11	Best:12.79
2025-01-07 18:49:46,148: Snapshot:3	Epoch:5	Loss:0.288	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.25                                                   	MRR:13.04	Hits@10:29.73	Best:13.04
2025-01-07 18:49:48,045: Snapshot:3	Epoch:6	Loss:0.239	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.216                                                   	MRR:13.17	Hits@10:30.05	Best:13.17
2025-01-07 18:49:49,898: Snapshot:3	Epoch:7	Loss:0.193	translation_Loss:0.016	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:13.21	Hits@10:30.3	Best:13.21
2025-01-07 18:49:51,707: Snapshot:3	Epoch:8	Loss:0.153	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:13.21	Hits@10:30.48	Best:13.21
2025-01-07 18:49:53,588: Snapshot:3	Epoch:9	Loss:0.121	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.108                                                   	MRR:13.23	Hits@10:30.54	Best:13.23
2025-01-07 18:49:55,336: Snapshot:3	Epoch:10	Loss:0.097	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.084                                                   	MRR:13.2	Hits@10:30.67	Best:13.23
2025-01-07 18:49:57,203: Snapshot:3	Epoch:11	Loss:0.08	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:13.24	Hits@10:30.81	Best:13.24
2025-01-07 18:49:58,949: Snapshot:3	Epoch:12	Loss:0.069	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.057                                                   	MRR:13.22	Hits@10:30.81	Best:13.24
2025-01-07 18:50:00,684: Snapshot:3	Epoch:13	Loss:0.06	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:13.22	Hits@10:30.97	Best:13.24
2025-01-07 18:50:02,812: Snapshot:3	Epoch:14	Loss:0.056	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.044                                                   	MRR:13.25	Hits@10:30.91	Best:13.25
2025-01-07 18:50:04,722: Snapshot:3	Epoch:15	Loss:0.05	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.041                                                   	MRR:13.27	Hits@10:30.97	Best:13.27
2025-01-07 18:50:06,633: Snapshot:3	Epoch:16	Loss:0.047	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.038                                                   	MRR:13.32	Hits@10:31.1	Best:13.32
2025-01-07 18:50:08,595: Snapshot:3	Epoch:17	Loss:0.046	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.036                                                   	MRR:13.37	Hits@10:30.86	Best:13.37
2025-01-07 18:50:10,338: Snapshot:3	Epoch:18	Loss:0.043	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.034                                                   	MRR:13.35	Hits@10:30.89	Best:13.37
2025-01-07 18:50:12,220: Snapshot:3	Epoch:19	Loss:0.042	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.033                                                   	MRR:13.41	Hits@10:31.02	Best:13.41
2025-01-07 18:50:14,106: Snapshot:3	Epoch:20	Loss:0.04	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.032                                                   	MRR:13.43	Hits@10:31.1	Best:13.43
2025-01-07 18:50:15,846: Snapshot:3	Epoch:21	Loss:0.039	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.031                                                   	MRR:13.43	Hits@10:31.13	Best:13.43
2025-01-07 18:50:17,733: Snapshot:3	Epoch:22	Loss:0.038	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.03                                                   	MRR:13.46	Hits@10:31.02	Best:13.46
2025-01-07 18:50:19,594: Snapshot:3	Epoch:23	Loss:0.037	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:13.48	Hits@10:31.13	Best:13.48
2025-01-07 18:50:21,347: Snapshot:3	Epoch:24	Loss:0.036	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:13.48	Hits@10:31.1	Best:13.48
2025-01-07 18:50:23,434: Snapshot:3	Epoch:25	Loss:0.036	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.028                                                   	MRR:13.5	Hits@10:31.34	Best:13.5
2025-01-07 18:50:25,179: Snapshot:3	Epoch:26	Loss:0.035	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.028                                                   	MRR:13.47	Hits@10:31.32	Best:13.5
2025-01-07 18:50:27,056: Snapshot:3	Epoch:27	Loss:0.034	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.027                                                   	MRR:13.51	Hits@10:31.26	Best:13.51
2025-01-07 18:50:28,922: Snapshot:3	Epoch:28	Loss:0.035	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.027                                                   	MRR:13.54	Hits@10:31.26	Best:13.54
2025-01-07 18:50:30,663: Snapshot:3	Epoch:29	Loss:0.034	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.027                                                   	MRR:13.5	Hits@10:31.32	Best:13.54
2025-01-07 18:50:32,393: Snapshot:3	Epoch:30	Loss:0.035	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.027                                                   	MRR:13.46	Hits@10:31.24	Best:13.54
2025-01-07 18:50:34,114: Early Stopping! Snapshot: 3 Epoch: 31 Best Results: 13.54
2025-01-07 18:50:34,114: Start to training tokens! Snapshot: 3 Epoch: 31 Loss:0.035 MRR:13.49 Best Results: 13.54
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:50:34,115: Snapshot:3	Epoch:31	Loss:0.035	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.027                                                   	MRR:13.49	Hits@10:31.29	Best:13.54
2025-01-07 18:50:35,800: Snapshot:3	Epoch:32	Loss:8.164	translation_Loss:1.126	token_training_loss:7.038	distillation_Loss:0.0                                                   	MRR:13.49	Hits@10:31.29	Best:13.54
2025-01-07 18:50:37,473: End of token training: 3 Epoch: 33 Loss:5.628 MRR:13.49 Best Results: 13.54
2025-01-07 18:50:37,473: Snapshot:3	Epoch:33	Loss:5.628	translation_Loss:1.126	token_training_loss:4.502	distillation_Loss:0.0                                                           	MRR:13.49	Hits@10:31.29	Best:13.54
2025-01-07 18:50:37,579: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:50:44,331: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1629 | 0.0081 | 0.2851 | 0.3552 |  0.4148 |
|     1      | 0.1365 | 0.007  | 0.2398 | 0.2844 |  0.3296 |
|     2      | 0.138  | 0.0059 | 0.2476 | 0.2855 |  0.3247 |
|     3      | 0.1388 | 0.0097 | 0.2444 | 0.2863 |  0.3255 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,377,200
Trainable params: 3,200
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:50:48,735: Snapshot:4	Epoch:0	Loss:2.621	translation_Loss:2.531	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:10.07	Hits@10:24.25	Best:10.07
2025-01-07 18:50:50,709: Snapshot:4	Epoch:1	Loss:1.589	translation_Loss:1.403	token_training_loss:0.0	distillation_Loss:0.186                                                   	MRR:11.87	Hits@10:28.82	Best:11.87
2025-01-07 18:50:52,695: Snapshot:4	Epoch:2	Loss:0.783	translation_Loss:0.573	token_training_loss:0.0	distillation_Loss:0.211                                                   	MRR:13.0	Hits@10:30.54	Best:13.0
2025-01-07 18:50:54,709: Snapshot:4	Epoch:3	Loss:0.4	translation_Loss:0.164	token_training_loss:0.0	distillation_Loss:0.236                                                   	MRR:13.46	Hits@10:31.59	Best:13.46
2025-01-07 18:50:56,690: Snapshot:4	Epoch:4	Loss:0.306	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.246                                                   	MRR:13.87	Hits@10:32.04	Best:13.87
2025-01-07 18:50:58,681: Snapshot:4	Epoch:5	Loss:0.264	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.233                                                   	MRR:14.09	Hits@10:32.58	Best:14.09
2025-01-07 18:51:00,697: Snapshot:4	Epoch:6	Loss:0.221	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:14.11	Hits@10:32.8	Best:14.11
2025-01-07 18:51:02,765: Snapshot:4	Epoch:7	Loss:0.18	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.167                                                   	MRR:14.07	Hits@10:32.96	Best:14.11
2025-01-07 18:51:04,624: Snapshot:4	Epoch:8	Loss:0.145	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.134                                                   	MRR:14.09	Hits@10:33.09	Best:14.11
2025-01-07 18:51:06,498: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 14.11
2025-01-07 18:51:06,498: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:0.115 MRR:14.06 Best Results: 14.11
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:51:06,498: Snapshot:4	Epoch:9	Loss:0.115	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.105                                                   	MRR:14.06	Hits@10:33.23	Best:14.11
2025-01-07 18:51:08,294: Snapshot:4	Epoch:10	Loss:8.287	translation_Loss:1.132	token_training_loss:7.156	distillation_Loss:0.0                                                   	MRR:14.06	Hits@10:33.23	Best:14.11
2025-01-07 18:51:10,076: End of token training: 4 Epoch: 11 Loss:5.791 MRR:14.06 Best Results: 14.11
2025-01-07 18:51:10,077: Snapshot:4	Epoch:11	Loss:5.791	translation_Loss:1.132	token_training_loss:4.658	distillation_Loss:0.0                                                           	MRR:14.06	Hits@10:33.23	Best:14.11
2025-01-07 18:51:10,153: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:51:17,957: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1611 | 0.0074 | 0.2817 | 0.3532 |  0.4125 |
|     1      | 0.1347 | 0.0067 | 0.2358 | 0.2793 |  0.3312 |
|     2      | 0.1388 | 0.0086 | 0.2468 | 0.2855 |  0.3218 |
|     3      | 0.138  | 0.0094 | 0.2406 | 0.2874 |  0.3285 |
|     4      | 0.1384 | 0.0091 | 0.2426 | 0.2902 |  0.3361 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,196,200
Trainable params: 3,200
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:51:17,960: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1641 | 0.0068 | 0.2942 | 0.3556 |  0.4088 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1654 | 0.0079 | 0.2932 | 0.3603 |  0.4164 |
|     1      | 0.1402 | 0.0062 | 0.2522 | 0.2876 |  0.3317 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1627 | 0.0081 | 0.2851 | 0.356  |  0.4143 |
|     1      | 0.1377 | 0.0073 | 0.2419 | 0.2844 |  0.3306 |
|     2      | 0.1395 | 0.0081 | 0.247  | 0.2852 |  0.3253 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1629 | 0.0081 | 0.2851 | 0.3552 |  0.4148 |
|     1      | 0.1365 | 0.007  | 0.2398 | 0.2844 |  0.3296 |
|     2      | 0.138  | 0.0059 | 0.2476 | 0.2855 |  0.3247 |
|     3      | 0.1388 | 0.0097 | 0.2444 | 0.2863 |  0.3255 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1611 | 0.0074 | 0.2817 | 0.3532 |  0.4125 |
|     1      | 0.1347 | 0.0067 | 0.2358 | 0.2793 |  0.3312 |
|     2      | 0.1388 | 0.0086 | 0.2468 | 0.2855 |  0.3218 |
|     3      | 0.138  | 0.0094 | 0.2406 | 0.2874 |  0.3285 |
|     4      | 0.1384 | 0.0091 | 0.2426 | 0.2902 |  0.3361 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:51:17,961: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 315.6532278060913  |   0.164   |    0.007     |    0.294     |     0.409     |
|    1     | 20.059396266937256 |   0.162   |    0.008     |    0.287     |     0.404     |
|    2     |  23.4098482131958  |   0.157   |    0.008     |    0.275     |     0.393     |
|    3     | 63.864216327667236 |   0.155   |    0.008     |    0.271     |     0.385     |
|    4     | 24.63702917098999  |   0.152   |    0.008     |    0.266     |     0.379     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:51:17,961: Sum_Training_Time:447.6237177848816
2025-01-07 18:51:17,961: Every_Training_Time:[315.6532278060913, 20.059396266937256, 23.4098482131958, 63.864216327667236, 24.63702917098999]
2025-01-07 18:51:17,961: Forward transfer: 0.060175000000000006 Backward transfer: -0.0025000000000000022
2025-01-07 18:51:30,664: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107185122/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=9, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 18:51:39,690: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 18:51:47,247: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 18:51:55,087: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-07 18:52:02,947: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.19	Best:13.18
2025-01-07 18:52:10,501: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.04	Hits@10:35.48	Best:14.04
2025-01-07 18:52:18,283: Snapshot:0	Epoch:5	Loss:0.534	translation_Loss:0.534	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.63	Hits@10:36.63	Best:14.63
2025-01-07 18:52:25,854: Snapshot:0	Epoch:6	Loss:0.349	translation_Loss:0.349	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.98	Hits@10:37.47	Best:14.98
2025-01-07 18:52:33,699: Snapshot:0	Epoch:7	Loss:0.244	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.21	Hits@10:38.0	Best:15.21
2025-01-07 18:52:41,245: Snapshot:0	Epoch:8	Loss:0.177	translation_Loss:0.177	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.36	Hits@10:38.26	Best:15.36
2025-01-07 18:52:49,037: Snapshot:0	Epoch:9	Loss:0.14	translation_Loss:0.14	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.52	Hits@10:38.62	Best:15.52
2025-01-07 18:52:56,819: Snapshot:0	Epoch:10	Loss:0.113	translation_Loss:0.113	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.64	Hits@10:38.69	Best:15.64
2025-01-07 18:53:04,373: Snapshot:0	Epoch:11	Loss:0.093	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.69	Hits@10:39.03	Best:15.69
2025-01-07 18:53:12,164: Snapshot:0	Epoch:12	Loss:0.078	translation_Loss:0.078	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.72	Hits@10:39.12	Best:15.72
2025-01-07 18:53:19,731: Snapshot:0	Epoch:13	Loss:0.07	translation_Loss:0.07	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.29	Best:15.79
2025-01-07 18:53:27,566: Snapshot:0	Epoch:14	Loss:0.059	translation_Loss:0.059	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.36	Best:15.79
2025-01-07 18:53:35,363: Snapshot:0	Epoch:15	Loss:0.052	translation_Loss:0.052	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.82	Hits@10:39.43	Best:15.82
2025-01-07 18:53:42,924: Snapshot:0	Epoch:16	Loss:0.049	translation_Loss:0.049	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.88	Hits@10:39.63	Best:15.88
2025-01-07 18:53:50,725: Snapshot:0	Epoch:17	Loss:0.044	translation_Loss:0.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.75	Best:15.92
2025-01-07 18:53:58,256: Snapshot:0	Epoch:18	Loss:0.041	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.74	Best:15.92
2025-01-07 18:54:06,042: Snapshot:0	Epoch:19	Loss:0.038	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.76	Best:15.96
2025-01-07 18:54:13,550: Snapshot:0	Epoch:20	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.95	Hits@10:39.92	Best:15.96
2025-01-07 18:54:21,348: Snapshot:0	Epoch:21	Loss:0.035	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.97	Hits@10:40.09	Best:15.97
2025-01-07 18:54:28,906: Snapshot:0	Epoch:22	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:40.13	Best:15.98
2025-01-07 18:54:36,759: Snapshot:0	Epoch:23	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:40.2	Best:16.03
2025-01-07 18:54:44,571: Snapshot:0	Epoch:24	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.08	Hits@10:40.3	Best:16.08
2025-01-07 18:54:52,121: Snapshot:0	Epoch:25	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.07	Hits@10:40.33	Best:16.08
2025-01-07 18:54:59,915: Snapshot:0	Epoch:26	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.11	Hits@10:40.36	Best:16.11
2025-01-07 18:55:07,516: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.44	Best:16.15
2025-01-07 18:55:15,276: Snapshot:0	Epoch:28	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.48	Best:16.15
2025-01-07 18:55:23,090: Snapshot:0	Epoch:29	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.45	Best:16.16
2025-01-07 18:55:30,609: Snapshot:0	Epoch:30	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.48	Best:16.16
2025-01-07 18:55:38,366: Snapshot:0	Epoch:31	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.56	Best:16.16
2025-01-07 18:55:45,935: Snapshot:0	Epoch:32	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.17	Hits@10:40.62	Best:16.17
2025-01-07 18:55:53,760: Snapshot:0	Epoch:33	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.59	Best:16.21
2025-01-07 18:56:01,338: Snapshot:0	Epoch:34	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.56	Best:16.23
2025-01-07 18:56:09,085: Snapshot:0	Epoch:35	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.79	Best:16.23
2025-01-07 18:56:16,642: Snapshot:0	Epoch:36	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.92	Best:16.25
2025-01-07 18:56:24,438: Snapshot:0	Epoch:37	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.29	Hits@10:40.93	Best:16.29
2025-01-07 18:56:31,959: Snapshot:0	Epoch:38	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.29	Hits@10:40.87	Best:16.29
2025-01-07 18:56:39,704: Snapshot:0	Epoch:39	Loss:0.023	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.86	Best:16.29
2025-01-07 18:56:47,492: Early Stopping! Snapshot: 0 Epoch: 40 Best Results: 16.29
2025-01-07 18:56:47,492: Start to training tokens! Snapshot: 0 Epoch: 40 Loss:0.022 MRR:16.27 Best Results: 16.29
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:56:47,492: Snapshot:0	Epoch:40	Loss:0.022	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.27	Hits@10:40.91	Best:16.29
2025-01-07 18:56:55,625: Snapshot:0	Epoch:41	Loss:22.263	translation_Loss:5.421	token_training_loss:16.842	distillation_Loss:0.0                                                   	MRR:16.27	Hits@10:40.91	Best:16.29
2025-01-07 18:57:03,420: End of token training: 0 Epoch: 42 Loss:5.972 MRR:16.27 Best Results: 16.29
2025-01-07 18:57:03,420: Snapshot:0	Epoch:42	Loss:5.972	translation_Loss:5.415	token_training_loss:0.557	distillation_Loss:0.0                                                           	MRR:16.27	Hits@10:40.91	Best:16.29
2025-01-07 18:57:03,520: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 18:57:07,912: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1646 | 0.0069 | 0.2932 | 0.3557 |  0.4112 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,921,400
Trainable params: 3,600
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:57:11,664: Snapshot:1	Epoch:0	Loss:2.753	translation_Loss:2.656	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:2.59	Hits@10:6.99	Best:2.59
2025-01-07 18:57:13,234: Snapshot:1	Epoch:1	Loss:1.785	translation_Loss:1.631	token_training_loss:0.0	distillation_Loss:0.154                                                   	MRR:7.42	Hits@10:19.68	Best:7.42
2025-01-07 18:57:14,796: Snapshot:1	Epoch:2	Loss:1.044	translation_Loss:0.834	token_training_loss:0.0	distillation_Loss:0.21                                                   	MRR:10.53	Hits@10:26.83	Best:10.53
2025-01-07 18:57:16,350: Snapshot:1	Epoch:3	Loss:0.594	translation_Loss:0.335	token_training_loss:0.0	distillation_Loss:0.259                                                   	MRR:12.21	Hits@10:30.22	Best:12.21
2025-01-07 18:57:17,926: Snapshot:1	Epoch:4	Loss:0.417	translation_Loss:0.138	token_training_loss:0.0	distillation_Loss:0.278                                                   	MRR:12.99	Hits@10:31.77	Best:12.99
2025-01-07 18:57:19,520: Snapshot:1	Epoch:5	Loss:0.329	translation_Loss:0.064	token_training_loss:0.0	distillation_Loss:0.266                                                   	MRR:13.37	Hits@10:32.31	Best:13.37
2025-01-07 18:57:21,115: Snapshot:1	Epoch:6	Loss:0.26	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.226                                                   	MRR:13.56	Hits@10:32.8	Best:13.56
2025-01-07 18:57:22,727: Snapshot:1	Epoch:7	Loss:0.204	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:13.58	Hits@10:33.15	Best:13.58
2025-01-07 18:57:24,224: Snapshot:1	Epoch:8	Loss:0.159	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:13.54	Hits@10:33.33	Best:13.58
2025-01-07 18:57:25,938: Snapshot:1	Epoch:9	Loss:0.125	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.105                                                   	MRR:13.44	Hits@10:33.36	Best:13.58
2025-01-07 18:57:27,434: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.58
2025-01-07 18:57:27,434: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.101 MRR:13.48 Best Results: 13.58
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:57:27,435: Snapshot:1	Epoch:10	Loss:0.101	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:13.48	Hits@10:33.39	Best:13.58
2025-01-07 18:57:28,886: Snapshot:1	Epoch:11	Loss:8.377	translation_Loss:1.19	token_training_loss:7.186	distillation_Loss:0.0                                                   	MRR:13.48	Hits@10:33.39	Best:13.58
2025-01-07 18:57:30,325: End of token training: 1 Epoch: 12 Loss:5.951 MRR:13.48 Best Results: 13.58
2025-01-07 18:57:30,326: Snapshot:1	Epoch:12	Loss:5.951	translation_Loss:1.19	token_training_loss:4.761	distillation_Loss:0.0                                                           	MRR:13.48	Hits@10:33.39	Best:13.58
2025-01-07 18:57:30,405: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 18:57:35,124: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1667 | 0.0082 | 0.2958 | 0.3616 |  0.4165 |
|     1      | 0.1379 | 0.0062 | 0.2468 | 0.2882 |  0.3293 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,740,000
Trainable params: 3,600
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:57:39,103: Snapshot:2	Epoch:0	Loss:2.67	translation_Loss:2.611	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:3.0	Hits@10:7.96	Best:3.0
2025-01-07 18:57:40,820: Snapshot:2	Epoch:1	Loss:1.681	translation_Loss:1.543	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:7.8	Hits@10:19.7	Best:7.8
2025-01-07 18:57:42,540: Snapshot:2	Epoch:2	Loss:0.878	translation_Loss:0.714	token_training_loss:0.0	distillation_Loss:0.164                                                   	MRR:10.63	Hits@10:25.91	Best:10.63
2025-01-07 18:57:44,267: Snapshot:2	Epoch:3	Loss:0.429	translation_Loss:0.239	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:12.19	Hits@10:28.44	Best:12.19
2025-01-07 18:57:45,986: Snapshot:2	Epoch:4	Loss:0.295	translation_Loss:0.086	token_training_loss:0.0	distillation_Loss:0.21                                                   	MRR:13.01	Hits@10:29.73	Best:13.01
2025-01-07 18:57:47,711: Snapshot:2	Epoch:5	Loss:0.25	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.211                                                   	MRR:13.34	Hits@10:30.51	Best:13.34
2025-01-07 18:57:49,430: Snapshot:2	Epoch:6	Loss:0.214	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:13.48	Hits@10:30.83	Best:13.48
2025-01-07 18:57:51,177: Snapshot:2	Epoch:7	Loss:0.183	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.171                                                   	MRR:13.54	Hits@10:31.1	Best:13.54
2025-01-07 18:57:52,777: Snapshot:2	Epoch:8	Loss:0.153	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:13.54	Hits@10:31.37	Best:13.54
2025-01-07 18:57:54,380: Snapshot:2	Epoch:9	Loss:0.125	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.117                                                   	MRR:13.51	Hits@10:31.45	Best:13.54
2025-01-07 18:57:55,978: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 13.54
2025-01-07 18:57:55,978: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.102 MRR:13.43 Best Results: 13.54
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:57:55,979: Snapshot:2	Epoch:10	Loss:0.102	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.094                                                   	MRR:13.43	Hits@10:31.51	Best:13.54
2025-01-07 18:57:57,515: Snapshot:2	Epoch:11	Loss:8.345	translation_Loss:1.159	token_training_loss:7.186	distillation_Loss:0.0                                                   	MRR:13.43	Hits@10:31.51	Best:13.54
2025-01-07 18:57:59,289: End of token training: 2 Epoch: 12 Loss:5.916 MRR:13.43 Best Results: 13.54
2025-01-07 18:57:59,290: Snapshot:2	Epoch:12	Loss:5.916	translation_Loss:1.165	token_training_loss:4.751	distillation_Loss:0.0                                                           	MRR:13.43	Hits@10:31.51	Best:13.54
2025-01-07 18:57:59,366: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 18:58:04,962: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.165  | 0.0082 | 0.2907 | 0.3587 |  0.4177 |
|     1      | 0.1364 | 0.0078 | 0.2374 | 0.286  |  0.3331 |
|     2      | 0.1399 | 0.0073 | 0.2522 | 0.2868 |  0.3269 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,800
Trainable params: 3,600
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:58:09,426: Snapshot:3	Epoch:0	Loss:2.673	translation_Loss:2.575	token_training_loss:0.0	distillation_Loss:0.098                                                   	MRR:2.95	Hits@10:7.1	Best:2.95
2025-01-07 18:58:11,289: Snapshot:3	Epoch:1	Loss:1.655	translation_Loss:1.474	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:7.48	Hits@10:18.74	Best:7.48
2025-01-07 18:58:13,142: Snapshot:3	Epoch:2	Loss:0.868	translation_Loss:0.651	token_training_loss:0.0	distillation_Loss:0.217                                                   	MRR:10.51	Hits@10:25.11	Best:10.51
2025-01-07 18:58:15,005: Snapshot:3	Epoch:3	Loss:0.469	translation_Loss:0.212	token_training_loss:0.0	distillation_Loss:0.256                                                   	MRR:11.98	Hits@10:27.72	Best:11.98
2025-01-07 18:58:16,872: Snapshot:3	Epoch:4	Loss:0.352	translation_Loss:0.082	token_training_loss:0.0	distillation_Loss:0.27                                                   	MRR:12.7	Hits@10:29.09	Best:12.7
2025-01-07 18:58:18,683: Snapshot:3	Epoch:5	Loss:0.295	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:13.03	Hits@10:29.54	Best:13.03
2025-01-07 18:58:20,531: Snapshot:3	Epoch:6	Loss:0.241	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.216                                                   	MRR:13.18	Hits@10:30.3	Best:13.18
2025-01-07 18:58:22,378: Snapshot:3	Epoch:7	Loss:0.191	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:13.27	Hits@10:30.4	Best:13.27
2025-01-07 18:58:24,092: Snapshot:3	Epoch:8	Loss:0.149	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.135                                                   	MRR:13.22	Hits@10:30.59	Best:13.27
2025-01-07 18:58:25,805: Snapshot:3	Epoch:9	Loss:0.117	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:13.27	Hits@10:30.73	Best:13.27
2025-01-07 18:58:28,035: Snapshot:3	Epoch:10	Loss:0.093	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:13.31	Hits@10:30.81	Best:13.31
2025-01-07 18:58:29,788: Snapshot:3	Epoch:11	Loss:0.078	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:13.25	Hits@10:30.81	Best:13.31
2025-01-07 18:58:31,496: Snapshot:3	Epoch:12	Loss:0.066	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.054                                                   	MRR:13.31	Hits@10:30.91	Best:13.31
2025-01-07 18:58:33,339: Snapshot:3	Epoch:13	Loss:0.06	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.047                                                   	MRR:13.38	Hits@10:31.13	Best:13.38
2025-01-07 18:58:35,210: Snapshot:3	Epoch:14	Loss:0.055	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:13.41	Hits@10:31.05	Best:13.41
2025-01-07 18:58:37,067: Snapshot:3	Epoch:15	Loss:0.05	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.04                                                   	MRR:13.44	Hits@10:31.08	Best:13.44
2025-01-07 18:58:38,777: Snapshot:3	Epoch:16	Loss:0.046	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.037                                                   	MRR:13.44	Hits@10:31.1	Best:13.44
2025-01-07 18:58:40,645: Snapshot:3	Epoch:17	Loss:0.044	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.035                                                   	MRR:13.46	Hits@10:31.18	Best:13.46
2025-01-07 18:58:42,499: Snapshot:3	Epoch:18	Loss:0.042	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.033                                                   	MRR:13.51	Hits@10:31.08	Best:13.51
2025-01-07 18:58:44,347: Snapshot:3	Epoch:19	Loss:0.041	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.032                                                   	MRR:13.54	Hits@10:31.21	Best:13.54
2025-01-07 18:58:46,191: Snapshot:3	Epoch:20	Loss:0.04	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.03                                                   	MRR:13.56	Hits@10:31.24	Best:13.56
2025-01-07 18:58:48,254: Snapshot:3	Epoch:21	Loss:0.038	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.03                                                   	MRR:13.61	Hits@10:31.16	Best:13.61
2025-01-07 18:58:49,974: Snapshot:3	Epoch:22	Loss:0.037	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:13.6	Hits@10:31.34	Best:13.61
2025-01-07 18:58:51,708: Snapshot:3	Epoch:23	Loss:0.037	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.028                                                   	MRR:13.58	Hits@10:31.24	Best:13.61
2025-01-07 18:58:53,439: Early Stopping! Snapshot: 3 Epoch: 24 Best Results: 13.61
2025-01-07 18:58:53,439: Start to training tokens! Snapshot: 3 Epoch: 24 Loss:0.036 MRR:13.54 Best Results: 13.61
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:58:53,440: Snapshot:3	Epoch:24	Loss:0.036	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.028                                                   	MRR:13.54	Hits@10:31.37	Best:13.61
2025-01-07 18:58:55,082: Snapshot:3	Epoch:25	Loss:8.227	translation_Loss:1.14	token_training_loss:7.087	distillation_Loss:0.0                                                   	MRR:13.54	Hits@10:31.37	Best:13.61
2025-01-07 18:58:56,740: End of token training: 3 Epoch: 26 Loss:5.746 MRR:13.54 Best Results: 13.61
2025-01-07 18:58:56,740: Snapshot:3	Epoch:26	Loss:5.746	translation_Loss:1.14	token_training_loss:4.606	distillation_Loss:0.0                                                           	MRR:13.54	Hits@10:31.37	Best:13.61
2025-01-07 18:58:56,840: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 18:59:03,585: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1649 | 0.0079 | 0.2903 | 0.3587 |  0.4185 |
|     1      | 0.1359 | 0.0073 | 0.2352 | 0.2844 |  0.3323 |
|     2      | 0.1389 | 0.0075 | 0.2492 | 0.2871 |  0.3274 |
|     3      | 0.1382 | 0.0081 | 0.2449 | 0.2898 |  0.3255 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,377,600
Trainable params: 3,600
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 18:59:07,782: Snapshot:4	Epoch:0	Loss:2.635	translation_Loss:2.537	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:10.17	Hits@10:24.73	Best:10.17
2025-01-07 18:59:09,722: Snapshot:4	Epoch:1	Loss:1.603	translation_Loss:1.413	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:11.92	Hits@10:28.52	Best:11.92
2025-01-07 18:59:11,914: Snapshot:4	Epoch:2	Loss:0.797	translation_Loss:0.58	token_training_loss:0.0	distillation_Loss:0.217                                                   	MRR:12.96	Hits@10:30.7	Best:12.96
2025-01-07 18:59:13,879: Snapshot:4	Epoch:3	Loss:0.413	translation_Loss:0.166	token_training_loss:0.0	distillation_Loss:0.246                                                   	MRR:13.57	Hits@10:31.61	Best:13.57
2025-01-07 18:59:15,847: Snapshot:4	Epoch:4	Loss:0.315	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.256                                                   	MRR:13.94	Hits@10:32.47	Best:13.94
2025-01-07 18:59:17,856: Snapshot:4	Epoch:5	Loss:0.27	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.238                                                   	MRR:14.13	Hits@10:32.96	Best:14.13
2025-01-07 18:59:19,826: Snapshot:4	Epoch:6	Loss:0.223	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:14.17	Hits@10:33.36	Best:14.17
2025-01-07 18:59:21,802: Snapshot:4	Epoch:7	Loss:0.178	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.165                                                   	MRR:14.21	Hits@10:33.41	Best:14.21
2025-01-07 18:59:23,779: Snapshot:4	Epoch:8	Loss:0.141	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.13                                                   	MRR:14.23	Hits@10:33.49	Best:14.23
2025-01-07 18:59:25,611: Snapshot:4	Epoch:9	Loss:0.11	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.1                                                   	MRR:14.16	Hits@10:33.49	Best:14.23
2025-01-07 18:59:27,426: Snapshot:4	Epoch:10	Loss:0.088	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.077                                                   	MRR:14.15	Hits@10:33.52	Best:14.23
2025-01-07 18:59:29,264: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 14.23
2025-01-07 18:59:29,265: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:0.073 MRR:14.19 Best Results: 14.23
Token added to optimizer, embeddings excluded successfully.
2025-01-07 18:59:29,265: Snapshot:4	Epoch:11	Loss:0.073	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:14.19	Hits@10:33.44	Best:14.23
2025-01-07 18:59:31,014: Snapshot:4	Epoch:12	Loss:8.278	translation_Loss:1.136	token_training_loss:7.142	distillation_Loss:0.0                                                   	MRR:14.19	Hits@10:33.44	Best:14.23
2025-01-07 18:59:33,054: End of token training: 4 Epoch: 13 Loss:5.833 MRR:14.19 Best Results: 14.23
2025-01-07 18:59:33,054: Snapshot:4	Epoch:13	Loss:5.833	translation_Loss:1.136	token_training_loss:4.697	distillation_Loss:0.0                                                           	MRR:14.19	Hits@10:33.44	Best:14.23
2025-01-07 18:59:33,153: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 18:59:40,946: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1645 | 0.0083 | 0.2895 | 0.3578 |  0.4178 |
|     1      | 0.1361 | 0.0073 | 0.2355 | 0.2879 |  0.3333 |
|     2      | 0.139  | 0.0083 | 0.2508 | 0.2858 |  0.3272 |
|     3      | 0.1393 | 0.0094 | 0.2441 | 0.2903 |  0.3263 |
|     4      | 0.1398 | 0.0091 | 0.2453 | 0.2926 |  0.3412 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,196,600
Trainable params: 3,600
Non-trainable params: 8,193,000
=================================================================
2025-01-07 18:59:40,948: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1646 | 0.0069 | 0.2932 | 0.3557 |  0.4112 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1667 | 0.0082 | 0.2958 | 0.3616 |  0.4165 |
|     1      | 0.1379 | 0.0062 | 0.2468 | 0.2882 |  0.3293 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.165  | 0.0082 | 0.2907 | 0.3587 |  0.4177 |
|     1      | 0.1364 | 0.0078 | 0.2374 | 0.286  |  0.3331 |
|     2      | 0.1399 | 0.0073 | 0.2522 | 0.2868 |  0.3269 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1649 | 0.0079 | 0.2903 | 0.3587 |  0.4185 |
|     1      | 0.1359 | 0.0073 | 0.2352 | 0.2844 |  0.3323 |
|     2      | 0.1389 | 0.0075 | 0.2492 | 0.2871 |  0.3274 |
|     3      | 0.1382 | 0.0081 | 0.2449 | 0.2898 |  0.3255 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1645 | 0.0083 | 0.2895 | 0.3578 |  0.4178 |
|     1      | 0.1361 | 0.0073 | 0.2355 | 0.2879 |  0.3333 |
|     2      | 0.139  | 0.0083 | 0.2508 | 0.2858 |  0.3272 |
|     3      | 0.1393 | 0.0094 | 0.2441 | 0.2903 |  0.3263 |
|     4      | 0.1398 | 0.0091 | 0.2453 | 0.2926 |  0.3412 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 18:59:40,949: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 332.75493144989014 |   0.165   |    0.007     |    0.293     |     0.411     |
|    1     |  21.3917453289032  |   0.163   |    0.008     |    0.289     |     0.404     |
|    2     | 23.24714732170105  |   0.158   |    0.008     |    0.279     |     0.396     |
|    3     | 50.50481867790222  |   0.156   |    0.008     |    0.275     |     0.388     |
|    4     | 28.35258913040161  |   0.154   |    0.008     |    0.271     |     0.383     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 18:59:40,949: Sum_Training_Time:456.2512319087982
2025-01-07 18:59:40,949: Every_Training_Time:[332.75493144989014, 21.3917453289032, 23.24714732170105, 50.50481867790222, 28.35258913040161]
2025-01-07 18:59:40,949: Forward transfer: 0.0607 Backward transfer: -0.0004249999999999879
2025-01-07 18:59:53,636: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107185944/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 19:00:02,590: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-07 19:00:10,174: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-07 19:00:17,922: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.92	Best:10.92
2025-01-07 19:00:25,715: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.18	Hits@10:33.17	Best:13.18
2025-01-07 19:00:33,206: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.03	Hits@10:35.5	Best:14.03
2025-01-07 19:00:40,891: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.63	Hits@10:36.61	Best:14.63
2025-01-07 19:00:48,359: Snapshot:0	Epoch:6	Loss:0.347	translation_Loss:0.347	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.01	Hits@10:37.4	Best:15.01
2025-01-07 19:00:56,132: Snapshot:0	Epoch:7	Loss:0.244	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.21	Hits@10:38.07	Best:15.21
2025-01-07 19:01:03,622: Snapshot:0	Epoch:8	Loss:0.179	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.36	Hits@10:38.25	Best:15.36
2025-01-07 19:01:11,423: Snapshot:0	Epoch:9	Loss:0.139	translation_Loss:0.139	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.51	Hits@10:38.4	Best:15.51
2025-01-07 19:01:19,130: Snapshot:0	Epoch:10	Loss:0.113	translation_Loss:0.113	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.63	Hits@10:38.73	Best:15.63
2025-01-07 19:01:26,657: Snapshot:0	Epoch:11	Loss:0.093	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.73	Hits@10:38.91	Best:15.73
2025-01-07 19:01:34,345: Snapshot:0	Epoch:12	Loss:0.078	translation_Loss:0.078	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.76	Hits@10:39.05	Best:15.76
2025-01-07 19:01:41,804: Snapshot:0	Epoch:13	Loss:0.07	translation_Loss:0.07	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.79	Hits@10:39.23	Best:15.79
2025-01-07 19:01:49,528: Snapshot:0	Epoch:14	Loss:0.06	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.82	Hits@10:39.42	Best:15.82
2025-01-07 19:01:57,263: Snapshot:0	Epoch:15	Loss:0.052	translation_Loss:0.052	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.85	Hits@10:39.57	Best:15.85
2025-01-07 19:02:04,745: Snapshot:0	Epoch:16	Loss:0.048	translation_Loss:0.048	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.88	Hits@10:39.73	Best:15.88
2025-01-07 19:02:12,425: Snapshot:0	Epoch:17	Loss:0.046	translation_Loss:0.046	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.95	Hits@10:39.75	Best:15.95
2025-01-07 19:02:19,900: Snapshot:0	Epoch:18	Loss:0.04	translation_Loss:0.04	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.99	Hits@10:39.76	Best:15.99
2025-01-07 19:02:27,594: Snapshot:0	Epoch:19	Loss:0.038	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:39.94	Best:16.0
2025-01-07 19:02:35,068: Snapshot:0	Epoch:20	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.02	Hits@10:39.91	Best:16.02
2025-01-07 19:02:42,752: Snapshot:0	Epoch:21	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:39.96	Best:16.03
2025-01-07 19:02:50,187: Snapshot:0	Epoch:22	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:40.0	Best:16.03
2025-01-07 19:02:57,959: Snapshot:0	Epoch:23	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.14	Best:16.06
2025-01-07 19:03:05,618: Snapshot:0	Epoch:24	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.17	Best:16.06
2025-01-07 19:03:13,062: Snapshot:0	Epoch:25	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.07	Hits@10:40.16	Best:16.07
2025-01-07 19:03:20,736: Snapshot:0	Epoch:26	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.1	Hits@10:40.29	Best:16.1
2025-01-07 19:03:28,245: Snapshot:0	Epoch:27	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.29	Best:16.15
2025-01-07 19:03:35,892: Snapshot:0	Epoch:28	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.5	Best:16.15
2025-01-07 19:03:43,588: Snapshot:0	Epoch:29	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.47	Best:16.19
2025-01-07 19:03:51,015: Snapshot:0	Epoch:30	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.58	Best:16.19
2025-01-07 19:03:58,685: Snapshot:0	Epoch:31	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.62	Best:16.19
2025-01-07 19:04:06,103: Early Stopping! Snapshot: 0 Epoch: 32 Best Results: 16.19
2025-01-07 19:04:06,104: Start to training tokens! Snapshot: 0 Epoch: 32 Loss:0.024 MRR:16.18 Best Results: 16.19
Token added to optimizer, embeddings excluded successfully.
2025-01-07 19:04:06,104: Snapshot:0	Epoch:32	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.65	Best:16.19
2025-01-07 19:04:14,356: Snapshot:0	Epoch:33	Loss:22.515	translation_Loss:5.486	token_training_loss:17.029	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.65	Best:16.19
2025-01-07 19:04:21,967: End of token training: 0 Epoch: 34 Loss:6.064 MRR:16.18 Best Results: 16.19
2025-01-07 19:04:21,967: Snapshot:0	Epoch:34	Loss:6.064	translation_Loss:5.483	token_training_loss:0.581	distillation_Loss:0.0                                                           	MRR:16.18	Hits@10:40.65	Best:16.19
2025-01-07 19:04:22,065: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 19:04:26,121: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1635 | 0.0061 | 0.294  | 0.354  |  0.4068 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,921,800
Trainable params: 4,000
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 19:04:29,865: Snapshot:1	Epoch:0	Loss:2.754	translation_Loss:2.648	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:2.49	Hits@10:6.45	Best:2.49
2025-01-07 19:04:31,430: Snapshot:1	Epoch:1	Loss:1.786	translation_Loss:1.621	token_training_loss:0.0	distillation_Loss:0.165                                                   	MRR:7.37	Hits@10:19.38	Best:7.37
2025-01-07 19:04:33,002: Snapshot:1	Epoch:2	Loss:1.048	translation_Loss:0.821	token_training_loss:0.0	distillation_Loss:0.227                                                   	MRR:10.6	Hits@10:26.67	Best:10.6
2025-01-07 19:04:34,556: Snapshot:1	Epoch:3	Loss:0.603	translation_Loss:0.327	token_training_loss:0.0	distillation_Loss:0.276                                                   	MRR:12.31	Hits@10:30.11	Best:12.31
2025-01-07 19:04:36,125: Snapshot:1	Epoch:4	Loss:0.426	translation_Loss:0.133	token_training_loss:0.0	distillation_Loss:0.293                                                   	MRR:13.1	Hits@10:31.75	Best:13.1
2025-01-07 19:04:37,836: Snapshot:1	Epoch:5	Loss:0.339	translation_Loss:0.062	token_training_loss:0.0	distillation_Loss:0.277                                                   	MRR:13.47	Hits@10:32.82	Best:13.47
2025-01-07 19:04:39,433: Snapshot:1	Epoch:6	Loss:0.266	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.232                                                   	MRR:13.59	Hits@10:33.09	Best:13.59
2025-01-07 19:04:40,917: Snapshot:1	Epoch:7	Loss:0.208	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.182                                                   	MRR:13.58	Hits@10:33.47	Best:13.59
2025-01-07 19:04:42,405: Snapshot:1	Epoch:8	Loss:0.162	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:13.57	Hits@10:33.39	Best:13.59
2025-01-07 19:04:43,893: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 13.59
2025-01-07 19:04:43,893: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:0.129 MRR:13.51 Best Results: 13.59
Token added to optimizer, embeddings excluded successfully.
2025-01-07 19:04:43,893: Snapshot:1	Epoch:9	Loss:0.129	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.107                                                   	MRR:13.51	Hits@10:33.44	Best:13.59
2025-01-07 19:04:45,317: Snapshot:1	Epoch:10	Loss:8.394	translation_Loss:1.196	token_training_loss:7.197	distillation_Loss:0.0                                                   	MRR:13.51	Hits@10:33.44	Best:13.59
2025-01-07 19:04:46,961: End of token training: 1 Epoch: 11 Loss:6.015 MRR:13.51 Best Results: 13.59
2025-01-07 19:04:46,961: Snapshot:1	Epoch:11	Loss:6.015	translation_Loss:1.197	token_training_loss:4.818	distillation_Loss:0.0                                                           	MRR:13.51	Hits@10:33.44	Best:13.59
2025-01-07 19:04:47,041: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 19:04:51,516: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1656 | 0.0076 | 0.2951 | 0.3602 |  0.4136 |
|     1      | 0.1373 | 0.0065 | 0.246  | 0.2871 |  0.3285 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,740,400
Trainable params: 4,000
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 19:04:55,494: Snapshot:2	Epoch:0	Loss:2.666	translation_Loss:2.6	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:2.94	Hits@10:7.85	Best:2.94
2025-01-07 19:04:57,203: Snapshot:2	Epoch:1	Loss:1.673	translation_Loss:1.525	token_training_loss:0.0	distillation_Loss:0.148                                                   	MRR:7.67	Hits@10:19.54	Best:7.67
2025-01-07 19:04:59,120: Snapshot:2	Epoch:2	Loss:0.872	translation_Loss:0.697	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:10.73	Hits@10:25.91	Best:10.73
2025-01-07 19:05:00,797: Snapshot:2	Epoch:3	Loss:0.434	translation_Loss:0.231	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:12.12	Hits@10:28.36	Best:12.12
2025-01-07 19:05:02,524: Snapshot:2	Epoch:4	Loss:0.307	translation_Loss:0.083	token_training_loss:0.0	distillation_Loss:0.224                                                   	MRR:12.89	Hits@10:29.6	Best:12.89
2025-01-07 19:05:04,238: Snapshot:2	Epoch:5	Loss:0.261	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.224                                                   	MRR:13.24	Hits@10:30.24	Best:13.24
2025-01-07 19:05:05,961: Snapshot:2	Epoch:6	Loss:0.225	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.205                                                   	MRR:13.41	Hits@10:30.75	Best:13.41
2025-01-07 19:05:07,647: Snapshot:2	Epoch:7	Loss:0.191	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:13.43	Hits@10:31.1	Best:13.43
2025-01-07 19:05:09,354: Snapshot:2	Epoch:8	Loss:0.159	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.149                                                   	MRR:13.45	Hits@10:31.26	Best:13.45
2025-01-07 19:05:10,950: Snapshot:2	Epoch:9	Loss:0.128	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.12                                                   	MRR:13.42	Hits@10:31.29	Best:13.45
2025-01-07 19:05:12,562: Snapshot:2	Epoch:10	Loss:0.105	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.095                                                   	MRR:13.4	Hits@10:31.48	Best:13.45
2025-01-07 19:05:14,181: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 13.45
2025-01-07 19:05:14,181: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:0.085 MRR:13.38 Best Results: 13.45
Token added to optimizer, embeddings excluded successfully.
2025-01-07 19:05:14,181: Snapshot:2	Epoch:11	Loss:0.085	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.076                                                   	MRR:13.38	Hits@10:31.45	Best:13.45
2025-01-07 19:05:15,751: Snapshot:2	Epoch:12	Loss:8.367	translation_Loss:1.16	token_training_loss:7.207	distillation_Loss:0.0                                                   	MRR:13.38	Hits@10:31.45	Best:13.45
2025-01-07 19:05:17,284: End of token training: 2 Epoch: 13 Loss:5.988 MRR:13.38 Best Results: 13.45
2025-01-07 19:05:17,285: Snapshot:2	Epoch:13	Loss:5.988	translation_Loss:1.163	token_training_loss:4.824	distillation_Loss:0.0                                                           	MRR:13.38	Hits@10:31.45	Best:13.45
2025-01-07 19:05:17,362: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 19:05:23,176: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.0074 | 0.2909 | 0.3587 |  0.414  |
|     1      | 0.1356 | 0.0075 | 0.236  | 0.2863 |  0.3312 |
|     2      | 0.1393 | 0.0078 | 0.2522 | 0.2825 |  0.3223 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,559,200
Trainable params: 4,000
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 19:05:27,614: Snapshot:3	Epoch:0	Loss:2.671	translation_Loss:2.565	token_training_loss:0.0	distillation_Loss:0.107                                                   	MRR:2.88	Hits@10:7.31	Best:2.88
2025-01-07 19:05:29,485: Snapshot:3	Epoch:1	Loss:1.657	translation_Loss:1.465	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:7.34	Hits@10:18.44	Best:7.34
2025-01-07 19:05:31,330: Snapshot:3	Epoch:2	Loss:0.871	translation_Loss:0.64	token_training_loss:0.0	distillation_Loss:0.231                                                   	MRR:10.39	Hits@10:24.52	Best:10.39
2025-01-07 19:05:33,162: Snapshot:3	Epoch:3	Loss:0.48	translation_Loss:0.207	token_training_loss:0.0	distillation_Loss:0.273                                                   	MRR:11.87	Hits@10:27.02	Best:11.87
2025-01-07 19:05:34,992: Snapshot:3	Epoch:4	Loss:0.365	translation_Loss:0.08	token_training_loss:0.0	distillation_Loss:0.286                                                   	MRR:12.63	Hits@10:28.52	Best:12.63
2025-01-07 19:05:36,850: Snapshot:3	Epoch:5	Loss:0.304	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.266                                                   	MRR:12.99	Hits@10:29.33	Best:12.99
2025-01-07 19:05:38,691: Snapshot:3	Epoch:6	Loss:0.248	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.223                                                   	MRR:13.09	Hits@10:29.76	Best:13.09
2025-01-07 19:05:40,528: Snapshot:3	Epoch:7	Loss:0.194	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:13.15	Hits@10:29.97	Best:13.15
2025-01-07 19:05:42,441: Snapshot:3	Epoch:8	Loss:0.152	translation_Loss:0.016	token_training_loss:0.0	distillation_Loss:0.136                                                   	MRR:13.12	Hits@10:30.3	Best:13.15
2025-01-07 19:05:44,409: Snapshot:3	Epoch:9	Loss:0.118	translation_Loss:0.016	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:13.03	Hits@10:30.38	Best:13.15
2025-01-07 19:05:46,122: Early Stopping! Snapshot: 3 Epoch: 10 Best Results: 13.15
2025-01-07 19:05:46,122: Start to training tokens! Snapshot: 3 Epoch: 10 Loss:0.095 MRR:13.06 Best Results: 13.15
Token added to optimizer, embeddings excluded successfully.
2025-01-07 19:05:46,123: Snapshot:3	Epoch:10	Loss:0.095	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:13.06	Hits@10:30.3	Best:13.15
2025-01-07 19:05:47,780: Snapshot:3	Epoch:11	Loss:8.385	translation_Loss:1.179	token_training_loss:7.206	distillation_Loss:0.0                                                   	MRR:13.06	Hits@10:30.3	Best:13.15
2025-01-07 19:05:49,418: End of token training: 3 Epoch: 12 Loss:5.966 MRR:13.06 Best Results: 13.15
2025-01-07 19:05:49,418: Snapshot:3	Epoch:12	Loss:5.966	translation_Loss:1.177	token_training_loss:4.789	distillation_Loss:0.0                                                           	MRR:13.06	Hits@10:30.3	Best:13.15
2025-01-07 19:05:49,510: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 19:05:56,073: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1631 | 0.0079 | 0.2885 | 0.357  |  0.4124 |
|     1      | 0.1344 | 0.0075 | 0.2341 | 0.2836 |  0.3285 |
|     2      | 0.1376 | 0.0078 | 0.2481 | 0.2801 |  0.3234 |
|     3      | 0.1363 | 0.0083 | 0.2419 | 0.2831 |  0.3215 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,378,000
Trainable params: 4,000
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 19:06:00,459: Snapshot:4	Epoch:0	Loss:2.61	translation_Loss:2.504	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:10.09	Hits@10:24.49	Best:10.09
2025-01-07 19:06:02,414: Snapshot:4	Epoch:1	Loss:1.596	translation_Loss:1.396	token_training_loss:0.0	distillation_Loss:0.2                                                   	MRR:11.77	Hits@10:28.23	Best:11.77
2025-01-07 19:06:04,382: Snapshot:4	Epoch:2	Loss:0.805	translation_Loss:0.575	token_training_loss:0.0	distillation_Loss:0.23                                                   	MRR:12.91	Hits@10:30.46	Best:12.91
2025-01-07 19:06:06,628: Snapshot:4	Epoch:3	Loss:0.431	translation_Loss:0.169	token_training_loss:0.0	distillation_Loss:0.262                                                   	MRR:13.44	Hits@10:31.42	Best:13.44
2025-01-07 19:06:08,586: Snapshot:4	Epoch:4	Loss:0.336	translation_Loss:0.066	token_training_loss:0.0	distillation_Loss:0.27                                                   	MRR:13.74	Hits@10:31.88	Best:13.74
2025-01-07 19:06:10,547: Snapshot:4	Epoch:5	Loss:0.284	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.249                                                   	MRR:13.88	Hits@10:32.28	Best:13.88
2025-01-07 19:06:12,527: Snapshot:4	Epoch:6	Loss:0.234	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.21                                                   	MRR:13.98	Hits@10:32.66	Best:13.98
2025-01-07 19:06:14,477: Snapshot:4	Epoch:7	Loss:0.186	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.169                                                   	MRR:14.0	Hits@10:32.53	Best:14.0
2025-01-07 19:06:16,437: Snapshot:4	Epoch:8	Loss:0.147	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.132                                                   	MRR:14.04	Hits@10:32.61	Best:14.04
2025-01-07 19:06:18,258: Snapshot:4	Epoch:9	Loss:0.116	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.101                                                   	MRR:14.04	Hits@10:32.72	Best:14.04
2025-01-07 19:06:20,076: Snapshot:4	Epoch:10	Loss:0.093	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.079                                                   	MRR:14.0	Hits@10:32.8	Best:14.04
2025-01-07 19:06:21,899: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 14.04
2025-01-07 19:06:21,899: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:0.078 MRR:14.0 Best Results: 14.04
Token added to optimizer, embeddings excluded successfully.
2025-01-07 19:06:21,899: Snapshot:4	Epoch:11	Loss:0.078	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:14.0	Hits@10:32.69	Best:14.04
2025-01-07 19:06:23,643: Snapshot:4	Epoch:12	Loss:8.342	translation_Loss:1.149	token_training_loss:7.192	distillation_Loss:0.0                                                   	MRR:14.0	Hits@10:32.69	Best:14.04
2025-01-07 19:06:25,405: End of token training: 4 Epoch: 13 Loss:5.96 MRR:14.0 Best Results: 14.04
2025-01-07 19:06:25,405: Snapshot:4	Epoch:13	Loss:5.96	translation_Loss:1.149	token_training_loss:4.812	distillation_Loss:0.0                                                           	MRR:14.0	Hits@10:32.69	Best:14.04
2025-01-07 19:06:25,483: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 19:06:33,442: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1623 | 0.0076 | 0.2862 | 0.3558 |  0.413  |
|     1      | 0.1328 | 0.0062 | 0.2328 | 0.2836 |  0.3253 |
|     2      | 0.1372 | 0.0075 | 0.246  | 0.2804 |  0.3207 |
|     3      | 0.1365 | 0.0089 | 0.2409 | 0.2836 |  0.325  |
|     4      | 0.1393 | 0.0089 | 0.2493 | 0.2963 |  0.3337 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,197,000
Trainable params: 4,000
Non-trainable params: 8,193,000
=================================================================
2025-01-07 19:06:33,445: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1635 | 0.0061 | 0.294  | 0.354  |  0.4068 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1656 | 0.0076 | 0.2951 | 0.3602 |  0.4136 |
|     1      | 0.1373 | 0.0065 | 0.246  | 0.2871 |  0.3285 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.0074 | 0.2909 | 0.3587 |  0.414  |
|     1      | 0.1356 | 0.0075 | 0.236  | 0.2863 |  0.3312 |
|     2      | 0.1393 | 0.0078 | 0.2522 | 0.2825 |  0.3223 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1631 | 0.0079 | 0.2885 | 0.357  |  0.4124 |
|     1      | 0.1344 | 0.0075 | 0.2341 | 0.2836 |  0.3285 |
|     2      | 0.1376 | 0.0078 | 0.2481 | 0.2801 |  0.3234 |
|     3      | 0.1363 | 0.0083 | 0.2419 | 0.2831 |  0.3215 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1623 | 0.0076 | 0.2862 | 0.3558 |  0.413  |
|     1      | 0.1328 | 0.0062 | 0.2328 | 0.2836 |  0.3253 |
|     2      | 0.1372 | 0.0075 | 0.246  | 0.2804 |  0.3207 |
|     3      | 0.1365 | 0.0089 | 0.2409 | 0.2836 |  0.325  |
|     4      | 0.1393 | 0.0089 | 0.2493 | 0.2963 |  0.3337 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 19:06:33,446: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 268.33082604408264 |   0.164   |    0.006     |    0.294     |     0.407     |
|    1     | 20.021000623703003 |   0.162   |    0.007     |    0.288     |     0.401     |
|    2     | 24.64716148376465  |   0.157   |    0.007     |    0.279     |     0.392     |
|    3     | 24.967915773391724 |   0.154   |    0.008     |    0.273     |     0.383     |
|    4     | 28.034534692764282 |   0.152   |    0.008     |    0.269     |     0.378     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 19:06:33,446: Sum_Training_Time:366.0014386177063
2025-01-07 19:06:33,446: Every_Training_Time:[268.33082604408264, 20.021000623703003, 24.64716148376465, 24.967915773391724, 28.034534692764282]
2025-01-07 19:06:33,446: Forward transfer: 0.0608 Backward transfer: -0.0019000000000000059
