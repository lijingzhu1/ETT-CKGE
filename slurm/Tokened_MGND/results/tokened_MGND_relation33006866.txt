2024-12-27 03:19:24,237: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227031852/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 03:19:39,063: Snapshot:0	Epoch:0	Loss:79.901	translation_Loss:79.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.65	Hits@10:9.44	Best:4.65
2024-12-27 03:19:49,679: Snapshot:0	Epoch:1	Loss:71.045	translation_Loss:71.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.6	Hits@10:18.87	Best:8.6
2024-12-27 03:20:00,295: Snapshot:0	Epoch:2	Loss:63.624	translation_Loss:63.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.67	Hits@10:22.36	Best:9.67
2024-12-27 03:20:11,073: Snapshot:0	Epoch:3	Loss:56.95	translation_Loss:56.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.75	Hits@10:25.61	Best:10.75
2024-12-27 03:20:21,609: Snapshot:0	Epoch:4	Loss:50.574	translation_Loss:50.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.68	Hits@10:29.79	Best:12.68
2024-12-27 03:20:32,233: Snapshot:0	Epoch:5	Loss:44.361	translation_Loss:44.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.15	Hits@10:34.33	Best:15.15
2024-12-27 03:20:43,410: Snapshot:0	Epoch:6	Loss:38.303	translation_Loss:38.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.43	Hits@10:37.67	Best:17.43
2024-12-27 03:20:53,945: Snapshot:0	Epoch:7	Loss:32.599	translation_Loss:32.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.32	Hits@10:40.11	Best:19.32
2024-12-27 03:21:04,680: Snapshot:0	Epoch:8	Loss:27.616	translation_Loss:27.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.78	Hits@10:41.75	Best:20.78
2024-12-27 03:21:15,258: Snapshot:0	Epoch:9	Loss:23.298	translation_Loss:23.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.95	Hits@10:43.21	Best:21.95
2024-12-27 03:21:25,826: Snapshot:0	Epoch:10	Loss:19.67	translation_Loss:19.67	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.87	Hits@10:44.19	Best:22.87
2024-12-27 03:21:36,386: Snapshot:0	Epoch:11	Loss:16.591	translation_Loss:16.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:45.1	Best:23.71
2024-12-27 03:21:47,500: Snapshot:0	Epoch:12	Loss:13.988	translation_Loss:13.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:45.8	Best:24.34
2024-12-27 03:21:58,142: Snapshot:0	Epoch:13	Loss:11.891	translation_Loss:11.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:46.19	Best:24.9
2024-12-27 03:22:08,793: Snapshot:0	Epoch:14	Loss:10.107	translation_Loss:10.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.57	Best:25.35
2024-12-27 03:22:19,369: Snapshot:0	Epoch:15	Loss:8.72	translation_Loss:8.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.9	Best:25.77
2024-12-27 03:22:29,980: Snapshot:0	Epoch:16	Loss:7.514	translation_Loss:7.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.14	Hits@10:47.27	Best:26.14
2024-12-27 03:22:40,596: Snapshot:0	Epoch:17	Loss:6.526	translation_Loss:6.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.34	Hits@10:47.47	Best:26.34
2024-12-27 03:22:51,386: Snapshot:0	Epoch:18	Loss:5.779	translation_Loss:5.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.57	Hits@10:47.72	Best:26.57
2024-12-27 03:23:02,057: Snapshot:0	Epoch:19	Loss:5.137	translation_Loss:5.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.72	Hits@10:48.05	Best:26.72
2024-12-27 03:23:12,553: Snapshot:0	Epoch:20	Loss:4.63	translation_Loss:4.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.82	Hits@10:47.94	Best:26.82
2024-12-27 03:23:23,071: Snapshot:0	Epoch:21	Loss:4.181	translation_Loss:4.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.91	Hits@10:47.93	Best:26.91
2024-12-27 03:23:33,624: Snapshot:0	Epoch:22	Loss:3.825	translation_Loss:3.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.96	Hits@10:48.08	Best:26.96
2024-12-27 03:23:44,164: Snapshot:0	Epoch:23	Loss:3.543	translation_Loss:3.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.06	Hits@10:48.07	Best:27.06
2024-12-27 03:23:54,736: Snapshot:0	Epoch:24	Loss:3.276	translation_Loss:3.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.19	Hits@10:48.08	Best:27.19
2024-12-27 03:24:05,859: Snapshot:0	Epoch:25	Loss:3.062	translation_Loss:3.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.26	Hits@10:48.15	Best:27.26
2024-12-27 03:24:16,417: Snapshot:0	Epoch:26	Loss:2.853	translation_Loss:2.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.32	Hits@10:48.19	Best:27.32
2024-12-27 03:24:26,939: Snapshot:0	Epoch:27	Loss:2.699	translation_Loss:2.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.3	Hits@10:48.3	Best:27.32
2024-12-27 03:24:37,583: Snapshot:0	Epoch:28	Loss:2.548	translation_Loss:2.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:48.28	Best:27.33
2024-12-27 03:24:48,223: Snapshot:0	Epoch:29	Loss:2.431	translation_Loss:2.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.25	Best:27.37
2024-12-27 03:24:58,780: Snapshot:0	Epoch:30	Loss:2.298	translation_Loss:2.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.36	Hits@10:48.13	Best:27.37
2024-12-27 03:25:09,894: Snapshot:0	Epoch:31	Loss:2.212	translation_Loss:2.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.34	Hits@10:48.25	Best:27.37
2024-12-27 03:25:20,386: Snapshot:0	Epoch:32	Loss:2.115	translation_Loss:2.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.47	Hits@10:48.22	Best:27.47
2024-12-27 03:25:30,909: Snapshot:0	Epoch:33	Loss:2.048	translation_Loss:2.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.43	Hits@10:48.28	Best:27.47
2024-12-27 03:25:41,391: Snapshot:0	Epoch:34	Loss:1.943	translation_Loss:1.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.43	Hits@10:48.24	Best:27.47
2024-12-27 03:25:51,878: Snapshot:0	Epoch:35	Loss:1.889	translation_Loss:1.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.49	Hits@10:48.18	Best:27.49
2024-12-27 03:26:02,637: Snapshot:0	Epoch:36	Loss:1.843	translation_Loss:1.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.51	Hits@10:48.12	Best:27.51
2024-12-27 03:26:13,227: Snapshot:0	Epoch:37	Loss:1.764	translation_Loss:1.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.5	Hits@10:48.14	Best:27.51
2024-12-27 03:26:23,744: Snapshot:0	Epoch:38	Loss:1.706	translation_Loss:1.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.52	Hits@10:48.22	Best:27.52
2024-12-27 03:26:34,363: Snapshot:0	Epoch:39	Loss:1.673	translation_Loss:1.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:48.02	Best:27.52
2024-12-27 03:26:45,042: Snapshot:0	Epoch:40	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.44	Hits@10:48.04	Best:27.52
2024-12-27 03:26:55,584: Early Stopping! Snapshot: 0 Epoch: 41 Best Results: 27.52
2024-12-27 03:26:55,584: Start to training tokens! Snapshot: 0 Epoch: 41 Loss:1.579 MRR:27.45 Best Results: 27.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:26:55,584: Snapshot:0	Epoch:41	Loss:1.579	translation_Loss:1.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.45	Hits@10:47.98	Best:27.52
2024-12-27 03:27:06,994: Snapshot:0	Epoch:42	Loss:169.429	translation_Loss:49.911	multi_layer_Loss:119.518	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.45	Hits@10:47.98	Best:27.52
2024-12-27 03:27:18,277: End of token training: 0 Epoch: 43 Loss:58.341 MRR:27.45 Best Results: 27.52
2024-12-27 03:27:18,278: Snapshot:0	Epoch:43	Loss:58.341	translation_Loss:49.843	multi_layer_Loss:8.498	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.45	Hits@10:47.98	Best:27.52
2024-12-27 03:27:18,556: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-27 03:27:22,886: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2765 | 0.1644 | 0.3426 | 0.4139 |  0.4829 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:27:55,887: Snapshot:1	Epoch:0	Loss:72.246	translation_Loss:71.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.608                                                   	MRR:3.29	Hits@10:9.19	Best:3.29
2024-12-27 03:28:05,721: Snapshot:1	Epoch:1	Loss:58.645	translation_Loss:56.857	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.788                                                   	MRR:6.88	Hits@10:17.15	Best:6.88
2024-12-27 03:28:15,413: Snapshot:1	Epoch:2	Loss:48.115	translation_Loss:45.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.949                                                   	MRR:9.84	Hits@10:22.05	Best:9.84
2024-12-27 03:28:25,181: Snapshot:1	Epoch:3	Loss:39.78	translation_Loss:35.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.841                                                   	MRR:12.06	Hits@10:26.05	Best:12.06
2024-12-27 03:28:34,850: Snapshot:1	Epoch:4	Loss:33.109	translation_Loss:28.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.456                                                   	MRR:13.65	Hits@10:28.61	Best:13.65
2024-12-27 03:28:44,706: Snapshot:1	Epoch:5	Loss:27.854	translation_Loss:22.987	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.867                                                   	MRR:15.02	Hits@10:30.28	Best:15.02
2024-12-27 03:28:54,901: Snapshot:1	Epoch:6	Loss:23.85	translation_Loss:18.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.116                                                   	MRR:15.73	Hits@10:31.31	Best:15.73
2024-12-27 03:29:04,640: Snapshot:1	Epoch:7	Loss:20.901	translation_Loss:15.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.254                                                   	MRR:16.16	Hits@10:31.74	Best:16.16
2024-12-27 03:29:14,325: Snapshot:1	Epoch:8	Loss:18.746	translation_Loss:13.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.307                                                   	MRR:16.24	Hits@10:31.95	Best:16.24
2024-12-27 03:29:24,063: Snapshot:1	Epoch:9	Loss:17.17	translation_Loss:11.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.314                                                   	MRR:16.15	Hits@10:32.07	Best:16.24
2024-12-27 03:29:33,878: Snapshot:1	Epoch:10	Loss:16.014	translation_Loss:10.73	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.284                                                   	MRR:16.19	Hits@10:32.19	Best:16.24
2024-12-27 03:29:43,617: Snapshot:1	Epoch:11	Loss:15.109	translation_Loss:9.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.238                                                   	MRR:16.43	Hits@10:32.17	Best:16.43
2024-12-27 03:29:53,360: Snapshot:1	Epoch:12	Loss:14.466	translation_Loss:9.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.179                                                   	MRR:16.44	Hits@10:32.34	Best:16.44
2024-12-27 03:30:03,264: Snapshot:1	Epoch:13	Loss:13.946	translation_Loss:8.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.12                                                   	MRR:16.13	Hits@10:32.18	Best:16.44
2024-12-27 03:30:13,222: Snapshot:1	Epoch:14	Loss:13.499	translation_Loss:8.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.064                                                   	MRR:16.06	Hits@10:32.14	Best:16.44
2024-12-27 03:30:23,359: Early Stopping! Snapshot: 1 Epoch: 15 Best Results: 16.44
2024-12-27 03:30:23,360: Start to training tokens! Snapshot: 1 Epoch: 15 Loss:13.202 MRR:16.35 Best Results: 16.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:30:23,360: Snapshot:1	Epoch:15	Loss:13.202	translation_Loss:8.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.006                                                   	MRR:16.35	Hits@10:32.23	Best:16.44
2024-12-27 03:30:33,053: Snapshot:1	Epoch:16	Loss:175.605	translation_Loss:57.14	multi_layer_Loss:118.465	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.35	Hits@10:32.23	Best:16.44
2024-12-27 03:30:42,876: End of token training: 1 Epoch: 17 Loss:66.701 MRR:16.35 Best Results: 16.44
2024-12-27 03:30:42,876: Snapshot:1	Epoch:17	Loss:66.701	translation_Loss:57.151	multi_layer_Loss:9.55	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:16.35	Hits@10:32.23	Best:16.44
2024-12-27 03:30:43,165: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-27 03:30:52,352: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2416 | 0.132  | 0.3038 | 0.373  |  0.4468 |
|     1      | 0.1646 | 0.0837 | 0.1946 | 0.2502 |  0.3221 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:31:18,483: Snapshot:2	Epoch:0	Loss:48.658	translation_Loss:48.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.492                                                   	MRR:1.6	Hits@10:4.3	Best:1.6
2024-12-27 03:31:25,749: Snapshot:2	Epoch:1	Loss:31.822	translation_Loss:30.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.797                                                   	MRR:8.45	Hits@10:19.59	Best:8.45
2024-12-27 03:31:33,023: Snapshot:2	Epoch:2	Loss:21.819	translation_Loss:18.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.072                                                   	MRR:14.29	Hits@10:30.55	Best:14.29
2024-12-27 03:31:40,271: Snapshot:2	Epoch:3	Loss:17.29	translation_Loss:13.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.731                                                   	MRR:17.94	Hits@10:34.74	Best:17.94
2024-12-27 03:31:47,600: Snapshot:2	Epoch:4	Loss:14.781	translation_Loss:10.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.065                                                   	MRR:19.67	Hits@10:37.25	Best:19.67
2024-12-27 03:31:54,934: Snapshot:2	Epoch:5	Loss:13.067	translation_Loss:8.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.226                                                   	MRR:20.67	Hits@10:38.24	Best:20.67
2024-12-27 03:32:02,327: Snapshot:2	Epoch:6	Loss:11.834	translation_Loss:7.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.273                                                   	MRR:21.42	Hits@10:39.49	Best:21.42
2024-12-27 03:32:09,640: Snapshot:2	Epoch:7	Loss:10.898	translation_Loss:6.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.247                                                   	MRR:21.75	Hits@10:39.9	Best:21.75
2024-12-27 03:32:16,953: Snapshot:2	Epoch:8	Loss:10.109	translation_Loss:5.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.179                                                   	MRR:22.09	Hits@10:40.07	Best:22.09
2024-12-27 03:32:24,315: Snapshot:2	Epoch:9	Loss:9.441	translation_Loss:5.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.085                                                   	MRR:22.36	Hits@10:40.47	Best:22.36
2024-12-27 03:32:31,605: Snapshot:2	Epoch:10	Loss:8.84	translation_Loss:4.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.971                                                   	MRR:22.67	Hits@10:40.57	Best:22.67
2024-12-27 03:32:38,890: Snapshot:2	Epoch:11	Loss:8.374	translation_Loss:4.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.847                                                   	MRR:22.68	Hits@10:40.49	Best:22.68
2024-12-27 03:32:46,216: Snapshot:2	Epoch:12	Loss:7.943	translation_Loss:4.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.725                                                   	MRR:22.5	Hits@10:40.6	Best:22.68
2024-12-27 03:32:53,394: Snapshot:2	Epoch:13	Loss:7.554	translation_Loss:3.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.603                                                   	MRR:22.52	Hits@10:40.47	Best:22.68
2024-12-27 03:33:00,621: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 22.68
2024-12-27 03:33:00,621: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:7.22 MRR:22.63 Best Results: 22.68
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:33:00,622: Snapshot:2	Epoch:14	Loss:7.22	translation_Loss:3.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.48                                                   	MRR:22.63	Hits@10:40.35	Best:22.68
2024-12-27 03:33:07,802: Snapshot:2	Epoch:15	Loss:143.657	translation_Loss:36.55	multi_layer_Loss:107.107	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:40.35	Best:22.68
2024-12-27 03:33:14,973: End of token training: 2 Epoch: 16 Loss:61.321 MRR:22.63 Best Results: 22.68
2024-12-27 03:33:14,973: Snapshot:2	Epoch:16	Loss:61.321	translation_Loss:36.583	multi_layer_Loss:24.738	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.63	Hits@10:40.35	Best:22.68
2024-12-27 03:33:15,347: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-27 03:33:26,828: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2019 | 0.1045 | 0.2553 | 0.319  |  0.3854 |
|     1      | 0.1376 | 0.0661 | 0.1617 | 0.2143 |  0.2774 |
|     2      | 0.2238 | 0.1356 | 0.2497 | 0.3122 |  0.4008 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:33:41,222: Snapshot:3	Epoch:0	Loss:20.699	translation_Loss:20.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.091                                                   	MRR:2.66	Hits@10:6.48	Best:2.66
2024-12-27 03:33:44,721: Snapshot:3	Epoch:1	Loss:17.054	translation_Loss:16.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.26                                                   	MRR:5.51	Hits@10:12.09	Best:5.51
2024-12-27 03:33:48,211: Snapshot:3	Epoch:2	Loss:14.356	translation_Loss:13.89	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.467                                                   	MRR:8.65	Hits@10:18.73	Best:8.65
2024-12-27 03:33:51,579: Snapshot:3	Epoch:3	Loss:12.162	translation_Loss:11.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.694                                                   	MRR:12.25	Hits@10:26.88	Best:12.25
2024-12-27 03:33:54,932: Snapshot:3	Epoch:4	Loss:10.269	translation_Loss:9.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.926                                                   	MRR:16.12	Hits@10:34.63	Best:16.12
2024-12-27 03:33:58,852: Snapshot:3	Epoch:5	Loss:8.688	translation_Loss:7.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.148                                                   	MRR:19.04	Hits@10:38.57	Best:19.04
2024-12-27 03:34:02,346: Snapshot:3	Epoch:6	Loss:7.422	translation_Loss:6.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:21.35	Hits@10:41.12	Best:21.35
2024-12-27 03:34:05,790: Snapshot:3	Epoch:7	Loss:6.391	translation_Loss:4.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.504                                                   	MRR:23.02	Hits@10:42.97	Best:23.02
2024-12-27 03:34:09,162: Snapshot:3	Epoch:8	Loss:5.567	translation_Loss:3.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.624                                                   	MRR:24.06	Hits@10:44.17	Best:24.06
2024-12-27 03:34:12,597: Snapshot:3	Epoch:9	Loss:4.932	translation_Loss:3.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:25.05	Hits@10:45.03	Best:25.05
2024-12-27 03:34:15,966: Snapshot:3	Epoch:10	Loss:4.444	translation_Loss:2.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.764                                                   	MRR:26.08	Hits@10:45.46	Best:26.08
2024-12-27 03:34:19,452: Snapshot:3	Epoch:11	Loss:4.085	translation_Loss:2.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.798                                                   	MRR:26.85	Hits@10:45.47	Best:26.85
2024-12-27 03:34:22,821: Snapshot:3	Epoch:12	Loss:3.812	translation_Loss:1.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.816                                                   	MRR:27.31	Hits@10:45.9	Best:27.31
2024-12-27 03:34:26,248: Snapshot:3	Epoch:13	Loss:3.573	translation_Loss:1.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.824                                                   	MRR:27.66	Hits@10:46.15	Best:27.66
2024-12-27 03:34:29,639: Snapshot:3	Epoch:14	Loss:3.411	translation_Loss:1.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.824                                                   	MRR:28.15	Hits@10:46.3	Best:28.15
2024-12-27 03:34:33,106: Snapshot:3	Epoch:15	Loss:3.231	translation_Loss:1.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.818                                                   	MRR:28.45	Hits@10:46.49	Best:28.45
2024-12-27 03:34:36,529: Snapshot:3	Epoch:16	Loss:3.111	translation_Loss:1.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.802                                                   	MRR:28.6	Hits@10:46.57	Best:28.6
2024-12-27 03:34:39,921: Snapshot:3	Epoch:17	Loss:2.986	translation_Loss:1.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.786                                                   	MRR:28.74	Hits@10:46.68	Best:28.74
2024-12-27 03:34:43,365: Snapshot:3	Epoch:18	Loss:2.884	translation_Loss:1.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.766                                                   	MRR:28.96	Hits@10:46.76	Best:28.96
2024-12-27 03:34:46,685: Snapshot:3	Epoch:19	Loss:2.782	translation_Loss:1.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.749                                                   	MRR:28.79	Hits@10:46.95	Best:28.96
2024-12-27 03:34:50,066: Snapshot:3	Epoch:20	Loss:2.707	translation_Loss:0.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.726                                                   	MRR:29.09	Hits@10:46.91	Best:29.09
2024-12-27 03:34:53,411: Snapshot:3	Epoch:21	Loss:2.623	translation_Loss:0.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.7                                                   	MRR:28.84	Hits@10:46.83	Best:29.09
2024-12-27 03:34:56,791: Snapshot:3	Epoch:22	Loss:2.555	translation_Loss:0.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.675                                                   	MRR:28.84	Hits@10:46.9	Best:29.09
2024-12-27 03:35:00,130: Early Stopping! Snapshot: 3 Epoch: 23 Best Results: 29.09
2024-12-27 03:35:00,131: Start to training tokens! Snapshot: 3 Epoch: 23 Loss:2.509 MRR:28.9 Best Results: 29.09
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:35:00,131: Snapshot:3	Epoch:23	Loss:2.509	translation_Loss:0.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.651                                                   	MRR:28.9	Hits@10:46.8	Best:29.09
2024-12-27 03:35:03,513: Snapshot:3	Epoch:24	Loss:70.936	translation_Loss:12.495	multi_layer_Loss:58.442	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.9	Hits@10:46.8	Best:29.09
2024-12-27 03:35:06,884: End of token training: 3 Epoch: 25 Loss:46.306 MRR:28.9 Best Results: 29.09
2024-12-27 03:35:06,885: Snapshot:3	Epoch:25	Loss:46.306	translation_Loss:12.494	multi_layer_Loss:33.812	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.9	Hits@10:46.8	Best:29.09
2024-12-27 03:35:07,249: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-27 03:35:20,769: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1985 | 0.1031 | 0.2508 | 0.3121 |  0.3783 |
|     1      | 0.1333 | 0.063  | 0.1559 | 0.2059 |  0.2714 |
|     2      | 0.1648 | 0.0858 | 0.1812 | 0.2351 |  0.3172 |
|     3      | 0.292  | 0.1924 | 0.3397 | 0.398  |  0.4754 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:35:32,687: Snapshot:4	Epoch:0	Loss:11.369	translation_Loss:11.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.055                                                   	MRR:5.67	Hits@10:16.31	Best:5.67
2024-12-27 03:35:35,102: Snapshot:4	Epoch:1	Loss:9.119	translation_Loss:8.952	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:7.63	Hits@10:20.65	Best:7.63
2024-12-27 03:35:37,532: Snapshot:4	Epoch:2	Loss:7.444	translation_Loss:7.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.278                                                   	MRR:9.39	Hits@10:23.95	Best:9.39
2024-12-27 03:35:39,987: Snapshot:4	Epoch:3	Loss:6.192	translation_Loss:5.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.383                                                   	MRR:11.35	Hits@10:27.81	Best:11.35
2024-12-27 03:35:42,389: Snapshot:4	Epoch:4	Loss:5.232	translation_Loss:4.749	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.483                                                   	MRR:13.57	Hits@10:32.15	Best:13.57
2024-12-27 03:35:44,830: Snapshot:4	Epoch:5	Loss:4.462	translation_Loss:3.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.576                                                   	MRR:15.49	Hits@10:36.46	Best:15.49
2024-12-27 03:35:47,238: Snapshot:4	Epoch:6	Loss:3.852	translation_Loss:3.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.661                                                   	MRR:17.18	Hits@10:39.82	Best:17.18
2024-12-27 03:35:49,684: Snapshot:4	Epoch:7	Loss:3.371	translation_Loss:2.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.736                                                   	MRR:18.75	Hits@10:42.74	Best:18.75
2024-12-27 03:35:52,082: Snapshot:4	Epoch:8	Loss:2.975	translation_Loss:2.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.798                                                   	MRR:20.22	Hits@10:44.28	Best:20.22
2024-12-27 03:35:54,529: Snapshot:4	Epoch:9	Loss:2.673	translation_Loss:1.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.848                                                   	MRR:21.12	Hits@10:46.11	Best:21.12
2024-12-27 03:35:57,040: Snapshot:4	Epoch:10	Loss:2.392	translation_Loss:1.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.885                                                   	MRR:21.97	Hits@10:47.51	Best:21.97
2024-12-27 03:35:59,452: Snapshot:4	Epoch:11	Loss:2.175	translation_Loss:1.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.915                                                   	MRR:23.55	Hits@10:48.16	Best:23.55
2024-12-27 03:36:01,854: Snapshot:4	Epoch:12	Loss:1.957	translation_Loss:1.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.937                                                   	MRR:24.42	Hits@10:48.8	Best:24.42
2024-12-27 03:36:04,281: Snapshot:4	Epoch:13	Loss:1.789	translation_Loss:0.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.95                                                   	MRR:25.18	Hits@10:49.33	Best:25.18
2024-12-27 03:36:06,786: Snapshot:4	Epoch:14	Loss:1.677	translation_Loss:0.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.957                                                   	MRR:25.41	Hits@10:49.65	Best:25.41
2024-12-27 03:36:09,234: Snapshot:4	Epoch:15	Loss:1.596	translation_Loss:0.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.96                                                   	MRR:25.72	Hits@10:49.65	Best:25.72
2024-12-27 03:36:11,684: Snapshot:4	Epoch:16	Loss:1.523	translation_Loss:0.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.959                                                   	MRR:25.79	Hits@10:50.07	Best:25.79
2024-12-27 03:36:14,135: Snapshot:4	Epoch:17	Loss:1.469	translation_Loss:0.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.952                                                   	MRR:26.17	Hits@10:50.39	Best:26.17
2024-12-27 03:36:16,589: Snapshot:4	Epoch:18	Loss:1.409	translation_Loss:0.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.946                                                   	MRR:26.69	Hits@10:51.02	Best:26.69
2024-12-27 03:36:18,997: Snapshot:4	Epoch:19	Loss:1.371	translation_Loss:0.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.938                                                   	MRR:26.82	Hits@10:51.0	Best:26.82
2024-12-27 03:36:21,409: Snapshot:4	Epoch:20	Loss:1.322	translation_Loss:0.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.926                                                   	MRR:26.89	Hits@10:51.0	Best:26.89
2024-12-27 03:36:23,804: Snapshot:4	Epoch:21	Loss:1.301	translation_Loss:0.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.916                                                   	MRR:27.24	Hits@10:51.23	Best:27.24
2024-12-27 03:36:26,199: Snapshot:4	Epoch:22	Loss:1.264	translation_Loss:0.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.907                                                   	MRR:27.47	Hits@10:51.78	Best:27.47
2024-12-27 03:36:28,550: Snapshot:4	Epoch:23	Loss:1.225	translation_Loss:0.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.895                                                   	MRR:27.21	Hits@10:51.82	Best:27.47
2024-12-27 03:36:30,882: Snapshot:4	Epoch:24	Loss:1.199	translation_Loss:0.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.882                                                   	MRR:27.46	Hits@10:51.68	Best:27.47
2024-12-27 03:36:33,258: Early Stopping! Snapshot: 4 Epoch: 25 Best Results: 27.47
2024-12-27 03:36:33,258: Start to training tokens! Snapshot: 4 Epoch: 25 Loss:1.178 MRR:27.43 Best Results: 27.47
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:36:33,259: Snapshot:4	Epoch:25	Loss:1.178	translation_Loss:0.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.866                                                   	MRR:27.43	Hits@10:51.86	Best:27.47
2024-12-27 03:36:35,599: Snapshot:4	Epoch:26	Loss:53.816	translation_Loss:6.982	multi_layer_Loss:46.835	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.43	Hits@10:51.86	Best:27.47
2024-12-27 03:36:38,120: End of token training: 4 Epoch: 27 Loss:41.061 MRR:27.43 Best Results: 27.47
2024-12-27 03:36:38,120: Snapshot:4	Epoch:27	Loss:41.061	translation_Loss:6.98	multi_layer_Loss:34.08	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.43	Hits@10:51.86	Best:27.47
2024-12-27 03:36:38,484: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-27 03:36:53,704: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1846 | 0.0956 | 0.2322 | 0.289  |  0.3476 |
|     1      | 0.1297 | 0.0607 | 0.1509 | 0.201  |  0.2673 |
|     2      | 0.157  | 0.0843 | 0.1736 | 0.2192 |  0.2989 |
|     3      | 0.2495 | 0.1633 | 0.286  | 0.3321 |  0.4022 |
|     4      | 0.2686 | 0.1495 | 0.307  | 0.3919 |  0.5213 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 03:36:53,707: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2765 | 0.1644 | 0.3426 | 0.4139 |  0.4829 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2416 | 0.132  | 0.3038 | 0.373  |  0.4468 |
|     1      | 0.1646 | 0.0837 | 0.1946 | 0.2502 |  0.3221 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2019 | 0.1045 | 0.2553 | 0.319  |  0.3854 |
|     1      | 0.1376 | 0.0661 | 0.1617 | 0.2143 |  0.2774 |
|     2      | 0.2238 | 0.1356 | 0.2497 | 0.3122 |  0.4008 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1985 | 0.1031 | 0.2508 | 0.3121 |  0.3783 |
|     1      | 0.1333 | 0.063  | 0.1559 | 0.2059 |  0.2714 |
|     2      | 0.1648 | 0.0858 | 0.1812 | 0.2351 |  0.3172 |
|     3      | 0.292  | 0.1924 | 0.3397 | 0.398  |  0.4754 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1846 | 0.0956 | 0.2322 | 0.289  |  0.3476 |
|     1      | 0.1297 | 0.0607 | 0.1509 | 0.201  |  0.2673 |
|     2      | 0.157  | 0.0843 | 0.1736 | 0.2192 |  0.2989 |
|     3      | 0.2495 | 0.1633 | 0.286  | 0.3321 |  0.4022 |
|     4      | 0.2686 | 0.1495 | 0.307  | 0.3919 |  0.5213 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 03:36:53,707: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 474.0400302410126  |   0.277   |    0.164     |    0.343     |     0.483     |
|    1     | 195.9031276702881  |   0.204   |    0.109     |    0.251     |     0.386     |
|    2     | 139.38811659812927 |   0.184   |    0.099     |     0.22     |      0.35     |
|    3     | 98.02748203277588  |   0.179   |    0.095     |    0.213     |      0.34     |
|    4     | 75.70622730255127  |   0.174   |    0.093     |    0.206     |      0.33     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 03:36:53,707: Sum_Training_Time:983.0649838447571
2024-12-27 03:36:53,707: Every_Training_Time:[474.0400302410126, 195.9031276702881, 139.38811659812927, 98.02748203277588, 75.70622730255127]
2024-12-27 03:36:53,707: Forward transfer: 0.018275 Backward transfer: -0.059025
2024-12-27 03:37:29,722: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227033658/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=1, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 03:37:44,727: Snapshot:0	Epoch:0	Loss:79.901	translation_Loss:79.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.65	Hits@10:9.44	Best:4.65
2024-12-27 03:37:55,612: Snapshot:0	Epoch:1	Loss:71.045	translation_Loss:71.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.6	Hits@10:18.87	Best:8.6
2024-12-27 03:38:06,443: Snapshot:0	Epoch:2	Loss:63.624	translation_Loss:63.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.67	Hits@10:22.37	Best:9.67
2024-12-27 03:38:17,445: Snapshot:0	Epoch:3	Loss:56.95	translation_Loss:56.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.75	Hits@10:25.62	Best:10.75
2024-12-27 03:38:28,167: Snapshot:0	Epoch:4	Loss:50.574	translation_Loss:50.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.68	Hits@10:29.79	Best:12.68
2024-12-27 03:38:38,829: Snapshot:0	Epoch:5	Loss:44.361	translation_Loss:44.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.15	Hits@10:34.32	Best:15.15
2024-12-27 03:38:49,929: Snapshot:0	Epoch:6	Loss:38.303	translation_Loss:38.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.44	Hits@10:37.7	Best:17.44
2024-12-27 03:39:00,780: Snapshot:0	Epoch:7	Loss:32.599	translation_Loss:32.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.32	Hits@10:40.1	Best:19.32
2024-12-27 03:39:11,330: Snapshot:0	Epoch:8	Loss:27.616	translation_Loss:27.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.75	Hits@10:41.81	Best:20.75
2024-12-27 03:39:21,926: Snapshot:0	Epoch:9	Loss:23.297	translation_Loss:23.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.96	Hits@10:43.29	Best:21.96
2024-12-27 03:39:32,612: Snapshot:0	Epoch:10	Loss:19.669	translation_Loss:19.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.85	Hits@10:44.16	Best:22.85
2024-12-27 03:39:43,161: Snapshot:0	Epoch:11	Loss:16.592	translation_Loss:16.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:45.1	Best:23.71
2024-12-27 03:39:54,246: Snapshot:0	Epoch:12	Loss:13.99	translation_Loss:13.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:45.78	Best:24.33
2024-12-27 03:40:05,051: Snapshot:0	Epoch:13	Loss:11.892	translation_Loss:11.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.85	Hits@10:46.18	Best:24.85
2024-12-27 03:40:15,664: Snapshot:0	Epoch:14	Loss:10.106	translation_Loss:10.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.61	Best:25.35
2024-12-27 03:40:26,204: Snapshot:0	Epoch:15	Loss:8.719	translation_Loss:8.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.9	Best:25.79
2024-12-27 03:40:36,851: Snapshot:0	Epoch:16	Loss:7.516	translation_Loss:7.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.14	Hits@10:47.18	Best:26.14
2024-12-27 03:40:47,397: Snapshot:0	Epoch:17	Loss:6.526	translation_Loss:6.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.38	Hits@10:47.48	Best:26.38
2024-12-27 03:40:58,094: Snapshot:0	Epoch:18	Loss:5.78	translation_Loss:5.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.59	Hits@10:47.69	Best:26.59
2024-12-27 03:41:08,813: Snapshot:0	Epoch:19	Loss:5.136	translation_Loss:5.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.68	Hits@10:47.95	Best:26.68
2024-12-27 03:41:19,382: Snapshot:0	Epoch:20	Loss:4.629	translation_Loss:4.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.81	Hits@10:47.98	Best:26.81
2024-12-27 03:41:29,920: Snapshot:0	Epoch:21	Loss:4.179	translation_Loss:4.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.91	Hits@10:47.95	Best:26.91
2024-12-27 03:41:40,476: Snapshot:0	Epoch:22	Loss:3.825	translation_Loss:3.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.94	Hits@10:47.99	Best:26.94
2024-12-27 03:41:50,993: Snapshot:0	Epoch:23	Loss:3.546	translation_Loss:3.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.03	Hits@10:48.01	Best:27.03
2024-12-27 03:42:01,657: Snapshot:0	Epoch:24	Loss:3.274	translation_Loss:3.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.16	Hits@10:48.12	Best:27.16
2024-12-27 03:42:12,703: Snapshot:0	Epoch:25	Loss:3.063	translation_Loss:3.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.2	Hits@10:48.14	Best:27.2
2024-12-27 03:42:23,242: Snapshot:0	Epoch:26	Loss:2.853	translation_Loss:2.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.31	Hits@10:48.25	Best:27.31
2024-12-27 03:42:33,844: Snapshot:0	Epoch:27	Loss:2.7	translation_Loss:2.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.35	Hits@10:48.32	Best:27.35
2024-12-27 03:42:44,599: Snapshot:0	Epoch:28	Loss:2.547	translation_Loss:2.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:48.23	Best:27.35
2024-12-27 03:42:55,115: Snapshot:0	Epoch:29	Loss:2.434	translation_Loss:2.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.38	Hits@10:48.29	Best:27.38
2024-12-27 03:43:05,704: Snapshot:0	Epoch:30	Loss:2.298	translation_Loss:2.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.35	Hits@10:48.14	Best:27.38
2024-12-27 03:43:16,796: Snapshot:0	Epoch:31	Loss:2.21	translation_Loss:2.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.3	Hits@10:48.32	Best:27.38
2024-12-27 03:43:27,308: Snapshot:0	Epoch:32	Loss:2.117	translation_Loss:2.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.49	Hits@10:48.17	Best:27.49
2024-12-27 03:43:37,923: Snapshot:0	Epoch:33	Loss:2.05	translation_Loss:2.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.4	Hits@10:48.17	Best:27.49
2024-12-27 03:43:48,402: Snapshot:0	Epoch:34	Loss:1.945	translation_Loss:1.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.42	Hits@10:48.08	Best:27.49
2024-12-27 03:43:58,993: Early Stopping! Snapshot: 0 Epoch: 35 Best Results: 27.49
2024-12-27 03:43:58,993: Start to training tokens! Snapshot: 0 Epoch: 35 Loss:1.888 MRR:27.42 Best Results: 27.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 03:43:58,994: Snapshot:0	Epoch:35	Loss:1.888	translation_Loss:1.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.42	Hits@10:48.17	Best:27.49
2024-12-27 03:44:10,209: Snapshot:0	Epoch:36	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.42	Hits@10:48.17	Best:27.49
2024-12-27 03:44:20,918: End of token training: 0 Epoch: 37 Loss:nan MRR:27.42 Best Results: 27.49
2024-12-27 03:44:20,918: Snapshot:0	Epoch:37	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.42	Hits@10:48.17	Best:27.49
2024-12-27 03:44:21,263: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-27 03:44:26,071: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.277 | 0.1651 | 0.3419 | 0.4127 |  0.4843 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:44:59,444: Snapshot:1	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 03:45:09,287: Snapshot:1	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 03:45:18,913: Snapshot:1	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 03:45:28,631: Early Stopping! Snapshot: 1 Epoch: 3 Best Results: 0.13
2024-12-27 03:45:28,632: Start to training tokens! Snapshot: 1 Epoch: 3 Loss:nan MRR:0.13 Best Results: 0.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 03:45:28,632: Snapshot:1	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 03:45:38,293: Snapshot:1	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 03:45:48,011: End of token training: 1 Epoch: 5 Loss:nan MRR:0.13 Best Results: 0.13
2024-12-27 03:45:48,011: Snapshot:1	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 03:45:48,367: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-27 03:45:56,713: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:46:22,323: Snapshot:2	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 03:46:29,571: Snapshot:2	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 03:46:36,800: Snapshot:2	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 03:46:43,960: Early Stopping! Snapshot: 2 Epoch: 3 Best Results: 0.19
2024-12-27 03:46:43,961: Start to training tokens! Snapshot: 2 Epoch: 3 Loss:nan MRR:0.19 Best Results: 0.19
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 03:46:43,961: Snapshot:2	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 03:46:51,140: Snapshot:2	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 03:46:58,420: End of token training: 2 Epoch: 5 Loss:nan MRR:0.19 Best Results: 0.19
2024-12-27 03:46:58,420: Snapshot:2	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 03:46:58,783: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-27 03:47:10,557: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:47:24,524: Snapshot:3	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 03:47:27,979: Snapshot:3	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 03:47:31,299: Snapshot:3	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 03:47:34,999: Early Stopping! Snapshot: 3 Epoch: 3 Best Results: 0.07
2024-12-27 03:47:35,000: Start to training tokens! Snapshot: 3 Epoch: 3 Loss:nan MRR:0.07 Best Results: 0.07
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 03:47:35,000: Snapshot:3	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 03:47:38,417: Snapshot:3	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 03:47:41,759: End of token training: 3 Epoch: 5 Loss:nan MRR:0.07 Best Results: 0.07
2024-12-27 03:47:41,759: Snapshot:3	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 03:47:41,982: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-27 03:47:55,530: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:48:07,118: Snapshot:4	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 03:48:09,446: Snapshot:4	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 03:48:11,773: Snapshot:4	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 03:48:14,090: Early Stopping! Snapshot: 4 Epoch: 3 Best Results: 0.06
2024-12-27 03:48:14,090: Start to training tokens! Snapshot: 4 Epoch: 3 Loss:nan MRR:0.06 Best Results: 0.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 03:48:14,091: Snapshot:4	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 03:48:16,473: Snapshot:4	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 03:48:18,800: End of token training: 4 Epoch: 5 Loss:nan MRR:0.06 Best Results: 0.06
2024-12-27 03:48:18,800: Snapshot:4	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 03:48:19,023: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-27 03:48:33,758: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
|     4      | 0.0007 |  0.0   |  0.0   | 0.0002 |  0.0005 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 03:48:33,760: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.277 | 0.1651 | 0.3419 | 0.4127 |  0.4843 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
|     4      | 0.0007 |  0.0   |  0.0   | 0.0002 |  0.0005 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 03:48:33,761: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 411.1949870586395  |   0.277   |    0.165     |    0.342     |     0.484     |
|    1     | 78.14597463607788  |   0.005   |     0.0      |     0.0      |     0.011     |
|    2     | 58.52841663360596  |   0.004   |     0.0      |     0.0      |     0.009     |
|    3     | 29.45755958557129  |   0.004   |     0.0      |     0.0      |     0.008     |
|    4     | 21.889522075653076 |   0.003   |     0.0      |     0.0      |     0.007     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 03:48:33,761: Sum_Training_Time:599.2164599895477
2024-12-27 03:48:33,761: Every_Training_Time:[411.1949870586395, 78.14597463607788, 58.52841663360596, 29.45755958557129, 21.889522075653076]
2024-12-27 03:48:33,761: Forward transfer: 0.006599999999999999 Backward transfer: -0.067275
2024-12-27 03:49:09,117: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227034838/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 03:49:23,987: Snapshot:0	Epoch:0	Loss:79.901	translation_Loss:79.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.65	Hits@10:9.44	Best:4.65
2024-12-27 03:49:34,725: Snapshot:0	Epoch:1	Loss:71.045	translation_Loss:71.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.6	Hits@10:18.87	Best:8.6
2024-12-27 03:49:45,380: Snapshot:0	Epoch:2	Loss:63.624	translation_Loss:63.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.67	Hits@10:22.36	Best:9.67
2024-12-27 03:49:56,115: Snapshot:0	Epoch:3	Loss:56.95	translation_Loss:56.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.75	Hits@10:25.62	Best:10.75
2024-12-27 03:50:07,063: Snapshot:0	Epoch:4	Loss:50.574	translation_Loss:50.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.68	Hits@10:29.78	Best:12.68
2024-12-27 03:50:18,145: Snapshot:0	Epoch:5	Loss:44.361	translation_Loss:44.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.15	Hits@10:34.3	Best:15.15
2024-12-27 03:50:29,364: Snapshot:0	Epoch:6	Loss:38.303	translation_Loss:38.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.42	Hits@10:37.68	Best:17.42
2024-12-27 03:50:40,080: Snapshot:0	Epoch:7	Loss:32.599	translation_Loss:32.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.32	Hits@10:40.1	Best:19.32
2024-12-27 03:50:50,808: Snapshot:0	Epoch:8	Loss:27.616	translation_Loss:27.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.78	Hits@10:41.79	Best:20.78
2024-12-27 03:51:01,566: Snapshot:0	Epoch:9	Loss:23.297	translation_Loss:23.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.99	Hits@10:43.26	Best:21.99
2024-12-27 03:51:12,330: Snapshot:0	Epoch:10	Loss:19.67	translation_Loss:19.67	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.86	Hits@10:44.19	Best:22.86
2024-12-27 03:51:23,091: Snapshot:0	Epoch:11	Loss:16.592	translation_Loss:16.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.73	Hits@10:45.11	Best:23.73
2024-12-27 03:51:34,357: Snapshot:0	Epoch:12	Loss:13.99	translation_Loss:13.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:45.77	Best:24.33
2024-12-27 03:51:45,009: Snapshot:0	Epoch:13	Loss:11.893	translation_Loss:11.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.86	Hits@10:46.22	Best:24.86
2024-12-27 03:51:55,726: Snapshot:0	Epoch:14	Loss:10.107	translation_Loss:10.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.36	Hits@10:46.61	Best:25.36
2024-12-27 03:52:06,716: Snapshot:0	Epoch:15	Loss:8.721	translation_Loss:8.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.95	Best:25.78
2024-12-27 03:52:17,410: Snapshot:0	Epoch:16	Loss:7.518	translation_Loss:7.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.12	Hits@10:47.24	Best:26.12
2024-12-27 03:52:28,096: Snapshot:0	Epoch:17	Loss:6.527	translation_Loss:6.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.33	Hits@10:47.46	Best:26.33
2024-12-27 03:52:38,965: Snapshot:0	Epoch:18	Loss:5.78	translation_Loss:5.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.61	Hits@10:47.69	Best:26.61
2024-12-27 03:52:49,628: Snapshot:0	Epoch:19	Loss:5.138	translation_Loss:5.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.68	Hits@10:47.99	Best:26.68
2024-12-27 03:53:00,424: Snapshot:0	Epoch:20	Loss:4.631	translation_Loss:4.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.81	Hits@10:47.98	Best:26.81
2024-12-27 03:53:11,239: Snapshot:0	Epoch:21	Loss:4.18	translation_Loss:4.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.93	Hits@10:48.01	Best:26.93
2024-12-27 03:53:21,901: Snapshot:0	Epoch:22	Loss:3.824	translation_Loss:3.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.92	Hits@10:48.0	Best:26.93
2024-12-27 03:53:32,600: Snapshot:0	Epoch:23	Loss:3.545	translation_Loss:3.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.08	Hits@10:48.1	Best:27.08
2024-12-27 03:53:43,267: Snapshot:0	Epoch:24	Loss:3.274	translation_Loss:3.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.16	Hits@10:48.16	Best:27.16
2024-12-27 03:53:54,490: Snapshot:0	Epoch:25	Loss:3.062	translation_Loss:3.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.18	Hits@10:48.18	Best:27.18
2024-12-27 03:54:05,303: Snapshot:0	Epoch:26	Loss:2.853	translation_Loss:2.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:48.21	Best:27.33
2024-12-27 03:54:16,118: Snapshot:0	Epoch:27	Loss:2.702	translation_Loss:2.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.38	Hits@10:48.3	Best:27.38
2024-12-27 03:54:26,803: Snapshot:0	Epoch:28	Loss:2.544	translation_Loss:2.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.35	Hits@10:48.18	Best:27.38
2024-12-27 03:54:37,563: Snapshot:0	Epoch:29	Loss:2.429	translation_Loss:2.429	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.36	Hits@10:48.26	Best:27.38
2024-12-27 03:54:48,193: Early Stopping! Snapshot: 0 Epoch: 30 Best Results: 27.38
2024-12-27 03:54:48,193: Start to training tokens! Snapshot: 0 Epoch: 30 Loss:2.299 MRR:27.37 Best Results: 27.38
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:54:48,193: Snapshot:0	Epoch:30	Loss:2.299	translation_Loss:2.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.12	Best:27.38
2024-12-27 03:55:00,167: Snapshot:0	Epoch:31	Loss:179.593	translation_Loss:50.628	multi_layer_Loss:128.965	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.12	Best:27.38
2024-12-27 03:55:11,234: End of token training: 0 Epoch: 32 Loss:62.585 MRR:27.37 Best Results: 27.38
2024-12-27 03:55:11,235: Snapshot:0	Epoch:32	Loss:62.585	translation_Loss:50.608	multi_layer_Loss:11.977	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.37	Hits@10:48.12	Best:27.38
2024-12-27 03:55:11,516: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-27 03:55:16,428: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2766 | 0.1633 | 0.3443 | 0.4141 |  0.4862 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:55:49,514: Snapshot:1	Epoch:0	Loss:72.845	translation_Loss:72.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.822                                                   	MRR:3.33	Hits@10:9.23	Best:3.33
2024-12-27 03:55:59,340: Snapshot:1	Epoch:1	Loss:61.212	translation_Loss:59.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.035                                                   	MRR:6.52	Hits@10:16.66	Best:6.52
2024-12-27 03:56:09,098: Snapshot:1	Epoch:2	Loss:52.0	translation_Loss:49.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.93                                                   	MRR:8.98	Hits@10:21.19	Best:8.98
2024-12-27 03:56:18,918: Snapshot:1	Epoch:3	Loss:44.339	translation_Loss:40.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.498                                                   	MRR:10.96	Hits@10:24.74	Best:10.96
2024-12-27 03:56:28,728: Snapshot:1	Epoch:4	Loss:37.855	translation_Loss:34.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.823                                                   	MRR:12.61	Hits@10:27.14	Best:12.61
2024-12-27 03:56:38,538: Snapshot:1	Epoch:5	Loss:32.604	translation_Loss:28.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.992                                                   	MRR:13.77	Hits@10:28.6	Best:13.77
2024-12-27 03:56:48,320: Snapshot:1	Epoch:6	Loss:28.504	translation_Loss:24.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.079                                                   	MRR:14.37	Hits@10:29.36	Best:14.37
2024-12-27 03:56:58,153: Snapshot:1	Epoch:7	Loss:25.43	translation_Loss:21.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.131                                                   	MRR:14.59	Hits@10:29.87	Best:14.59
2024-12-27 03:57:08,052: Snapshot:1	Epoch:8	Loss:23.152	translation_Loss:18.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.16                                                   	MRR:14.77	Hits@10:29.93	Best:14.77
2024-12-27 03:57:17,800: Snapshot:1	Epoch:9	Loss:21.459	translation_Loss:17.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.171                                                   	MRR:14.67	Hits@10:30.05	Best:14.77
2024-12-27 03:57:27,602: Snapshot:1	Epoch:10	Loss:20.166	translation_Loss:15.999	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.167                                                   	MRR:14.94	Hits@10:30.14	Best:14.94
2024-12-27 03:57:37,915: Snapshot:1	Epoch:11	Loss:19.249	translation_Loss:15.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.156                                                   	MRR:14.95	Hits@10:30.03	Best:14.95
2024-12-27 03:57:47,781: Snapshot:1	Epoch:12	Loss:18.483	translation_Loss:14.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.144                                                   	MRR:14.91	Hits@10:29.97	Best:14.95
2024-12-27 03:57:57,634: Snapshot:1	Epoch:13	Loss:17.923	translation_Loss:13.801	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.122                                                   	MRR:14.97	Hits@10:30.02	Best:14.97
2024-12-27 03:58:07,483: Snapshot:1	Epoch:14	Loss:17.427	translation_Loss:13.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.1                                                   	MRR:15.07	Hits@10:29.99	Best:15.07
2024-12-27 03:58:17,252: Snapshot:1	Epoch:15	Loss:17.093	translation_Loss:13.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.076                                                   	MRR:14.98	Hits@10:29.93	Best:15.07
2024-12-27 03:58:26,960: Snapshot:1	Epoch:16	Loss:16.838	translation_Loss:12.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.052                                                   	MRR:15.03	Hits@10:29.92	Best:15.07
2024-12-27 03:58:36,725: Early Stopping! Snapshot: 1 Epoch: 17 Best Results: 15.07
2024-12-27 03:58:36,726: Start to training tokens! Snapshot: 1 Epoch: 17 Loss:16.574 MRR:15.03 Best Results: 15.07
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:58:36,726: Snapshot:1	Epoch:17	Loss:16.574	translation_Loss:12.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.028                                                   	MRR:15.03	Hits@10:29.92	Best:15.07
2024-12-27 03:58:46,584: Snapshot:1	Epoch:18	Loss:191.243	translation_Loss:61.314	multi_layer_Loss:129.929	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.03	Hits@10:29.92	Best:15.07
2024-12-27 03:58:56,830: End of token training: 1 Epoch: 19 Loss:76.694 MRR:15.03 Best Results: 15.07
2024-12-27 03:58:56,830: Snapshot:1	Epoch:19	Loss:76.694	translation_Loss:61.311	multi_layer_Loss:15.383	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.03	Hits@10:29.92	Best:15.07
2024-12-27 03:58:57,121: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-27 03:59:06,419: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2491 | 0.136  | 0.3142 | 0.3863 |  0.4583 |
|     1      | 0.1516 | 0.0756 | 0.1804 | 0.2301 |  0.2991 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:59:31,927: Snapshot:2	Epoch:0	Loss:50.118	translation_Loss:49.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.715                                                   	MRR:1.95	Hits@10:4.78	Best:1.95
2024-12-27 03:59:39,347: Snapshot:2	Epoch:1	Loss:35.882	translation_Loss:33.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.482                                                   	MRR:7.42	Hits@10:17.96	Best:7.42
2024-12-27 03:59:46,649: Snapshot:2	Epoch:2	Loss:26.835	translation_Loss:22.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.217                                                   	MRR:13.64	Hits@10:29.13	Best:13.64
2024-12-27 03:59:54,035: Snapshot:2	Epoch:3	Loss:22.223	translation_Loss:17.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.043                                                   	MRR:16.87	Hits@10:33.75	Best:16.87
2024-12-27 04:00:01,436: Snapshot:2	Epoch:4	Loss:19.587	translation_Loss:14.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.308                                                   	MRR:18.67	Hits@10:35.89	Best:18.67
2024-12-27 04:00:09,499: Snapshot:2	Epoch:5	Loss:17.692	translation_Loss:12.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.336                                                   	MRR:19.39	Hits@10:36.99	Best:19.39
2024-12-27 04:00:16,948: Snapshot:2	Epoch:6	Loss:16.254	translation_Loss:10.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.259                                                   	MRR:20.21	Hits@10:37.64	Best:20.21
2024-12-27 04:00:24,281: Snapshot:2	Epoch:7	Loss:15.067	translation_Loss:9.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.121                                                   	MRR:20.56	Hits@10:37.85	Best:20.56
2024-12-27 04:00:31,553: Snapshot:2	Epoch:8	Loss:14.101	translation_Loss:9.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.954                                                   	MRR:20.51	Hits@10:38.01	Best:20.56
2024-12-27 04:00:38,875: Snapshot:2	Epoch:9	Loss:13.223	translation_Loss:8.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.779                                                   	MRR:21.03	Hits@10:38.26	Best:21.03
2024-12-27 04:00:46,158: Snapshot:2	Epoch:10	Loss:12.457	translation_Loss:7.864	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.594                                                   	MRR:20.86	Hits@10:38.48	Best:21.03
2024-12-27 04:00:53,425: Snapshot:2	Epoch:11	Loss:11.812	translation_Loss:7.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.42                                                   	MRR:20.93	Hits@10:38.56	Best:21.03
2024-12-27 04:01:00,785: Snapshot:2	Epoch:12	Loss:11.234	translation_Loss:6.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.251                                                   	MRR:21.14	Hits@10:38.31	Best:21.14
2024-12-27 04:01:08,327: Snapshot:2	Epoch:13	Loss:10.75	translation_Loss:6.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.086                                                   	MRR:21.23	Hits@10:38.32	Best:21.23
2024-12-27 04:01:15,601: Snapshot:2	Epoch:14	Loss:10.317	translation_Loss:6.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.946                                                   	MRR:21.02	Hits@10:38.09	Best:21.23
2024-12-27 04:01:22,972: Snapshot:2	Epoch:15	Loss:9.978	translation_Loss:6.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.811                                                   	MRR:21.15	Hits@10:38.0	Best:21.23
2024-12-27 04:01:30,295: Early Stopping! Snapshot: 2 Epoch: 16 Best Results: 21.23
2024-12-27 04:01:30,295: Start to training tokens! Snapshot: 2 Epoch: 16 Loss:9.666 MRR:21.05 Best Results: 21.23
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:01:30,296: Snapshot:2	Epoch:16	Loss:9.666	translation_Loss:5.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.701                                                   	MRR:21.05	Hits@10:37.78	Best:21.23
2024-12-27 04:01:38,195: Snapshot:2	Epoch:17	Loss:153.634	translation_Loss:42.281	multi_layer_Loss:111.354	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.05	Hits@10:37.78	Best:21.23
2024-12-27 04:01:45,474: End of token training: 2 Epoch: 18 Loss:72.439 MRR:21.05 Best Results: 21.23
2024-12-27 04:01:45,474: Snapshot:2	Epoch:18	Loss:72.439	translation_Loss:42.188	multi_layer_Loss:30.251	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.05	Hits@10:37.78	Best:21.23
2024-12-27 04:01:45,762: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-27 04:01:58,033: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2137 | 0.1116 | 0.2723 | 0.3362 |  0.4035 |
|     1      |  0.13  | 0.0591 | 0.155  | 0.2029 |  0.2668 |
|     2      | 0.2089 | 0.1268 | 0.2333 | 0.2903 |  0.3738 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:02:12,208: Snapshot:3	Epoch:0	Loss:21.689	translation_Loss:21.551	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.139                                                   	MRR:2.55	Hits@10:5.93	Best:2.55
2024-12-27 04:02:15,610: Snapshot:3	Epoch:1	Loss:18.387	translation_Loss:18.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.367                                                   	MRR:4.7	Hits@10:10.74	Best:4.7
2024-12-27 04:02:19,062: Snapshot:3	Epoch:2	Loss:15.94	translation_Loss:15.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.658                                                   	MRR:7.99	Hits@10:17.23	Best:7.99
2024-12-27 04:02:22,639: Snapshot:3	Epoch:3	Loss:13.986	translation_Loss:13.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.949                                                   	MRR:11.47	Hits@10:25.38	Best:11.47
2024-12-27 04:02:26,193: Snapshot:3	Epoch:4	Loss:12.344	translation_Loss:11.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.218                                                   	MRR:15.04	Hits@10:32.39	Best:15.04
2024-12-27 04:02:29,625: Snapshot:3	Epoch:5	Loss:10.895	translation_Loss:9.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.462                                                   	MRR:17.84	Hits@10:36.18	Best:17.84
2024-12-27 04:02:33,456: Snapshot:3	Epoch:6	Loss:9.686	translation_Loss:8.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.672                                                   	MRR:19.69	Hits@10:38.85	Best:19.69
2024-12-27 04:02:36,929: Snapshot:3	Epoch:7	Loss:8.711	translation_Loss:6.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.84                                                   	MRR:21.19	Hits@10:40.79	Best:21.19
2024-12-27 04:02:40,337: Snapshot:3	Epoch:8	Loss:7.832	translation_Loss:5.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.969                                                   	MRR:22.39	Hits@10:42.03	Best:22.39
2024-12-27 04:02:43,846: Snapshot:3	Epoch:9	Loss:7.12	translation_Loss:5.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.064                                                   	MRR:23.52	Hits@10:42.83	Best:23.52
2024-12-27 04:02:47,250: Snapshot:3	Epoch:10	Loss:6.579	translation_Loss:4.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.127                                                   	MRR:24.32	Hits@10:43.56	Best:24.32
2024-12-27 04:02:50,764: Snapshot:3	Epoch:11	Loss:6.122	translation_Loss:3.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.165                                                   	MRR:25.13	Hits@10:43.8	Best:25.13
2024-12-27 04:02:54,191: Snapshot:3	Epoch:12	Loss:5.768	translation_Loss:3.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.188                                                   	MRR:25.75	Hits@10:43.95	Best:25.75
2024-12-27 04:02:57,701: Snapshot:3	Epoch:13	Loss:5.489	translation_Loss:3.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.196                                                   	MRR:26.18	Hits@10:44.45	Best:26.18
2024-12-27 04:03:01,190: Snapshot:3	Epoch:14	Loss:5.239	translation_Loss:3.044	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.194                                                   	MRR:26.35	Hits@10:44.53	Best:26.35
2024-12-27 04:03:04,634: Snapshot:3	Epoch:15	Loss:5.011	translation_Loss:2.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.183                                                   	MRR:26.69	Hits@10:44.61	Best:26.69
2024-12-27 04:03:08,096: Snapshot:3	Epoch:16	Loss:4.84	translation_Loss:2.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.166                                                   	MRR:26.94	Hits@10:44.98	Best:26.94
2024-12-27 04:03:11,529: Snapshot:3	Epoch:17	Loss:4.668	translation_Loss:2.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.147                                                   	MRR:26.93	Hits@10:45.04	Best:26.94
2024-12-27 04:03:15,012: Snapshot:3	Epoch:18	Loss:4.519	translation_Loss:2.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.121                                                   	MRR:27.36	Hits@10:45.13	Best:27.36
2024-12-27 04:03:18,499: Snapshot:3	Epoch:19	Loss:4.407	translation_Loss:2.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.101                                                   	MRR:27.49	Hits@10:45.4	Best:27.49
2024-12-27 04:03:21,949: Snapshot:3	Epoch:20	Loss:4.276	translation_Loss:2.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.078                                                   	MRR:27.59	Hits@10:45.47	Best:27.59
2024-12-27 04:03:25,303: Snapshot:3	Epoch:21	Loss:4.14	translation_Loss:2.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.048                                                   	MRR:27.54	Hits@10:45.28	Best:27.59
2024-12-27 04:03:28,790: Snapshot:3	Epoch:22	Loss:4.038	translation_Loss:2.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.021                                                   	MRR:27.7	Hits@10:45.51	Best:27.7
2024-12-27 04:03:32,243: Snapshot:3	Epoch:23	Loss:3.96	translation_Loss:1.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.995                                                   	MRR:27.81	Hits@10:45.48	Best:27.81
2024-12-27 04:03:35,674: Snapshot:3	Epoch:24	Loss:3.869	translation_Loss:1.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.97                                                   	MRR:27.96	Hits@10:45.35	Best:27.96
2024-12-27 04:03:39,133: Snapshot:3	Epoch:25	Loss:3.779	translation_Loss:1.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.943                                                   	MRR:28.0	Hits@10:45.51	Best:28.0
2024-12-27 04:03:42,627: Snapshot:3	Epoch:26	Loss:3.697	translation_Loss:1.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.917                                                   	MRR:28.15	Hits@10:45.51	Best:28.15
2024-12-27 04:03:46,067: Snapshot:3	Epoch:27	Loss:3.654	translation_Loss:1.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.899                                                   	MRR:28.3	Hits@10:45.51	Best:28.3
2024-12-27 04:03:49,445: Snapshot:3	Epoch:28	Loss:3.588	translation_Loss:1.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.877                                                   	MRR:28.25	Hits@10:45.63	Best:28.3
2024-12-27 04:03:52,878: Snapshot:3	Epoch:29	Loss:3.507	translation_Loss:1.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.857                                                   	MRR:28.26	Hits@10:45.37	Best:28.3
2024-12-27 04:03:56,296: Snapshot:3	Epoch:30	Loss:3.439	translation_Loss:1.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.834                                                   	MRR:28.49	Hits@10:45.51	Best:28.49
2024-12-27 04:03:59,720: Snapshot:3	Epoch:31	Loss:3.407	translation_Loss:1.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.813                                                   	MRR:28.49	Hits@10:45.51	Best:28.49
2024-12-27 04:04:03,156: Snapshot:3	Epoch:32	Loss:3.344	translation_Loss:1.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.794                                                   	MRR:28.61	Hits@10:45.49	Best:28.61
2024-12-27 04:04:06,561: Snapshot:3	Epoch:33	Loss:3.286	translation_Loss:1.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.772                                                   	MRR:28.52	Hits@10:45.28	Best:28.61
2024-12-27 04:04:09,939: Snapshot:3	Epoch:34	Loss:3.241	translation_Loss:1.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.754                                                   	MRR:28.54	Hits@10:45.36	Best:28.61
2024-12-27 04:04:13,337: Snapshot:3	Epoch:35	Loss:3.203	translation_Loss:1.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.734                                                   	MRR:28.68	Hits@10:45.36	Best:28.68
2024-12-27 04:04:16,748: Snapshot:3	Epoch:36	Loss:3.163	translation_Loss:1.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.716                                                   	MRR:28.75	Hits@10:45.56	Best:28.75
2024-12-27 04:04:20,114: Snapshot:3	Epoch:37	Loss:3.129	translation_Loss:1.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.702                                                   	MRR:28.66	Hits@10:45.45	Best:28.75
2024-12-27 04:04:23,466: Snapshot:3	Epoch:38	Loss:3.104	translation_Loss:1.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.69                                                   	MRR:28.57	Hits@10:45.41	Best:28.75
2024-12-27 04:04:26,803: Early Stopping! Snapshot: 3 Epoch: 39 Best Results: 28.75
2024-12-27 04:04:26,803: Start to training tokens! Snapshot: 3 Epoch: 39 Loss:3.063 MRR:28.7 Best Results: 28.75
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:04:26,804: Snapshot:3	Epoch:39	Loss:3.063	translation_Loss:1.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.676                                                   	MRR:28.7	Hits@10:45.29	Best:28.75
2024-12-27 04:04:30,189: Snapshot:3	Epoch:40	Loss:78.243	translation_Loss:15.452	multi_layer_Loss:62.791	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.7	Hits@10:45.29	Best:28.75
2024-12-27 04:04:33,539: End of token training: 3 Epoch: 41 Loss:56.569 MRR:28.7 Best Results: 28.75
2024-12-27 04:04:33,539: Snapshot:3	Epoch:41	Loss:56.569	translation_Loss:15.408	multi_layer_Loss:41.162	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.7	Hits@10:45.29	Best:28.75
2024-12-27 04:04:33,905: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-27 04:04:48,099: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2113 | 0.1104 | 0.2678 | 0.3327 |  0.3996 |
|     1      | 0.1293 | 0.059  | 0.1521 | 0.2002 |  0.2661 |
|     2      | 0.1668 | 0.0909 | 0.1842 | 0.2338 |  0.3112 |
|     3      | 0.2906 | 0.2009 | 0.3306 | 0.3841 |  0.4578 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:04:59,726: Snapshot:4	Epoch:0	Loss:12.412	translation_Loss:12.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.089                                                   	MRR:4.84	Hits@10:14.07	Best:4.84
2024-12-27 04:05:02,215: Snapshot:4	Epoch:1	Loss:10.232	translation_Loss:9.997	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.234                                                   	MRR:6.93	Hits@10:19.02	Best:6.93
2024-12-27 04:05:04,691: Snapshot:4	Epoch:2	Loss:8.619	translation_Loss:8.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.39                                                   	MRR:8.68	Hits@10:22.46	Best:8.68
2024-12-27 04:05:07,139: Snapshot:4	Epoch:3	Loss:7.437	translation_Loss:6.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.555                                                   	MRR:10.31	Hits@10:25.84	Best:10.31
2024-12-27 04:05:09,534: Snapshot:4	Epoch:4	Loss:6.51	translation_Loss:5.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.715                                                   	MRR:12.1	Hits@10:29.4	Best:12.1
2024-12-27 04:05:11,951: Snapshot:4	Epoch:5	Loss:5.772	translation_Loss:4.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.86                                                   	MRR:14.12	Hits@10:33.1	Best:14.12
2024-12-27 04:05:14,431: Snapshot:4	Epoch:6	Loss:5.185	translation_Loss:4.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.986                                                   	MRR:15.86	Hits@10:36.41	Best:15.86
2024-12-27 04:05:16,914: Snapshot:4	Epoch:7	Loss:4.726	translation_Loss:3.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.09                                                   	MRR:17.38	Hits@10:39.3	Best:17.38
2024-12-27 04:05:19,309: Snapshot:4	Epoch:8	Loss:4.325	translation_Loss:3.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.173                                                   	MRR:18.6	Hits@10:41.24	Best:18.6
2024-12-27 04:05:21,810: Snapshot:4	Epoch:9	Loss:3.993	translation_Loss:2.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.238                                                   	MRR:19.68	Hits@10:43.12	Best:19.68
2024-12-27 04:05:24,294: Snapshot:4	Epoch:10	Loss:3.707	translation_Loss:2.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.287                                                   	MRR:20.48	Hits@10:44.32	Best:20.48
2024-12-27 04:05:26,753: Snapshot:4	Epoch:11	Loss:3.447	translation_Loss:2.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.324                                                   	MRR:21.7	Hits@10:45.46	Best:21.7
2024-12-27 04:05:29,245: Snapshot:4	Epoch:12	Loss:3.197	translation_Loss:1.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.35                                                   	MRR:22.84	Hits@10:46.3	Best:22.84
2024-12-27 04:05:31,665: Snapshot:4	Epoch:13	Loss:3.003	translation_Loss:1.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.367                                                   	MRR:23.71	Hits@10:46.85	Best:23.71
2024-12-27 04:05:34,149: Snapshot:4	Epoch:14	Loss:2.841	translation_Loss:1.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.376                                                   	MRR:24.17	Hits@10:47.11	Best:24.17
2024-12-27 04:05:36,599: Snapshot:4	Epoch:15	Loss:2.724	translation_Loss:1.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.379                                                   	MRR:24.53	Hits@10:47.65	Best:24.53
2024-12-27 04:05:39,046: Snapshot:4	Epoch:16	Loss:2.623	translation_Loss:1.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.381                                                   	MRR:24.88	Hits@10:48.23	Best:24.88
2024-12-27 04:05:41,509: Snapshot:4	Epoch:17	Loss:2.532	translation_Loss:1.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.378                                                   	MRR:25.23	Hits@10:48.3	Best:25.23
2024-12-27 04:05:43,929: Snapshot:4	Epoch:18	Loss:2.448	translation_Loss:1.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.369                                                   	MRR:25.42	Hits@10:48.44	Best:25.42
2024-12-27 04:05:46,433: Snapshot:4	Epoch:19	Loss:2.374	translation_Loss:1.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.359                                                   	MRR:25.59	Hits@10:48.71	Best:25.59
2024-12-27 04:05:48,869: Snapshot:4	Epoch:20	Loss:2.309	translation_Loss:0.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.348                                                   	MRR:25.7	Hits@10:48.78	Best:25.7
2024-12-27 04:05:51,313: Snapshot:4	Epoch:21	Loss:2.226	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.334                                                   	MRR:25.93	Hits@10:49.14	Best:25.93
2024-12-27 04:05:53,745: Snapshot:4	Epoch:22	Loss:2.189	translation_Loss:0.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.319                                                   	MRR:26.23	Hits@10:48.97	Best:26.23
2024-12-27 04:05:56,236: Snapshot:4	Epoch:23	Loss:2.128	translation_Loss:0.823	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.306                                                   	MRR:26.34	Hits@10:49.1	Best:26.34
2024-12-27 04:05:58,651: Snapshot:4	Epoch:24	Loss:2.078	translation_Loss:0.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.289                                                   	MRR:26.65	Hits@10:49.29	Best:26.65
2024-12-27 04:06:01,128: Snapshot:4	Epoch:25	Loss:2.019	translation_Loss:0.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.273                                                   	MRR:26.8	Hits@10:49.72	Best:26.8
2024-12-27 04:06:03,499: Snapshot:4	Epoch:26	Loss:1.965	translation_Loss:0.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.255                                                   	MRR:26.75	Hits@10:49.51	Best:26.8
2024-12-27 04:06:05,858: Snapshot:4	Epoch:27	Loss:1.93	translation_Loss:0.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.236                                                   	MRR:26.73	Hits@10:49.25	Best:26.8
2024-12-27 04:06:08,226: Early Stopping! Snapshot: 4 Epoch: 28 Best Results: 26.8
2024-12-27 04:06:08,226: Start to training tokens! Snapshot: 4 Epoch: 28 Loss:1.883 MRR:26.74 Best Results: 26.8
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:06:08,227: Snapshot:4	Epoch:28	Loss:1.883	translation_Loss:0.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.222                                                   	MRR:26.74	Hits@10:49.73	Best:26.8
2024-12-27 04:06:10,559: Snapshot:4	Epoch:29	Loss:55.486	translation_Loss:8.208	multi_layer_Loss:47.277	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.74	Hits@10:49.73	Best:26.8
2024-12-27 04:06:13,002: End of token training: 4 Epoch: 30 Loss:44.478 MRR:26.74 Best Results: 26.8
2024-12-27 04:06:13,002: Snapshot:4	Epoch:30	Loss:44.478	translation_Loss:8.188	multi_layer_Loss:36.29	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:26.74	Hits@10:49.73	Best:26.8
2024-12-27 04:06:13,368: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-27 04:06:28,670: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1996 | 0.1034 | 0.2529 | 0.3137 |  0.3739 |
|     1      | 0.1269 | 0.0569 | 0.1488 | 0.1974 |  0.2651 |
|     2      | 0.1551 | 0.0832 | 0.173  | 0.2176 |  0.2919 |
|     3      | 0.2523 | 0.1727 | 0.2834 | 0.3226 |  0.3942 |
|     4      | 0.2585 | 0.1419 | 0.2931 | 0.3797 |  0.4991 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 04:06:28,673: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2766 | 0.1633 | 0.3443 | 0.4141 |  0.4862 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2491 | 0.136  | 0.3142 | 0.3863 |  0.4583 |
|     1      | 0.1516 | 0.0756 | 0.1804 | 0.2301 |  0.2991 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2137 | 0.1116 | 0.2723 | 0.3362 |  0.4035 |
|     1      |  0.13  | 0.0591 | 0.155  | 0.2029 |  0.2668 |
|     2      | 0.2089 | 0.1268 | 0.2333 | 0.2903 |  0.3738 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2113 | 0.1104 | 0.2678 | 0.3327 |  0.3996 |
|     1      | 0.1293 | 0.059  | 0.1521 | 0.2002 |  0.2661 |
|     2      | 0.1668 | 0.0909 | 0.1842 | 0.2338 |  0.3112 |
|     3      | 0.2906 | 0.2009 | 0.3306 | 0.3841 |  0.4578 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1996 | 0.1034 | 0.2529 | 0.3137 |  0.3739 |
|     1      | 0.1269 | 0.0569 | 0.1488 | 0.1974 |  0.2651 |
|     2      | 0.1551 | 0.0832 | 0.173  | 0.2176 |  0.2919 |
|     3      | 0.2523 | 0.1727 | 0.2834 | 0.3226 |  0.3942 |
|     4      | 0.2585 | 0.1419 | 0.2931 | 0.3797 |  0.4991 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 04:06:28,673: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 362.11669874191284 |   0.277   |    0.163     |    0.344     |     0.486     |
|    1     | 216.57292318344116 |   0.202   |    0.107     |    0.249     |     0.381     |
|    2     | 155.7644443511963  |   0.182   |    0.096     |     0.22     |     0.346     |
|    3     | 154.01618933677673 |   0.183   |    0.099     |    0.218     |     0.342     |
|    4     | 83.52792572975159  |   0.177   |    0.094     |     0.21     |     0.334     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 04:06:28,673: Sum_Training_Time:971.9981813430786
2024-12-27 04:06:28,673: Every_Training_Time:[362.11669874191284, 216.57292318344116, 155.7644443511963, 154.01618933677673, 83.52792572975159]
2024-12-27 04:06:28,673: Forward transfer: 0.01765 Backward transfer: -0.04845000000000001
