2024-12-27 02:38:22,085: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227023749/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=100000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 02:38:38,411: Snapshot:0	Epoch:0	Loss:64.234	translation_Loss:64.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:35.5	Best:16.66
2024-12-27 02:38:50,447: Snapshot:0	Epoch:1	Loss:30.863	translation_Loss:30.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.26	Hits@10:44.13	Best:23.26
2024-12-27 02:39:02,623: Snapshot:0	Epoch:2	Loss:14.474	translation_Loss:14.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.91	Hits@10:46.51	Best:25.91
2024-12-27 02:39:14,648: Snapshot:0	Epoch:3	Loss:7.683	translation_Loss:7.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.73	Hits@10:47.25	Best:26.73
2024-12-27 02:39:26,722: Snapshot:0	Epoch:4	Loss:4.989	translation_Loss:4.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.03	Hits@10:47.52	Best:27.03
2024-12-27 02:39:38,849: Snapshot:0	Epoch:5	Loss:3.771	translation_Loss:3.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.14	Hits@10:47.28	Best:27.14
2024-12-27 02:39:51,384: Snapshot:0	Epoch:6	Loss:3.132	translation_Loss:3.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.79	Hits@10:46.88	Best:27.14
2024-12-27 02:40:03,636: Snapshot:0	Epoch:7	Loss:2.768	translation_Loss:2.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.61	Hits@10:46.79	Best:27.14
2024-12-27 02:40:15,805: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 27.14
2024-12-27 02:40:15,805: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:2.505 MRR:26.51 Best Results: 27.14
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 02:40:15,805: Snapshot:0	Epoch:8	Loss:2.505	translation_Loss:2.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.51	Hits@10:46.6	Best:27.14
2024-12-27 02:40:28,757: Snapshot:0	Epoch:9	Loss:64.992	translation_Loss:49.323	multi_layer_Loss:15.669	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.51	Hits@10:46.6	Best:27.14
2024-12-27 02:40:41,085: End of token training: 0 Epoch: 10 Loss:49.338 MRR:26.51 Best Results: 27.14
2024-12-27 02:40:41,085: Snapshot:0	Epoch:10	Loss:49.338	translation_Loss:49.338	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:26.51	Hits@10:46.6	Best:27.14
2024-12-27 02:40:41,436: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-27 02:40:46,597: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2746 | 0.1663 | 0.3367 | 0.4044 |  0.4775 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:41:23,036: Snapshot:1	Epoch:0	Loss:52.335	translation_Loss:47.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.873                                                   	MRR:11.92	Hits@10:24.67	Best:11.92
2024-12-27 02:41:33,182: Snapshot:1	Epoch:1	Loss:27.453	translation_Loss:22.935	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.518                                                   	MRR:12.55	Hits@10:25.55	Best:12.55
2024-12-27 02:41:43,072: Snapshot:1	Epoch:2	Loss:24.039	translation_Loss:19.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.23                                                   	MRR:12.56	Hits@10:25.39	Best:12.56
2024-12-27 02:41:54,275: Snapshot:1	Epoch:3	Loss:23.16	translation_Loss:19.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.129                                                   	MRR:12.6	Hits@10:25.37	Best:12.6
2024-12-27 02:42:06,139: Snapshot:1	Epoch:4	Loss:22.722	translation_Loss:18.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.087                                                   	MRR:12.59	Hits@10:25.67	Best:12.6
2024-12-27 02:42:17,258: Snapshot:1	Epoch:5	Loss:22.461	translation_Loss:18.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.066                                                   	MRR:12.6	Hits@10:25.53	Best:12.6
2024-12-27 02:42:28,533: Snapshot:1	Epoch:6	Loss:22.327	translation_Loss:18.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.052                                                   	MRR:12.64	Hits@10:25.59	Best:12.64
2024-12-27 02:42:39,951: Snapshot:1	Epoch:7	Loss:22.155	translation_Loss:18.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.05                                                   	MRR:12.58	Hits@10:25.46	Best:12.64
2024-12-27 02:42:51,105: Snapshot:1	Epoch:8	Loss:22.105	translation_Loss:18.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.034                                                   	MRR:12.67	Hits@10:25.72	Best:12.67
2024-12-27 02:43:01,822: Snapshot:1	Epoch:9	Loss:22.032	translation_Loss:17.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.048                                                   	MRR:12.56	Hits@10:25.63	Best:12.67
2024-12-27 02:43:11,571: Snapshot:1	Epoch:10	Loss:21.994	translation_Loss:17.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.052                                                   	MRR:12.6	Hits@10:25.52	Best:12.67
2024-12-27 02:43:21,324: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 12.67
2024-12-27 02:43:21,325: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:21.882 MRR:12.57 Best Results: 12.67
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 02:43:21,325: Snapshot:1	Epoch:11	Loss:21.882	translation_Loss:17.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.044                                                   	MRR:12.57	Hits@10:25.45	Best:12.67
2024-12-27 02:43:31,750: Snapshot:1	Epoch:12	Loss:81.913	translation_Loss:66.11	multi_layer_Loss:15.802	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.57	Hits@10:25.45	Best:12.67
2024-12-27 02:43:43,434: End of token training: 1 Epoch: 13 Loss:66.11 MRR:12.57 Best Results: 12.67
2024-12-27 02:43:43,434: Snapshot:1	Epoch:13	Loss:66.11	translation_Loss:66.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:12.57	Hits@10:25.45	Best:12.67
2024-12-27 02:43:43,714: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-27 02:43:53,111: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2712 | 0.1614 | 0.3349 | 0.4044 |  0.4781 |
|     1      | 0.1279 | 0.0645 | 0.1464 | 0.1891 |  0.254  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:44:21,100: Snapshot:2	Epoch:0	Loss:41.07	translation_Loss:35.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.998                                                   	MRR:14.78	Hits@10:25.51	Best:14.78
2024-12-27 02:44:29,499: Snapshot:2	Epoch:1	Loss:21.614	translation_Loss:16.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.47                                                   	MRR:15.89	Hits@10:26.15	Best:15.89
2024-12-27 02:44:37,860: Snapshot:2	Epoch:2	Loss:18.812	translation_Loss:13.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.09                                                   	MRR:16.1	Hits@10:26.16	Best:16.1
2024-12-27 02:44:46,078: Snapshot:2	Epoch:3	Loss:18.334	translation_Loss:13.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.027                                                   	MRR:15.76	Hits@10:25.85	Best:16.1
2024-12-27 02:44:54,313: Snapshot:2	Epoch:4	Loss:18.13	translation_Loss:13.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.994                                                   	MRR:15.87	Hits@10:26.01	Best:16.1
2024-12-27 02:45:02,572: Early Stopping! Snapshot: 2 Epoch: 5 Best Results: 16.1
2024-12-27 02:45:02,573: Start to training tokens! Snapshot: 2 Epoch: 5 Loss:18.003 MRR:15.76 Best Results: 16.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 02:45:02,573: Snapshot:2	Epoch:5	Loss:18.003	translation_Loss:13.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.971                                                   	MRR:15.76	Hits@10:25.95	Best:16.1
2024-12-27 02:45:11,315: Snapshot:2	Epoch:6	Loss:70.996	translation_Loss:54.467	multi_layer_Loss:16.529	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.76	Hits@10:25.95	Best:16.1
2024-12-27 02:45:19,663: End of token training: 2 Epoch: 7 Loss:54.43 MRR:15.76 Best Results: 16.1
2024-12-27 02:45:19,663: Snapshot:2	Epoch:7	Loss:54.43	translation_Loss:54.428	multi_layer_Loss:0.002	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.76	Hits@10:25.95	Best:16.1
2024-12-27 02:45:19,950: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-27 02:45:32,738: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1425 | 0.3111 | 0.3789 |  0.4512 |
|     1      | 0.1262 | 0.0619 | 0.1449 | 0.1886 |  0.2549 |
|     2      | 0.1575 | 0.1044 | 0.1687 | 0.2039 |  0.2587 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:45:47,843: Snapshot:3	Epoch:0	Loss:19.602	translation_Loss:17.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.951                                                   	MRR:13.69	Hits@10:25.8	Best:13.69
2024-12-27 02:45:51,730: Snapshot:3	Epoch:1	Loss:12.042	translation_Loss:10.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.826                                                   	MRR:19.94	Hits@10:32.66	Best:19.94
2024-12-27 02:45:55,699: Snapshot:3	Epoch:2	Loss:8.996	translation_Loss:7.338	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.659                                                   	MRR:21.85	Hits@10:33.8	Best:21.85
2024-12-27 02:46:00,109: Snapshot:3	Epoch:3	Loss:7.839	translation_Loss:6.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.574                                                   	MRR:22.42	Hits@10:33.79	Best:22.42
2024-12-27 02:46:04,098: Snapshot:3	Epoch:4	Loss:7.378	translation_Loss:5.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.541                                                   	MRR:22.5	Hits@10:33.81	Best:22.5
2024-12-27 02:46:07,965: Snapshot:3	Epoch:5	Loss:7.149	translation_Loss:5.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.509                                                   	MRR:22.3	Hits@10:33.72	Best:22.5
2024-12-27 02:46:11,919: Snapshot:3	Epoch:6	Loss:7.023	translation_Loss:5.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.481                                                   	MRR:22.48	Hits@10:33.75	Best:22.5
2024-12-27 02:46:15,847: Early Stopping! Snapshot: 3 Epoch: 7 Best Results: 22.5
2024-12-27 02:46:15,847: Start to training tokens! Snapshot: 3 Epoch: 7 Loss:6.946 MRR:22.27 Best Results: 22.5
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 02:46:15,847: Snapshot:3	Epoch:7	Loss:6.946	translation_Loss:5.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.47                                                   	MRR:22.27	Hits@10:33.71	Best:22.5
2024-12-27 02:46:19,741: Snapshot:3	Epoch:8	Loss:34.193	translation_Loss:20.075	multi_layer_Loss:14.118	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.27	Hits@10:33.71	Best:22.5
2024-12-27 02:46:23,575: End of token training: 3 Epoch: 9 Loss:20.169 MRR:22.27 Best Results: 22.5
2024-12-27 02:46:23,575: Snapshot:3	Epoch:9	Loss:20.169	translation_Loss:20.057	multi_layer_Loss:0.112	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.27	Hits@10:33.71	Best:22.5
2024-12-27 02:46:23,803: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-27 02:46:38,419: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2445 | 0.1378 | 0.304  | 0.3731 |  0.4488 |
|     1      | 0.1263 | 0.0618 | 0.145  | 0.1884 |  0.2552 |
|     2      | 0.159  | 0.1042 | 0.1713 | 0.2074 |  0.2635 |
|     3      | 0.2221 | 0.1557 | 0.2501 | 0.2854 |  0.3398 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:46:50,797: Snapshot:4	Epoch:0	Loss:14.192	translation_Loss:12.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.78                                                   	MRR:9.02	Hits@10:22.26	Best:9.02
2024-12-27 02:46:53,594: Snapshot:4	Epoch:1	Loss:10.256	translation_Loss:8.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.2                                                   	MRR:16.42	Hits@10:31.81	Best:16.42
2024-12-27 02:46:56,383: Snapshot:4	Epoch:2	Loss:7.947	translation_Loss:5.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.127                                                   	MRR:18.62	Hits@10:33.82	Best:18.62
2024-12-27 02:46:58,917: Snapshot:4	Epoch:3	Loss:6.872	translation_Loss:4.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.039                                                   	MRR:19.26	Hits@10:33.97	Best:19.26
2024-12-27 02:47:01,357: Snapshot:4	Epoch:4	Loss:6.353	translation_Loss:4.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.986                                                   	MRR:19.38	Hits@10:34.59	Best:19.38
2024-12-27 02:47:03,760: Snapshot:4	Epoch:5	Loss:6.087	translation_Loss:4.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.952                                                   	MRR:19.12	Hits@10:34.42	Best:19.38
2024-12-27 02:47:06,181: Snapshot:4	Epoch:6	Loss:5.962	translation_Loss:4.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.933                                                   	MRR:19.28	Hits@10:34.53	Best:19.38
2024-12-27 02:47:08,600: Snapshot:4	Epoch:7	Loss:5.847	translation_Loss:3.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.918                                                   	MRR:19.56	Hits@10:34.81	Best:19.56
2024-12-27 02:47:11,055: Snapshot:4	Epoch:8	Loss:5.828	translation_Loss:3.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.918                                                   	MRR:19.67	Hits@10:34.87	Best:19.67
2024-12-27 02:47:13,484: Snapshot:4	Epoch:9	Loss:5.778	translation_Loss:3.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.911                                                   	MRR:19.62	Hits@10:35.18	Best:19.67
2024-12-27 02:47:15,875: Snapshot:4	Epoch:10	Loss:5.754	translation_Loss:3.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.905                                                   	MRR:19.52	Hits@10:35.09	Best:19.67
2024-12-27 02:47:18,237: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 19.67
2024-12-27 02:47:18,237: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:5.713 MRR:19.54 Best Results: 19.67
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 02:47:18,238: Snapshot:4	Epoch:11	Loss:5.713	translation_Loss:3.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.897                                                   	MRR:19.54	Hits@10:35.12	Best:19.67
2024-12-27 02:47:20,611: Snapshot:4	Epoch:12	Loss:28.846	translation_Loss:13.555	multi_layer_Loss:15.29	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.54	Hits@10:35.12	Best:19.67
2024-12-27 02:47:23,000: End of token training: 4 Epoch: 13 Loss:13.828 MRR:19.54 Best Results: 19.67
2024-12-27 02:47:23,000: Snapshot:4	Epoch:13	Loss:13.828	translation_Loss:13.542	multi_layer_Loss:0.286	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.54	Hits@10:35.12	Best:19.67
2024-12-27 02:47:23,229: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-27 02:47:38,095: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.139  | 0.3064 | 0.3755 |  0.4477 |
|     1      | 0.1263 | 0.0615 | 0.1451 | 0.1893 |  0.2561 |
|     2      | 0.1519 | 0.0954 | 0.1646 | 0.2026 |  0.2607 |
|     3      | 0.2266 | 0.1579 | 0.255  | 0.2938 |  0.3488 |
|     4      | 0.191  | 0.113  | 0.2012 | 0.2549 |  0.348  |
+------------+--------+--------+--------+--------+---------+
2024-12-27 02:47:38,098: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2746 | 0.1663 | 0.3367 | 0.4044 |  0.4775 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2712 | 0.1614 | 0.3349 | 0.4044 |  0.4781 |
|     1      | 0.1279 | 0.0645 | 0.1464 | 0.1891 |  0.254  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1425 | 0.3111 | 0.3789 |  0.4512 |
|     1      | 0.1262 | 0.0619 | 0.1449 | 0.1886 |  0.2549 |
|     2      | 0.1575 | 0.1044 | 0.1687 | 0.2039 |  0.2587 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2445 | 0.1378 | 0.304  | 0.3731 |  0.4488 |
|     1      | 0.1263 | 0.0618 | 0.145  | 0.1884 |  0.2552 |
|     2      | 0.159  | 0.1042 | 0.1713 | 0.2074 |  0.2635 |
|     3      | 0.2221 | 0.1557 | 0.2501 | 0.2854 |  0.3398 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.139  | 0.3064 | 0.3755 |  0.4477 |
|     1      | 0.1263 | 0.0615 | 0.1451 | 0.1893 |  0.2561 |
|     2      | 0.1519 | 0.0954 | 0.1646 | 0.2026 |  0.2607 |
|     3      | 0.2266 | 0.1579 | 0.255  | 0.2938 |  0.3488 |
|     4      | 0.191  | 0.113  | 0.2012 | 0.2549 |  0.348  |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 02:47:38,098: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 138.99897694587708 |   0.275   |    0.166     |    0.337     |     0.477     |
|    1     | 172.66896271705627 |   0.202   |    0.114     |    0.243     |     0.369     |
|    2     |  82.8161633014679  |   0.181   |    0.104     |    0.215     |     0.331     |
|    3     | 48.969414472579956 |   0.184   |    0.107     |    0.216     |     0.332     |
|    4     | 43.13235688209534  |   0.184   |    0.106     |    0.215     |     0.334     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 02:47:38,098: Sum_Training_Time:486.58587431907654
2024-12-27 02:47:38,098: Every_Training_Time:[138.99897694587708, 172.66896271705627, 82.8161633014679, 48.969414472579956, 43.13235688209534]
2024-12-27 02:47:38,098: Forward transfer: 0.0132 Backward transfer: -0.007875000000000007
2024-12-27 02:48:13,472: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227024742/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=100000.0, token_num=1, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 02:48:29,745: Snapshot:0	Epoch:0	Loss:64.234	translation_Loss:64.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:35.48	Best:16.66
2024-12-27 02:48:41,989: Snapshot:0	Epoch:1	Loss:30.863	translation_Loss:30.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.29	Hits@10:44.25	Best:23.29
2024-12-27 02:48:54,330: Snapshot:0	Epoch:2	Loss:14.473	translation_Loss:14.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.15	Hits@10:46.61	Best:26.15
2024-12-27 02:49:06,485: Snapshot:0	Epoch:3	Loss:7.689	translation_Loss:7.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.83	Hits@10:47.19	Best:26.83
2024-12-27 02:49:18,735: Snapshot:0	Epoch:4	Loss:4.987	translation_Loss:4.987	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.99	Hits@10:47.36	Best:26.99
2024-12-27 02:49:30,925: Snapshot:0	Epoch:5	Loss:3.776	translation_Loss:3.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.07	Hits@10:47.41	Best:27.07
2024-12-27 02:49:43,531: Snapshot:0	Epoch:6	Loss:3.144	translation_Loss:3.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.83	Hits@10:46.96	Best:27.07
2024-12-27 02:49:55,742: Snapshot:0	Epoch:7	Loss:2.765	translation_Loss:2.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.69	Hits@10:46.83	Best:27.07
2024-12-27 02:50:08,000: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 27.07
2024-12-27 02:50:08,001: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:2.517 MRR:26.66 Best Results: 27.07
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 02:50:08,001: Snapshot:0	Epoch:8	Loss:2.517	translation_Loss:2.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.66	Hits@10:46.75	Best:27.07
2024-12-27 02:50:20,886: Snapshot:0	Epoch:9	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.66	Hits@10:46.75	Best:27.07
2024-12-27 02:50:33,327: End of token training: 0 Epoch: 10 Loss:nan MRR:26.66 Best Results: 27.07
2024-12-27 02:50:33,327: Snapshot:0	Epoch:10	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:26.66	Hits@10:46.75	Best:27.07
2024-12-27 02:50:33,666: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-27 02:50:38,775: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.274 | 0.1655 | 0.3357 | 0.4048 |  0.4768 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:51:15,030: Snapshot:1	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 02:51:26,255: Snapshot:1	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 02:51:36,996: Snapshot:1	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 02:51:47,050: Early Stopping! Snapshot: 1 Epoch: 3 Best Results: 0.13
2024-12-27 02:51:47,050: Start to training tokens! Snapshot: 1 Epoch: 3 Loss:nan MRR:0.13 Best Results: 0.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 02:51:47,050: Snapshot:1	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 02:51:57,645: Snapshot:1	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 02:52:07,503: End of token training: 1 Epoch: 5 Loss:nan MRR:0.13 Best Results: 0.13
2024-12-27 02:52:07,504: Snapshot:1	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.13	Hits@10:0.13	Best:0.13
2024-12-27 02:52:07,818: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-27 02:52:16,422: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:52:41,921: Snapshot:2	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 02:52:49,324: Snapshot:2	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 02:52:56,651: Snapshot:2	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 02:53:03,892: Early Stopping! Snapshot: 2 Epoch: 3 Best Results: 0.19
2024-12-27 02:53:03,892: Start to training tokens! Snapshot: 2 Epoch: 3 Loss:nan MRR:0.19 Best Results: 0.19
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 02:53:03,892: Snapshot:2	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 02:53:11,110: Snapshot:2	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 02:53:18,465: End of token training: 2 Epoch: 5 Loss:nan MRR:0.19 Best Results: 0.19
2024-12-27 02:53:18,465: Snapshot:2	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.19	Hits@10:0.04	Best:0.19
2024-12-27 02:53:18,821: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-27 02:53:30,731: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:53:45,186: Snapshot:3	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 02:53:49,052: Snapshot:3	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 02:53:52,907: Snapshot:3	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 02:53:56,784: Early Stopping! Snapshot: 3 Epoch: 3 Best Results: 0.07
2024-12-27 02:53:56,785: Start to training tokens! Snapshot: 3 Epoch: 3 Loss:nan MRR:0.07 Best Results: 0.07
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 02:53:56,785: Snapshot:3	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 02:54:00,704: Snapshot:3	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 02:54:04,567: End of token training: 3 Epoch: 5 Loss:nan MRR:0.07 Best Results: 0.07
2024-12-27 02:54:04,567: Snapshot:3	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.07	Hits@10:0.01	Best:0.07
2024-12-27 02:54:04,817: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-27 02:54:19,817: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:54:32,165: Snapshot:4	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 02:54:34,955: Snapshot:4	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 02:54:37,710: Snapshot:4	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 02:54:40,444: Early Stopping! Snapshot: 4 Epoch: 3 Best Results: 0.06
2024-12-27 02:54:40,445: Start to training tokens! Snapshot: 4 Epoch: 3 Loss:nan MRR:0.06 Best Results: 0.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2024-12-27 02:54:40,445: Snapshot:4	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 02:54:43,240: Snapshot:4	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 02:54:46,018: End of token training: 4 Epoch: 5 Loss:nan MRR:0.06 Best Results: 0.06
2024-12-27 02:54:46,018: Snapshot:4	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.06	Hits@10:0.03	Best:0.06
2024-12-27 02:54:46,251: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-27 02:55:02,318: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
|     4      | 0.0007 |  0.0   |  0.0   | 0.0002 |  0.0005 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 02:55:02,320: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.274 | 0.1655 | 0.3357 | 0.4048 |  0.4768 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0079 | 0.0002 | 0.0004 | 0.0006 |  0.0206 |
|     1      | 0.0012 | 0.0002 | 0.0006 | 0.0007 |  0.0012 |
|     2      | 0.002  |  0.0   | 0.0002 | 0.0003 |  0.0008 |
|     3      | 0.0006 |  0.0   |  0.0   | 0.0001 |  0.0001 |
|     4      | 0.0007 |  0.0   |  0.0   | 0.0002 |  0.0005 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 02:55:02,321: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 139.8544909954071  |   0.274   |    0.166     |    0.336     |     0.477     |
|    1     | 84.64312267303467  |   0.005   |     0.0      |     0.0      |     0.011     |
|    2     | 58.85765743255615  |   0.004   |     0.0      |     0.0      |     0.009     |
|    3     | 32.110501527786255 |   0.004   |     0.0      |     0.0      |     0.008     |
|    4     | 25.06812882423401  |   0.003   |     0.0      |     0.0      |     0.007     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 02:55:02,321: Sum_Training_Time:340.5339014530182
2024-12-27 02:55:02,321: Every_Training_Time:[139.8544909954071, 84.64312267303467, 58.85765743255615, 32.110501527786255, 25.06812882423401]
2024-12-27 02:55:02,321: Forward transfer: 0.006549999999999999 Backward transfer: -0.066525
2024-12-27 02:55:39,066: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227025508/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=100000.0, token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 02:55:55,393: Snapshot:0	Epoch:0	Loss:64.234	translation_Loss:64.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:35.48	Best:16.66
2024-12-27 02:56:07,786: Snapshot:0	Epoch:1	Loss:30.865	translation_Loss:30.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:44.09	Best:23.24
2024-12-27 02:56:20,235: Snapshot:0	Epoch:2	Loss:14.473	translation_Loss:14.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.03	Hits@10:46.46	Best:26.03
2024-12-27 02:56:32,379: Snapshot:0	Epoch:3	Loss:7.683	translation_Loss:7.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.72	Hits@10:47.14	Best:26.72
2024-12-27 02:56:44,598: Snapshot:0	Epoch:4	Loss:4.978	translation_Loss:4.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.01	Hits@10:47.51	Best:27.01
2024-12-27 02:56:56,942: Snapshot:0	Epoch:5	Loss:3.774	translation_Loss:3.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.0	Hits@10:47.28	Best:27.01
2024-12-27 02:57:09,736: Snapshot:0	Epoch:6	Loss:3.14	translation_Loss:3.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.78	Hits@10:46.94	Best:27.01
2024-12-27 02:57:20,399: Early Stopping! Snapshot: 0 Epoch: 7 Best Results: 27.01
2024-12-27 02:57:20,400: Start to training tokens! Snapshot: 0 Epoch: 7 Loss:2.757 MRR:26.72 Best Results: 27.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 02:57:20,400: Snapshot:0	Epoch:7	Loss:2.757	translation_Loss:2.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.72	Hits@10:46.68	Best:27.01
2024-12-27 02:57:31,917: Snapshot:0	Epoch:8	Loss:67.367	translation_Loss:49.697	multi_layer_Loss:17.671	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.72	Hits@10:46.68	Best:27.01
2024-12-27 02:57:42,931: End of token training: 0 Epoch: 9 Loss:49.603 MRR:26.72 Best Results: 27.01
2024-12-27 02:57:42,931: Snapshot:0	Epoch:9	Loss:49.603	translation_Loss:49.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:26.72	Hits@10:46.68	Best:27.01
2024-12-27 02:57:43,273: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-27 02:57:47,991: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.274 | 0.1637 | 0.3394 | 0.4079 |  0.479  |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:58:21,390: Snapshot:1	Epoch:0	Loss:53.53	translation_Loss:48.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.72                                                   	MRR:11.72	Hits@10:24.46	Best:11.72
2024-12-27 02:58:31,563: Snapshot:1	Epoch:1	Loss:28.523	translation_Loss:24.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.218                                                   	MRR:12.32	Hits@10:24.83	Best:12.32
2024-12-27 02:58:41,520: Snapshot:1	Epoch:2	Loss:25.055	translation_Loss:21.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.007                                                   	MRR:12.41	Hits@10:24.97	Best:12.41
2024-12-27 02:58:51,456: Snapshot:1	Epoch:3	Loss:24.077	translation_Loss:20.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.923                                                   	MRR:12.56	Hits@10:25.09	Best:12.56
2024-12-27 02:59:01,495: Snapshot:1	Epoch:4	Loss:23.644	translation_Loss:19.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.868                                                   	MRR:12.45	Hits@10:25.05	Best:12.56
2024-12-27 02:59:11,381: Snapshot:1	Epoch:5	Loss:23.413	translation_Loss:19.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.867                                                   	MRR:12.51	Hits@10:25.04	Best:12.56
2024-12-27 02:59:21,351: Early Stopping! Snapshot: 1 Epoch: 6 Best Results: 12.56
2024-12-27 02:59:21,351: Start to training tokens! Snapshot: 1 Epoch: 6 Loss:23.177 MRR:12.44 Best Results: 12.56
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 02:59:21,351: Snapshot:1	Epoch:6	Loss:23.177	translation_Loss:19.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.843                                                   	MRR:12.44	Hits@10:24.98	Best:12.56
2024-12-27 02:59:31,272: Snapshot:1	Epoch:7	Loss:84.72	translation_Loss:66.475	multi_layer_Loss:18.246	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.44	Hits@10:24.98	Best:12.56
2024-12-27 02:59:42,569: End of token training: 1 Epoch: 8 Loss:66.475 MRR:12.44 Best Results: 12.56
2024-12-27 02:59:42,569: Snapshot:1	Epoch:8	Loss:66.475	translation_Loss:66.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:12.44	Hits@10:24.98	Best:12.56
2024-12-27 02:59:42,924: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-27 02:59:52,282: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2736 | 0.1632 | 0.3384 | 0.4079 |  0.4787 |
|     1      | 0.1258 | 0.0624 | 0.146  | 0.187  |  0.2495 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:00:20,901: Snapshot:2	Epoch:0	Loss:43.45	translation_Loss:37.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.522                                                   	MRR:12.63	Hits@10:21.43	Best:12.63
2024-12-27 03:00:29,355: Snapshot:2	Epoch:1	Loss:23.966	translation_Loss:18.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.016                                                   	MRR:14.41	Hits@10:23.3	Best:14.41
2024-12-27 03:00:37,773: Snapshot:2	Epoch:2	Loss:21.116	translation_Loss:16.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.753                                                   	MRR:14.47	Hits@10:23.16	Best:14.47
2024-12-27 03:00:46,289: Snapshot:2	Epoch:3	Loss:20.669	translation_Loss:15.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.686                                                   	MRR:14.52	Hits@10:23.19	Best:14.52
2024-12-27 03:00:55,074: Snapshot:2	Epoch:4	Loss:20.439	translation_Loss:15.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.659                                                   	MRR:14.49	Hits@10:23.22	Best:14.52
2024-12-27 03:01:03,558: Snapshot:2	Epoch:5	Loss:20.353	translation_Loss:15.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.649                                                   	MRR:14.52	Hits@10:23.14	Best:14.52
2024-12-27 03:01:11,954: Early Stopping! Snapshot: 2 Epoch: 6 Best Results: 14.52
2024-12-27 03:01:11,955: Start to training tokens! Snapshot: 2 Epoch: 6 Loss:20.288 MRR:14.5 Best Results: 14.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:01:11,955: Snapshot:2	Epoch:6	Loss:20.288	translation_Loss:15.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.646                                                   	MRR:14.5	Hits@10:23.3	Best:14.52
2024-12-27 03:01:20,460: Snapshot:2	Epoch:7	Loss:74.167	translation_Loss:55.876	multi_layer_Loss:18.291	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.5	Hits@10:23.3	Best:14.52
2024-12-27 03:01:28,919: End of token training: 2 Epoch: 8 Loss:55.873 MRR:14.5 Best Results: 14.52
2024-12-27 03:01:28,919: Snapshot:2	Epoch:8	Loss:55.873	translation_Loss:55.871	multi_layer_Loss:0.002	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:14.5	Hits@10:23.3	Best:14.52
2024-12-27 03:01:29,289: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-27 03:01:41,889: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.254  | 0.1454 | 0.3169 | 0.3861 |  0.4558 |
|     1      | 0.125  | 0.0608 | 0.146  | 0.1869 |  0.2489 |
|     2      | 0.1427 | 0.0943 | 0.1521 | 0.1831 |  0.2335 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:01:57,385: Snapshot:3	Epoch:0	Loss:20.621	translation_Loss:18.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.151                                                   	MRR:11.79	Hits@10:23.69	Best:11.79
2024-12-27 03:02:01,434: Snapshot:3	Epoch:1	Loss:12.902	translation_Loss:11.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.656                                                   	MRR:18.83	Hits@10:31.5	Best:18.83
2024-12-27 03:02:05,409: Snapshot:3	Epoch:2	Loss:9.779	translation_Loss:8.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.565                                                   	MRR:20.7	Hits@10:33.04	Best:20.7
2024-12-27 03:02:09,430: Snapshot:3	Epoch:3	Loss:8.604	translation_Loss:7.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.518                                                   	MRR:21.51	Hits@10:32.75	Best:21.51
2024-12-27 03:02:13,445: Snapshot:3	Epoch:4	Loss:8.148	translation_Loss:6.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.494                                                   	MRR:21.63	Hits@10:33.09	Best:21.63
2024-12-27 03:02:17,521: Snapshot:3	Epoch:5	Loss:7.891	translation_Loss:6.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.458                                                   	MRR:21.76	Hits@10:33.06	Best:21.76
2024-12-27 03:02:21,432: Snapshot:3	Epoch:6	Loss:7.761	translation_Loss:6.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.439                                                   	MRR:21.57	Hits@10:32.67	Best:21.76
2024-12-27 03:02:25,368: Snapshot:3	Epoch:7	Loss:7.687	translation_Loss:6.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.42                                                   	MRR:21.67	Hits@10:32.55	Best:21.76
2024-12-27 03:02:29,250: Snapshot:3	Epoch:8	Loss:7.642	translation_Loss:6.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.406                                                   	MRR:21.8	Hits@10:32.85	Best:21.8
2024-12-27 03:02:33,182: Snapshot:3	Epoch:9	Loss:7.611	translation_Loss:6.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.394                                                   	MRR:21.83	Hits@10:32.94	Best:21.83
2024-12-27 03:02:37,169: Snapshot:3	Epoch:10	Loss:7.565	translation_Loss:6.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.391                                                   	MRR:21.74	Hits@10:32.78	Best:21.83
2024-12-27 03:02:41,183: Snapshot:3	Epoch:11	Loss:7.539	translation_Loss:6.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.374                                                   	MRR:21.91	Hits@10:32.89	Best:21.91
2024-12-27 03:02:45,097: Snapshot:3	Epoch:12	Loss:7.53	translation_Loss:6.159	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.37                                                   	MRR:21.77	Hits@10:32.8	Best:21.91
2024-12-27 03:02:48,993: Snapshot:3	Epoch:13	Loss:7.511	translation_Loss:6.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.365                                                   	MRR:21.81	Hits@10:32.85	Best:21.91
2024-12-27 03:02:53,201: Snapshot:3	Epoch:14	Loss:7.483	translation_Loss:6.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.359                                                   	MRR:21.92	Hits@10:32.85	Best:21.92
2024-12-27 03:02:56,698: Snapshot:3	Epoch:15	Loss:7.452	translation_Loss:6.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.357                                                   	MRR:21.8	Hits@10:32.79	Best:21.92
2024-12-27 03:03:00,183: Snapshot:3	Epoch:16	Loss:7.438	translation_Loss:6.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.352                                                   	MRR:21.86	Hits@10:32.84	Best:21.92
2024-12-27 03:03:03,616: Early Stopping! Snapshot: 3 Epoch: 17 Best Results: 21.92
2024-12-27 03:03:03,616: Start to training tokens! Snapshot: 3 Epoch: 17 Loss:7.447 MRR:21.75 Best Results: 21.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:03:03,616: Snapshot:3	Epoch:17	Loss:7.447	translation_Loss:6.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.353                                                   	MRR:21.75	Hits@10:33.14	Best:21.92
2024-12-27 03:03:07,084: Snapshot:3	Epoch:18	Loss:38.265	translation_Loss:20.665	multi_layer_Loss:17.6	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.75	Hits@10:33.14	Best:21.92
2024-12-27 03:03:10,536: End of token training: 3 Epoch: 19 Loss:20.781 MRR:21.75 Best Results: 21.92
2024-12-27 03:03:10,536: Snapshot:3	Epoch:19	Loss:20.781	translation_Loss:20.66	multi_layer_Loss:0.121	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.75	Hits@10:33.14	Best:21.92
2024-12-27 03:03:10,824: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-27 03:03:24,448: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.251  | 0.1416 | 0.3144 | 0.3845 |  0.4543 |
|     1      | 0.125  | 0.0609 | 0.1454 | 0.1869 |  0.2492 |
|     2      | 0.1449 | 0.096  | 0.1553 | 0.1863 |  0.2362 |
|     3      | 0.219  | 0.1533 | 0.2476 | 0.2804 |  0.3305 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:03:35,636: Snapshot:4	Epoch:0	Loss:15.321	translation_Loss:13.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.934                                                   	MRR:7.29	Hits@10:18.88	Best:7.29
2024-12-27 03:03:38,157: Snapshot:4	Epoch:1	Loss:11.417	translation_Loss:9.724	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.693                                                   	MRR:14.3	Hits@10:28.22	Best:14.3
2024-12-27 03:03:41,093: Snapshot:4	Epoch:2	Loss:9.13	translation_Loss:7.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.7                                                   	MRR:17.0	Hits@10:30.69	Best:17.0
2024-12-27 03:03:43,607: Snapshot:4	Epoch:3	Loss:7.971	translation_Loss:6.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.66                                                   	MRR:17.76	Hits@10:31.41	Best:17.76
2024-12-27 03:03:46,132: Snapshot:4	Epoch:4	Loss:7.423	translation_Loss:5.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.623                                                   	MRR:17.98	Hits@10:31.6	Best:17.98
2024-12-27 03:03:48,614: Snapshot:4	Epoch:5	Loss:7.161	translation_Loss:5.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.601                                                   	MRR:17.95	Hits@10:31.76	Best:17.98
2024-12-27 03:03:51,091: Snapshot:4	Epoch:6	Loss:7.032	translation_Loss:5.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.6                                                   	MRR:17.53	Hits@10:31.85	Best:17.98
2024-12-27 03:03:53,535: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 17.98
2024-12-27 03:03:53,535: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:6.952 MRR:17.71 Best Results: 17.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:03:53,535: Snapshot:4	Epoch:7	Loss:6.952	translation_Loss:5.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.6                                                   	MRR:17.71	Hits@10:31.76	Best:17.98
2024-12-27 03:03:56,007: Snapshot:4	Epoch:8	Loss:31.677	translation_Loss:14.527	multi_layer_Loss:17.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.71	Hits@10:31.76	Best:17.98
2024-12-27 03:03:58,460: End of token training: 4 Epoch: 9 Loss:14.857 MRR:17.71 Best Results: 17.98
2024-12-27 03:03:58,460: Snapshot:4	Epoch:9	Loss:14.857	translation_Loss:14.497	multi_layer_Loss:0.36	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.71	Hits@10:31.76	Best:17.98
2024-12-27 03:03:58,811: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-27 03:04:13,313: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2514 | 0.1429 | 0.314  | 0.384  |  0.4536 |
|     1      | 0.1249 | 0.0606 | 0.1455 | 0.1875 |  0.2496 |
|     2      | 0.1424 | 0.0929 | 0.1527 | 0.1849 |  0.2341 |
|     3      | 0.2233 | 0.1563 | 0.2508 | 0.2843 |   0.34  |
|     4      | 0.1763 | 0.1074 | 0.1857 | 0.2367 |  0.3163 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 03:04:13,315: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.274 | 0.1637 | 0.3394 | 0.4079 |  0.479  |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2736 | 0.1632 | 0.3384 | 0.4079 |  0.4787 |
|     1      | 0.1258 | 0.0624 | 0.146  | 0.187  |  0.2495 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.254  | 0.1454 | 0.3169 | 0.3861 |  0.4558 |
|     1      | 0.125  | 0.0608 | 0.146  | 0.1869 |  0.2489 |
|     2      | 0.1427 | 0.0943 | 0.1521 | 0.1831 |  0.2335 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.251  | 0.1416 | 0.3144 | 0.3845 |  0.4543 |
|     1      | 0.125  | 0.0609 | 0.1454 | 0.1869 |  0.2492 |
|     2      | 0.1449 | 0.096  | 0.1553 | 0.1863 |  0.2362 |
|     3      | 0.219  | 0.1533 | 0.2476 | 0.2804 |  0.3305 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2514 | 0.1429 | 0.314  | 0.384  |  0.4536 |
|     1      | 0.1249 | 0.0606 | 0.1455 | 0.1875 |  0.2496 |
|     2      | 0.1424 | 0.0929 | 0.1527 | 0.1849 |  0.2341 |
|     3      | 0.2233 | 0.1563 | 0.2508 | 0.2843 |   0.34  |
|     4      | 0.1763 | 0.1074 | 0.1857 | 0.2367 |  0.3163 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 03:04:13,316: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 123.86452746391296 |   0.274   |    0.164     |    0.339     |     0.479     |
|    1     | 110.80987119674683 |   0.202   |    0.114     |    0.245     |     0.367     |
|    2     |  93.1658284664154  |   0.179   |    0.102     |    0.213     |     0.324     |
|    3     | 86.76258659362793  |   0.183   |    0.106     |    0.216     |     0.325     |
|    4     | 32.63525891304016  |   0.182   |    0.106     |    0.214     |     0.325     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 03:04:13,316: Sum_Training_Time:447.2380726337433
2024-12-27 03:04:13,316: Every_Training_Time:[123.86452746391296, 110.80987119674683, 93.1658284664154, 86.76258659362793, 32.63525891304016]
2024-12-27 03:04:13,316: Forward transfer: 0.012825 Backward transfer: -0.004875000000000001
