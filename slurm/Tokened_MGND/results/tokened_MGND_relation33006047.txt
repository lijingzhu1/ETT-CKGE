2024-12-26 23:28:21,463: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241226232742/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-26 23:28:37,209: Snapshot:0	Epoch:0	Loss:34.762	translation_Loss:34.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.3	Hits@10:29.6	Best:12.3
2024-12-26 23:28:48,022: Snapshot:0	Epoch:1	Loss:19.584	translation_Loss:19.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.23	Hits@10:42.56	Best:21.23
2024-12-26 23:28:58,377: Snapshot:0	Epoch:2	Loss:10.295	translation_Loss:10.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:46.32	Best:25.04
2024-12-26 23:29:08,861: Snapshot:0	Epoch:3	Loss:5.407	translation_Loss:5.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.57	Hits@10:47.59	Best:26.57
2024-12-26 23:29:19,311: Snapshot:0	Epoch:4	Loss:3.235	translation_Loss:3.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.13	Hits@10:48.0	Best:27.13
2024-12-26 23:29:30,149: Snapshot:0	Epoch:5	Loss:2.269	translation_Loss:2.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.47	Hits@10:48.16	Best:27.47
2024-12-26 23:29:40,503: Snapshot:0	Epoch:6	Loss:1.784	translation_Loss:1.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.32	Hits@10:47.92	Best:27.47
2024-12-26 23:29:50,842: Snapshot:0	Epoch:7	Loss:1.506	translation_Loss:1.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.27	Hits@10:47.82	Best:27.47
2024-12-26 23:30:01,620: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 27.47
2024-12-26 23:30:01,621: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:1.328 MRR:27.27 Best Results: 27.47
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:30:01,621: Snapshot:0	Epoch:8	Loss:1.328	translation_Loss:1.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.27	Hits@10:47.77	Best:27.47
2024-12-26 23:30:12,719: Snapshot:0	Epoch:9	Loss:40.531	translation_Loss:24.874	multi_layer_Loss:15.656	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.27	Hits@10:47.77	Best:27.47
2024-12-26 23:30:23,246: End of token training: 0 Epoch: 10 Loss:24.895 MRR:27.27 Best Results: 27.47
2024-12-26 23:30:23,246: Snapshot:0	Epoch:10	Loss:24.895	translation_Loss:24.883	multi_layer_Loss:0.013	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.27	Hits@10:47.77	Best:27.47
2024-12-26 23:30:23,528: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-26 23:30:28,333: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2792 | 0.1681 | 0.3452 | 0.4135 |  0.4859 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:31:01,608: Snapshot:1	Epoch:0	Loss:27.12	translation_Loss:25.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.336                                                   	MRR:13.25	Hits@10:29.43	Best:13.25
2024-12-26 23:31:11,148: Snapshot:1	Epoch:1	Loss:10.676	translation_Loss:7.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.805                                                   	MRR:18.09	Hits@10:35.21	Best:18.09
2024-12-26 23:31:21,771: Snapshot:1	Epoch:2	Loss:6.311	translation_Loss:3.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.121                                                   	MRR:18.51	Hits@10:35.91	Best:18.51
2024-12-26 23:31:31,790: Snapshot:1	Epoch:3	Loss:5.252	translation_Loss:2.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.983                                                   	MRR:18.49	Hits@10:35.5	Best:18.51
2024-12-26 23:31:42,374: Snapshot:1	Epoch:4	Loss:4.914	translation_Loss:2.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.897                                                   	MRR:18.48	Hits@10:35.24	Best:18.51
2024-12-26 23:31:53,305: Early Stopping! Snapshot: 1 Epoch: 5 Best Results: 18.51
2024-12-26 23:31:53,305: Start to training tokens! Snapshot: 1 Epoch: 5 Loss:4.767 MRR:18.31 Best Results: 18.51
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:31:53,305: Snapshot:1	Epoch:5	Loss:4.767	translation_Loss:1.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.836                                                   	MRR:18.31	Hits@10:35.09	Best:18.51
2024-12-26 23:32:04,144: Snapshot:1	Epoch:6	Loss:42.242	translation_Loss:26.458	multi_layer_Loss:15.784	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.31	Hits@10:35.09	Best:18.51
2024-12-26 23:32:15,012: End of token training: 1 Epoch: 7 Loss:26.514 MRR:18.31 Best Results: 18.51
2024-12-26 23:32:15,012: Snapshot:1	Epoch:7	Loss:26.514	translation_Loss:26.495	multi_layer_Loss:0.018	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.31	Hits@10:35.09	Best:18.51
2024-12-26 23:32:15,368: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-26 23:32:24,488: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2222 | 0.1196 | 0.2783 | 0.3399 |  0.4104 |
|     1      | 0.1864 | 0.0976 | 0.2214 | 0.2804 |  0.3596 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:32:52,009: Snapshot:2	Epoch:0	Loss:15.301	translation_Loss:14.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.968                                                   	MRR:14.84	Hits@10:31.21	Best:14.84
2024-12-26 23:32:59,825: Snapshot:2	Epoch:1	Loss:4.671	translation_Loss:3.009	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.662                                                   	MRR:20.3	Hits@10:37.94	Best:20.3
2024-12-26 23:33:06,928: Snapshot:2	Epoch:2	Loss:2.826	translation_Loss:1.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.565                                                   	MRR:21.93	Hits@10:39.97	Best:21.93
2024-12-26 23:33:14,019: Snapshot:2	Epoch:3	Loss:2.292	translation_Loss:0.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.42                                                   	MRR:22.09	Hits@10:39.99	Best:22.09
2024-12-26 23:33:21,254: Snapshot:2	Epoch:4	Loss:2.043	translation_Loss:0.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.328                                                   	MRR:22.18	Hits@10:40.12	Best:22.18
2024-12-26 23:33:28,527: Snapshot:2	Epoch:5	Loss:1.918	translation_Loss:0.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.259                                                   	MRR:22.28	Hits@10:40.26	Best:22.28
2024-12-26 23:33:35,581: Snapshot:2	Epoch:6	Loss:1.821	translation_Loss:0.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.21                                                   	MRR:22.02	Hits@10:39.9	Best:22.28
2024-12-26 23:33:42,754: Snapshot:2	Epoch:7	Loss:1.764	translation_Loss:0.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.174                                                   	MRR:22.27	Hits@10:39.83	Best:22.28
2024-12-26 23:33:50,450: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 22.28
2024-12-26 23:33:50,451: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:1.731 MRR:22.06 Best Results: 22.28
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:33:50,451: Snapshot:2	Epoch:8	Loss:1.731	translation_Loss:0.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.148                                                   	MRR:22.06	Hits@10:39.73	Best:22.28
2024-12-26 23:33:57,470: Snapshot:2	Epoch:9	Loss:34.701	translation_Loss:18.27	multi_layer_Loss:16.431	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.06	Hits@10:39.73	Best:22.28
2024-12-26 23:34:04,458: End of token training: 2 Epoch: 10 Loss:18.378 MRR:22.06 Best Results: 22.28
2024-12-26 23:34:04,458: Snapshot:2	Epoch:10	Loss:18.378	translation_Loss:18.28	multi_layer_Loss:0.098	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.06	Hits@10:39.73	Best:22.28
2024-12-26 23:34:04,741: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-26 23:34:16,448: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1814 | 0.097  | 0.2212 | 0.2771 |  0.3433 |
|     1      | 0.1535 | 0.0756 | 0.1827 | 0.2346 |  0.2991 |
|     2      | 0.2211 | 0.1338 | 0.2424 | 0.3061 |  0.3987 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:34:30,488: Snapshot:3	Epoch:0	Loss:7.826	translation_Loss:7.61	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.216                                                   	MRR:13.4	Hits@10:31.23	Best:13.4
2024-12-26 23:34:33,744: Snapshot:3	Epoch:1	Loss:3.01	translation_Loss:2.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.5                                                   	MRR:23.91	Hits@10:43.72	Best:23.91
2024-12-26 23:34:36,999: Snapshot:3	Epoch:2	Loss:1.301	translation_Loss:0.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.615                                                   	MRR:27.99	Hits@10:46.86	Best:27.99
2024-12-26 23:34:40,723: Snapshot:3	Epoch:3	Loss:0.823	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.59                                                   	MRR:29.78	Hits@10:47.91	Best:29.78
2024-12-26 23:34:43,971: Snapshot:3	Epoch:4	Loss:0.657	translation_Loss:0.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.517                                                   	MRR:30.36	Hits@10:47.94	Best:30.36
2024-12-26 23:34:47,331: Snapshot:3	Epoch:5	Loss:0.574	translation_Loss:0.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.459                                                   	MRR:30.65	Hits@10:48.19	Best:30.65
2024-12-26 23:34:50,652: Snapshot:3	Epoch:6	Loss:0.536	translation_Loss:0.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.426                                                   	MRR:30.56	Hits@10:48.12	Best:30.65
2024-12-26 23:34:53,930: Snapshot:3	Epoch:7	Loss:0.509	translation_Loss:0.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.407                                                   	MRR:30.56	Hits@10:48.25	Best:30.65
2024-12-26 23:34:57,161: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 30.65
2024-12-26 23:34:57,161: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:0.483 MRR:30.4 Best Results: 30.65
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:34:57,161: Snapshot:3	Epoch:8	Loss:0.483	translation_Loss:0.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:30.4	Hits@10:48.5	Best:30.65
2024-12-26 23:35:00,413: Snapshot:3	Epoch:9	Loss:18.88	translation_Loss:5.357	multi_layer_Loss:13.523	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.4	Hits@10:48.5	Best:30.65
2024-12-26 23:35:03,611: End of token training: 3 Epoch: 10 Loss:6.0 MRR:30.4 Best Results: 30.65
2024-12-26 23:35:03,611: Snapshot:3	Epoch:10	Loss:6.0	translation_Loss:5.405	multi_layer_Loss:0.596	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.4	Hits@10:48.5	Best:30.65
2024-12-26 23:35:03,886: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-26 23:35:17,476: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1742 | 0.0931 | 0.2104 | 0.2662 |  0.3308 |
|     1      | 0.1456 | 0.0708 | 0.1729 | 0.2206 |  0.2866 |
|     2      | 0.1689 | 0.0938 | 0.1811 | 0.2301 |  0.3168 |
|     3      | 0.3071 | 0.2114 | 0.3501 | 0.4044 |  0.4824 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:35:28,773: Snapshot:4	Epoch:0	Loss:3.975	translation_Loss:3.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.123                                                   	MRR:11.81	Hits@10:28.27	Best:11.81
2024-12-26 23:35:31,189: Snapshot:4	Epoch:1	Loss:1.742	translation_Loss:1.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.287                                                   	MRR:19.13	Hits@10:41.38	Best:19.13
2024-12-26 23:35:33,590: Snapshot:4	Epoch:2	Loss:0.836	translation_Loss:0.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.31                                                   	MRR:24.06	Hits@10:45.96	Best:24.06
2024-12-26 23:35:35,937: Snapshot:4	Epoch:3	Loss:0.459	translation_Loss:0.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.303                                                   	MRR:25.72	Hits@10:48.19	Best:25.72
2024-12-26 23:35:38,284: Snapshot:4	Epoch:4	Loss:0.345	translation_Loss:0.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.274                                                   	MRR:26.57	Hits@10:48.93	Best:26.57
2024-12-26 23:35:40,698: Snapshot:4	Epoch:5	Loss:0.287	translation_Loss:0.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:26.79	Hits@10:48.96	Best:26.79
2024-12-26 23:35:43,331: Snapshot:4	Epoch:6	Loss:0.254	translation_Loss:0.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.212                                                   	MRR:26.98	Hits@10:49.55	Best:26.98
2024-12-26 23:35:46,046: Snapshot:4	Epoch:7	Loss:0.234	translation_Loss:0.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.196                                                   	MRR:27.13	Hits@10:49.61	Best:27.13
2024-12-26 23:35:48,714: Snapshot:4	Epoch:8	Loss:0.226	translation_Loss:0.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.185                                                   	MRR:27.38	Hits@10:49.98	Best:27.38
2024-12-26 23:35:51,383: Snapshot:4	Epoch:9	Loss:0.216	translation_Loss:0.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:27.34	Hits@10:50.28	Best:27.38
2024-12-26 23:35:54,054: Snapshot:4	Epoch:10	Loss:0.207	translation_Loss:0.036	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:27.58	Hits@10:50.66	Best:27.58
2024-12-26 23:35:56,731: Snapshot:4	Epoch:11	Loss:0.199	translation_Loss:0.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:27.87	Hits@10:51.03	Best:27.87
2024-12-26 23:35:59,429: Snapshot:4	Epoch:12	Loss:0.192	translation_Loss:0.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.16                                                   	MRR:28.11	Hits@10:51.89	Best:28.11
2024-12-26 23:36:02,149: Snapshot:4	Epoch:13	Loss:0.189	translation_Loss:0.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:28.2	Hits@10:51.61	Best:28.2
2024-12-26 23:36:04,860: Snapshot:4	Epoch:14	Loss:0.184	translation_Loss:0.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:28.03	Hits@10:51.89	Best:28.2
2024-12-26 23:36:08,065: Snapshot:4	Epoch:15	Loss:0.178	translation_Loss:0.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.147                                                   	MRR:28.4	Hits@10:52.21	Best:28.4
2024-12-26 23:36:10,759: Snapshot:4	Epoch:16	Loss:0.174	translation_Loss:0.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.144                                                   	MRR:28.69	Hits@10:52.76	Best:28.69
2024-12-26 23:36:13,424: Snapshot:4	Epoch:17	Loss:0.172	translation_Loss:0.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.141                                                   	MRR:28.64	Hits@10:53.3	Best:28.69
2024-12-26 23:36:16,074: Snapshot:4	Epoch:18	Loss:0.168	translation_Loss:0.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.14                                                   	MRR:28.41	Hits@10:53.26	Best:28.69
2024-12-26 23:36:18,767: Early Stopping! Snapshot: 4 Epoch: 19 Best Results: 28.69
2024-12-26 23:36:18,767: Start to training tokens! Snapshot: 4 Epoch: 19 Loss:0.167 MRR:28.19 Best Results: 28.69
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:36:18,767: Snapshot:4	Epoch:19	Loss:0.167	translation_Loss:0.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.138                                                   	MRR:28.19	Hits@10:52.77	Best:28.69
2024-12-26 23:36:21,423: Snapshot:4	Epoch:20	Loss:17.329	translation_Loss:3.484	multi_layer_Loss:13.845	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.19	Hits@10:52.77	Best:28.69
2024-12-26 23:36:24,056: End of token training: 4 Epoch: 21 Loss:4.91 MRR:28.19 Best Results: 28.69
2024-12-26 23:36:24,056: Snapshot:4	Epoch:21	Loss:4.91	translation_Loss:3.465	multi_layer_Loss:1.445	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.19	Hits@10:52.77	Best:28.69
2024-12-26 23:36:24,344: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-26 23:36:40,277: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.164  | 0.0884 | 0.1975 | 0.2493 |  0.3083 |
|     1      | 0.1415 | 0.0686 | 0.167  | 0.2141 |  0.2773 |
|     2      | 0.1591 | 0.0877 | 0.168  | 0.2181 |  0.301  |
|     3      | 0.2673 | 0.1797 | 0.3012 | 0.3479 |   0.43  |
|     4      | 0.2843 | 0.1661 | 0.3237 | 0.4059 |  0.5266 |
+------------+--------+--------+--------+--------+---------+
2024-12-26 23:36:40,279: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2792 | 0.1681 | 0.3452 | 0.4135 |  0.4859 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2222 | 0.1196 | 0.2783 | 0.3399 |  0.4104 |
|     1      | 0.1864 | 0.0976 | 0.2214 | 0.2804 |  0.3596 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1814 | 0.097  | 0.2212 | 0.2771 |  0.3433 |
|     1      | 0.1535 | 0.0756 | 0.1827 | 0.2346 |  0.2991 |
|     2      | 0.2211 | 0.1338 | 0.2424 | 0.3061 |  0.3987 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1742 | 0.0931 | 0.2104 | 0.2662 |  0.3308 |
|     1      | 0.1456 | 0.0708 | 0.1729 | 0.2206 |  0.2866 |
|     2      | 0.1689 | 0.0938 | 0.1811 | 0.2301 |  0.3168 |
|     3      | 0.3071 | 0.2114 | 0.3501 | 0.4044 |  0.4824 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.164  | 0.0884 | 0.1975 | 0.2493 |  0.3083 |
|     1      | 0.1415 | 0.0686 | 0.167  | 0.2141 |  0.2773 |
|     2      | 0.1591 | 0.0877 | 0.168  | 0.2181 |  0.301  |
|     3      | 0.2673 | 0.1797 | 0.3012 | 0.3479 |   0.43  |
|     4      | 0.2843 | 0.1661 | 0.3237 | 0.4059 |  0.5266 |
+------------+--------+--------+--------+--------+---------+]
2024-12-26 23:36:40,280: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 121.78204417228699 |   0.279   |    0.168     |    0.345     |     0.486     |
|    1     | 102.8841462135315  |   0.205   |    0.109     |    0.251     |     0.386     |
|    2     | 96.56940531730652  |   0.181   |    0.099     |    0.213     |     0.341     |
|    3     | 45.129709005355835 |   0.178   |    0.098     |    0.206     |     0.329     |
|    4     | 65.22282981872559  |   0.175   |    0.097     |    0.201     |     0.324     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-26 23:36:40,280: Sum_Training_Time:431.5881345272064
2024-12-26 23:36:40,280: Every_Training_Time:[121.78204417228699, 102.8841462135315, 96.56940531730652, 45.129709005355835, 65.22282981872559]
2024-12-26 23:36:40,280: Forward transfer: 0.017974999999999998 Backward transfer: -0.065475
2024-12-26 23:37:16,398: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241226233645/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-26 23:37:32,634: Snapshot:0	Epoch:0	Loss:34.762	translation_Loss:34.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.3	Hits@10:29.6	Best:12.3
2024-12-26 23:37:45,106: Snapshot:0	Epoch:1	Loss:19.584	translation_Loss:19.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.22	Hits@10:42.58	Best:21.22
2024-12-26 23:37:55,743: Snapshot:0	Epoch:2	Loss:10.298	translation_Loss:10.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:46.3	Best:25.07
2024-12-26 23:38:07,973: Snapshot:0	Epoch:3	Loss:5.408	translation_Loss:5.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.52	Hits@10:47.48	Best:26.52
2024-12-26 23:38:20,043: Snapshot:0	Epoch:4	Loss:3.234	translation_Loss:3.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.11	Hits@10:47.87	Best:27.11
2024-12-26 23:38:32,653: Snapshot:0	Epoch:5	Loss:2.266	translation_Loss:2.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.35	Hits@10:48.2	Best:27.35
2024-12-26 23:38:44,711: Snapshot:0	Epoch:6	Loss:1.786	translation_Loss:1.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.24	Hits@10:48.02	Best:27.35
2024-12-26 23:38:56,678: Snapshot:0	Epoch:7	Loss:1.504	translation_Loss:1.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.18	Hits@10:47.91	Best:27.35
2024-12-26 23:39:09,133: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 27.35
2024-12-26 23:39:09,133: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:1.332 MRR:27.23 Best Results: 27.35
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-26 23:39:09,134: Snapshot:0	Epoch:8	Loss:1.332	translation_Loss:1.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.23	Hits@10:47.84	Best:27.35
2024-12-26 23:39:21,740: Snapshot:0	Epoch:9	Loss:35.476	translation_Loss:24.725	multi_layer_Loss:10.75	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.23	Hits@10:47.84	Best:27.35
2024-12-26 23:39:33,460: End of token training: 0 Epoch: 10 Loss:24.744 MRR:27.23 Best Results: 27.35
2024-12-26 23:39:33,461: Snapshot:0	Epoch:10	Loss:24.744	translation_Loss:24.734	multi_layer_Loss:0.01	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.23	Hits@10:47.84	Best:27.35
2024-12-26 23:39:33,806: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-26 23:39:38,563: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2776 | 0.1657 | 0.3435 | 0.4126 |  0.485  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:40:12,187: Snapshot:1	Epoch:0	Loss:26.497	translation_Loss:25.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.757                                                   	MRR:13.45	Hits@10:29.65	Best:13.45
2024-12-26 23:40:23,116: Snapshot:1	Epoch:1	Loss:8.871	translation_Loss:7.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.821                                                   	MRR:18.49	Hits@10:36.06	Best:18.49
2024-12-26 23:40:33,000: Snapshot:1	Epoch:2	Loss:4.318	translation_Loss:2.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.166                                                   	MRR:19.34	Hits@10:36.89	Best:19.34
2024-12-26 23:40:43,661: Snapshot:1	Epoch:3	Loss:3.307	translation_Loss:1.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.053                                                   	MRR:19.33	Hits@10:37.02	Best:19.34
2024-12-26 23:40:54,577: Snapshot:1	Epoch:4	Loss:3.011	translation_Loss:1.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.968                                                   	MRR:19.46	Hits@10:36.92	Best:19.46
2024-12-26 23:41:05,613: Snapshot:1	Epoch:5	Loss:2.887	translation_Loss:0.972	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.915                                                   	MRR:19.17	Hits@10:36.71	Best:19.46
2024-12-26 23:41:16,522: Snapshot:1	Epoch:6	Loss:2.849	translation_Loss:0.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.894                                                   	MRR:19.26	Hits@10:36.49	Best:19.46
2024-12-26 23:41:27,378: Early Stopping! Snapshot: 1 Epoch: 7 Best Results: 19.46
2024-12-26 23:41:27,378: Start to training tokens! Snapshot: 1 Epoch: 7 Loss:2.801 MRR:19.16 Best Results: 19.46
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-26 23:41:27,378: Snapshot:1	Epoch:7	Loss:2.801	translation_Loss:0.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.887                                                   	MRR:19.16	Hits@10:36.01	Best:19.46
2024-12-26 23:41:38,278: Snapshot:1	Epoch:8	Loss:34.92	translation_Loss:25.194	multi_layer_Loss:9.727	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.16	Hits@10:36.01	Best:19.46
2024-12-26 23:41:49,215: End of token training: 1 Epoch: 9 Loss:25.243 MRR:19.16 Best Results: 19.46
2024-12-26 23:41:49,215: Snapshot:1	Epoch:9	Loss:25.243	translation_Loss:25.23	multi_layer_Loss:0.013	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.16	Hits@10:36.01	Best:19.46
2024-12-26 23:41:49,571: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-26 23:41:59,172: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1975 | 0.1004 | 0.2453 | 0.308  |  0.3863 |
|     1      | 0.1953 | 0.1041 | 0.2354 | 0.2938 |  0.3675 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:42:26,361: Snapshot:2	Epoch:0	Loss:14.581	translation_Loss:14.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.515                                                   	MRR:15.35	Hits@10:31.35	Best:15.35
2024-12-26 23:42:34,498: Snapshot:2	Epoch:1	Loss:3.655	translation_Loss:2.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.989                                                   	MRR:19.74	Hits@10:36.99	Best:19.74
2024-12-26 23:42:42,355: Snapshot:2	Epoch:2	Loss:1.906	translation_Loss:0.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.982                                                   	MRR:20.77	Hits@10:38.6	Best:20.77
2024-12-26 23:42:49,519: Snapshot:2	Epoch:3	Loss:1.43	translation_Loss:0.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.899                                                   	MRR:21.02	Hits@10:39.19	Best:21.02
2024-12-26 23:42:56,621: Snapshot:2	Epoch:4	Loss:1.243	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.837                                                   	MRR:21.38	Hits@10:39.0	Best:21.38
2024-12-26 23:43:03,771: Snapshot:2	Epoch:5	Loss:1.153	translation_Loss:0.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.801                                                   	MRR:21.12	Hits@10:38.92	Best:21.38
2024-12-26 23:43:11,390: Snapshot:2	Epoch:6	Loss:1.093	translation_Loss:0.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.767                                                   	MRR:21.87	Hits@10:39.59	Best:21.87
2024-12-26 23:43:19,249: Snapshot:2	Epoch:7	Loss:1.063	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.748                                                   	MRR:22.04	Hits@10:39.44	Best:22.04
2024-12-26 23:43:27,378: Snapshot:2	Epoch:8	Loss:1.043	translation_Loss:0.308	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.735                                                   	MRR:22.23	Hits@10:39.47	Best:22.23
2024-12-26 23:43:35,498: Snapshot:2	Epoch:9	Loss:1.025	translation_Loss:0.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.725                                                   	MRR:22.42	Hits@10:39.83	Best:22.42
2024-12-26 23:43:43,553: Snapshot:2	Epoch:10	Loss:1.01	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.718                                                   	MRR:22.04	Hits@10:39.67	Best:22.42
2024-12-26 23:43:52,068: Snapshot:2	Epoch:11	Loss:0.995	translation_Loss:0.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.707                                                   	MRR:21.97	Hits@10:40.04	Best:22.42
2024-12-26 23:44:00,117: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 22.42
2024-12-26 23:44:00,117: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:0.988 MRR:22.1 Best Results: 22.42
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-26 23:44:00,117: Snapshot:2	Epoch:12	Loss:0.988	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.704                                                   	MRR:22.1	Hits@10:39.48	Best:22.42
2024-12-26 23:44:08,206: Snapshot:2	Epoch:13	Loss:29.096	translation_Loss:17.037	multi_layer_Loss:12.059	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.1	Hits@10:39.48	Best:22.42
2024-12-26 23:44:16,354: End of token training: 2 Epoch: 14 Loss:17.126 MRR:22.1 Best Results: 22.42
2024-12-26 23:44:16,354: Snapshot:2	Epoch:14	Loss:17.126	translation_Loss:17.047	multi_layer_Loss:0.08	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.1	Hits@10:39.48	Best:22.42
2024-12-26 23:44:16,691: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
