2024-12-29 03:52:39,875: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229035204/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=50000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 03:52:54,655: Snapshot:0	Epoch:0	Loss:24.263	translation_Loss:24.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.68	Hits@10:26.04	Best:10.68
2024-12-29 03:53:05,017: Snapshot:0	Epoch:1	Loss:15.05	translation_Loss:15.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.45	Hits@10:40.39	Best:18.45
2024-12-29 03:53:15,735: Snapshot:0	Epoch:2	Loss:8.578	translation_Loss:8.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:45.2	Best:23.51
2024-12-29 03:53:26,652: Snapshot:0	Epoch:3	Loss:4.719	translation_Loss:4.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:47.26	Best:25.76
2024-12-29 03:53:36,971: Snapshot:0	Epoch:4	Loss:2.731	translation_Loss:2.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.72	Hits@10:48.01	Best:26.72
2024-12-29 03:53:47,687: Snapshot:0	Epoch:5	Loss:1.813	translation_Loss:1.813	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.27	Hits@10:48.21	Best:27.27
2024-12-29 03:53:58,489: Snapshot:0	Epoch:6	Loss:1.353	translation_Loss:1.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.38	Hits@10:48.26	Best:27.38
2024-12-29 03:54:08,836: Snapshot:0	Epoch:7	Loss:1.102	translation_Loss:1.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.38	Hits@10:48.35	Best:27.38
2024-12-29 03:54:19,636: Snapshot:0	Epoch:8	Loss:0.955	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.5	Hits@10:48.2	Best:27.5
2024-12-29 03:54:30,366: Snapshot:0	Epoch:9	Loss:0.844	translation_Loss:0.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.48	Hits@10:48.01	Best:27.5
2024-12-29 03:54:40,668: Snapshot:0	Epoch:10	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.32	Hits@10:47.97	Best:27.5
2024-12-29 03:54:51,240: Snapshot:0	Epoch:11	Loss:0.722	translation_Loss:0.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.3	Hits@10:47.69	Best:27.5
2024-12-29 03:55:02,107: Snapshot:0	Epoch:12	Loss:0.678	translation_Loss:0.678	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.28	Hits@10:47.54	Best:27.5
2024-12-29 03:55:12,468: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 27.5
2024-12-29 03:55:12,469: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.639 MRR:27.0 Best Results: 27.5
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:55:12,469: Snapshot:0	Epoch:13	Loss:0.639	translation_Loss:0.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.0	Hits@10:47.54	Best:27.5
2024-12-29 03:55:23,600: Snapshot:0	Epoch:14	Loss:32.048	translation_Loss:16.47	multi_layer_Loss:15.578	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.0	Hits@10:47.54	Best:27.5
2024-12-29 03:55:34,266: End of token training: 0 Epoch: 15 Loss:16.576 MRR:27.0 Best Results: 27.5
2024-12-29 03:55:34,267: Snapshot:0	Epoch:15	Loss:16.576	translation_Loss:16.487	multi_layer_Loss:0.09	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.0	Hits@10:47.54	Best:27.5
2024-12-29 03:55:34,541: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-29 03:55:39,168: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.277 | 0.1659 | 0.3423 | 0.4123 |  0.4837 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:56:11,999: Snapshot:1	Epoch:0	Loss:21.554	translation_Loss:20.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.541                                                   	MRR:8.12	Hits@10:20.59	Best:8.12
2024-12-29 03:56:21,225: Snapshot:1	Epoch:1	Loss:13.148	translation_Loss:11.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.481                                                   	MRR:12.43	Hits@10:25.99	Best:12.43
2024-12-29 03:56:30,566: Snapshot:1	Epoch:2	Loss:9.452	translation_Loss:8.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.405                                                   	MRR:13.05	Hits@10:26.42	Best:13.05
2024-12-29 03:56:39,914: Snapshot:1	Epoch:3	Loss:7.947	translation_Loss:6.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.325                                                   	MRR:13.2	Hits@10:26.76	Best:13.2
2024-12-29 03:56:49,210: Snapshot:1	Epoch:4	Loss:7.35	translation_Loss:6.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.267                                                   	MRR:13.36	Hits@10:26.69	Best:13.36
2024-12-29 03:56:59,186: Snapshot:1	Epoch:5	Loss:7.063	translation_Loss:5.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.228                                                   	MRR:13.28	Hits@10:26.59	Best:13.36
2024-12-29 03:57:08,490: Snapshot:1	Epoch:6	Loss:6.916	translation_Loss:5.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.203                                                   	MRR:13.29	Hits@10:26.75	Best:13.36
2024-12-29 03:57:18,167: Snapshot:1	Epoch:7	Loss:6.825	translation_Loss:5.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.189                                                   	MRR:13.28	Hits@10:26.76	Best:13.36
2024-12-29 03:57:27,449: Snapshot:1	Epoch:8	Loss:6.776	translation_Loss:5.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.181                                                   	MRR:13.17	Hits@10:26.57	Best:13.36
2024-12-29 03:57:37,075: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 13.36
2024-12-29 03:57:37,076: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:6.732 MRR:13.14 Best Results: 13.36
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:57:37,076: Snapshot:1	Epoch:9	Loss:6.732	translation_Loss:5.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.176                                                   	MRR:13.14	Hits@10:26.73	Best:13.36
2024-12-29 03:57:46,695: Snapshot:1	Epoch:10	Loss:37.365	translation_Loss:21.676	multi_layer_Loss:15.689	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.14	Hits@10:26.73	Best:13.36
2024-12-29 03:57:55,946: End of token training: 1 Epoch: 11 Loss:21.786 MRR:13.14 Best Results: 13.36
2024-12-29 03:57:55,947: Snapshot:1	Epoch:11	Loss:21.786	translation_Loss:21.675	multi_layer_Loss:0.111	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:13.14	Hits@10:26.73	Best:13.36
2024-12-29 03:57:56,290: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-29 03:58:05,063: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2718 | 0.1589 | 0.3377 | 0.4097 |  0.4861 |
|     1      | 0.1342 | 0.0676 | 0.1552 | 0.1994 |  0.2671 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:58:30,230: Snapshot:2	Epoch:0	Loss:15.94	translation_Loss:13.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.128                                                   	MRR:8.1	Hits@10:19.01	Best:8.1
2024-12-29 03:58:37,344: Snapshot:2	Epoch:1	Loss:10.554	translation_Loss:7.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.863                                                   	MRR:16.14	Hits@10:29.82	Best:16.14
2024-12-29 03:58:44,430: Snapshot:2	Epoch:2	Loss:7.846	translation_Loss:5.429	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.417                                                   	MRR:18.5	Hits@10:32.37	Best:18.5
2024-12-29 03:58:51,489: Snapshot:2	Epoch:3	Loss:6.36	translation_Loss:4.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.979                                                   	MRR:18.96	Hits@10:32.72	Best:18.96
2024-12-29 03:58:58,525: Snapshot:2	Epoch:4	Loss:5.606	translation_Loss:3.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.717                                                   	MRR:18.99	Hits@10:32.34	Best:18.99
2024-12-29 03:59:05,598: Snapshot:2	Epoch:5	Loss:5.244	translation_Loss:3.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.576                                                   	MRR:18.77	Hits@10:32.13	Best:18.99
2024-12-29 03:59:13,050: Snapshot:2	Epoch:6	Loss:5.097	translation_Loss:3.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.521                                                   	MRR:18.52	Hits@10:31.87	Best:18.99
2024-12-29 03:59:20,004: Snapshot:2	Epoch:7	Loss:5.006	translation_Loss:3.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.491                                                   	MRR:18.47	Hits@10:31.73	Best:18.99
2024-12-29 03:59:27,455: Snapshot:2	Epoch:8	Loss:4.965	translation_Loss:3.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.475                                                   	MRR:18.41	Hits@10:31.82	Best:18.99
2024-12-29 03:59:34,369: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 18.99
2024-12-29 03:59:34,369: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:4.933 MRR:18.43 Best Results: 18.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:59:34,369: Snapshot:2	Epoch:9	Loss:4.933	translation_Loss:3.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.474                                                   	MRR:18.43	Hits@10:31.66	Best:18.99
2024-12-29 03:59:41,678: Snapshot:2	Epoch:10	Loss:33.236	translation_Loss:17.038	multi_layer_Loss:16.199	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.43	Hits@10:31.66	Best:18.99
2024-12-29 03:59:48,511: End of token training: 2 Epoch: 11 Loss:17.362 MRR:18.43 Best Results: 18.99
2024-12-29 03:59:48,511: Snapshot:2	Epoch:11	Loss:17.362	translation_Loss:17.056	multi_layer_Loss:0.306	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.43	Hits@10:31.66	Best:18.99
2024-12-29 03:59:48,791: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-29 04:00:00,455: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1314 | 0.3085 | 0.3797 |  0.4499 |
|     1      | 0.1296 | 0.0628 |  0.15  | 0.1939 |  0.262  |
|     2      | 0.1868 | 0.1204 | 0.2051 | 0.2523 |  0.3183 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:00:14,473: Snapshot:3	Epoch:0	Loss:7.236	translation_Loss:6.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.564                                                   	MRR:5.36	Hits@10:11.59	Best:5.36
2024-12-29 04:00:18,210: Snapshot:3	Epoch:1	Loss:5.297	translation_Loss:4.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.707                                                   	MRR:13.34	Hits@10:29.12	Best:13.34
2024-12-29 04:00:21,517: Snapshot:3	Epoch:2	Loss:4.061	translation_Loss:3.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.693                                                   	MRR:17.5	Hits@10:32.98	Best:17.5
2024-12-29 04:00:24,837: Snapshot:3	Epoch:3	Loss:3.298	translation_Loss:2.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.651                                                   	MRR:20.28	Hits@10:35.08	Best:20.28
2024-12-29 04:00:28,142: Snapshot:3	Epoch:4	Loss:2.85	translation_Loss:2.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.614                                                   	MRR:21.85	Hits@10:36.15	Best:21.85
2024-12-29 04:00:31,494: Snapshot:3	Epoch:5	Loss:2.551	translation_Loss:1.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.586                                                   	MRR:22.96	Hits@10:36.87	Best:22.96
2024-12-29 04:00:34,819: Snapshot:3	Epoch:6	Loss:2.331	translation_Loss:1.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.555                                                   	MRR:23.49	Hits@10:37.1	Best:23.49
2024-12-29 04:00:38,539: Snapshot:3	Epoch:7	Loss:2.163	translation_Loss:1.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.528                                                   	MRR:23.79	Hits@10:37.38	Best:23.79
2024-12-29 04:00:41,824: Snapshot:3	Epoch:8	Loss:2.041	translation_Loss:1.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.504                                                   	MRR:23.97	Hits@10:37.36	Best:23.97
2024-12-29 04:00:45,146: Snapshot:3	Epoch:9	Loss:1.952	translation_Loss:1.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.485                                                   	MRR:24.37	Hits@10:37.41	Best:24.37
2024-12-29 04:00:48,503: Snapshot:3	Epoch:10	Loss:1.893	translation_Loss:1.422	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.471                                                   	MRR:24.38	Hits@10:37.39	Best:24.38
2024-12-29 04:00:51,748: Snapshot:3	Epoch:11	Loss:1.852	translation_Loss:1.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:24.32	Hits@10:37.47	Best:24.38
2024-12-29 04:00:55,123: Snapshot:3	Epoch:12	Loss:1.817	translation_Loss:1.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.455                                                   	MRR:24.47	Hits@10:37.61	Best:24.47
2024-12-29 04:00:58,785: Snapshot:3	Epoch:13	Loss:1.795	translation_Loss:1.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.451                                                   	MRR:24.51	Hits@10:37.42	Best:24.51
2024-12-29 04:01:01,974: Snapshot:3	Epoch:14	Loss:1.777	translation_Loss:1.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.447                                                   	MRR:24.68	Hits@10:37.56	Best:24.68
2024-12-29 04:01:05,380: Snapshot:3	Epoch:15	Loss:1.762	translation_Loss:1.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.442                                                   	MRR:24.71	Hits@10:37.27	Best:24.71
2024-12-29 04:01:08,674: Snapshot:3	Epoch:16	Loss:1.745	translation_Loss:1.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.438                                                   	MRR:24.38	Hits@10:37.61	Best:24.71
2024-12-29 04:01:12,303: Snapshot:3	Epoch:17	Loss:1.747	translation_Loss:1.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.438                                                   	MRR:24.52	Hits@10:37.62	Best:24.71
2024-12-29 04:01:15,492: Snapshot:3	Epoch:18	Loss:1.735	translation_Loss:1.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.436                                                   	MRR:24.38	Hits@10:37.33	Best:24.71
2024-12-29 04:01:18,791: Snapshot:3	Epoch:19	Loss:1.72	translation_Loss:1.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.433                                                   	MRR:24.49	Hits@10:37.31	Best:24.71
2024-12-29 04:01:21,948: Early Stopping! Snapshot: 3 Epoch: 20 Best Results: 24.71
2024-12-29 04:01:21,948: Start to training tokens! Snapshot: 3 Epoch: 20 Loss:1.718 MRR:24.55 Best Results: 24.71
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:01:21,949: Snapshot:3	Epoch:20	Loss:1.718	translation_Loss:1.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.429                                                   	MRR:24.55	Hits@10:37.35	Best:24.71
2024-12-29 04:01:25,170: Snapshot:3	Epoch:21	Loss:18.822	translation_Loss:6.36	multi_layer_Loss:12.462	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.55	Hits@10:37.35	Best:24.71
2024-12-29 04:01:28,471: End of token training: 3 Epoch: 22 Loss:7.761 MRR:24.55 Best Results: 24.71
2024-12-29 04:01:28,471: Snapshot:3	Epoch:22	Loss:7.761	translation_Loss:6.356	multi_layer_Loss:1.405	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.55	Hits@10:37.35	Best:24.71
2024-12-29 04:01:28,799: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-29 04:01:42,595: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2387 | 0.1293 | 0.3025 | 0.3706 |  0.4424 |
|     1      | 0.1303 | 0.0636 | 0.1498 | 0.1935 |  0.2625 |
|     2      | 0.1771 | 0.1092 | 0.193  | 0.2439 |  0.3171 |
|     3      | 0.246  | 0.1731 | 0.2753 | 0.3175 |  0.3746 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:01:53,925: Snapshot:4	Epoch:0	Loss:4.785	translation_Loss:4.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.406                                                   	MRR:5.71	Hits@10:15.8	Best:5.71
2024-12-29 04:01:56,392: Snapshot:4	Epoch:1	Loss:3.512	translation_Loss:2.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.719                                                   	MRR:11.16	Hits@10:28.32	Best:11.16
2024-12-29 04:01:58,844: Snapshot:4	Epoch:2	Loss:2.885	translation_Loss:2.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.81                                                   	MRR:14.47	Hits@10:34.52	Best:14.47
2024-12-29 04:02:01,232: Snapshot:4	Epoch:3	Loss:2.415	translation_Loss:1.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.77                                                   	MRR:18.29	Hits@10:37.5	Best:18.29
2024-12-29 04:02:03,656: Snapshot:4	Epoch:4	Loss:2.077	translation_Loss:1.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.699                                                   	MRR:20.26	Hits@10:39.3	Best:20.26
2024-12-29 04:02:06,097: Snapshot:4	Epoch:5	Loss:1.846	translation_Loss:1.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.646                                                   	MRR:21.44	Hits@10:41.03	Best:21.44
2024-12-29 04:02:08,470: Snapshot:4	Epoch:6	Loss:1.674	translation_Loss:1.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.604                                                   	MRR:22.1	Hits@10:42.23	Best:22.1
2024-12-29 04:02:10,796: Snapshot:4	Epoch:7	Loss:1.529	translation_Loss:0.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.567                                                   	MRR:22.77	Hits@10:43.19	Best:22.77
2024-12-29 04:02:13,600: Snapshot:4	Epoch:8	Loss:1.423	translation_Loss:0.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.537                                                   	MRR:23.15	Hits@10:43.6	Best:23.15
2024-12-29 04:02:16,007: Snapshot:4	Epoch:9	Loss:1.338	translation_Loss:0.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.509                                                   	MRR:23.37	Hits@10:43.57	Best:23.37
2024-12-29 04:02:18,261: Snapshot:4	Epoch:10	Loss:1.267	translation_Loss:0.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.492                                                   	MRR:23.08	Hits@10:43.61	Best:23.37
2024-12-29 04:02:20,609: Snapshot:4	Epoch:11	Loss:1.213	translation_Loss:0.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:23.13	Hits@10:43.51	Best:23.37
2024-12-29 04:02:22,884: Snapshot:4	Epoch:12	Loss:1.178	translation_Loss:0.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:22.97	Hits@10:43.46	Best:23.37
2024-12-29 04:02:25,222: Snapshot:4	Epoch:13	Loss:1.155	translation_Loss:0.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.453                                                   	MRR:22.94	Hits@10:43.22	Best:23.37
2024-12-29 04:02:27,523: Early Stopping! Snapshot: 4 Epoch: 14 Best Results: 23.37
2024-12-29 04:02:27,524: Start to training tokens! Snapshot: 4 Epoch: 14 Loss:1.131 MRR:22.82 Best Results: 23.37
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:02:27,524: Snapshot:4	Epoch:14	Loss:1.131	translation_Loss:0.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.443                                                   	MRR:22.82	Hits@10:43.35	Best:23.37
2024-12-29 04:02:30,318: Snapshot:4	Epoch:15	Loss:16.135	translation_Loss:4.126	multi_layer_Loss:12.01	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.82	Hits@10:43.35	Best:23.37
2024-12-29 04:02:32,548: End of token training: 4 Epoch: 16 Loss:6.925 MRR:22.82 Best Results: 23.37
2024-12-29 04:02:32,549: Snapshot:4	Epoch:16	Loss:6.925	translation_Loss:4.119	multi_layer_Loss:2.806	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.82	Hits@10:43.35	Best:23.37
2024-12-29 04:02:32,834: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-29 04:02:47,583: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2329 | 0.1253 | 0.2955 | 0.3608 |  0.4299 |
|     1      | 0.127  | 0.0604 | 0.1467 |  0.19  |  0.259  |
|     2      | 0.1666 | 0.1028 | 0.1822 | 0.2288 |  0.2968 |
|     3      | 0.2387 | 0.1647 | 0.2645 | 0.308  |  0.3776 |
|     4      | 0.2274 | 0.1263 | 0.2576 | 0.329  |  0.4375 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:02:47,585: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.277 | 0.1659 | 0.3423 | 0.4123 |  0.4837 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2718 | 0.1589 | 0.3377 | 0.4097 |  0.4861 |
|     1      | 0.1342 | 0.0676 | 0.1552 | 0.1994 |  0.2671 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1314 | 0.3085 | 0.3797 |  0.4499 |
|     1      | 0.1296 | 0.0628 |  0.15  | 0.1939 |  0.262  |
|     2      | 0.1868 | 0.1204 | 0.2051 | 0.2523 |  0.3183 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2387 | 0.1293 | 0.3025 | 0.3706 |  0.4424 |
|     1      | 0.1303 | 0.0636 | 0.1498 | 0.1935 |  0.2625 |
|     2      | 0.1771 | 0.1092 | 0.193  | 0.2439 |  0.3171 |
|     3      | 0.246  | 0.1731 | 0.2753 | 0.3175 |  0.3746 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2329 | 0.1253 | 0.2955 | 0.3608 |  0.4299 |
|     1      | 0.127  | 0.0604 | 0.1467 |  0.19  |  0.259  |
|     2      | 0.1666 | 0.1028 | 0.1822 | 0.2288 |  0.2968 |
|     3      | 0.2387 | 0.1647 | 0.2645 | 0.308  |  0.3776 |
|     4      | 0.2274 | 0.1263 | 0.2576 | 0.329  |  0.4375 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:02:47,586: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 174.39139127731323 |   0.277   |    0.166     |    0.342     |     0.484     |
|    1     | 132.9905824661255  |   0.205   |    0.115     |    0.249     |      0.38     |
|    2     | 100.27192807197571 |   0.187   |    0.104     |    0.225     |     0.348     |
|    3     | 86.26845979690552  |    0.19   |    0.108     |    0.225     |     0.348     |
|    4     | 48.59933257102966  |   0.187   |    0.105     |    0.221     |     0.345     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:02:47,586: Sum_Training_Time:542.5216941833496
2024-12-29 04:02:47,586: Every_Training_Time:[174.39139127731323, 132.9905824661255, 100.27192807197571, 86.26845979690552, 48.59933257102966]
2024-12-29 04:02:47,586: Forward transfer: 0.015274999999999999 Backward transfer: -0.01970000000000001
2024-12-29 04:03:23,089: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229040252/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=100000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:03:37,903: Snapshot:0	Epoch:0	Loss:24.263	translation_Loss:24.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.68	Hits@10:26.04	Best:10.68
2024-12-29 04:03:48,328: Snapshot:0	Epoch:1	Loss:15.05	translation_Loss:15.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.44	Hits@10:40.39	Best:18.44
2024-12-29 04:03:59,148: Snapshot:0	Epoch:2	Loss:8.578	translation_Loss:8.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:45.2	Best:23.51
2024-12-29 04:04:10,090: Snapshot:0	Epoch:3	Loss:4.718	translation_Loss:4.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:47.28	Best:25.7
2024-12-29 04:04:20,532: Snapshot:0	Epoch:4	Loss:2.731	translation_Loss:2.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.74	Hits@10:47.93	Best:26.74
2024-12-29 04:04:31,311: Snapshot:0	Epoch:5	Loss:1.812	translation_Loss:1.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.18	Hits@10:48.19	Best:27.18
2024-12-29 04:04:42,073: Snapshot:0	Epoch:6	Loss:1.353	translation_Loss:1.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.29	Best:27.37
2024-12-29 04:04:52,417: Snapshot:0	Epoch:7	Loss:1.101	translation_Loss:1.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.34	Hits@10:48.2	Best:27.37
2024-12-29 04:05:03,219: Snapshot:0	Epoch:8	Loss:0.954	translation_Loss:0.954	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.48	Hits@10:48.18	Best:27.48
2024-12-29 04:05:14,038: Snapshot:0	Epoch:9	Loss:0.844	translation_Loss:0.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.41	Hits@10:48.1	Best:27.48
2024-12-29 04:05:24,423: Snapshot:0	Epoch:10	Loss:0.775	translation_Loss:0.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.29	Hits@10:47.96	Best:27.48
2024-12-29 04:05:35,157: Snapshot:0	Epoch:11	Loss:0.722	translation_Loss:0.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.34	Hits@10:47.51	Best:27.48
2024-12-29 04:05:45,931: Snapshot:0	Epoch:12	Loss:0.679	translation_Loss:0.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:47.65	Best:27.48
2024-12-29 04:05:56,321: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 27.48
2024-12-29 04:05:56,322: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.64 MRR:27.06 Best Results: 27.48
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:05:56,322: Snapshot:0	Epoch:13	Loss:0.64	translation_Loss:0.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.06	Hits@10:47.41	Best:27.48
2024-12-29 04:06:07,581: Snapshot:0	Epoch:14	Loss:32.088	translation_Loss:16.51	multi_layer_Loss:15.578	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.06	Hits@10:47.41	Best:27.48
2024-12-29 04:06:18,464: End of token training: 0 Epoch: 15 Loss:16.617 MRR:27.06 Best Results: 27.48
2024-12-29 04:06:18,465: Snapshot:0	Epoch:15	Loss:16.617	translation_Loss:16.528	multi_layer_Loss:0.09	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.06	Hits@10:47.41	Best:27.48
2024-12-29 04:06:18,745: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-29 04:06:23,379: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2759 | 0.1641 | 0.3408 | 0.4108 |  0.4828 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:06:56,003: Snapshot:1	Epoch:0	Loss:21.855	translation_Loss:20.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.582                                                   	MRR:7.57	Hits@10:19.51	Best:7.57
2024-12-29 04:07:05,498: Snapshot:1	Epoch:1	Loss:13.404	translation_Loss:12.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.254                                                   	MRR:11.94	Hits@10:25.08	Best:11.94
2024-12-29 04:07:15,034: Snapshot:1	Epoch:2	Loss:9.675	translation_Loss:8.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.17                                                   	MRR:12.78	Hits@10:25.76	Best:12.78
2024-12-29 04:07:24,322: Snapshot:1	Epoch:3	Loss:8.136	translation_Loss:7.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.099                                                   	MRR:12.84	Hits@10:25.91	Best:12.84
2024-12-29 04:07:33,674: Snapshot:1	Epoch:4	Loss:7.533	translation_Loss:6.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.052                                                   	MRR:12.93	Hits@10:25.96	Best:12.93
2024-12-29 04:07:43,526: Snapshot:1	Epoch:5	Loss:7.238	translation_Loss:6.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.018                                                   	MRR:12.98	Hits@10:25.9	Best:12.98
2024-12-29 04:07:52,970: Snapshot:1	Epoch:6	Loss:7.092	translation_Loss:6.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.998                                                   	MRR:13.02	Hits@10:26.06	Best:13.02
2024-12-29 04:08:02,714: Snapshot:1	Epoch:7	Loss:6.997	translation_Loss:6.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.991                                                   	MRR:12.96	Hits@10:25.96	Best:13.02
2024-12-29 04:08:12,072: Snapshot:1	Epoch:8	Loss:6.95	translation_Loss:5.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.984                                                   	MRR:12.91	Hits@10:25.93	Best:13.02
2024-12-29 04:08:21,778: Snapshot:1	Epoch:9	Loss:6.903	translation_Loss:5.922	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.981                                                   	MRR:12.94	Hits@10:25.98	Best:13.02
2024-12-29 04:08:31,598: Snapshot:1	Epoch:10	Loss:6.883	translation_Loss:5.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.979                                                   	MRR:12.89	Hits@10:25.91	Best:13.02
2024-12-29 04:08:40,856: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 13.02
2024-12-29 04:08:40,856: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:6.843 MRR:12.95 Best Results: 13.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:08:40,856: Snapshot:1	Epoch:11	Loss:6.843	translation_Loss:5.864	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.979                                                   	MRR:12.95	Hits@10:26.21	Best:13.02
2024-12-29 04:08:50,540: Snapshot:1	Epoch:12	Loss:37.663	translation_Loss:21.974	multi_layer_Loss:15.689	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.95	Hits@10:26.21	Best:13.02
2024-12-29 04:08:59,821: End of token training: 1 Epoch: 13 Loss:22.079 MRR:12.95 Best Results: 13.02
2024-12-29 04:08:59,822: Snapshot:1	Epoch:13	Loss:22.079	translation_Loss:21.968	multi_layer_Loss:0.111	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:12.95	Hits@10:26.21	Best:13.02
2024-12-29 04:09:00,099: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-29 04:09:09,006: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2743 | 0.1608 | 0.3422 | 0.4117 |  0.4844 |
|     1      | 0.1315 | 0.0664 | 0.1517 | 0.1935 |  0.2603 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:09:33,984: Snapshot:2	Epoch:0	Loss:16.76	translation_Loss:14.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.123                                                   	MRR:5.36	Hits@10:12.6	Best:5.36
2024-12-29 04:09:41,117: Snapshot:2	Epoch:1	Loss:11.524	translation_Loss:9.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.294                                                   	MRR:14.11	Hits@10:27.09	Best:14.11
2024-12-29 04:09:48,108: Snapshot:2	Epoch:2	Loss:8.569	translation_Loss:6.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.134                                                   	MRR:17.6	Hits@10:30.35	Best:17.6
2024-12-29 04:09:55,093: Snapshot:2	Epoch:3	Loss:6.863	translation_Loss:5.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.797                                                   	MRR:17.88	Hits@10:30.59	Best:17.88
2024-12-29 04:10:02,205: Snapshot:2	Epoch:4	Loss:6.029	translation_Loss:4.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.583                                                   	MRR:17.5	Hits@10:29.86	Best:17.88
2024-12-29 04:10:09,357: Snapshot:2	Epoch:5	Loss:5.676	translation_Loss:4.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.484                                                   	MRR:17.52	Hits@10:29.46	Best:17.88
2024-12-29 04:10:16,882: Snapshot:2	Epoch:6	Loss:5.502	translation_Loss:4.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.441                                                   	MRR:17.33	Hits@10:29.45	Best:17.88
2024-12-29 04:10:23,992: Snapshot:2	Epoch:7	Loss:5.437	translation_Loss:4.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.421                                                   	MRR:17.29	Hits@10:29.26	Best:17.88
2024-12-29 04:10:31,071: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 17.88
2024-12-29 04:10:31,071: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:5.378 MRR:17.43 Best Results: 17.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:10:31,072: Snapshot:2	Epoch:8	Loss:5.378	translation_Loss:3.967	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.411                                                   	MRR:17.43	Hits@10:29.38	Best:17.88
2024-12-29 04:10:38,363: Snapshot:2	Epoch:9	Loss:33.845	translation_Loss:17.646	multi_layer_Loss:16.199	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.43	Hits@10:29.38	Best:17.88
2024-12-29 04:10:45,277: End of token training: 2 Epoch: 10 Loss:17.947 MRR:17.43 Best Results: 17.88
2024-12-29 04:10:45,277: Snapshot:2	Epoch:10	Loss:17.947	translation_Loss:17.641	multi_layer_Loss:0.306	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.43	Hits@10:29.38	Best:17.88
2024-12-29 04:10:45,632: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-29 04:10:57,594: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1418 | 0.3155 | 0.383  |  0.4528 |
|     1      | 0.1288 | 0.0637 | 0.1481 | 0.1915 |  0.2571 |
|     2      | 0.1764 | 0.1138 | 0.1946 | 0.2372 |  0.3002 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:11:11,330: Snapshot:3	Epoch:0	Loss:7.497	translation_Loss:6.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.721                                                   	MRR:4.54	Hits@10:9.93	Best:4.54
2024-12-29 04:11:15,043: Snapshot:3	Epoch:1	Loss:5.657	translation_Loss:5.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.652                                                   	MRR:12.07	Hits@10:25.41	Best:12.07
2024-12-29 04:11:18,449: Snapshot:3	Epoch:2	Loss:4.354	translation_Loss:3.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.561                                                   	MRR:16.1	Hits@10:29.87	Best:16.1
2024-12-29 04:11:21,867: Snapshot:3	Epoch:3	Loss:3.603	translation_Loss:3.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.487                                                   	MRR:18.69	Hits@10:32.09	Best:18.69
2024-12-29 04:11:25,271: Snapshot:3	Epoch:4	Loss:3.131	translation_Loss:2.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.451                                                   	MRR:20.43	Hits@10:33.84	Best:20.43
2024-12-29 04:11:28,629: Snapshot:3	Epoch:5	Loss:2.815	translation_Loss:2.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.423                                                   	MRR:21.48	Hits@10:34.68	Best:21.48
2024-12-29 04:11:32,358: Snapshot:3	Epoch:6	Loss:2.569	translation_Loss:2.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:22.13	Hits@10:35.1	Best:22.13
2024-12-29 04:11:35,759: Snapshot:3	Epoch:7	Loss:2.4	translation_Loss:2.021	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.379                                                   	MRR:22.52	Hits@10:35.35	Best:22.52
2024-12-29 04:11:39,068: Snapshot:3	Epoch:8	Loss:2.276	translation_Loss:1.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.368                                                   	MRR:22.86	Hits@10:35.41	Best:22.86
2024-12-29 04:11:42,485: Snapshot:3	Epoch:9	Loss:2.188	translation_Loss:1.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.357                                                   	MRR:23.13	Hits@10:35.56	Best:23.13
2024-12-29 04:11:45,929: Snapshot:3	Epoch:10	Loss:2.124	translation_Loss:1.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.351                                                   	MRR:23.29	Hits@10:35.51	Best:23.29
2024-12-29 04:11:49,815: Snapshot:3	Epoch:11	Loss:2.084	translation_Loss:1.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.345                                                   	MRR:23.32	Hits@10:35.55	Best:23.32
2024-12-29 04:11:53,236: Snapshot:3	Epoch:12	Loss:2.044	translation_Loss:1.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.341                                                   	MRR:23.39	Hits@10:35.66	Best:23.39
2024-12-29 04:11:56,669: Snapshot:3	Epoch:13	Loss:2.024	translation_Loss:1.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.338                                                   	MRR:23.46	Hits@10:35.57	Best:23.46
2024-12-29 04:12:00,015: Snapshot:3	Epoch:14	Loss:1.995	translation_Loss:1.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.334                                                   	MRR:23.26	Hits@10:35.61	Best:23.46
2024-12-29 04:12:03,266: Snapshot:3	Epoch:15	Loss:1.987	translation_Loss:1.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.332                                                   	MRR:23.35	Hits@10:35.31	Best:23.46
2024-12-29 04:12:07,023: Snapshot:3	Epoch:16	Loss:1.976	translation_Loss:1.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:23.52	Hits@10:35.52	Best:23.52
2024-12-29 04:12:10,224: Snapshot:3	Epoch:17	Loss:1.968	translation_Loss:1.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.33                                                   	MRR:23.38	Hits@10:35.3	Best:23.52
2024-12-29 04:12:13,417: Snapshot:3	Epoch:18	Loss:1.96	translation_Loss:1.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:23.46	Hits@10:35.53	Best:23.52
2024-12-29 04:12:16,669: Snapshot:3	Epoch:19	Loss:1.946	translation_Loss:1.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.33                                                   	MRR:23.33	Hits@10:35.51	Best:23.52
2024-12-29 04:12:19,907: Snapshot:3	Epoch:20	Loss:1.949	translation_Loss:1.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:23.4	Hits@10:35.49	Best:23.52
2024-12-29 04:12:23,550: Early Stopping! Snapshot: 3 Epoch: 21 Best Results: 23.52
2024-12-29 04:12:23,550: Start to training tokens! Snapshot: 3 Epoch: 21 Loss:1.937 MRR:23.3 Best Results: 23.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:12:23,550: Snapshot:3	Epoch:21	Loss:1.937	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:23.3	Hits@10:35.51	Best:23.52
2024-12-29 04:12:26,709: Snapshot:3	Epoch:22	Loss:19.032	translation_Loss:6.57	multi_layer_Loss:12.462	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.3	Hits@10:35.51	Best:23.52
2024-12-29 04:12:29,856: End of token training: 3 Epoch: 23 Loss:7.974 MRR:23.3 Best Results: 23.52
2024-12-29 04:12:29,856: Snapshot:3	Epoch:23	Loss:7.974	translation_Loss:6.569	multi_layer_Loss:1.405	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.3	Hits@10:35.51	Best:23.52
2024-12-29 04:12:30,135: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-29 04:12:43,435: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1359 | 0.3081 | 0.3777 |  0.451  |
|     1      | 0.1293 | 0.0641 | 0.1479 | 0.1911 |  0.2579 |
|     2      | 0.1731 | 0.1069 | 0.1949 | 0.2399 |  0.3017 |
|     3      | 0.2341 | 0.1648 | 0.2625 | 0.3033 |  0.3571 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:12:54,933: Snapshot:4	Epoch:0	Loss:5.251	translation_Loss:4.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.571                                                   	MRR:4.3	Hits@10:11.9	Best:4.3
2024-12-29 04:12:57,345: Snapshot:4	Epoch:1	Loss:4.158	translation_Loss:3.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.761                                                   	MRR:8.86	Hits@10:23.36	Best:8.86
2024-12-29 04:12:59,662: Snapshot:4	Epoch:2	Loss:3.476	translation_Loss:2.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.787                                                   	MRR:12.09	Hits@10:29.94	Best:12.09
2024-12-29 04:13:02,123: Snapshot:4	Epoch:3	Loss:2.972	translation_Loss:2.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.767                                                   	MRR:16.27	Hits@10:32.92	Best:16.27
2024-12-29 04:13:04,562: Snapshot:4	Epoch:4	Loss:2.603	translation_Loss:1.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.73                                                   	MRR:18.07	Hits@10:35.15	Best:18.07
2024-12-29 04:13:06,958: Snapshot:4	Epoch:5	Loss:2.334	translation_Loss:1.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.692                                                   	MRR:19.21	Hits@10:36.41	Best:19.21
2024-12-29 04:13:09,279: Snapshot:4	Epoch:6	Loss:2.123	translation_Loss:1.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.655                                                   	MRR:19.96	Hits@10:37.51	Best:19.96
2024-12-29 04:13:12,143: Snapshot:4	Epoch:7	Loss:1.96	translation_Loss:1.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.623                                                   	MRR:20.26	Hits@10:38.16	Best:20.26
2024-12-29 04:13:14,577: Snapshot:4	Epoch:8	Loss:1.835	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.595                                                   	MRR:20.67	Hits@10:38.39	Best:20.67
2024-12-29 04:13:16,950: Snapshot:4	Epoch:9	Loss:1.739	translation_Loss:1.163	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.576                                                   	MRR:20.76	Hits@10:38.45	Best:20.76
2024-12-29 04:13:19,301: Snapshot:4	Epoch:10	Loss:1.67	translation_Loss:1.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.559                                                   	MRR:20.68	Hits@10:38.5	Best:20.76
2024-12-29 04:13:21,665: Snapshot:4	Epoch:11	Loss:1.613	translation_Loss:1.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.547                                                   	MRR:20.6	Hits@10:38.5	Best:20.76
2024-12-29 04:13:24,042: Snapshot:4	Epoch:12	Loss:1.577	translation_Loss:1.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.539                                                   	MRR:20.7	Hits@10:38.37	Best:20.76
2024-12-29 04:13:26,808: Snapshot:4	Epoch:13	Loss:1.543	translation_Loss:1.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.532                                                   	MRR:20.71	Hits@10:38.18	Best:20.76
2024-12-29 04:13:29,232: Snapshot:4	Epoch:14	Loss:1.521	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.526                                                   	MRR:20.82	Hits@10:38.1	Best:20.82
2024-12-29 04:13:31,590: Snapshot:4	Epoch:15	Loss:1.505	translation_Loss:0.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.52                                                   	MRR:20.67	Hits@10:38.1	Best:20.82
2024-12-29 04:13:33,907: Snapshot:4	Epoch:16	Loss:1.492	translation_Loss:0.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.522                                                   	MRR:20.61	Hits@10:38.06	Best:20.82
2024-12-29 04:13:36,250: Snapshot:4	Epoch:17	Loss:1.481	translation_Loss:0.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.515                                                   	MRR:20.66	Hits@10:38.1	Best:20.82
2024-12-29 04:13:38,493: Snapshot:4	Epoch:18	Loss:1.477	translation_Loss:0.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.513                                                   	MRR:20.7	Hits@10:38.04	Best:20.82
2024-12-29 04:13:40,867: Snapshot:4	Epoch:19	Loss:1.464	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.509                                                   	MRR:20.86	Hits@10:38.01	Best:20.86
2024-12-29 04:13:43,252: Snapshot:4	Epoch:20	Loss:1.45	translation_Loss:0.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.507                                                   	MRR:20.68	Hits@10:38.03	Best:20.86
2024-12-29 04:13:46,023: Snapshot:4	Epoch:21	Loss:1.449	translation_Loss:0.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.506                                                   	MRR:20.72	Hits@10:38.04	Best:20.86
2024-12-29 04:13:48,368: Snapshot:4	Epoch:22	Loss:1.443	translation_Loss:0.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.504                                                   	MRR:20.69	Hits@10:38.2	Best:20.86
2024-12-29 04:13:50,823: Snapshot:4	Epoch:23	Loss:1.441	translation_Loss:0.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.503                                                   	MRR:20.87	Hits@10:37.98	Best:20.87
2024-12-29 04:13:53,272: Snapshot:4	Epoch:24	Loss:1.439	translation_Loss:0.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.502                                                   	MRR:20.87	Hits@10:38.33	Best:20.87
2024-12-29 04:13:55,628: Snapshot:4	Epoch:25	Loss:1.436	translation_Loss:0.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.5                                                   	MRR:20.6	Hits@10:38.12	Best:20.87
2024-12-29 04:13:57,971: Snapshot:4	Epoch:26	Loss:1.432	translation_Loss:0.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.501                                                   	MRR:20.58	Hits@10:38.2	Best:20.87
2024-12-29 04:14:00,309: Snapshot:4	Epoch:27	Loss:1.424	translation_Loss:0.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.499                                                   	MRR:20.77	Hits@10:38.12	Best:20.87
2024-12-29 04:14:02,930: Early Stopping! Snapshot: 4 Epoch: 28 Best Results: 20.87
2024-12-29 04:14:02,930: Start to training tokens! Snapshot: 4 Epoch: 28 Loss:1.43 MRR:20.47 Best Results: 20.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:14:02,930: Snapshot:4	Epoch:28	Loss:1.43	translation_Loss:0.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.497                                                   	MRR:20.47	Hits@10:37.91	Best:20.87
2024-12-29 04:14:05,278: Snapshot:4	Epoch:29	Loss:16.49	translation_Loss:4.481	multi_layer_Loss:12.01	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.47	Hits@10:37.91	Best:20.87
2024-12-29 04:14:07,598: End of token training: 4 Epoch: 30 Loss:7.296 MRR:20.47 Best Results: 20.87
2024-12-29 04:14:07,598: Snapshot:4	Epoch:30	Loss:7.296	translation_Loss:4.49	multi_layer_Loss:2.806	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.47	Hits@10:37.91	Best:20.87
2024-12-29 04:14:07,877: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-29 04:14:22,756: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2454 | 0.1356 | 0.309  | 0.3795 |  0.4493 |
|     1      | 0.1288 | 0.063  | 0.148  | 0.1912 |  0.2593 |
|     2      | 0.1644 | 0.1008 | 0.1814 | 0.2263 |  0.2921 |
|     3      | 0.237  | 0.1636 | 0.2682 | 0.3106 |  0.3684 |
|     4      | 0.2014 | 0.1139 | 0.2175 | 0.2817 |  0.3781 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:14:22,758: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2759 | 0.1641 | 0.3408 | 0.4108 |  0.4828 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2743 | 0.1608 | 0.3422 | 0.4117 |  0.4844 |
|     1      | 0.1315 | 0.0664 | 0.1517 | 0.1935 |  0.2603 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1418 | 0.3155 | 0.383  |  0.4528 |
|     1      | 0.1288 | 0.0637 | 0.1481 | 0.1915 |  0.2571 |
|     2      | 0.1764 | 0.1138 | 0.1946 | 0.2372 |  0.3002 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1359 | 0.3081 | 0.3777 |  0.451  |
|     1      | 0.1293 | 0.0641 | 0.1479 | 0.1911 |  0.2579 |
|     2      | 0.1731 | 0.1069 | 0.1949 | 0.2399 |  0.3017 |
|     3      | 0.2341 | 0.1648 | 0.2625 | 0.3033 |  0.3571 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2454 | 0.1356 | 0.309  | 0.3795 |  0.4493 |
|     1      | 0.1288 | 0.063  | 0.148  | 0.1912 |  0.2593 |
|     2      | 0.1644 | 0.1008 | 0.1814 | 0.2263 |  0.2921 |
|     3      | 0.237  | 0.1636 | 0.2682 | 0.3106 |  0.3684 |
|     4      | 0.2014 | 0.1139 | 0.2175 | 0.2817 |  0.3781 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:14:22,758: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  175.374609708786  |   0.276   |    0.164     |    0.341     |     0.483     |
|    1     | 152.66365909576416 |   0.205   |    0.115     |     0.25     |     0.375     |
|    2     | 93.08656024932861  |   0.188   |    0.106     |    0.224     |     0.343     |
|    3     | 90.49818181991577  |    0.19   |    0.109     |    0.225     |     0.344     |
|    4     | 82.84748029708862  |   0.189   |    0.107     |    0.223     |     0.346     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:14:22,758: Sum_Training_Time:594.4704911708832
2024-12-29 04:14:22,758: Every_Training_Time:[175.374609708786, 152.66365909576416, 93.08656024932861, 90.49818181991577, 82.84748029708862]
2024-12-29 04:14:22,758: Forward transfer: 0.013874999999999998 Backward transfer: -0.010575000000000001
2024-12-29 04:14:58,387: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229041427/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=8000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:15:13,202: Snapshot:0	Epoch:0	Loss:24.263	translation_Loss:24.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.68	Hits@10:26.04	Best:10.68
2024-12-29 04:15:23,828: Snapshot:0	Epoch:1	Loss:15.05	translation_Loss:15.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.45	Hits@10:40.39	Best:18.45
2024-12-29 04:15:34,788: Snapshot:0	Epoch:2	Loss:8.578	translation_Loss:8.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:45.19	Best:23.51
2024-12-29 04:15:46,011: Snapshot:0	Epoch:3	Loss:4.719	translation_Loss:4.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:47.19	Best:25.73
2024-12-29 04:15:56,670: Snapshot:0	Epoch:4	Loss:2.731	translation_Loss:2.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.76	Hits@10:48.03	Best:26.76
2024-12-29 04:16:07,544: Snapshot:0	Epoch:5	Loss:1.812	translation_Loss:1.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.2	Hits@10:48.29	Best:27.2
2024-12-29 04:16:18,389: Snapshot:0	Epoch:6	Loss:1.354	translation_Loss:1.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.36	Hits@10:48.33	Best:27.36
2024-12-29 04:16:28,813: Snapshot:0	Epoch:7	Loss:1.103	translation_Loss:1.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.19	Best:27.37
2024-12-29 04:16:39,821: Snapshot:0	Epoch:8	Loss:0.953	translation_Loss:0.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.52	Hits@10:48.12	Best:27.52
2024-12-29 04:16:50,647: Snapshot:0	Epoch:9	Loss:0.846	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.4	Hits@10:48.06	Best:27.52
2024-12-29 04:17:01,125: Snapshot:0	Epoch:10	Loss:0.775	translation_Loss:0.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:47.97	Best:27.52
2024-12-29 04:17:12,046: Snapshot:0	Epoch:11	Loss:0.722	translation_Loss:0.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.26	Hits@10:47.64	Best:27.52
2024-12-29 04:17:22,816: Snapshot:0	Epoch:12	Loss:0.679	translation_Loss:0.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.23	Hits@10:47.59	Best:27.52
2024-12-29 04:17:33,179: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 27.52
2024-12-29 04:17:33,179: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.638 MRR:27.01 Best Results: 27.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:17:33,179: Snapshot:0	Epoch:13	Loss:0.638	translation_Loss:0.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.01	Hits@10:47.45	Best:27.52
2024-12-29 04:17:44,428: Snapshot:0	Epoch:14	Loss:32.095	translation_Loss:16.517	multi_layer_Loss:15.578	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.01	Hits@10:47.45	Best:27.52
2024-12-29 04:17:55,086: End of token training: 0 Epoch: 15 Loss:16.624 MRR:27.01 Best Results: 27.52
2024-12-29 04:17:55,086: Snapshot:0	Epoch:15	Loss:16.624	translation_Loss:16.534	multi_layer_Loss:0.09	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.01	Hits@10:47.45	Best:27.52
2024-12-29 04:17:55,358: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-29 04:18:00,091: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2777 | 0.1669 | 0.3426 | 0.4117 |  0.4834 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:18:32,701: Snapshot:1	Epoch:0	Loss:20.688	translation_Loss:19.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.536                                                   	MRR:9.85	Hits@10:23.88	Best:9.85
2024-12-29 04:18:42,142: Snapshot:1	Epoch:1	Loss:11.926	translation_Loss:9.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.018                                                   	MRR:14.59	Hits@10:29.54	Best:14.59
2024-12-29 04:18:51,369: Snapshot:1	Epoch:2	Loss:8.234	translation_Loss:6.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.898                                                   	MRR:14.82	Hits@10:29.96	Best:14.82
2024-12-29 04:19:00,899: Snapshot:1	Epoch:3	Loss:6.833	translation_Loss:5.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.826                                                   	MRR:14.9	Hits@10:30.08	Best:14.9
2024-12-29 04:19:10,294: Snapshot:1	Epoch:4	Loss:6.307	translation_Loss:4.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.767                                                   	MRR:15.11	Hits@10:29.99	Best:15.11
2024-12-29 04:19:20,163: Snapshot:1	Epoch:5	Loss:6.055	translation_Loss:4.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.726                                                   	MRR:15.33	Hits@10:29.89	Best:15.33
2024-12-29 04:19:29,731: Snapshot:1	Epoch:6	Loss:5.922	translation_Loss:4.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.698                                                   	MRR:15.38	Hits@10:29.99	Best:15.38
2024-12-29 04:19:39,454: Snapshot:1	Epoch:7	Loss:5.834	translation_Loss:4.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.676                                                   	MRR:15.29	Hits@10:30.02	Best:15.38
2024-12-29 04:19:48,770: Snapshot:1	Epoch:8	Loss:5.79	translation_Loss:4.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.662                                                   	MRR:15.27	Hits@10:29.65	Best:15.38
2024-12-29 04:19:58,480: Snapshot:1	Epoch:9	Loss:5.748	translation_Loss:4.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.651                                                   	MRR:15.17	Hits@10:29.87	Best:15.38
2024-12-29 04:20:08,580: Snapshot:1	Epoch:10	Loss:5.729	translation_Loss:4.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.65                                                   	MRR:15.09	Hits@10:29.82	Best:15.38
2024-12-29 04:20:17,908: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 15.38
2024-12-29 04:20:17,909: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:5.687 MRR:15.09 Best Results: 15.38
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:20:17,909: Snapshot:1	Epoch:11	Loss:5.687	translation_Loss:4.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.642                                                   	MRR:15.09	Hits@10:29.92	Best:15.38
2024-12-29 04:20:27,509: Snapshot:1	Epoch:12	Loss:35.956	translation_Loss:20.267	multi_layer_Loss:15.689	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.09	Hits@10:29.92	Best:15.38
2024-12-29 04:20:36,796: End of token training: 1 Epoch: 13 Loss:20.375 MRR:15.09 Best Results: 15.38
2024-12-29 04:20:36,796: Snapshot:1	Epoch:13	Loss:20.375	translation_Loss:20.264	multi_layer_Loss:0.111	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.09	Hits@10:29.92	Best:15.38
2024-12-29 04:20:37,072: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-29 04:20:46,452: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2599 | 0.1435 | 0.328  | 0.4008 |  0.4746 |
|     1      | 0.1549 | 0.0798 | 0.183  | 0.233  |  0.2998 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:21:11,649: Snapshot:2	Epoch:0	Loss:13.516	translation_Loss:12.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.431                                                   	MRR:10.82	Hits@10:25.39	Best:10.82
2024-12-29 04:21:18,746: Snapshot:2	Epoch:1	Loss:6.915	translation_Loss:4.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.423                                                   	MRR:19.68	Hits@10:36.99	Best:19.68
2024-12-29 04:21:25,988: Snapshot:2	Epoch:2	Loss:4.834	translation_Loss:2.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.935                                                   	MRR:20.32	Hits@10:37.38	Best:20.32
2024-12-29 04:21:33,079: Snapshot:2	Epoch:3	Loss:3.871	translation_Loss:2.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.624                                                   	MRR:20.89	Hits@10:37.25	Best:20.89
2024-12-29 04:21:40,011: Snapshot:2	Epoch:4	Loss:3.354	translation_Loss:1.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.41                                                   	MRR:20.97	Hits@10:37.36	Best:20.97
2024-12-29 04:21:46,982: Snapshot:2	Epoch:5	Loss:3.069	translation_Loss:1.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.284                                                   	MRR:20.74	Hits@10:36.91	Best:20.97
2024-12-29 04:21:54,423: Snapshot:2	Epoch:6	Loss:2.906	translation_Loss:1.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.204                                                   	MRR:20.8	Hits@10:36.91	Best:20.97
2024-12-29 04:22:01,454: Snapshot:2	Epoch:7	Loss:2.825	translation_Loss:1.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.16                                                   	MRR:20.7	Hits@10:36.92	Best:20.97
2024-12-29 04:22:08,547: Snapshot:2	Epoch:8	Loss:2.768	translation_Loss:1.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.132                                                   	MRR:20.81	Hits@10:36.71	Best:20.97
2024-12-29 04:22:15,994: Snapshot:2	Epoch:9	Loss:2.741	translation_Loss:1.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.116                                                   	MRR:21.06	Hits@10:36.86	Best:21.06
2024-12-29 04:22:23,104: Snapshot:2	Epoch:10	Loss:2.723	translation_Loss:1.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.111                                                   	MRR:20.76	Hits@10:36.83	Best:21.06
2024-12-29 04:22:30,550: Snapshot:2	Epoch:11	Loss:2.701	translation_Loss:1.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.1                                                   	MRR:20.92	Hits@10:36.74	Best:21.06
2024-12-29 04:22:37,506: Snapshot:2	Epoch:12	Loss:2.689	translation_Loss:1.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:20.83	Hits@10:36.67	Best:21.06
2024-12-29 04:22:44,828: Snapshot:2	Epoch:13	Loss:2.676	translation_Loss:1.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.091                                                   	MRR:20.79	Hits@10:36.79	Best:21.06
2024-12-29 04:22:51,794: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 21.06
2024-12-29 04:22:51,795: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:2.689 MRR:20.6 Best Results: 21.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:22:51,795: Snapshot:2	Epoch:14	Loss:2.689	translation_Loss:1.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:20.6	Hits@10:36.25	Best:21.06
2024-12-29 04:22:58,766: Snapshot:2	Epoch:15	Loss:31.334	translation_Loss:15.135	multi_layer_Loss:16.199	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.6	Hits@10:36.25	Best:21.06
2024-12-29 04:23:05,987: End of token training: 2 Epoch: 16 Loss:15.441 MRR:20.6 Best Results: 21.06
2024-12-29 04:23:05,987: Snapshot:2	Epoch:16	Loss:15.441	translation_Loss:15.135	multi_layer_Loss:0.306	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.6	Hits@10:36.25	Best:21.06
2024-12-29 04:23:06,263: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-29 04:23:18,026: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2231 | 0.1179 | 0.2799 | 0.3471 |  0.421  |
|     1      | 0.1491 | 0.0732 | 0.1765 | 0.2273 |  0.2952 |
|     2      | 0.2073 |  0.13  | 0.2303 | 0.2826 |  0.3643 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:23:32,042: Snapshot:3	Epoch:0	Loss:6.503	translation_Loss:6.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:7.11	Hits@10:16.56	Best:7.11
2024-12-29 04:23:35,370: Snapshot:3	Epoch:1	Loss:3.891	translation_Loss:3.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.692                                                   	MRR:18.8	Hits@10:37.7	Best:18.8
2024-12-29 04:23:38,671: Snapshot:3	Epoch:2	Loss:2.584	translation_Loss:1.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.886                                                   	MRR:24.25	Hits@10:42.34	Best:24.25
2024-12-29 04:23:42,332: Snapshot:3	Epoch:3	Loss:1.925	translation_Loss:1.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.88                                                   	MRR:26.31	Hits@10:44.22	Best:26.31
2024-12-29 04:23:45,675: Snapshot:3	Epoch:4	Loss:1.587	translation_Loss:0.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.802                                                   	MRR:27.3	Hits@10:44.98	Best:27.3
2024-12-29 04:23:49,035: Snapshot:3	Epoch:5	Loss:1.38	translation_Loss:0.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.725                                                   	MRR:27.69	Hits@10:45.33	Best:27.69
2024-12-29 04:23:52,436: Snapshot:3	Epoch:6	Loss:1.238	translation_Loss:0.568	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.67                                                   	MRR:28.01	Hits@10:45.37	Best:28.01
2024-12-29 04:23:55,766: Snapshot:3	Epoch:7	Loss:1.138	translation_Loss:0.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.629                                                   	MRR:28.04	Hits@10:44.83	Best:28.04
2024-12-29 04:23:59,516: Snapshot:3	Epoch:8	Loss:1.06	translation_Loss:0.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.592                                                   	MRR:28.43	Hits@10:44.89	Best:28.43
2024-12-29 04:24:02,879: Snapshot:3	Epoch:9	Loss:0.991	translation_Loss:0.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.564                                                   	MRR:28.64	Hits@10:44.9	Best:28.64
2024-12-29 04:24:06,292: Snapshot:3	Epoch:10	Loss:0.954	translation_Loss:0.415	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.54                                                   	MRR:28.74	Hits@10:44.86	Best:28.74
2024-12-29 04:24:09,577: Snapshot:3	Epoch:11	Loss:0.921	translation_Loss:0.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.521                                                   	MRR:28.55	Hits@10:44.47	Best:28.74
2024-12-29 04:24:12,788: Snapshot:3	Epoch:12	Loss:0.895	translation_Loss:0.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.509                                                   	MRR:28.32	Hits@10:44.69	Best:28.74
2024-12-29 04:24:16,458: Snapshot:3	Epoch:13	Loss:0.869	translation_Loss:0.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.495                                                   	MRR:28.2	Hits@10:44.74	Best:28.74
2024-12-29 04:24:19,612: Snapshot:3	Epoch:14	Loss:0.853	translation_Loss:0.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.484                                                   	MRR:28.53	Hits@10:44.56	Best:28.74
2024-12-29 04:24:22,784: Early Stopping! Snapshot: 3 Epoch: 15 Best Results: 28.74
2024-12-29 04:24:22,784: Start to training tokens! Snapshot: 3 Epoch: 15 Loss:0.84 MRR:28.15 Best Results: 28.74
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:24:22,784: Snapshot:3	Epoch:15	Loss:0.84	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:28.15	Hits@10:43.93	Best:28.74
2024-12-29 04:24:25,955: Snapshot:3	Epoch:16	Loss:17.673	translation_Loss:5.212	multi_layer_Loss:12.462	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.15	Hits@10:43.93	Best:28.74
2024-12-29 04:24:29,187: End of token training: 3 Epoch: 17 Loss:6.607 MRR:28.15 Best Results: 28.74
2024-12-29 04:24:29,187: Snapshot:3	Epoch:17	Loss:6.607	translation_Loss:5.202	multi_layer_Loss:1.405	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.15	Hits@10:43.93	Best:28.74
2024-12-29 04:24:29,464: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-29 04:24:43,069: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2202 | 0.1173 | 0.2767 | 0.3429 |  0.4154 |
|     1      | 0.147  | 0.0715 | 0.172  | 0.2236 |  0.293  |
|     2      | 0.1826 | 0.1099 | 0.2011 | 0.2489 |  0.3253 |
|     3      | 0.2862 | 0.1998 | 0.3188 | 0.3756 |  0.4495 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:24:54,330: Snapshot:4	Epoch:0	Loss:3.71	translation_Loss:3.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.203                                                   	MRR:8.61	Hits@10:22.2	Best:8.61
2024-12-29 04:24:56,732: Snapshot:4	Epoch:1	Loss:2.189	translation_Loss:1.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.427                                                   	MRR:15.29	Hits@10:34.93	Best:15.29
2024-12-29 04:24:59,152: Snapshot:4	Epoch:2	Loss:1.544	translation_Loss:0.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.574                                                   	MRR:19.75	Hits@10:42.14	Best:19.75
2024-12-29 04:25:01,546: Snapshot:4	Epoch:3	Loss:1.162	translation_Loss:0.555	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:23.26	Hits@10:44.98	Best:23.26
2024-12-29 04:25:03,961: Snapshot:4	Epoch:4	Loss:0.928	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.566                                                   	MRR:24.8	Hits@10:46.67	Best:24.8
2024-12-29 04:25:06,781: Snapshot:4	Epoch:5	Loss:0.794	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.511                                                   	MRR:25.97	Hits@10:47.42	Best:25.97
2024-12-29 04:25:09,225: Snapshot:4	Epoch:6	Loss:0.7	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.463                                                   	MRR:26.48	Hits@10:48.34	Best:26.48
2024-12-29 04:25:11,673: Snapshot:4	Epoch:7	Loss:0.625	translation_Loss:0.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.425                                                   	MRR:26.55	Hits@10:48.83	Best:26.55
2024-12-29 04:25:14,069: Snapshot:4	Epoch:8	Loss:0.57	translation_Loss:0.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:26.99	Hits@10:49.36	Best:26.99
2024-12-29 04:25:16,373: Snapshot:4	Epoch:9	Loss:0.526	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.364                                                   	MRR:27.23	Hits@10:49.97	Best:27.23
2024-12-29 04:25:18,808: Snapshot:4	Epoch:10	Loss:0.491	translation_Loss:0.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:27.34	Hits@10:49.91	Best:27.34
2024-12-29 04:25:21,210: Snapshot:4	Epoch:11	Loss:0.462	translation_Loss:0.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.321                                                   	MRR:27.24	Hits@10:50.1	Best:27.34
2024-12-29 04:25:23,645: Snapshot:4	Epoch:12	Loss:0.44	translation_Loss:0.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.304                                                   	MRR:27.49	Hits@10:50.17	Best:27.49
2024-12-29 04:25:26,435: Snapshot:4	Epoch:13	Loss:0.422	translation_Loss:0.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.292                                                   	MRR:27.4	Hits@10:50.02	Best:27.49
2024-12-29 04:25:28,757: Snapshot:4	Epoch:14	Loss:0.412	translation_Loss:0.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.282                                                   	MRR:27.22	Hits@10:50.02	Best:27.49
2024-12-29 04:25:31,088: Snapshot:4	Epoch:15	Loss:0.4	translation_Loss:0.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.274                                                   	MRR:27.05	Hits@10:50.29	Best:27.49
2024-12-29 04:25:33,429: Snapshot:4	Epoch:16	Loss:0.387	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.266                                                   	MRR:27.48	Hits@10:49.98	Best:27.49
2024-12-29 04:25:35,739: Early Stopping! Snapshot: 4 Epoch: 17 Best Results: 27.49
2024-12-29 04:25:35,739: Start to training tokens! Snapshot: 4 Epoch: 17 Loss:0.376 MRR:27.29 Best Results: 27.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:25:35,739: Snapshot:4	Epoch:17	Loss:0.376	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:27.29	Hits@10:50.03	Best:27.49
2024-12-29 04:25:38,128: Snapshot:4	Epoch:18	Loss:15.228	translation_Loss:3.218	multi_layer_Loss:12.01	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.29	Hits@10:50.03	Best:27.49
2024-12-29 04:25:40,388: End of token training: 4 Epoch: 19 Loss:6.038 MRR:27.29 Best Results: 27.49
2024-12-29 04:25:40,389: Snapshot:4	Epoch:19	Loss:6.038	translation_Loss:3.232	multi_layer_Loss:2.806	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.29	Hits@10:50.03	Best:27.49
2024-12-29 04:25:40,611: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-29 04:25:55,730: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2085 | 0.1086 | 0.2621 | 0.3252 |  0.3955 |
|     1      | 0.1462 | 0.0713 | 0.1717 | 0.2229 |  0.2928 |
|     2      | 0.1716 | 0.1036 | 0.1873 | 0.2341 |  0.3076 |
|     3      | 0.2676 | 0.1907 | 0.2944 | 0.3383 |  0.4072 |
|     4      | 0.2701 | 0.1571 | 0.3062 | 0.3846 |  0.502  |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:25:55,732: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2777 | 0.1669 | 0.3426 | 0.4117 |  0.4834 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2599 | 0.1435 | 0.328  | 0.4008 |  0.4746 |
|     1      | 0.1549 | 0.0798 | 0.183  | 0.233  |  0.2998 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2231 | 0.1179 | 0.2799 | 0.3471 |  0.421  |
|     1      | 0.1491 | 0.0732 | 0.1765 | 0.2273 |  0.2952 |
|     2      | 0.2073 |  0.13  | 0.2303 | 0.2826 |  0.3643 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2202 | 0.1173 | 0.2767 | 0.3429 |  0.4154 |
|     1      | 0.147  | 0.0715 | 0.172  | 0.2236 |  0.293  |
|     2      | 0.1826 | 0.1099 | 0.2011 | 0.2489 |  0.3253 |
|     3      | 0.2862 | 0.1998 | 0.3188 | 0.3756 |  0.4495 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2085 | 0.1086 | 0.2621 | 0.3252 |  0.3955 |
|     1      | 0.1462 | 0.0713 | 0.1717 | 0.2229 |  0.2928 |
|     2      | 0.1716 | 0.1036 | 0.1873 | 0.2341 |  0.3076 |
|     3      | 0.2676 | 0.1907 | 0.2944 | 0.3383 |  0.4072 |
|     4      | 0.2701 | 0.1571 | 0.3062 | 0.3846 |  0.502  |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:25:55,732: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 176.69837856292725 |   0.278   |    0.167     |    0.343     |     0.483     |
|    1     | 152.94279599189758 |   0.209   |    0.113     |    0.257     |      0.39     |
|    2     | 136.35947394371033 |   0.192   |    0.105     |     0.23     |     0.361     |
|    3     | 69.42521929740906  |   0.195   |    0.109     |     0.23     |     0.359     |
|    4     | 55.980815410614014 |   0.192   |    0.108     |    0.225     |     0.354     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:25:55,732: Sum_Training_Time:591.4066832065582
2024-12-29 04:25:55,732: Every_Training_Time:[176.69837856292725, 152.94279599189758, 136.35947394371033, 69.42521929740906, 55.980815410614014]
2024-12-29 04:25:55,733: Forward transfer: 0.0177 Backward transfer: -0.03305000000000001
