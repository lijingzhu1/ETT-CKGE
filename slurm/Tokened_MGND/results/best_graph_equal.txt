[lijing@p0316 IncDE]$ python main.py -dataset graph_equal -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 5 -multi_layer_weight 1 -token_distillation_weight 10000 3000 3000 10000 -token_num 5
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2025-01-03 01:02:46,003: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/graph_equal/', dataset='graph_equal', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103010217/graph_equal', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/graph_equal', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 3000.0, 3000.0, 10000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 01:02:55,233: Snapshot:0	Epoch:0	Loss:15.838	translation_Loss:15.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:13.23	Best:5.6
2025-01-03 01:03:01,172: Snapshot:0	Epoch:1	Loss:12.827	translation_Loss:12.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.13	Hits@10:32.26	Best:12.13
2025-01-03 01:03:06,708: Snapshot:0	Epoch:2	Loss:9.68	translation_Loss:9.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.65	Hits@10:44.98	Best:19.65
2025-01-03 01:03:12,234: Snapshot:0	Epoch:3	Loss:6.289	translation_Loss:6.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:51.51	Best:25.75
2025-01-03 01:03:18,144: Snapshot:0	Epoch:4	Loss:4.053	translation_Loss:4.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.43	Hits@10:54.86	Best:29.43
2025-01-03 01:03:23,668: Snapshot:0	Epoch:5	Loss:2.623	translation_Loss:2.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:31.01	Hits@10:56.49	Best:31.01
2025-01-03 01:03:29,522: Snapshot:0	Epoch:6	Loss:1.761	translation_Loss:1.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:31.87	Hits@10:57.47	Best:31.87
2025-01-03 01:03:35,072: Snapshot:0	Epoch:7	Loss:1.242	translation_Loss:1.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.33	Hits@10:57.92	Best:32.33
2025-01-03 01:03:40,950: Snapshot:0	Epoch:8	Loss:0.931	translation_Loss:0.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.63	Hits@10:58.0	Best:32.63
2025-01-03 01:03:46,485: Snapshot:0	Epoch:9	Loss:0.729	translation_Loss:0.729	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.6	Hits@10:57.96	Best:32.63
2025-01-03 01:03:52,027: Snapshot:0	Epoch:10	Loss:0.594	translation_Loss:0.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.71	Hits@10:57.92	Best:32.71
2025-01-03 01:03:57,949: Snapshot:0	Epoch:11	Loss:0.505	translation_Loss:0.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.86	Hits@10:58.07	Best:32.86
2025-01-03 01:04:03,472: Snapshot:0	Epoch:12	Loss:0.437	translation_Loss:0.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.86	Hits@10:57.94	Best:32.86
2025-01-03 01:04:09,370: Snapshot:0	Epoch:13	Loss:0.386	translation_Loss:0.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.56	Hits@10:57.84	Best:32.86
2025-01-03 01:04:14,928: Snapshot:0	Epoch:14	Loss:0.347	translation_Loss:0.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.62	Hits@10:57.73	Best:32.86
2025-01-03 01:04:20,786: Snapshot:0	Epoch:15	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.61	Hits@10:57.55	Best:32.86
2025-01-03 01:04:26,289: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 32.86
2025-01-03 01:04:26,289: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.295 MRR:32.68 Best Results: 32.86
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 01:04:26,290: Snapshot:0	Epoch:16	Loss:0.295	translation_Loss:0.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.68	Hits@10:57.58	Best:32.86
2025-01-03 01:04:32,636: Snapshot:0	Epoch:17	Loss:26.348	translation_Loss:11.281	multi_layer_Loss:15.067	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:32.68	Hits@10:57.58	Best:32.86
2025-01-03 01:04:38,237: End of token training: 0 Epoch: 18 Loss:11.67 MRR:32.68 Best Results: 32.86
2025-01-03 01:04:38,237: Snapshot:0	Epoch:18	Loss:11.67	translation_Loss:11.3	multi_layer_Loss:0.37	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:32.68	Hits@10:57.58	Best:32.86
2025-01-03 01:04:38,468: => loading checkpoint './checkpoint/graph_equal/0model_best.tar'
2025-01-03 01:04:40,140: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3315 | 0.1997 | 0.3946 | 0.4797 |  0.5857 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 01:05:03,365: Snapshot:1	Epoch:0	Loss:11.827	translation_Loss:11.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.414                                                   	MRR:12.5	Hits@10:23.71	Best:12.5
2025-01-03 01:05:09,859: Snapshot:1	Epoch:1	Loss:5.113	translation_Loss:4.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.297                                                   	MRR:16.83	Hits@10:33.16	Best:16.83
2025-01-03 01:05:16,432: Snapshot:1	Epoch:2	Loss:2.53	translation_Loss:2.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.194                                                   	MRR:19.62	Hits@10:38.47	Best:19.62
2025-01-03 01:05:23,206: Snapshot:1	Epoch:3	Loss:1.696	translation_Loss:1.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:20.97	Hits@10:40.7	Best:20.97
2025-01-03 01:05:29,636: Snapshot:1	Epoch:4	Loss:1.351	translation_Loss:1.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.131                                                   	MRR:21.63	Hits@10:41.8	Best:21.63
2025-01-03 01:05:36,471: Snapshot:1	Epoch:5	Loss:1.169	translation_Loss:1.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.122                                                   	MRR:21.81	Hits@10:42.1	Best:21.81
2025-01-03 01:05:43,100: Snapshot:1	Epoch:6	Loss:1.044	translation_Loss:0.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.117                                                   	MRR:22.03	Hits@10:42.49	Best:22.03
2025-01-03 01:05:50,017: Snapshot:1	Epoch:7	Loss:0.972	translation_Loss:0.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.114                                                   	MRR:22.2	Hits@10:42.51	Best:22.2
2025-01-03 01:05:56,443: Snapshot:1	Epoch:8	Loss:0.923	translation_Loss:0.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.111                                                   	MRR:22.37	Hits@10:42.85	Best:22.37
2025-01-03 01:06:03,191: Snapshot:1	Epoch:9	Loss:0.876	translation_Loss:0.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.11                                                   	MRR:22.41	Hits@10:42.8	Best:22.41
2025-01-03 01:06:09,604: Snapshot:1	Epoch:10	Loss:0.841	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.108                                                   	MRR:22.36	Hits@10:42.89	Best:22.41
2025-01-03 01:06:16,367: Snapshot:1	Epoch:11	Loss:0.811	translation_Loss:0.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.108                                                   	MRR:22.61	Hits@10:42.84	Best:22.61
2025-01-03 01:06:22,813: Snapshot:1	Epoch:12	Loss:0.793	translation_Loss:0.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.106                                                   	MRR:22.55	Hits@10:42.88	Best:22.61
2025-01-03 01:06:29,593: Snapshot:1	Epoch:13	Loss:0.771	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.107                                                   	MRR:22.52	Hits@10:42.92	Best:22.61
2025-01-03 01:06:36,046: Snapshot:1	Epoch:14	Loss:0.762	translation_Loss:0.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.107                                                   	MRR:22.64	Hits@10:43.12	Best:22.64
2025-01-03 01:06:42,573: Snapshot:1	Epoch:15	Loss:0.74	translation_Loss:0.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.105                                                   	MRR:22.59	Hits@10:43.01	Best:22.64
2025-01-03 01:06:49,470: Snapshot:1	Epoch:16	Loss:0.736	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.105                                                   	MRR:22.66	Hits@10:43.07	Best:22.66
2025-01-03 01:06:55,875: Snapshot:1	Epoch:17	Loss:0.718	translation_Loss:0.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:22.66	Hits@10:43.17	Best:22.66
2025-01-03 01:07:02,624: Snapshot:1	Epoch:18	Loss:0.711	translation_Loss:0.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:22.59	Hits@10:43.16	Best:22.66
2025-01-03 01:07:09,357: Snapshot:1	Epoch:19	Loss:0.703	translation_Loss:0.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:22.55	Hits@10:43.22	Best:22.66
2025-01-03 01:07:15,778: Snapshot:1	Epoch:20	Loss:0.699	translation_Loss:0.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.105                                                   	MRR:22.58	Hits@10:43.23	Best:22.66
2025-01-03 01:07:22,203: Early Stopping! Snapshot: 1 Epoch: 21 Best Results: 22.66
2025-01-03 01:07:22,203: Start to training tokens! Snapshot: 1 Epoch: 21 Loss:0.687 MRR:22.63 Best Results: 22.66
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 01:07:22,204: Snapshot:1	Epoch:21	Loss:0.687	translation_Loss:0.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.103                                                   	MRR:22.63	Hits@10:43.25	Best:22.66
2025-01-03 01:07:28,962: Snapshot:1	Epoch:22	Loss:28.293	translation_Loss:12.446	multi_layer_Loss:15.847	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:43.25	Best:22.66
2025-01-03 01:07:35,466: End of token training: 1 Epoch: 23 Loss:12.755 MRR:22.63 Best Results: 22.66
2025-01-03 01:07:35,466: Snapshot:1	Epoch:23	Loss:12.755	translation_Loss:12.446	multi_layer_Loss:0.31	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.63	Hits@10:43.25	Best:22.66
2025-01-03 01:07:35,699: => loading checkpoint './checkpoint/graph_equal/1model_best.tar'
2025-01-03 01:07:39,994: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3312 | 0.1991 | 0.3952 | 0.4799 |  0.5851 |
|     1      | 0.2266 | 0.1244 | 0.2577 | 0.3327 |  0.4334 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 01:08:03,801: Snapshot:2	Epoch:0	Loss:10.605	translation_Loss:10.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.518                                                   	MRR:13.84	Hits@10:26.07	Best:13.84
2025-01-03 01:08:10,633: Snapshot:2	Epoch:1	Loss:4.029	translation_Loss:3.508	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.521                                                   	MRR:17.09	Hits@10:32.34	Best:17.09
2025-01-03 01:08:17,833: Snapshot:2	Epoch:2	Loss:2.175	translation_Loss:1.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:18.74	Hits@10:35.6	Best:18.74
2025-01-03 01:08:24,741: Snapshot:2	Epoch:3	Loss:1.581	translation_Loss:1.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:19.56	Hits@10:37.29	Best:19.56
2025-01-03 01:08:31,892: Snapshot:2	Epoch:4	Loss:1.332	translation_Loss:1.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.209                                                   	MRR:20.19	Hits@10:38.27	Best:20.19
2025-01-03 01:08:38,717: Snapshot:2	Epoch:5	Loss:1.202	translation_Loss:1.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.195                                                   	MRR:20.65	Hits@10:38.75	Best:20.65
2025-01-03 01:08:45,989: Snapshot:2	Epoch:6	Loss:1.113	translation_Loss:0.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.189                                                   	MRR:20.8	Hits@10:39.2	Best:20.8
2025-01-03 01:08:52,884: Snapshot:2	Epoch:7	Loss:1.055	translation_Loss:0.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.185                                                   	MRR:20.96	Hits@10:39.45	Best:20.96
2025-01-03 01:08:59,859: Snapshot:2	Epoch:8	Loss:1.008	translation_Loss:0.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:21.16	Hits@10:39.63	Best:21.16
2025-01-03 01:09:07,151: Snapshot:2	Epoch:9	Loss:0.976	translation_Loss:0.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:21.22	Hits@10:39.66	Best:21.22
2025-01-03 01:09:14,109: Snapshot:2	Epoch:10	Loss:0.945	translation_Loss:0.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:21.3	Hits@10:39.91	Best:21.3
2025-01-03 01:09:21,277: Snapshot:2	Epoch:11	Loss:0.926	translation_Loss:0.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:21.54	Hits@10:40.19	Best:21.54
2025-01-03 01:09:28,397: Snapshot:2	Epoch:12	Loss:0.912	translation_Loss:0.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:21.42	Hits@10:40.33	Best:21.54
2025-01-03 01:09:35,218: Snapshot:2	Epoch:13	Loss:0.889	translation_Loss:0.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:21.58	Hits@10:40.39	Best:21.58
2025-01-03 01:09:42,129: Snapshot:2	Epoch:14	Loss:0.876	translation_Loss:0.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:21.58	Hits@10:40.38	Best:21.58
2025-01-03 01:09:49,367: Snapshot:2	Epoch:15	Loss:0.86	translation_Loss:0.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.172                                                   	MRR:21.58	Hits@10:40.47	Best:21.58
2025-01-03 01:09:56,229: Snapshot:2	Epoch:16	Loss:0.848	translation_Loss:0.678	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:21.6	Hits@10:40.47	Best:21.6
2025-01-03 01:10:03,428: Snapshot:2	Epoch:17	Loss:0.849	translation_Loss:0.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.172                                                   	MRR:21.63	Hits@10:40.6	Best:21.63
2025-01-03 01:10:10,259: Snapshot:2	Epoch:18	Loss:0.84	translation_Loss:0.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:21.65	Hits@10:40.55	Best:21.65
2025-01-03 01:10:17,541: Snapshot:2	Epoch:19	Loss:0.829	translation_Loss:0.659	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:21.6	Hits@10:40.64	Best:21.65
2025-01-03 01:10:24,409: Snapshot:2	Epoch:20	Loss:0.827	translation_Loss:0.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:21.79	Hits@10:40.68	Best:21.79
2025-01-03 01:10:31,547: Snapshot:2	Epoch:21	Loss:0.81	translation_Loss:0.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:21.69	Hits@10:40.75	Best:21.79
2025-01-03 01:10:38,336: Snapshot:2	Epoch:22	Loss:0.815	translation_Loss:0.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:21.54	Hits@10:40.64	Best:21.79
2025-01-03 01:10:45,420: Snapshot:2	Epoch:23	Loss:0.801	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:21.66	Hits@10:40.56	Best:21.79
2025-01-03 01:10:52,282: Snapshot:2	Epoch:24	Loss:0.802	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:21.58	Hits@10:40.62	Best:21.79
2025-01-03 01:10:59,406: Early Stopping! Snapshot: 2 Epoch: 25 Best Results: 21.79
2025-01-03 01:10:59,407: Start to training tokens! Snapshot: 2 Epoch: 25 Loss:0.796 MRR:21.63 Best Results: 21.79
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 01:10:59,407: Snapshot:2	Epoch:25	Loss:0.796	translation_Loss:0.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:21.63	Hits@10:40.77	Best:21.79
2025-01-03 01:11:06,185: Snapshot:2	Epoch:26	Loss:27.365	translation_Loss:12.256	multi_layer_Loss:15.108	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.63	Hits@10:40.77	Best:21.79
2025-01-03 01:11:13,490: End of token training: 2 Epoch: 27 Loss:12.573 MRR:21.63 Best Results: 21.79
2025-01-03 01:11:13,490: Snapshot:2	Epoch:27	Loss:12.573	translation_Loss:12.258	multi_layer_Loss:0.315	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.63	Hits@10:40.77	Best:21.79
2025-01-03 01:11:13,737: => loading checkpoint './checkpoint/graph_equal/2model_best.tar'
2025-01-03 01:11:20,279: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3305 | 0.199  | 0.395  | 0.4782 |  0.5828 |
|     1      | 0.2277 | 0.1254 | 0.2578 | 0.3356 |  0.4376 |
|     2      | 0.2139 | 0.1198 | 0.2446 | 0.3109 |  0.4031 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 01:11:44,861: Snapshot:3	Epoch:0	Loss:9.84	translation_Loss:9.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.563                                                   	MRR:16.66	Hits@10:30.27	Best:16.66
2025-01-03 01:11:52,208: Snapshot:3	Epoch:1	Loss:3.559	translation_Loss:2.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.618                                                   	MRR:19.11	Hits@10:34.22	Best:19.11
2025-01-03 01:11:59,942: Snapshot:3	Epoch:2	Loss:2.023	translation_Loss:1.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.409                                                   	MRR:20.06	Hits@10:35.89	Best:20.06
2025-01-03 01:12:07,181: Snapshot:3	Epoch:3	Loss:1.55	translation_Loss:1.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:20.45	Hits@10:36.45	Best:20.45
2025-01-03 01:12:14,861: Snapshot:3	Epoch:4	Loss:1.344	translation_Loss:1.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.283                                                   	MRR:20.7	Hits@10:37.15	Best:20.7
2025-01-03 01:12:22,172: Snapshot:3	Epoch:5	Loss:1.238	translation_Loss:0.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.267                                                   	MRR:20.98	Hits@10:37.43	Best:20.98
2025-01-03 01:12:29,894: Snapshot:3	Epoch:6	Loss:1.151	translation_Loss:0.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:21.18	Hits@10:37.89	Best:21.18
2025-01-03 01:12:37,153: Snapshot:3	Epoch:7	Loss:1.1	translation_Loss:0.849	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.251                                                   	MRR:21.1	Hits@10:37.83	Best:21.18
2025-01-03 01:12:44,410: Snapshot:3	Epoch:8	Loss:1.063	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.248                                                   	MRR:21.21	Hits@10:38.21	Best:21.21
2025-01-03 01:12:51,995: Snapshot:3	Epoch:9	Loss:1.037	translation_Loss:0.791	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.246                                                   	MRR:21.41	Hits@10:38.4	Best:21.41
2025-01-03 01:12:59,348: Snapshot:3	Epoch:10	Loss:1.01	translation_Loss:0.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.243                                                   	MRR:21.53	Hits@10:38.48	Best:21.53
2025-01-03 01:13:06,902: Snapshot:3	Epoch:11	Loss:0.994	translation_Loss:0.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.24                                                   	MRR:21.39	Hits@10:38.52	Best:21.53
2025-01-03 01:13:14,201: Snapshot:3	Epoch:12	Loss:0.972	translation_Loss:0.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.24                                                   	MRR:21.62	Hits@10:38.61	Best:21.62
2025-01-03 01:13:21,813: Snapshot:3	Epoch:13	Loss:0.958	translation_Loss:0.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:21.54	Hits@10:38.74	Best:21.62
2025-01-03 01:13:29,124: Snapshot:3	Epoch:14	Loss:0.941	translation_Loss:0.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.236                                                   	MRR:21.59	Hits@10:38.54	Best:21.62
2025-01-03 01:13:36,757: Snapshot:3	Epoch:15	Loss:0.933	translation_Loss:0.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:21.6	Hits@10:38.78	Best:21.62
2025-01-03 01:13:44,060: Snapshot:3	Epoch:16	Loss:0.932	translation_Loss:0.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.236                                                   	MRR:21.68	Hits@10:38.63	Best:21.68
2025-01-03 01:13:51,658: Snapshot:3	Epoch:17	Loss:0.92	translation_Loss:0.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.236                                                   	MRR:21.59	Hits@10:38.78	Best:21.68
2025-01-03 01:13:58,905: Snapshot:3	Epoch:18	Loss:0.906	translation_Loss:0.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.234                                                   	MRR:21.75	Hits@10:38.93	Best:21.75
2025-01-03 01:14:06,494: Snapshot:3	Epoch:19	Loss:0.903	translation_Loss:0.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.233                                                   	MRR:21.58	Hits@10:38.82	Best:21.75
2025-01-03 01:14:13,746: Snapshot:3	Epoch:20	Loss:0.898	translation_Loss:0.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.235                                                   	MRR:21.81	Hits@10:38.91	Best:21.81
2025-01-03 01:14:21,344: Snapshot:3	Epoch:21	Loss:0.888	translation_Loss:0.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:21.72	Hits@10:38.94	Best:21.81
2025-01-03 01:14:28,564: Snapshot:3	Epoch:22	Loss:0.89	translation_Loss:0.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.234                                                   	MRR:21.78	Hits@10:38.71	Best:21.81
2025-01-03 01:14:36,273: Snapshot:3	Epoch:23	Loss:0.883	translation_Loss:0.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.234                                                   	MRR:21.62	Hits@10:39.09	Best:21.81
2025-01-03 01:14:43,523: Snapshot:3	Epoch:24	Loss:0.882	translation_Loss:0.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.234                                                   	MRR:21.7	Hits@10:38.88	Best:21.81
2025-01-03 01:14:51,112: Early Stopping! Snapshot: 3 Epoch: 25 Best Results: 21.81
2025-01-03 01:14:51,112: Start to training tokens! Snapshot: 3 Epoch: 25 Loss:0.867 MRR:21.73 Best Results: 21.81
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 01:14:51,113: Snapshot:3	Epoch:25	Loss:0.867	translation_Loss:0.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:21.73	Hits@10:38.94	Best:21.81
2025-01-03 01:14:58,457: Snapshot:3	Epoch:26	Loss:26.496	translation_Loss:12.12	multi_layer_Loss:14.376	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.73	Hits@10:38.94	Best:21.81
2025-01-03 01:15:06,071: End of token training: 3 Epoch: 27 Loss:12.439 MRR:21.73 Best Results: 21.81
2025-01-03 01:15:06,071: Snapshot:3	Epoch:27	Loss:12.439	translation_Loss:12.125	multi_layer_Loss:0.314	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.73	Hits@10:38.94	Best:21.81
2025-01-03 01:15:06,309: => loading checkpoint './checkpoint/graph_equal/3model_best.tar'
2025-01-03 01:15:15,889: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3295 |  0.2   | 0.3903 | 0.4729 |  0.5772 |
|     1      | 0.2262 | 0.1231 | 0.2581 | 0.3328 |  0.4352 |
|     2      | 0.214  | 0.1192 | 0.2456 | 0.3117 |  0.4054 |
|     3      | 0.1995 | 0.1132 | 0.2262 | 0.288  |  0.3724 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 01:15:43,147: Snapshot:4	Epoch:0	Loss:8.171	translation_Loss:7.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.712                                                   	MRR:22.29	Hits@10:38.56	Best:22.29
2025-01-03 01:15:51,426: Snapshot:4	Epoch:1	Loss:3.633	translation_Loss:3.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.586                                                   	MRR:22.67	Hits@10:39.0	Best:22.67
2025-01-03 01:15:59,618: Snapshot:4	Epoch:2	Loss:2.296	translation_Loss:1.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.422                                                   	MRR:22.71	Hits@10:39.27	Best:22.71
2025-01-03 01:16:07,896: Snapshot:4	Epoch:3	Loss:1.921	translation_Loss:1.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.376                                                   	MRR:22.74	Hits@10:39.22	Best:22.74
2025-01-03 01:16:16,048: Snapshot:4	Epoch:4	Loss:1.761	translation_Loss:1.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.364                                                   	MRR:22.67	Hits@10:39.23	Best:22.74
2025-01-03 01:16:24,164: Snapshot:4	Epoch:5	Loss:1.678	translation_Loss:1.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.356                                                   	MRR:22.63	Hits@10:39.2	Best:22.74
2025-01-03 01:16:32,615: Snapshot:4	Epoch:6	Loss:1.622	translation_Loss:1.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.356                                                   	MRR:22.67	Hits@10:39.16	Best:22.74
2025-01-03 01:16:41,108: Snapshot:4	Epoch:7	Loss:1.589	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:22.63	Hits@10:39.05	Best:22.74
2025-01-03 01:16:49,334: Early Stopping! Snapshot: 4 Epoch: 8 Best Results: 22.74
2025-01-03 01:16:49,334: Start to training tokens! Snapshot: 4 Epoch: 8 Loss:1.564 MRR:22.63 Best Results: 22.74
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 01:16:49,335: Snapshot:4	Epoch:8	Loss:1.564	translation_Loss:1.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.353                                                   	MRR:22.63	Hits@10:39.09	Best:22.74
2025-01-03 01:16:57,891: Snapshot:4	Epoch:9	Loss:27.533	translation_Loss:12.298	multi_layer_Loss:15.235	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:39.09	Best:22.74
2025-01-03 01:17:06,029: End of token training: 4 Epoch: 10 Loss:12.61 MRR:22.63 Best Results: 22.74
2025-01-03 01:17:06,029: Snapshot:4	Epoch:10	Loss:12.61	translation_Loss:12.317	multi_layer_Loss:0.292	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.63	Hits@10:39.09	Best:22.74
2025-01-03 01:17:06,267: => loading checkpoint './checkpoint/graph_equal/4model_best.tar'
2025-01-03 01:17:19,281: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3288 | 0.1987 | 0.3898 | 0.4728 |  0.578  |
|     1      | 0.2281 | 0.1248 | 0.2599 | 0.3357 |  0.4382 |
|     2      | 0.215  | 0.1187 | 0.2481 | 0.3157 |   0.41  |
|     3      | 0.2018 | 0.1153 | 0.2286 | 0.2899 |  0.3756 |
|     4      | 0.2237 | 0.1368 | 0.2552 | 0.317  |   0.39  |
+------------+--------+--------+--------+--------+---------+
2025-01-03 01:17:19,284: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3315 | 0.1997 | 0.3946 | 0.4797 |  0.5857 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3312 | 0.1991 | 0.3952 | 0.4799 |  0.5851 |
|     1      | 0.2266 | 0.1244 | 0.2577 | 0.3327 |  0.4334 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3305 | 0.199  | 0.395  | 0.4782 |  0.5828 |
|     1      | 0.2277 | 0.1254 | 0.2578 | 0.3356 |  0.4376 |
|     2      | 0.2139 | 0.1198 | 0.2446 | 0.3109 |  0.4031 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3295 |  0.2   | 0.3903 | 0.4729 |  0.5772 |
|     1      | 0.2262 | 0.1231 | 0.2581 | 0.3328 |  0.4352 |
|     2      | 0.214  | 0.1192 | 0.2456 | 0.3117 |  0.4054 |
|     3      | 0.1995 | 0.1132 | 0.2262 | 0.288  |  0.3724 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.3288 | 0.1987 | 0.3898 | 0.4728 |  0.578  |
|     1      | 0.2281 | 0.1248 | 0.2599 | 0.3357 |  0.4382 |
|     2      | 0.215  | 0.1187 | 0.2481 | 0.3157 |   0.41  |
|     3      | 0.2018 | 0.1153 | 0.2286 | 0.2899 |  0.3756 |
|     4      | 0.2237 | 0.1368 | 0.2552 | 0.317  |   0.39  |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 01:17:19,285: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 112.23378396034241 |   0.332   |     0.2      |    0.395     |     0.586     |
|    1     | 172.89554691314697 |   0.277   |     0.16     |    0.324     |     0.506     |
|    2     | 210.95225071907043 |   0.256   |    0.147     |    0.297     |     0.472     |
|    3     | 222.6366684436798  |   0.241   |    0.138     |    0.278     |     0.445     |
|    4     | 106.60047960281372 |   0.238   |    0.138     |    0.274     |     0.436     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 01:17:19,285: Sum_Training_Time:825.3187296390533
2025-01-03 01:17:19,285: Every_Training_Time:[112.23378396034241, 172.89554691314697, 210.95225071907043, 222.6366684436798, 106.60047960281372]
2025-01-03 01:17:19,285: Forward transfer: 0.119975 Backward transfer: 0.000549999999999988