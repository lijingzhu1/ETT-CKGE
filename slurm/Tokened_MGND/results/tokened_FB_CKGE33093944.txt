2025-01-07 15:49:08,668: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107154843/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=1, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 15:49:32,849: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.19	Hits@10:34.63	Best:15.19
2025-01-07 15:49:52,338: Snapshot:0	Epoch:1	Loss:18.581	translation_Loss:18.581	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.8	Hits@10:45.21	Best:23.8
2025-01-07 15:50:11,766: Snapshot:0	Epoch:2	Loss:7.138	translation_Loss:7.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.58	Hits@10:46.76	Best:25.58
2025-01-07 15:50:31,093: Snapshot:0	Epoch:3	Loss:3.628	translation_Loss:3.628	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.88	Hits@10:46.9	Best:25.88
2025-01-07 15:50:50,992: Snapshot:0	Epoch:4	Loss:2.335	translation_Loss:2.335	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.85	Hits@10:46.59	Best:25.88
2025-01-07 15:51:10,267: Snapshot:0	Epoch:5	Loss:1.745	translation_Loss:1.745	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.47	Hits@10:46.38	Best:25.88
2025-01-07 15:51:29,605: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.88
2025-01-07 15:51:29,606: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.417 MRR:25.48 Best Results: 25.88
Token added to optimizer, embeddings excluded successfully.
2025-01-07 15:51:29,606: Snapshot:0	Epoch:6	Loss:1.417	translation_Loss:1.417	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.48	Hits@10:46.2	Best:25.88
2025-01-07 15:51:49,577: Snapshot:0	Epoch:7	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:25.48	Hits@10:46.2	Best:25.88
2025-01-07 15:52:08,923: End of token training: 0 Epoch: 8 Loss:nan MRR:25.48 Best Results: 25.88
2025-01-07 15:52:08,924: Snapshot:0	Epoch:8	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:25.48	Hits@10:46.2	Best:25.88
2025-01-07 15:52:09,230: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 15:52:15,725: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1507 | 0.304  | 0.3761 |  0.4669 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,596,200
Trainable params: 400
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 15:52:26,054: Snapshot:1	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.51	Hits@10:0.66	Best:0.51
2025-01-07 15:52:29,494: Snapshot:1	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.51	Hits@10:0.66	Best:0.51
2025-01-07 15:52:32,937: Snapshot:1	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.51	Hits@10:0.66	Best:0.51
2025-01-07 15:52:36,805: Early Stopping! Snapshot: 1 Epoch: 3 Best Results: 0.51
2025-01-07 15:52:36,805: Start to training tokens! Snapshot: 1 Epoch: 3 Loss:nan MRR:0.51 Best Results: 0.51
Token added to optimizer, embeddings excluded successfully.
2025-01-07 15:52:36,806: Snapshot:1	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.51	Hits@10:0.66	Best:0.51
2025-01-07 15:52:40,256: Snapshot:1	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.51	Hits@10:0.66	Best:0.51
2025-01-07 15:52:43,719: End of token training: 1 Epoch: 5 Loss:nan MRR:0.51 Best Results: 0.51
2025-01-07 15:52:43,720: Snapshot:1	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.51	Hits@10:0.66	Best:0.51
2025-01-07 15:52:43,965: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 15:52:52,096: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,346,800
Trainable params: 400
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 15:53:02,804: Snapshot:2	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.48	Hits@10:0.59	Best:0.48
2025-01-07 15:53:06,397: Snapshot:2	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.48	Hits@10:0.59	Best:0.48
2025-01-07 15:53:09,991: Snapshot:2	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.48	Hits@10:0.59	Best:0.48
2025-01-07 15:53:13,585: Early Stopping! Snapshot: 2 Epoch: 3 Best Results: 0.48
2025-01-07 15:53:13,585: Start to training tokens! Snapshot: 2 Epoch: 3 Loss:nan MRR:0.48 Best Results: 0.48
Token added to optimizer, embeddings excluded successfully.
2025-01-07 15:53:13,585: Snapshot:2	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.48	Hits@10:0.59	Best:0.48
2025-01-07 15:53:17,168: Snapshot:2	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.48	Hits@10:0.59	Best:0.48
2025-01-07 15:53:21,276: End of token training: 2 Epoch: 5 Loss:nan MRR:0.48 Best Results: 0.48
2025-01-07 15:53:21,276: Snapshot:2	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.48	Hits@10:0.59	Best:0.48
2025-01-07 15:53:21,594: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 15:53:31,709: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
|     2      | 0.0052 |  0.0   | 0.0064 | 0.0064 |  0.0066 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,722,000
Trainable params: 400
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 15:53:42,245: Snapshot:3	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.62	Hits@10:0.87	Best:0.62
2025-01-07 15:53:45,938: Snapshot:3	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.62	Hits@10:0.87	Best:0.62
2025-01-07 15:53:49,613: Snapshot:3	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.62	Hits@10:0.87	Best:0.62
2025-01-07 15:53:53,731: Early Stopping! Snapshot: 3 Epoch: 3 Best Results: 0.62
2025-01-07 15:53:53,732: Start to training tokens! Snapshot: 3 Epoch: 3 Loss:nan MRR:0.62 Best Results: 0.62
Token added to optimizer, embeddings excluded successfully.
2025-01-07 15:53:53,732: Snapshot:3	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.62	Hits@10:0.87	Best:0.62
2025-01-07 15:53:57,424: Snapshot:3	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.62	Hits@10:0.87	Best:0.62
2025-01-07 15:54:01,129: End of token training: 3 Epoch: 5 Loss:nan MRR:0.62 Best Results: 0.62
2025-01-07 15:54:01,129: Snapshot:3	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.62	Hits@10:0.87	Best:0.62
2025-01-07 15:54:01,376: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 15:54:13,514: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
|     2      | 0.0052 |  0.0   | 0.0064 | 0.0064 |  0.0066 |
|     3      | 0.0053 | 0.0001 | 0.0064 | 0.0064 |  0.0068 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,909,600
Trainable params: 400
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 15:54:24,387: Snapshot:4	Epoch:0	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.52	Hits@10:0.65	Best:0.52
2025-01-07 15:54:28,151: Snapshot:4	Epoch:1	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.52	Hits@10:0.65	Best:0.52
2025-01-07 15:54:31,866: Snapshot:4	Epoch:2	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.52	Hits@10:0.65	Best:0.52
2025-01-07 15:54:36,021: Early Stopping! Snapshot: 4 Epoch: 3 Best Results: 0.52
2025-01-07 15:54:36,022: Start to training tokens! Snapshot: 4 Epoch: 3 Loss:nan MRR:0.52 Best Results: 0.52
Token added to optimizer, embeddings excluded successfully.
2025-01-07 15:54:36,022: Snapshot:4	Epoch:3	Loss:nan	translation_Loss:nan	token_training_loss:0.0	distillation_Loss:nan                                                   	MRR:0.52	Hits@10:0.65	Best:0.52
2025-01-07 15:54:39,750: Snapshot:4	Epoch:4	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                   	MRR:0.52	Hits@10:0.65	Best:0.52
2025-01-07 15:54:43,510: End of token training: 4 Epoch: 5 Loss:nan MRR:0.52 Best Results: 0.52
2025-01-07 15:54:43,510: Snapshot:4	Epoch:5	Loss:nan	translation_Loss:nan	token_training_loss:nan	distillation_Loss:0.0                                                           	MRR:0.52	Hits@10:0.65	Best:0.52
2025-01-07 15:54:43,755: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 15:54:58,240: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
|     2      | 0.0052 |  0.0   | 0.0064 | 0.0064 |  0.0066 |
|     3      | 0.0053 | 0.0001 | 0.0064 | 0.0064 |  0.0068 |
|     4      | 0.0057 | 0.0001 | 0.0068 | 0.0069 |  0.0072 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   400
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,003,400
Trainable params: 400
Non-trainable params: 3,003,000
=================================================================
2025-01-07 15:54:58,243: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1507 | 0.304  | 0.3761 |  0.4669 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
|     2      | 0.0052 |  0.0   | 0.0064 | 0.0064 |  0.0066 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
|     2      | 0.0052 |  0.0   | 0.0064 | 0.0064 |  0.0066 |
|     3      | 0.0053 | 0.0001 | 0.0064 | 0.0064 |  0.0068 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0057 |  0.0   | 0.0058 | 0.0059 |  0.0063 |
|     1      | 0.0043 |  0.0   | 0.0046 | 0.0046 |  0.005  |
|     2      | 0.0052 |  0.0   | 0.0064 | 0.0064 |  0.0066 |
|     3      | 0.0053 | 0.0001 | 0.0064 | 0.0064 |  0.0068 |
|     4      | 0.0057 | 0.0001 | 0.0068 | 0.0069 |  0.0072 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 15:54:58,244: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 180.25506734848022 |   0.258   |    0.151     |    0.304     |     0.467     |
|    1     | 25.829870462417603 |   0.005   |     0.0      |    0.006     |     0.006     |
|    2     | 27.213852405548096 |   0.005   |     0.0      |    0.006     |     0.006     |
|    3     | 27.352222442626953 |   0.005   |     0.0      |    0.006     |     0.006     |
|    4     | 27.89903497695923  |   0.005   |     0.0      |    0.006     |     0.006     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 15:54:58,244: Sum_Training_Time:288.5500476360321
2025-01-07 15:54:58,244: Every_Training_Time:[180.25506734848022, 25.829870462417603, 27.213852405548096, 27.352222442626953, 27.89903497695923]
2025-01-07 15:54:58,244: Forward transfer: 0.0171 Backward transfer: -0.0631
2025-01-07 15:55:22,608: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107155502/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 15:55:46,503: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.19	Hits@10:34.63	Best:15.19
2025-01-07 15:56:05,499: Snapshot:0	Epoch:1	Loss:18.581	translation_Loss:18.581	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.81	Hits@10:45.22	Best:23.81
2025-01-07 15:56:24,462: Snapshot:0	Epoch:2	Loss:7.139	translation_Loss:7.139	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.59	Hits@10:46.8	Best:25.59
2025-01-07 15:56:43,463: Snapshot:0	Epoch:3	Loss:3.628	translation_Loss:3.628	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.84	Hits@10:46.96	Best:25.84
2025-01-07 15:57:02,851: Snapshot:0	Epoch:4	Loss:2.339	translation_Loss:2.339	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.74	Hits@10:46.59	Best:25.84
2025-01-07 15:57:21,810: Snapshot:0	Epoch:5	Loss:1.746	translation_Loss:1.746	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.43	Hits@10:46.36	Best:25.84
2025-01-07 15:57:40,905: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.84
2025-01-07 15:57:40,906: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.416 MRR:25.34 Best Results: 25.84
Token added to optimizer, embeddings excluded successfully.
2025-01-07 15:57:40,906: Snapshot:0	Epoch:6	Loss:1.416	translation_Loss:1.416	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.34	Hits@10:46.15	Best:25.84
2025-01-07 15:58:00,452: Snapshot:0	Epoch:7	Loss:46.166	translation_Loss:36.153	token_training_loss:10.013	distillation_Loss:0.0                                                   	MRR:25.34	Hits@10:46.15	Best:25.84
2025-01-07 15:58:19,488: End of token training: 0 Epoch: 8 Loss:36.164 MRR:25.34 Best Results: 25.84
2025-01-07 15:58:19,489: Snapshot:0	Epoch:8	Loss:36.164	translation_Loss:36.162	token_training_loss:0.002	distillation_Loss:0.0                                                           	MRR:25.34	Hits@10:46.15	Best:25.84
2025-01-07 15:58:19,787: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 15:58:26,257: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 | 0.1494 | 0.3036 | 0.3759 |  0.467  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,596,600
Trainable params: 800
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 15:58:36,580: Snapshot:1	Epoch:0	Loss:6.422	translation_Loss:6.18	token_training_loss:0.0	distillation_Loss:0.242                                                   	MRR:10.3	Hits@10:19.96	Best:10.3
2025-01-07 15:58:39,998: Snapshot:1	Epoch:1	Loss:3.238	translation_Loss:2.836	token_training_loss:0.0	distillation_Loss:0.402                                                   	MRR:13.06	Hits@10:24.49	Best:13.06
2025-01-07 15:58:43,405: Snapshot:1	Epoch:2	Loss:1.706	translation_Loss:1.328	token_training_loss:0.0	distillation_Loss:0.378                                                   	MRR:14.72	Hits@10:27.58	Best:14.72
2025-01-07 15:58:47,309: Snapshot:1	Epoch:3	Loss:1.017	translation_Loss:0.742	token_training_loss:0.0	distillation_Loss:0.275                                                   	MRR:15.83	Hits@10:29.33	Best:15.83
2025-01-07 15:58:50,962: Snapshot:1	Epoch:4	Loss:0.702	translation_Loss:0.506	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:16.45	Hits@10:30.72	Best:16.45
2025-01-07 15:58:54,435: Snapshot:1	Epoch:5	Loss:0.539	translation_Loss:0.391	token_training_loss:0.0	distillation_Loss:0.148                                                   	MRR:16.81	Hits@10:31.51	Best:16.81
2025-01-07 15:58:57,858: Snapshot:1	Epoch:6	Loss:0.447	translation_Loss:0.324	token_training_loss:0.0	distillation_Loss:0.123                                                   	MRR:17.06	Hits@10:32.06	Best:17.06
2025-01-07 15:59:01,281: Snapshot:1	Epoch:7	Loss:0.393	translation_Loss:0.284	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:17.23	Hits@10:32.44	Best:17.23
2025-01-07 15:59:05,192: Snapshot:1	Epoch:8	Loss:0.363	translation_Loss:0.263	token_training_loss:0.0	distillation_Loss:0.1                                                   	MRR:17.54	Hits@10:32.84	Best:17.54
2025-01-07 15:59:08,614: Snapshot:1	Epoch:9	Loss:0.336	translation_Loss:0.241	token_training_loss:0.0	distillation_Loss:0.095                                                   	MRR:17.66	Hits@10:32.99	Best:17.66
2025-01-07 15:59:12,031: Snapshot:1	Epoch:10	Loss:0.311	translation_Loss:0.221	token_training_loss:0.0	distillation_Loss:0.091                                                   	MRR:17.77	Hits@10:33.24	Best:17.77
2025-01-07 15:59:15,440: Snapshot:1	Epoch:11	Loss:0.297	translation_Loss:0.21	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:17.89	Hits@10:33.52	Best:17.89
2025-01-07 15:59:18,845: Snapshot:1	Epoch:12	Loss:0.281	translation_Loss:0.196	token_training_loss:0.0	distillation_Loss:0.084                                                   	MRR:17.94	Hits@10:33.52	Best:17.94
2025-01-07 15:59:22,640: Snapshot:1	Epoch:13	Loss:0.275	translation_Loss:0.193	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:17.87	Hits@10:33.43	Best:17.94
2025-01-07 15:59:26,039: Snapshot:1	Epoch:14	Loss:0.268	translation_Loss:0.187	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:18.0	Hits@10:33.55	Best:18.0
2025-01-07 15:59:29,443: Snapshot:1	Epoch:15	Loss:0.254	translation_Loss:0.174	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:17.99	Hits@10:33.47	Best:18.0
2025-01-07 15:59:32,816: Snapshot:1	Epoch:16	Loss:0.246	translation_Loss:0.17	token_training_loss:0.0	distillation_Loss:0.076                                                   	MRR:18.0	Hits@10:33.62	Best:18.0
2025-01-07 15:59:36,222: Snapshot:1	Epoch:17	Loss:0.24	translation_Loss:0.164	token_training_loss:0.0	distillation_Loss:0.075                                                   	MRR:18.15	Hits@10:34.0	Best:18.15
2025-01-07 15:59:40,106: Snapshot:1	Epoch:18	Loss:0.233	translation_Loss:0.159	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:18.24	Hits@10:34.16	Best:18.24
2025-01-07 15:59:43,507: Snapshot:1	Epoch:19	Loss:0.23	translation_Loss:0.157	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:18.25	Hits@10:34.2	Best:18.25
2025-01-07 15:59:46,877: Snapshot:1	Epoch:20	Loss:0.223	translation_Loss:0.15	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:18.14	Hits@10:34.01	Best:18.25
2025-01-07 15:59:50,253: Snapshot:1	Epoch:21	Loss:0.226	translation_Loss:0.151	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:18.16	Hits@10:34.01	Best:18.25
2025-01-07 15:59:53,617: Early Stopping! Snapshot: 1 Epoch: 22 Best Results: 18.25
2025-01-07 15:59:53,617: Start to training tokens! Snapshot: 1 Epoch: 22 Loss:0.219 MRR:18.15 Best Results: 18.25
Token added to optimizer, embeddings excluded successfully.
2025-01-07 15:59:53,617: Snapshot:1	Epoch:22	Loss:0.219	translation_Loss:0.147	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:18.15	Hits@10:34.17	Best:18.25
2025-01-07 15:59:57,424: Snapshot:1	Epoch:23	Loss:15.857	translation_Loss:6.139	token_training_loss:9.718	distillation_Loss:0.0                                                   	MRR:18.15	Hits@10:34.17	Best:18.25
2025-01-07 16:00:00,806: End of token training: 1 Epoch: 24 Loss:6.706 MRR:18.15 Best Results: 18.25
2025-01-07 16:00:00,806: Snapshot:1	Epoch:24	Loss:6.706	translation_Loss:6.14	token_training_loss:0.566	distillation_Loss:0.0                                                           	MRR:18.15	Hits@10:34.17	Best:18.25
2025-01-07 16:00:01,117: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:00:09,517: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2547 | 0.1473 | 0.3008 | 0.3722 |  0.4624 |
|     1      | 0.1892 | 0.1107 | 0.2131 | 0.2659 |  0.3472 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,347,200
Trainable params: 800
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:00:19,753: Snapshot:2	Epoch:0	Loss:4.323	translation_Loss:4.061	token_training_loss:0.0	distillation_Loss:0.263                                                   	MRR:18.5	Hits@10:33.51	Best:18.5
2025-01-07 16:00:23,330: Snapshot:2	Epoch:1	Loss:2.15	translation_Loss:1.702	token_training_loss:0.0	distillation_Loss:0.448                                                   	MRR:19.8	Hits@10:35.19	Best:19.8
2025-01-07 16:00:27,351: Snapshot:2	Epoch:2	Loss:1.275	translation_Loss:0.81	token_training_loss:0.0	distillation_Loss:0.465                                                   	MRR:20.28	Hits@10:35.69	Best:20.28
2025-01-07 16:00:30,951: Snapshot:2	Epoch:3	Loss:0.923	translation_Loss:0.538	token_training_loss:0.0	distillation_Loss:0.385                                                   	MRR:20.37	Hits@10:36.01	Best:20.37
2025-01-07 16:00:34,474: Snapshot:2	Epoch:4	Loss:0.773	translation_Loss:0.44	token_training_loss:0.0	distillation_Loss:0.333                                                   	MRR:20.33	Hits@10:36.25	Best:20.37
2025-01-07 16:00:38,056: Snapshot:2	Epoch:5	Loss:0.702	translation_Loss:0.397	token_training_loss:0.0	distillation_Loss:0.306                                                   	MRR:20.54	Hits@10:36.7	Best:20.54
2025-01-07 16:00:41,598: Snapshot:2	Epoch:6	Loss:0.663	translation_Loss:0.372	token_training_loss:0.0	distillation_Loss:0.292                                                   	MRR:20.47	Hits@10:36.58	Best:20.54
2025-01-07 16:00:45,574: Snapshot:2	Epoch:7	Loss:0.64	translation_Loss:0.352	token_training_loss:0.0	distillation_Loss:0.288                                                   	MRR:20.44	Hits@10:36.54	Best:20.54
2025-01-07 16:00:49,107: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 20.54
2025-01-07 16:00:49,107: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:0.626 MRR:20.19 Best Results: 20.54
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:00:49,107: Snapshot:2	Epoch:8	Loss:0.626	translation_Loss:0.339	token_training_loss:0.0	distillation_Loss:0.286                                                   	MRR:20.19	Hits@10:36.49	Best:20.54
2025-01-07 16:00:52,636: Snapshot:2	Epoch:9	Loss:16.126	translation_Loss:6.263	token_training_loss:9.863	distillation_Loss:0.0                                                   	MRR:20.19	Hits@10:36.49	Best:20.54
2025-01-07 16:00:56,223: End of token training: 2 Epoch: 10 Loss:6.905 MRR:20.19 Best Results: 20.54
2025-01-07 16:00:56,223: Snapshot:2	Epoch:10	Loss:6.905	translation_Loss:6.271	token_training_loss:0.634	distillation_Loss:0.0                                                           	MRR:20.19	Hits@10:36.49	Best:20.54
2025-01-07 16:00:56,471: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:01:06,986: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2495 | 0.1432 | 0.2926 | 0.3646 |  0.4562 |
|     1      | 0.1939 | 0.1103 | 0.2202 | 0.2763 |  0.3596 |
|     2      | 0.205  | 0.1233 | 0.2336 | 0.291  |  0.3644 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,722,400
Trainable params: 800
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:01:17,486: Snapshot:3	Epoch:0	Loss:2.759	translation_Loss:2.511	token_training_loss:0.0	distillation_Loss:0.248                                                   	MRR:21.08	Hits@10:37.22	Best:21.08
2025-01-07 16:01:21,187: Snapshot:3	Epoch:1	Loss:1.526	translation_Loss:1.078	token_training_loss:0.0	distillation_Loss:0.448                                                   	MRR:21.77	Hits@10:38.13	Best:21.77
2025-01-07 16:01:24,874: Snapshot:3	Epoch:2	Loss:1.029	translation_Loss:0.564	token_training_loss:0.0	distillation_Loss:0.465                                                   	MRR:21.91	Hits@10:38.54	Best:21.91
2025-01-07 16:01:29,010: Snapshot:3	Epoch:3	Loss:0.851	translation_Loss:0.439	token_training_loss:0.0	distillation_Loss:0.412                                                   	MRR:22.07	Hits@10:38.51	Best:22.07
2025-01-07 16:01:32,635: Snapshot:3	Epoch:4	Loss:0.767	translation_Loss:0.377	token_training_loss:0.0	distillation_Loss:0.39                                                   	MRR:21.99	Hits@10:38.33	Best:22.07
2025-01-07 16:01:36,257: Snapshot:3	Epoch:5	Loss:0.729	translation_Loss:0.351	token_training_loss:0.0	distillation_Loss:0.378                                                   	MRR:21.81	Hits@10:38.57	Best:22.07
2025-01-07 16:01:39,910: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 22.07
2025-01-07 16:01:39,910: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:0.707 MRR:21.76 Best Results: 22.07
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:01:39,910: Snapshot:3	Epoch:6	Loss:0.707	translation_Loss:0.337	token_training_loss:0.0	distillation_Loss:0.37                                                   	MRR:21.76	Hits@10:38.33	Best:22.07
2025-01-07 16:01:43,544: Snapshot:3	Epoch:7	Loss:15.413	translation_Loss:6.278	token_training_loss:9.135	distillation_Loss:0.0                                                   	MRR:21.76	Hits@10:38.33	Best:22.07
2025-01-07 16:01:47,604: End of token training: 3 Epoch: 8 Loss:6.86 MRR:21.76 Best Results: 22.07
2025-01-07 16:01:47,604: Snapshot:3	Epoch:8	Loss:6.86	translation_Loss:6.262	token_training_loss:0.598	distillation_Loss:0.0                                                           	MRR:21.76	Hits@10:38.33	Best:22.07
2025-01-07 16:01:47,851: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 16:01:59,779: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2424 | 0.1379 | 0.2835 | 0.3547 |  0.4453 |
|     1      | 0.1987 | 0.1112 | 0.2273 | 0.2853 |  0.3723 |
|     2      | 0.203  | 0.118  | 0.2332 | 0.2888 |  0.3709 |
|     3      | 0.2197 | 0.1331 | 0.2542 | 0.3108 |  0.3866 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,910,000
Trainable params: 800
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:02:10,655: Snapshot:4	Epoch:0	Loss:1.783	translation_Loss:1.475	token_training_loss:0.0	distillation_Loss:0.307                                                   	MRR:23.45	Hits@10:39.96	Best:23.45
2025-01-07 16:02:14,387: Snapshot:4	Epoch:1	Loss:1.212	translation_Loss:0.707	token_training_loss:0.0	distillation_Loss:0.504                                                   	MRR:23.58	Hits@10:40.08	Best:23.58
2025-01-07 16:02:18,112: Snapshot:4	Epoch:2	Loss:1.044	translation_Loss:0.601	token_training_loss:0.0	distillation_Loss:0.443                                                   	MRR:23.7	Hits@10:39.95	Best:23.7
2025-01-07 16:02:22,325: Snapshot:4	Epoch:3	Loss:0.965	translation_Loss:0.547	token_training_loss:0.0	distillation_Loss:0.418                                                   	MRR:23.75	Hits@10:40.27	Best:23.75
2025-01-07 16:02:26,005: Snapshot:4	Epoch:4	Loss:0.92	translation_Loss:0.51	token_training_loss:0.0	distillation_Loss:0.41                                                   	MRR:23.64	Hits@10:40.27	Best:23.75
2025-01-07 16:02:29,678: Snapshot:4	Epoch:5	Loss:0.915	translation_Loss:0.515	token_training_loss:0.0	distillation_Loss:0.399                                                   	MRR:23.6	Hits@10:40.2	Best:23.75
2025-01-07 16:02:33,401: Snapshot:4	Epoch:6	Loss:0.907	translation_Loss:0.505	token_training_loss:0.0	distillation_Loss:0.402                                                   	MRR:23.76	Hits@10:40.13	Best:23.76
2025-01-07 16:02:37,062: Snapshot:4	Epoch:7	Loss:0.905	translation_Loss:0.496	token_training_loss:0.0	distillation_Loss:0.409                                                   	MRR:23.74	Hits@10:40.29	Best:23.76
2025-01-07 16:02:41,190: Snapshot:4	Epoch:8	Loss:0.914	translation_Loss:0.506	token_training_loss:0.0	distillation_Loss:0.407                                                   	MRR:23.66	Hits@10:40.24	Best:23.76
2025-01-07 16:02:44,864: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 23.76
2025-01-07 16:02:44,864: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:0.908 MRR:23.71 Best Results: 23.76
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:02:44,864: Snapshot:4	Epoch:9	Loss:0.908	translation_Loss:0.502	token_training_loss:0.0	distillation_Loss:0.406                                                   	MRR:23.71	Hits@10:40.18	Best:23.76
2025-01-07 16:02:48,529: Snapshot:4	Epoch:10	Loss:16.125	translation_Loss:6.416	token_training_loss:9.709	distillation_Loss:0.0                                                   	MRR:23.71	Hits@10:40.18	Best:23.76
2025-01-07 16:02:52,197: End of token training: 4 Epoch: 11 Loss:6.97 MRR:23.71 Best Results: 23.76
2025-01-07 16:02:52,198: Snapshot:4	Epoch:11	Loss:6.97	translation_Loss:6.415	token_training_loss:0.554	distillation_Loss:0.0                                                           	MRR:23.71	Hits@10:40.18	Best:23.76
2025-01-07 16:02:52,509: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 16:03:07,225: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2399 | 0.1352 | 0.2805 | 0.3535 |  0.4443 |
|     1      | 0.2002 | 0.1112 | 0.2267 | 0.2929 |  0.3801 |
|     2      | 0.2077 | 0.1213 | 0.2363 | 0.2966 |  0.3793 |
|     3      | 0.2167 | 0.1254 | 0.2536 | 0.314  |  0.3923 |
|     4      | 0.2309 | 0.1376 | 0.2694 | 0.3312 |  0.4095 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   800
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,003,800
Trainable params: 800
Non-trainable params: 3,003,000
=================================================================
2025-01-07 16:03:07,228: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 | 0.1494 | 0.3036 | 0.3759 |  0.467  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2547 | 0.1473 | 0.3008 | 0.3722 |  0.4624 |
|     1      | 0.1892 | 0.1107 | 0.2131 | 0.2659 |  0.3472 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2495 | 0.1432 | 0.2926 | 0.3646 |  0.4562 |
|     1      | 0.1939 | 0.1103 | 0.2202 | 0.2763 |  0.3596 |
|     2      | 0.205  | 0.1233 | 0.2336 | 0.291  |  0.3644 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2424 | 0.1379 | 0.2835 | 0.3547 |  0.4453 |
|     1      | 0.1987 | 0.1112 | 0.2273 | 0.2853 |  0.3723 |
|     2      | 0.203  | 0.118  | 0.2332 | 0.2888 |  0.3709 |
|     3      | 0.2197 | 0.1331 | 0.2542 | 0.3108 |  0.3866 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2399 | 0.1352 | 0.2805 | 0.3535 |  0.4443 |
|     1      | 0.2002 | 0.1112 | 0.2267 | 0.2929 |  0.3801 |
|     2      | 0.2077 | 0.1213 | 0.2363 | 0.2966 |  0.3793 |
|     3      | 0.2167 | 0.1254 | 0.2536 | 0.314  |  0.3923 |
|     4      | 0.2309 | 0.1376 | 0.2694 | 0.3312 |  0.4095 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 16:03:07,228: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 176.88036584854126 |   0.257   |    0.149     |    0.304     |     0.467     |
|    1     |  92.3832643032074  |   0.245   |    0.142     |    0.288     |     0.446     |
|    2     | 44.728440284729004 |   0.237   |    0.137     |    0.276     |     0.433     |
|    3     | 38.495222091674805 |   0.231   |    0.132     |    0.268     |     0.422     |
|    4     | 49.970916748046875 |   0.229   |    0.131     |    0.267     |     0.423     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 16:03:07,228: Sum_Training_Time:402.45820927619934
2025-01-07 16:03:07,228: Every_Training_Time:[176.88036584854126, 92.3832643032074, 44.728440284729004, 38.495222091674805, 49.970916748046875]
2025-01-07 16:03:07,228: Forward transfer: 0.163725 Backward transfer: -0.001674999999999996
2025-01-07 16:03:30,851: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107160311/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=4, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 16:03:55,050: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.2	Hits@10:34.64	Best:15.2
2025-01-07 16:04:14,387: Snapshot:0	Epoch:1	Loss:18.581	translation_Loss:18.581	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.75	Hits@10:45.24	Best:23.75
2025-01-07 16:04:33,794: Snapshot:0	Epoch:2	Loss:7.139	translation_Loss:7.139	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.51	Hits@10:46.8	Best:25.51
2025-01-07 16:04:53,159: Snapshot:0	Epoch:3	Loss:3.626	translation_Loss:3.626	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.82	Hits@10:46.94	Best:25.82
2025-01-07 16:05:12,882: Snapshot:0	Epoch:4	Loss:2.338	translation_Loss:2.338	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.73	Hits@10:46.49	Best:25.82
2025-01-07 16:05:32,238: Snapshot:0	Epoch:5	Loss:1.745	translation_Loss:1.745	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.53	Hits@10:46.43	Best:25.82
2025-01-07 16:05:51,640: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.82
2025-01-07 16:05:51,641: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.416 MRR:25.37 Best Results: 25.82
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:05:51,641: Snapshot:0	Epoch:6	Loss:1.416	translation_Loss:1.416	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.37	Hits@10:46.3	Best:25.82
2025-01-07 16:06:11,533: Snapshot:0	Epoch:7	Loss:50.23	translation_Loss:36.114	token_training_loss:14.117	distillation_Loss:0.0                                                   	MRR:25.37	Hits@10:46.3	Best:25.82
2025-01-07 16:06:31,037: End of token training: 0 Epoch: 8 Loss:36.125 MRR:25.37 Best Results: 25.82
2025-01-07 16:06:31,037: Snapshot:0	Epoch:8	Loss:36.125	translation_Loss:36.122	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.37	Hits@10:46.3	Best:25.82
2025-01-07 16:06:31,349: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 16:06:37,833: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1496 | 0.3034 | 0.3761 |  0.4676 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,597,400
Trainable params: 1,600
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:06:48,193: Snapshot:1	Epoch:0	Loss:6.496	translation_Loss:6.201	token_training_loss:0.0	distillation_Loss:0.295                                                   	MRR:10.17	Hits@10:19.86	Best:10.17
2025-01-07 16:06:51,648: Snapshot:1	Epoch:1	Loss:3.332	translation_Loss:2.944	token_training_loss:0.0	distillation_Loss:0.388                                                   	MRR:13.02	Hits@10:24.19	Best:13.02
2025-01-07 16:06:55,083: Snapshot:1	Epoch:2	Loss:1.749	translation_Loss:1.442	token_training_loss:0.0	distillation_Loss:0.307                                                   	MRR:14.76	Hits@10:27.25	Best:14.76
2025-01-07 16:06:59,030: Snapshot:1	Epoch:3	Loss:1.037	translation_Loss:0.823	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:15.81	Hits@10:29.53	Best:15.81
2025-01-07 16:07:02,486: Snapshot:1	Epoch:4	Loss:0.717	translation_Loss:0.567	token_training_loss:0.0	distillation_Loss:0.15                                                   	MRR:16.5	Hits@10:30.95	Best:16.5
2025-01-07 16:07:06,450: Snapshot:1	Epoch:5	Loss:0.551	translation_Loss:0.436	token_training_loss:0.0	distillation_Loss:0.115                                                   	MRR:16.98	Hits@10:31.72	Best:16.98
2025-01-07 16:07:09,969: Snapshot:1	Epoch:6	Loss:0.458	translation_Loss:0.36	token_training_loss:0.0	distillation_Loss:0.098                                                   	MRR:17.14	Hits@10:32.31	Best:17.14
2025-01-07 16:07:13,434: Snapshot:1	Epoch:7	Loss:0.402	translation_Loss:0.314	token_training_loss:0.0	distillation_Loss:0.088                                                   	MRR:17.25	Hits@10:32.84	Best:17.25
2025-01-07 16:07:17,360: Snapshot:1	Epoch:8	Loss:0.375	translation_Loss:0.29	token_training_loss:0.0	distillation_Loss:0.084                                                   	MRR:17.54	Hits@10:33.19	Best:17.54
2025-01-07 16:07:20,838: Snapshot:1	Epoch:9	Loss:0.349	translation_Loss:0.268	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:17.68	Hits@10:33.3	Best:17.68
2025-01-07 16:07:24,268: Snapshot:1	Epoch:10	Loss:0.322	translation_Loss:0.245	token_training_loss:0.0	distillation_Loss:0.078                                                   	MRR:17.61	Hits@10:33.3	Best:17.68
2025-01-07 16:07:27,752: Snapshot:1	Epoch:11	Loss:0.308	translation_Loss:0.233	token_training_loss:0.0	distillation_Loss:0.075                                                   	MRR:17.79	Hits@10:33.63	Best:17.79
2025-01-07 16:07:31,216: Snapshot:1	Epoch:12	Loss:0.289	translation_Loss:0.217	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:17.86	Hits@10:33.76	Best:17.86
2025-01-07 16:07:35,074: Snapshot:1	Epoch:13	Loss:0.288	translation_Loss:0.215	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:17.84	Hits@10:33.47	Best:17.86
2025-01-07 16:07:38,581: Snapshot:1	Epoch:14	Loss:0.279	translation_Loss:0.207	token_training_loss:0.0	distillation_Loss:0.072                                                   	MRR:17.89	Hits@10:33.63	Best:17.89
2025-01-07 16:07:42,014: Snapshot:1	Epoch:15	Loss:0.265	translation_Loss:0.194	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:17.82	Hits@10:33.55	Best:17.89
2025-01-07 16:07:45,473: Snapshot:1	Epoch:16	Loss:0.26	translation_Loss:0.19	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:17.97	Hits@10:33.64	Best:17.97
2025-01-07 16:07:48,936: Snapshot:1	Epoch:17	Loss:0.25	translation_Loss:0.181	token_training_loss:0.0	distillation_Loss:0.069                                                   	MRR:18.03	Hits@10:34.05	Best:18.03
2025-01-07 16:07:52,800: Snapshot:1	Epoch:18	Loss:0.244	translation_Loss:0.177	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.92	Hits@10:34.09	Best:18.03
2025-01-07 16:07:56,226: Snapshot:1	Epoch:19	Loss:0.241	translation_Loss:0.175	token_training_loss:0.0	distillation_Loss:0.066                                                   	MRR:17.93	Hits@10:34.03	Best:18.03
2025-01-07 16:07:59,658: Early Stopping! Snapshot: 1 Epoch: 20 Best Results: 18.03
2025-01-07 16:07:59,658: Start to training tokens! Snapshot: 1 Epoch: 20 Loss:0.235 MRR:17.93 Best Results: 18.03
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:07:59,658: Snapshot:1	Epoch:20	Loss:0.235	translation_Loss:0.167	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.93	Hits@10:33.92	Best:18.03
2025-01-07 16:08:03,116: Snapshot:1	Epoch:21	Loss:19.596	translation_Loss:6.171	token_training_loss:13.424	distillation_Loss:0.0                                                   	MRR:17.93	Hits@10:33.92	Best:18.03
2025-01-07 16:08:06,592: End of token training: 1 Epoch: 22 Loss:7.301 MRR:17.93 Best Results: 18.03
2025-01-07 16:08:06,592: Snapshot:1	Epoch:22	Loss:7.301	translation_Loss:6.174	token_training_loss:1.127	distillation_Loss:0.0                                                           	MRR:17.93	Hits@10:33.92	Best:18.03
2025-01-07 16:08:06,924: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:08:15,207: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1479 | 0.3025 | 0.3745 |  0.4664 |
|     1      | 0.1879 | 0.1087 | 0.2101 | 0.266  |  0.3477 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,348,000
Trainable params: 1,600
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:08:25,923: Snapshot:2	Epoch:0	Loss:4.394	translation_Loss:4.07	token_training_loss:0.0	distillation_Loss:0.324                                                   	MRR:18.39	Hits@10:33.81	Best:18.39
2025-01-07 16:08:29,691: Snapshot:2	Epoch:1	Loss:2.298	translation_Loss:1.798	token_training_loss:0.0	distillation_Loss:0.5                                                   	MRR:19.53	Hits@10:35.07	Best:19.53
2025-01-07 16:08:33,407: Snapshot:2	Epoch:2	Loss:1.435	translation_Loss:1.003	token_training_loss:0.0	distillation_Loss:0.431                                                   	MRR:19.76	Hits@10:35.57	Best:19.76
2025-01-07 16:08:37,532: Snapshot:2	Epoch:3	Loss:1.046	translation_Loss:0.697	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:20.05	Hits@10:35.57	Best:20.05
2025-01-07 16:08:41,251: Snapshot:2	Epoch:4	Loss:0.91	translation_Loss:0.608	token_training_loss:0.0	distillation_Loss:0.302                                                   	MRR:20.11	Hits@10:36.0	Best:20.11
2025-01-07 16:08:44,902: Snapshot:2	Epoch:5	Loss:0.833	translation_Loss:0.548	token_training_loss:0.0	distillation_Loss:0.286                                                   	MRR:19.97	Hits@10:36.37	Best:20.11
2025-01-07 16:08:48,596: Snapshot:2	Epoch:6	Loss:0.793	translation_Loss:0.516	token_training_loss:0.0	distillation_Loss:0.276                                                   	MRR:20.12	Hits@10:36.49	Best:20.12
2025-01-07 16:08:52,247: Snapshot:2	Epoch:7	Loss:0.762	translation_Loss:0.491	token_training_loss:0.0	distillation_Loss:0.271                                                   	MRR:19.95	Hits@10:36.23	Best:20.12
2025-01-07 16:08:56,335: Snapshot:2	Epoch:8	Loss:0.756	translation_Loss:0.484	token_training_loss:0.0	distillation_Loss:0.272                                                   	MRR:20.04	Hits@10:36.1	Best:20.12
2025-01-07 16:09:00,024: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 20.12
2025-01-07 16:09:00,024: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:0.735 MRR:20.09 Best Results: 20.12
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:09:00,025: Snapshot:2	Epoch:9	Loss:0.735	translation_Loss:0.467	token_training_loss:0.0	distillation_Loss:0.268                                                   	MRR:20.09	Hits@10:36.23	Best:20.12
2025-01-07 16:09:03,625: Snapshot:2	Epoch:10	Loss:19.48	translation_Loss:6.354	token_training_loss:13.126	distillation_Loss:0.0                                                   	MRR:20.09	Hits@10:36.23	Best:20.12
2025-01-07 16:09:07,220: End of token training: 2 Epoch: 11 Loss:7.507 MRR:20.09 Best Results: 20.12
2025-01-07 16:09:07,221: Snapshot:2	Epoch:11	Loss:7.507	translation_Loss:6.358	token_training_loss:1.149	distillation_Loss:0.0                                                           	MRR:20.09	Hits@10:36.23	Best:20.12
2025-01-07 16:09:07,467: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:09:17,677: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2535 | 0.146  | 0.2988 | 0.3705 |  0.4632 |
|     1      | 0.1924 | 0.109  | 0.2196 | 0.2758 |  0.3611 |
|     2      | 0.2028 | 0.1214 | 0.2299 | 0.2904 |  0.361  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,723,200
Trainable params: 1,600
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:09:28,583: Snapshot:3	Epoch:0	Loss:2.83	translation_Loss:2.508	token_training_loss:0.0	distillation_Loss:0.322                                                   	MRR:20.75	Hits@10:37.29	Best:20.75
2025-01-07 16:09:32,504: Snapshot:3	Epoch:1	Loss:1.701	translation_Loss:1.172	token_training_loss:0.0	distillation_Loss:0.529                                                   	MRR:21.5	Hits@10:37.97	Best:21.5
2025-01-07 16:09:36,412: Snapshot:3	Epoch:2	Loss:1.246	translation_Loss:0.775	token_training_loss:0.0	distillation_Loss:0.471                                                   	MRR:21.61	Hits@10:38.19	Best:21.61
2025-01-07 16:09:40,777: Snapshot:3	Epoch:3	Loss:1.043	translation_Loss:0.624	token_training_loss:0.0	distillation_Loss:0.42                                                   	MRR:21.53	Hits@10:38.12	Best:21.61
2025-01-07 16:09:44,652: Snapshot:3	Epoch:4	Loss:0.958	translation_Loss:0.565	token_training_loss:0.0	distillation_Loss:0.392                                                   	MRR:21.57	Hits@10:38.44	Best:21.61
2025-01-07 16:09:48,530: Early Stopping! Snapshot: 3 Epoch: 5 Best Results: 21.61
2025-01-07 16:09:48,530: Start to training tokens! Snapshot: 3 Epoch: 5 Loss:0.931 MRR:21.61 Best Results: 21.61
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:09:48,531: Snapshot:3	Epoch:5	Loss:0.931	translation_Loss:0.549	token_training_loss:0.0	distillation_Loss:0.382                                                   	MRR:21.61	Hits@10:38.19	Best:21.61
2025-01-07 16:09:52,232: Snapshot:3	Epoch:6	Loss:18.779	translation_Loss:6.412	token_training_loss:12.367	distillation_Loss:0.0                                                   	MRR:21.61	Hits@10:38.19	Best:21.61
2025-01-07 16:09:55,915: End of token training: 3 Epoch: 7 Loss:7.378 MRR:21.61 Best Results: 21.61
2025-01-07 16:09:55,915: Snapshot:3	Epoch:7	Loss:7.378	translation_Loss:6.402	token_training_loss:0.975	distillation_Loss:0.0                                                           	MRR:21.61	Hits@10:38.19	Best:21.61
2025-01-07 16:09:56,159: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 16:10:08,899: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2495 | 0.1426 | 0.2935 | 0.3649 |  0.4564 |
|     1      | 0.1971 | 0.1108 | 0.2251 | 0.2842 |  0.3711 |
|     2      | 0.2052 | 0.1212 | 0.2354 | 0.2933 |  0.3692 |
|     3      | 0.2164 | 0.1306 | 0.2499 | 0.3076 |  0.3799 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,910,800
Trainable params: 1,600
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:10:19,736: Snapshot:4	Epoch:0	Loss:1.931	translation_Loss:1.515	token_training_loss:0.0	distillation_Loss:0.417                                                   	MRR:23.54	Hits@10:39.61	Best:23.54
2025-01-07 16:10:23,530: Snapshot:4	Epoch:1	Loss:1.395	translation_Loss:0.884	token_training_loss:0.0	distillation_Loss:0.511                                                   	MRR:23.45	Hits@10:39.6	Best:23.54
2025-01-07 16:10:27,439: Snapshot:4	Epoch:2	Loss:1.312	translation_Loss:0.91	token_training_loss:0.0	distillation_Loss:0.401                                                   	MRR:23.55	Hits@10:39.52	Best:23.55
2025-01-07 16:10:31,372: Snapshot:4	Epoch:3	Loss:1.167	translation_Loss:0.78	token_training_loss:0.0	distillation_Loss:0.387                                                   	MRR:23.47	Hits@10:39.54	Best:23.55
2025-01-07 16:10:35,868: Snapshot:4	Epoch:4	Loss:1.148	translation_Loss:0.771	token_training_loss:0.0	distillation_Loss:0.377                                                   	MRR:23.61	Hits@10:39.8	Best:23.61
2025-01-07 16:10:39,846: Snapshot:4	Epoch:5	Loss:1.134	translation_Loss:0.758	token_training_loss:0.0	distillation_Loss:0.376                                                   	MRR:23.74	Hits@10:39.81	Best:23.74
2025-01-07 16:10:43,687: Snapshot:4	Epoch:6	Loss:1.128	translation_Loss:0.751	token_training_loss:0.0	distillation_Loss:0.377                                                   	MRR:23.59	Hits@10:39.6	Best:23.74
2025-01-07 16:10:47,541: Snapshot:4	Epoch:7	Loss:1.121	translation_Loss:0.738	token_training_loss:0.0	distillation_Loss:0.383                                                   	MRR:23.46	Hits@10:39.46	Best:23.74
2025-01-07 16:10:51,388: Early Stopping! Snapshot: 4 Epoch: 8 Best Results: 23.74
2025-01-07 16:10:51,388: Start to training tokens! Snapshot: 4 Epoch: 8 Loss:1.129 MRR:23.73 Best Results: 23.74
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:10:51,389: Snapshot:4	Epoch:8	Loss:1.129	translation_Loss:0.75	token_training_loss:0.0	distillation_Loss:0.379                                                   	MRR:23.73	Hits@10:39.85	Best:23.74
2025-01-07 16:10:55,598: Snapshot:4	Epoch:9	Loss:18.396	translation_Loss:6.596	token_training_loss:11.801	distillation_Loss:0.0                                                   	MRR:23.73	Hits@10:39.85	Best:23.74
2025-01-07 16:10:59,353: End of token training: 4 Epoch: 10 Loss:7.462 MRR:23.73 Best Results: 23.74
2025-01-07 16:10:59,353: Snapshot:4	Epoch:10	Loss:7.462	translation_Loss:6.598	token_training_loss:0.864	distillation_Loss:0.0                                                           	MRR:23.73	Hits@10:39.85	Best:23.74
2025-01-07 16:10:59,681: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 16:11:14,072: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2484 | 0.1423 | 0.2912 | 0.3628 |  0.4532 |
|     1      | 0.2001 | 0.1121 | 0.2287 | 0.2879 |  0.3734 |
|     2      | 0.2087 | 0.1235 | 0.2392 | 0.2983 |  0.3794 |
|     3      | 0.2199 | 0.1319 | 0.2579 | 0.314  |  0.3885 |
|     4      | 0.2353 | 0.1422 | 0.2743 | 0.3351 |  0.4098 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,600
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,004,600
Trainable params: 1,600
Non-trainable params: 3,003,000
=================================================================
2025-01-07 16:11:14,075: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1496 | 0.3034 | 0.3761 |  0.4676 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1479 | 0.3025 | 0.3745 |  0.4664 |
|     1      | 0.1879 | 0.1087 | 0.2101 | 0.266  |  0.3477 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2535 | 0.146  | 0.2988 | 0.3705 |  0.4632 |
|     1      | 0.1924 | 0.109  | 0.2196 | 0.2758 |  0.3611 |
|     2      | 0.2028 | 0.1214 | 0.2299 | 0.2904 |  0.361  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2495 | 0.1426 | 0.2935 | 0.3649 |  0.4564 |
|     1      | 0.1971 | 0.1108 | 0.2251 | 0.2842 |  0.3711 |
|     2      | 0.2052 | 0.1212 | 0.2354 | 0.2933 |  0.3692 |
|     3      | 0.2164 | 0.1306 | 0.2499 | 0.3076 |  0.3799 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2484 | 0.1423 | 0.2912 | 0.3628 |  0.4532 |
|     1      | 0.2001 | 0.1121 | 0.2287 | 0.2879 |  0.3734 |
|     2      | 0.2087 | 0.1235 | 0.2392 | 0.2983 |  0.3794 |
|     3      | 0.2199 | 0.1319 | 0.2579 | 0.314  |  0.3885 |
|     4      | 0.2353 | 0.1422 | 0.2743 | 0.3351 |  0.4098 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 16:11:14,076: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 180.18525171279907 |   0.258   |     0.15     |    0.303     |     0.468     |
|    1     | 86.58117890357971  |   0.246   |    0.142     |    0.289     |     0.449     |
|    2     | 49.69565463066101  |    0.24   |    0.138     |     0.28     |     0.438     |
|    3     | 35.84035515785217  |   0.235   |    0.135     |    0.275     |     0.429     |
|    4     |  48.2906060218811  |   0.235   |    0.136     |    0.275     |     0.427     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 16:11:14,076: Sum_Training_Time:400.59304642677307
2025-01-07 16:11:14,076: Every_Training_Time:[180.18525171279907, 86.58117890357971, 49.69565463066101, 35.84035515785217, 48.2906060218811]
2025-01-07 16:11:14,076: Forward transfer: 0.1637 Backward transfer: 0.003124999999999996
2025-01-07 16:11:37,779: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107161118/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 16:12:01,852: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.2	Hits@10:34.64	Best:15.2
2025-01-07 16:12:20,963: Snapshot:0	Epoch:1	Loss:18.582	translation_Loss:18.582	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.76	Hits@10:45.23	Best:23.76
2025-01-07 16:12:39,974: Snapshot:0	Epoch:2	Loss:7.138	translation_Loss:7.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.54	Hits@10:46.85	Best:25.54
2025-01-07 16:12:58,862: Snapshot:0	Epoch:3	Loss:3.625	translation_Loss:3.625	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.79	Hits@10:46.96	Best:25.79
2025-01-07 16:13:18,148: Snapshot:0	Epoch:4	Loss:2.339	translation_Loss:2.339	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.75	Hits@10:46.58	Best:25.79
2025-01-07 16:13:37,257: Snapshot:0	Epoch:5	Loss:1.743	translation_Loss:1.743	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.47	Hits@10:46.5	Best:25.79
2025-01-07 16:13:56,592: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.79
2025-01-07 16:13:56,593: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.415 MRR:25.41 Best Results: 25.79
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:13:56,593: Snapshot:0	Epoch:6	Loss:1.415	translation_Loss:1.415	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.41	Hits@10:46.22	Best:25.79
2025-01-07 16:14:16,332: Snapshot:0	Epoch:7	Loss:51.572	translation_Loss:36.157	token_training_loss:15.415	distillation_Loss:0.0                                                   	MRR:25.41	Hits@10:46.22	Best:25.79
2025-01-07 16:14:35,170: End of token training: 0 Epoch: 8 Loss:36.167 MRR:25.41 Best Results: 25.79
2025-01-07 16:14:35,170: Snapshot:0	Epoch:8	Loss:36.167	translation_Loss:36.164	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.41	Hits@10:46.22	Best:25.79
2025-01-07 16:14:35,478: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 16:14:41,982: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1499 | 0.3032 | 0.3756 |  0.467  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,597,800
Trainable params: 2,000
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:14:52,433: Snapshot:1	Epoch:0	Loss:6.525	translation_Loss:6.211	token_training_loss:0.0	distillation_Loss:0.315                                                   	MRR:10.15	Hits@10:19.87	Best:10.15
2025-01-07 16:14:55,955: Snapshot:1	Epoch:1	Loss:3.361	translation_Loss:2.99	token_training_loss:0.0	distillation_Loss:0.371                                                   	MRR:13.05	Hits@10:24.22	Best:13.05
2025-01-07 16:14:59,510: Snapshot:1	Epoch:2	Loss:1.757	translation_Loss:1.473	token_training_loss:0.0	distillation_Loss:0.284                                                   	MRR:14.8	Hits@10:27.19	Best:14.8
2025-01-07 16:15:03,612: Snapshot:1	Epoch:3	Loss:1.04	translation_Loss:0.843	token_training_loss:0.0	distillation_Loss:0.197                                                   	MRR:15.85	Hits@10:29.26	Best:15.85
2025-01-07 16:15:07,278: Snapshot:1	Epoch:4	Loss:0.719	translation_Loss:0.581	token_training_loss:0.0	distillation_Loss:0.137                                                   	MRR:16.54	Hits@10:30.78	Best:16.54
2025-01-07 16:15:10,907: Snapshot:1	Epoch:5	Loss:0.551	translation_Loss:0.445	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:16.96	Hits@10:31.61	Best:16.96
2025-01-07 16:15:14,559: Snapshot:1	Epoch:6	Loss:0.46	translation_Loss:0.369	token_training_loss:0.0	distillation_Loss:0.091                                                   	MRR:17.31	Hits@10:32.24	Best:17.31
2025-01-07 16:15:18,203: Snapshot:1	Epoch:7	Loss:0.406	translation_Loss:0.322	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:17.43	Hits@10:32.62	Best:17.43
2025-01-07 16:15:22,334: Snapshot:1	Epoch:8	Loss:0.377	translation_Loss:0.298	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:17.62	Hits@10:33.02	Best:17.62
2025-01-07 16:15:25,975: Snapshot:1	Epoch:9	Loss:0.353	translation_Loss:0.275	token_training_loss:0.0	distillation_Loss:0.078                                                   	MRR:17.76	Hits@10:33.26	Best:17.76
2025-01-07 16:15:29,682: Snapshot:1	Epoch:10	Loss:0.327	translation_Loss:0.251	token_training_loss:0.0	distillation_Loss:0.075                                                   	MRR:17.77	Hits@10:33.54	Best:17.77
2025-01-07 16:15:33,261: Snapshot:1	Epoch:11	Loss:0.31	translation_Loss:0.238	token_training_loss:0.0	distillation_Loss:0.072                                                   	MRR:17.9	Hits@10:33.53	Best:17.9
2025-01-07 16:15:36,754: Snapshot:1	Epoch:12	Loss:0.292	translation_Loss:0.222	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:17.86	Hits@10:33.61	Best:17.9
2025-01-07 16:15:40,793: Snapshot:1	Epoch:13	Loss:0.288	translation_Loss:0.219	token_training_loss:0.0	distillation_Loss:0.069                                                   	MRR:17.9	Hits@10:33.34	Best:17.9
2025-01-07 16:15:44,364: Snapshot:1	Epoch:14	Loss:0.28	translation_Loss:0.211	token_training_loss:0.0	distillation_Loss:0.069                                                   	MRR:18.01	Hits@10:33.47	Best:18.01
2025-01-07 16:15:47,974: Snapshot:1	Epoch:15	Loss:0.267	translation_Loss:0.199	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:17.89	Hits@10:33.89	Best:18.01
2025-01-07 16:15:51,506: Snapshot:1	Epoch:16	Loss:0.261	translation_Loss:0.193	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:18.01	Hits@10:33.83	Best:18.01
2025-01-07 16:15:55,024: Snapshot:1	Epoch:17	Loss:0.253	translation_Loss:0.185	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:18.15	Hits@10:34.08	Best:18.15
2025-01-07 16:15:58,942: Snapshot:1	Epoch:18	Loss:0.247	translation_Loss:0.182	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:18.13	Hits@10:33.94	Best:18.15
2025-01-07 16:16:02,459: Snapshot:1	Epoch:19	Loss:0.243	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:18.22	Hits@10:33.93	Best:18.22
2025-01-07 16:16:05,928: Snapshot:1	Epoch:20	Loss:0.237	translation_Loss:0.171	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:18.21	Hits@10:34.2	Best:18.22
2025-01-07 16:16:09,384: Snapshot:1	Epoch:21	Loss:0.238	translation_Loss:0.173	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:18.13	Hits@10:34.25	Best:18.22
2025-01-07 16:16:12,878: Early Stopping! Snapshot: 1 Epoch: 22 Best Results: 18.22
2025-01-07 16:16:12,878: Start to training tokens! Snapshot: 1 Epoch: 22 Loss:0.231 MRR:18.17 Best Results: 18.22
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:16:12,878: Snapshot:1	Epoch:22	Loss:0.231	translation_Loss:0.166	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:18.17	Hits@10:34.28	Best:18.22
2025-01-07 16:16:16,751: Snapshot:1	Epoch:23	Loss:19.705	translation_Loss:6.178	token_training_loss:13.527	distillation_Loss:0.0                                                   	MRR:18.17	Hits@10:34.28	Best:18.22
2025-01-07 16:16:20,171: End of token training: 1 Epoch: 24 Loss:7.542 MRR:18.17 Best Results: 18.22
2025-01-07 16:16:20,171: Snapshot:1	Epoch:24	Loss:7.542	translation_Loss:6.179	token_training_loss:1.363	distillation_Loss:0.0                                                           	MRR:18.17	Hits@10:34.28	Best:18.22
2025-01-07 16:16:20,501: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:16:28,784: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2556 | 0.1478 | 0.301  | 0.3748 |  0.4651 |
|     1      | 0.1881 | 0.1099 | 0.2091 | 0.2643 |  0.3468 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,348,400
Trainable params: 2,000
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:16:39,135: Snapshot:2	Epoch:0	Loss:4.406	translation_Loss:4.061	token_training_loss:0.0	distillation_Loss:0.346                                                   	MRR:18.42	Hits@10:33.77	Best:18.42
2025-01-07 16:16:42,850: Snapshot:2	Epoch:1	Loss:2.345	translation_Loss:1.848	token_training_loss:0.0	distillation_Loss:0.497                                                   	MRR:19.35	Hits@10:35.1	Best:19.35
2025-01-07 16:16:47,069: Snapshot:2	Epoch:2	Loss:1.469	translation_Loss:1.066	token_training_loss:0.0	distillation_Loss:0.404                                                   	MRR:19.7	Hits@10:35.37	Best:19.7
2025-01-07 16:16:50,851: Snapshot:2	Epoch:3	Loss:1.08	translation_Loss:0.753	token_training_loss:0.0	distillation_Loss:0.327                                                   	MRR:19.96	Hits@10:35.87	Best:19.96
2025-01-07 16:16:54,528: Snapshot:2	Epoch:4	Loss:0.939	translation_Loss:0.652	token_training_loss:0.0	distillation_Loss:0.287                                                   	MRR:19.92	Hits@10:36.17	Best:19.96
2025-01-07 16:16:58,315: Snapshot:2	Epoch:5	Loss:0.87	translation_Loss:0.599	token_training_loss:0.0	distillation_Loss:0.271                                                   	MRR:20.07	Hits@10:36.26	Best:20.07
2025-01-07 16:17:02,142: Snapshot:2	Epoch:6	Loss:0.827	translation_Loss:0.563	token_training_loss:0.0	distillation_Loss:0.264                                                   	MRR:20.1	Hits@10:35.94	Best:20.1
2025-01-07 16:17:06,330: Snapshot:2	Epoch:7	Loss:0.804	translation_Loss:0.542	token_training_loss:0.0	distillation_Loss:0.262                                                   	MRR:20.16	Hits@10:36.34	Best:20.16
2025-01-07 16:17:10,049: Snapshot:2	Epoch:8	Loss:0.786	translation_Loss:0.527	token_training_loss:0.0	distillation_Loss:0.259                                                   	MRR:20.03	Hits@10:36.19	Best:20.16
2025-01-07 16:17:13,721: Snapshot:2	Epoch:9	Loss:0.778	translation_Loss:0.516	token_training_loss:0.0	distillation_Loss:0.262                                                   	MRR:19.99	Hits@10:36.03	Best:20.16
2025-01-07 16:17:17,385: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 20.16
2025-01-07 16:17:17,385: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.769 MRR:19.88 Best Results: 20.16
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:17:17,385: Snapshot:2	Epoch:10	Loss:0.769	translation_Loss:0.51	token_training_loss:0.0	distillation_Loss:0.259                                                   	MRR:19.88	Hits@10:36.21	Best:20.16
2025-01-07 16:17:20,976: Snapshot:2	Epoch:11	Loss:20.489	translation_Loss:6.373	token_training_loss:14.116	distillation_Loss:0.0                                                   	MRR:19.88	Hits@10:36.21	Best:20.16
2025-01-07 16:17:25,028: End of token training: 2 Epoch: 12 Loss:7.866 MRR:19.88 Best Results: 20.16
2025-01-07 16:17:25,028: Snapshot:2	Epoch:12	Loss:7.866	translation_Loss:6.379	token_training_loss:1.488	distillation_Loss:0.0                                                           	MRR:19.88	Hits@10:36.21	Best:20.16
2025-01-07 16:17:25,319: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:17:35,229: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2545 | 0.1475 | 0.299  | 0.3721 |  0.464  |
|     1      | 0.1945 | 0.1133 | 0.2186 | 0.2745 |  0.3578 |
|     2      | 0.2041 | 0.1222 | 0.2338 | 0.2897 |  0.364  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,723,600
Trainable params: 2,000
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:17:46,130: Snapshot:3	Epoch:0	Loss:2.854	translation_Loss:2.505	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:21.24	Hits@10:37.36	Best:21.24
2025-01-07 16:17:50,156: Snapshot:3	Epoch:1	Loss:1.753	translation_Loss:1.219	token_training_loss:0.0	distillation_Loss:0.533                                                   	MRR:21.6	Hits@10:38.18	Best:21.6
2025-01-07 16:17:54,174: Snapshot:3	Epoch:2	Loss:1.314	translation_Loss:0.864	token_training_loss:0.0	distillation_Loss:0.45                                                   	MRR:21.7	Hits@10:38.11	Best:21.7
2025-01-07 16:17:58,553: Snapshot:3	Epoch:3	Loss:1.093	translation_Loss:0.688	token_training_loss:0.0	distillation_Loss:0.405                                                   	MRR:21.73	Hits@10:38.29	Best:21.73
2025-01-07 16:18:02,425: Snapshot:3	Epoch:4	Loss:1.015	translation_Loss:0.638	token_training_loss:0.0	distillation_Loss:0.377                                                   	MRR:21.53	Hits@10:38.12	Best:21.73
2025-01-07 16:18:06,288: Snapshot:3	Epoch:5	Loss:0.978	translation_Loss:0.61	token_training_loss:0.0	distillation_Loss:0.368                                                   	MRR:21.65	Hits@10:38.3	Best:21.73
2025-01-07 16:18:10,161: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 21.73
2025-01-07 16:18:10,162: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:0.957 MRR:21.52 Best Results: 21.73
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:18:10,162: Snapshot:3	Epoch:6	Loss:0.957	translation_Loss:0.595	token_training_loss:0.0	distillation_Loss:0.363                                                   	MRR:21.52	Hits@10:38.0	Best:21.73
2025-01-07 16:18:14,283: Snapshot:3	Epoch:7	Loss:19.369	translation_Loss:6.447	token_training_loss:12.922	distillation_Loss:0.0                                                   	MRR:21.52	Hits@10:38.0	Best:21.73
2025-01-07 16:18:17,967: End of token training: 3 Epoch: 8 Loss:7.657 MRR:21.52 Best Results: 21.73
2025-01-07 16:18:17,968: Snapshot:3	Epoch:8	Loss:7.657	translation_Loss:6.462	token_training_loss:1.196	distillation_Loss:0.0                                                           	MRR:21.52	Hits@10:38.0	Best:21.73
2025-01-07 16:18:18,216: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 16:18:30,629: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2505 | 0.1433 | 0.2948 | 0.3676 |  0.4588 |
|     1      | 0.199  | 0.1142 | 0.2276 | 0.2847 |  0.3687 |
|     2      | 0.2052 | 0.1221 | 0.2349 | 0.2924 |  0.3711 |
|     3      | 0.216  | 0.1315 |  0.25  | 0.3053 |  0.3799 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,911,200
Trainable params: 2,000
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:18:41,407: Snapshot:4	Epoch:0	Loss:1.951	translation_Loss:1.503	token_training_loss:0.0	distillation_Loss:0.448                                                   	MRR:23.38	Hits@10:39.58	Best:23.38
2025-01-07 16:18:45,257: Snapshot:4	Epoch:1	Loss:1.435	translation_Loss:0.951	token_training_loss:0.0	distillation_Loss:0.484                                                   	MRR:23.31	Hits@10:39.43	Best:23.38
2025-01-07 16:18:49,180: Snapshot:4	Epoch:2	Loss:1.339	translation_Loss:0.972	token_training_loss:0.0	distillation_Loss:0.367                                                   	MRR:23.51	Hits@10:39.64	Best:23.51
2025-01-07 16:18:53,469: Snapshot:4	Epoch:3	Loss:1.203	translation_Loss:0.844	token_training_loss:0.0	distillation_Loss:0.358                                                   	MRR:23.39	Hits@10:39.75	Best:23.51
2025-01-07 16:18:57,297: Snapshot:4	Epoch:4	Loss:1.18	translation_Loss:0.832	token_training_loss:0.0	distillation_Loss:0.348                                                   	MRR:23.36	Hits@10:39.55	Best:23.51
2025-01-07 16:19:01,203: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 23.51
2025-01-07 16:19:01,204: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:1.161 MRR:23.39 Best Results: 23.51
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:19:01,204: Snapshot:4	Epoch:5	Loss:1.161	translation_Loss:0.805	token_training_loss:0.0	distillation_Loss:0.356                                                   	MRR:23.39	Hits@10:39.55	Best:23.51
2025-01-07 16:19:04,921: Snapshot:4	Epoch:6	Loss:20.117	translation_Loss:6.636	token_training_loss:13.481	distillation_Loss:0.0                                                   	MRR:23.39	Hits@10:39.55	Best:23.51
2025-01-07 16:19:08,664: End of token training: 4 Epoch: 7 Loss:7.994 MRR:23.39 Best Results: 23.51
2025-01-07 16:19:08,665: Snapshot:4	Epoch:7	Loss:7.994	translation_Loss:6.618	token_training_loss:1.376	distillation_Loss:0.0                                                           	MRR:23.39	Hits@10:39.55	Best:23.51
2025-01-07 16:19:08,917: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 16:19:23,657: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2495 | 0.1426 | 0.2936 | 0.3669 |  0.457  |
|     1      | 0.2015 | 0.1154 | 0.2292 | 0.2895 |  0.3727 |
|     2      | 0.2076 | 0.122  | 0.2396 | 0.2976 |  0.3751 |
|     3      | 0.2179 | 0.1318 | 0.2508 | 0.3111 |  0.3859 |
|     4      | 0.2348 | 0.1421 | 0.2722 | 0.3371 |  0.4109 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,000
Trainable params: 2,000
Non-trainable params: 3,003,000
=================================================================
2025-01-07 16:19:23,659: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1499 | 0.3032 | 0.3756 |  0.467  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2556 | 0.1478 | 0.301  | 0.3748 |  0.4651 |
|     1      | 0.1881 | 0.1099 | 0.2091 | 0.2643 |  0.3468 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2545 | 0.1475 | 0.299  | 0.3721 |  0.464  |
|     1      | 0.1945 | 0.1133 | 0.2186 | 0.2745 |  0.3578 |
|     2      | 0.2041 | 0.1222 | 0.2338 | 0.2897 |  0.364  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2505 | 0.1433 | 0.2948 | 0.3676 |  0.4588 |
|     1      | 0.199  | 0.1142 | 0.2276 | 0.2847 |  0.3687 |
|     2      | 0.2052 | 0.1221 | 0.2349 | 0.2924 |  0.3711 |
|     3      | 0.216  | 0.1315 |  0.25  | 0.3053 |  0.3799 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2495 | 0.1426 | 0.2936 | 0.3669 |  0.457  |
|     1      | 0.2015 | 0.1154 | 0.2292 | 0.2895 |  0.3727 |
|     2      | 0.2076 | 0.122  | 0.2396 | 0.2976 |  0.3751 |
|     3      | 0.2179 | 0.1318 | 0.2508 | 0.3111 |  0.3859 |
|     4      | 0.2348 | 0.1421 | 0.2722 | 0.3371 |  0.4109 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 16:19:23,660: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 177.39026927947998 |   0.258   |     0.15     |    0.303     |     0.467     |
|    1     | 96.02168488502502  |   0.246   |    0.142     |    0.288     |     0.448     |
|    2     | 54.263444662094116 |   0.241   |     0.14     |    0.281     |     0.438     |
|    3     | 40.352654695510864 |   0.236   |    0.136     |    0.276     |      0.43     |
|    4     |  35.8905873298645  |   0.236   |    0.137     |    0.275     |     0.429     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 16:19:23,660: Sum_Training_Time:403.9186408519745
2025-01-07 16:19:23,660: Every_Training_Time:[177.39026927947998, 96.02168488502502, 54.263444662094116, 40.352654695510864, 35.8905873298645]
2025-01-07 16:19:23,660: Forward transfer: 0.163625 Backward transfer: 0.002700000000000008
2025-01-07 16:19:47,374: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107161927/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=6, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 16:20:11,387: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.19	Hits@10:34.63	Best:15.19
2025-01-07 16:20:30,507: Snapshot:0	Epoch:1	Loss:18.581	translation_Loss:18.581	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.79	Hits@10:45.16	Best:23.79
2025-01-07 16:20:49,578: Snapshot:0	Epoch:2	Loss:7.14	translation_Loss:7.14	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.55	Hits@10:46.8	Best:25.55
2025-01-07 16:21:08,750: Snapshot:0	Epoch:3	Loss:3.627	translation_Loss:3.627	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.83	Hits@10:47.05	Best:25.83
2025-01-07 16:21:28,236: Snapshot:0	Epoch:4	Loss:2.338	translation_Loss:2.338	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.76	Hits@10:46.58	Best:25.83
2025-01-07 16:21:47,336: Snapshot:0	Epoch:5	Loss:1.745	translation_Loss:1.745	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.52	Hits@10:46.4	Best:25.83
2025-01-07 16:22:06,494: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.83
2025-01-07 16:22:06,495: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.419 MRR:25.46 Best Results: 25.83
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:22:06,495: Snapshot:0	Epoch:6	Loss:1.419	translation_Loss:1.419	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.46	Hits@10:46.18	Best:25.83
2025-01-07 16:22:26,239: Snapshot:0	Epoch:7	Loss:52.179	translation_Loss:36.2	token_training_loss:15.979	distillation_Loss:0.0                                                   	MRR:25.46	Hits@10:46.18	Best:25.83
2025-01-07 16:22:45,540: End of token training: 0 Epoch: 8 Loss:36.209 MRR:25.46 Best Results: 25.83
2025-01-07 16:22:45,541: Snapshot:0	Epoch:8	Loss:36.209	translation_Loss:36.206	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.46	Hits@10:46.18	Best:25.83
2025-01-07 16:22:45,854: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 16:22:52,325: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1497 | 0.3034 | 0.376  |  0.4674 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,598,200
Trainable params: 2,400
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:23:02,851: Snapshot:1	Epoch:0	Loss:6.547	translation_Loss:6.215	token_training_loss:0.0	distillation_Loss:0.332                                                   	MRR:10.1	Hits@10:19.77	Best:10.1
2025-01-07 16:23:06,470: Snapshot:1	Epoch:1	Loss:3.381	translation_Loss:3.025	token_training_loss:0.0	distillation_Loss:0.356                                                   	MRR:12.85	Hits@10:23.94	Best:12.85
2025-01-07 16:23:10,035: Snapshot:1	Epoch:2	Loss:1.764	translation_Loss:1.497	token_training_loss:0.0	distillation_Loss:0.267                                                   	MRR:14.69	Hits@10:27.02	Best:14.69
2025-01-07 16:23:14,023: Snapshot:1	Epoch:3	Loss:1.042	translation_Loss:0.858	token_training_loss:0.0	distillation_Loss:0.184                                                   	MRR:15.77	Hits@10:29.5	Best:15.77
2025-01-07 16:23:17,596: Snapshot:1	Epoch:4	Loss:0.719	translation_Loss:0.591	token_training_loss:0.0	distillation_Loss:0.129                                                   	MRR:16.51	Hits@10:31.01	Best:16.51
2025-01-07 16:23:21,140: Snapshot:1	Epoch:5	Loss:0.553	translation_Loss:0.454	token_training_loss:0.0	distillation_Loss:0.099                                                   	MRR:16.99	Hits@10:31.89	Best:16.99
2025-01-07 16:23:24,698: Snapshot:1	Epoch:6	Loss:0.462	translation_Loss:0.376	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:17.2	Hits@10:32.57	Best:17.2
2025-01-07 16:23:28,198: Snapshot:1	Epoch:7	Loss:0.407	translation_Loss:0.326	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:17.34	Hits@10:32.9	Best:17.34
2025-01-07 16:23:32,250: Snapshot:1	Epoch:8	Loss:0.378	translation_Loss:0.302	token_training_loss:0.0	distillation_Loss:0.076                                                   	MRR:17.46	Hits@10:33.17	Best:17.46
2025-01-07 16:23:35,819: Snapshot:1	Epoch:9	Loss:0.354	translation_Loss:0.28	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:17.67	Hits@10:33.27	Best:17.67
2025-01-07 16:23:39,484: Snapshot:1	Epoch:10	Loss:0.328	translation_Loss:0.255	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:17.68	Hits@10:33.53	Best:17.68
2025-01-07 16:23:43,096: Snapshot:1	Epoch:11	Loss:0.313	translation_Loss:0.243	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:17.81	Hits@10:33.68	Best:17.81
2025-01-07 16:23:46,623: Snapshot:1	Epoch:12	Loss:0.293	translation_Loss:0.225	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:17.77	Hits@10:33.57	Best:17.81
2025-01-07 16:23:50,637: Snapshot:1	Epoch:13	Loss:0.291	translation_Loss:0.222	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:17.91	Hits@10:33.58	Best:17.91
2025-01-07 16:23:54,208: Snapshot:1	Epoch:14	Loss:0.282	translation_Loss:0.215	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.89	Hits@10:33.75	Best:17.91
2025-01-07 16:23:57,731: Snapshot:1	Epoch:15	Loss:0.267	translation_Loss:0.2	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.79	Hits@10:33.8	Best:17.91
2025-01-07 16:24:01,293: Snapshot:1	Epoch:16	Loss:0.262	translation_Loss:0.196	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:17.93	Hits@10:33.67	Best:17.93
2025-01-07 16:24:04,787: Snapshot:1	Epoch:17	Loss:0.254	translation_Loss:0.188	token_training_loss:0.0	distillation_Loss:0.066                                                   	MRR:17.87	Hits@10:33.84	Best:17.93
2025-01-07 16:24:08,737: Snapshot:1	Epoch:18	Loss:0.247	translation_Loss:0.183	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:17.98	Hits@10:33.93	Best:17.98
2025-01-07 16:24:12,212: Snapshot:1	Epoch:19	Loss:0.243	translation_Loss:0.181	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.94	Hits@10:34.14	Best:17.98
2025-01-07 16:24:15,750: Snapshot:1	Epoch:20	Loss:0.237	translation_Loss:0.173	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:18.02	Hits@10:34.38	Best:18.02
2025-01-07 16:24:19,263: Snapshot:1	Epoch:21	Loss:0.239	translation_Loss:0.175	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:18.04	Hits@10:34.13	Best:18.04
2025-01-07 16:24:22,761: Snapshot:1	Epoch:22	Loss:0.232	translation_Loss:0.168	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:18.11	Hits@10:34.17	Best:18.11
2025-01-07 16:24:26,679: Snapshot:1	Epoch:23	Loss:0.226	translation_Loss:0.164	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:18.19	Hits@10:34.27	Best:18.19
2025-01-07 16:24:30,382: Snapshot:1	Epoch:24	Loss:0.228	translation_Loss:0.166	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:18.22	Hits@10:34.25	Best:18.22
2025-01-07 16:24:33,969: Snapshot:1	Epoch:25	Loss:0.225	translation_Loss:0.162	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:18.18	Hits@10:34.33	Best:18.22
2025-01-07 16:24:37,524: Snapshot:1	Epoch:26	Loss:0.225	translation_Loss:0.16	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:18.12	Hits@10:34.33	Best:18.22
2025-01-07 16:24:41,084: Early Stopping! Snapshot: 1 Epoch: 27 Best Results: 18.22
2025-01-07 16:24:41,085: Start to training tokens! Snapshot: 1 Epoch: 27 Loss:0.224 MRR:18.15 Best Results: 18.22
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:24:41,085: Snapshot:1	Epoch:27	Loss:0.224	translation_Loss:0.159	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:18.15	Hits@10:34.17	Best:18.22
2025-01-07 16:24:45,073: Snapshot:1	Epoch:28	Loss:20.621	translation_Loss:6.145	token_training_loss:14.476	distillation_Loss:0.0                                                   	MRR:18.15	Hits@10:34.17	Best:18.22
2025-01-07 16:24:48,482: End of token training: 1 Epoch: 29 Loss:7.848 MRR:18.15 Best Results: 18.22
2025-01-07 16:24:48,482: Snapshot:1	Epoch:29	Loss:7.848	translation_Loss:6.15	token_training_loss:1.698	distillation_Loss:0.0                                                           	MRR:18.15	Hits@10:34.17	Best:18.22
2025-01-07 16:24:48,798: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:24:57,036: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1479 | 0.302  | 0.3746 |  0.4662 |
|     1      | 0.1893 | 0.1106 | 0.2115 | 0.2671 |  0.3509 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,348,800
Trainable params: 2,400
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:25:07,486: Snapshot:2	Epoch:0	Loss:4.416	translation_Loss:4.052	token_training_loss:0.0	distillation_Loss:0.364                                                   	MRR:18.48	Hits@10:33.76	Best:18.48
2025-01-07 16:25:11,283: Snapshot:2	Epoch:1	Loss:2.361	translation_Loss:1.879	token_training_loss:0.0	distillation_Loss:0.482                                                   	MRR:19.58	Hits@10:34.96	Best:19.58
2025-01-07 16:25:15,466: Snapshot:2	Epoch:2	Loss:1.493	translation_Loss:1.118	token_training_loss:0.0	distillation_Loss:0.375                                                   	MRR:19.86	Hits@10:35.47	Best:19.86
2025-01-07 16:25:19,264: Snapshot:2	Epoch:3	Loss:1.101	translation_Loss:0.799	token_training_loss:0.0	distillation_Loss:0.301                                                   	MRR:19.88	Hits@10:35.84	Best:19.88
2025-01-07 16:25:23,075: Snapshot:2	Epoch:4	Loss:0.961	translation_Loss:0.693	token_training_loss:0.0	distillation_Loss:0.268                                                   	MRR:20.02	Hits@10:35.77	Best:20.02
2025-01-07 16:25:26,855: Snapshot:2	Epoch:5	Loss:0.891	translation_Loss:0.633	token_training_loss:0.0	distillation_Loss:0.258                                                   	MRR:20.03	Hits@10:36.01	Best:20.03
2025-01-07 16:25:30,640: Snapshot:2	Epoch:6	Loss:0.854	translation_Loss:0.603	token_training_loss:0.0	distillation_Loss:0.252                                                   	MRR:20.0	Hits@10:35.98	Best:20.03
2025-01-07 16:25:34,880: Snapshot:2	Epoch:7	Loss:0.825	translation_Loss:0.578	token_training_loss:0.0	distillation_Loss:0.247                                                   	MRR:20.06	Hits@10:36.0	Best:20.06
2025-01-07 16:25:38,640: Snapshot:2	Epoch:8	Loss:0.806	translation_Loss:0.559	token_training_loss:0.0	distillation_Loss:0.247                                                   	MRR:20.06	Hits@10:36.25	Best:20.06
2025-01-07 16:25:42,439: Snapshot:2	Epoch:9	Loss:0.799	translation_Loss:0.553	token_training_loss:0.0	distillation_Loss:0.246                                                   	MRR:20.16	Hits@10:36.1	Best:20.16
2025-01-07 16:25:46,181: Snapshot:2	Epoch:10	Loss:0.782	translation_Loss:0.539	token_training_loss:0.0	distillation_Loss:0.244                                                   	MRR:20.07	Hits@10:36.32	Best:20.16
2025-01-07 16:25:49,915: Snapshot:2	Epoch:11	Loss:0.781	translation_Loss:0.537	token_training_loss:0.0	distillation_Loss:0.244                                                   	MRR:20.03	Hits@10:36.24	Best:20.16
2025-01-07 16:25:54,188: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 20.16
2025-01-07 16:25:54,189: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:0.763 MRR:20.03 Best Results: 20.16
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:25:54,189: Snapshot:2	Epoch:12	Loss:0.763	translation_Loss:0.519	token_training_loss:0.0	distillation_Loss:0.244                                                   	MRR:20.03	Hits@10:36.19	Best:20.16
2025-01-07 16:25:57,775: Snapshot:2	Epoch:13	Loss:21.166	translation_Loss:6.364	token_training_loss:14.801	distillation_Loss:0.0                                                   	MRR:20.03	Hits@10:36.19	Best:20.16
2025-01-07 16:26:01,364: End of token training: 2 Epoch: 14 Loss:8.213 MRR:20.03 Best Results: 20.16
2025-01-07 16:26:01,364: Snapshot:2	Epoch:14	Loss:8.213	translation_Loss:6.362	token_training_loss:1.851	distillation_Loss:0.0                                                           	MRR:20.03	Hits@10:36.19	Best:20.16
2025-01-07 16:26:01,713: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:26:11,887: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2542 | 0.1471 | 0.299  | 0.3716 |  0.464  |
|     1      | 0.1944 | 0.1124 | 0.218  | 0.2765 |  0.3605 |
|     2      | 0.202  | 0.1187 | 0.2327 | 0.2915 |  0.3655 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,724,000
Trainable params: 2,400
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:26:22,689: Snapshot:3	Epoch:0	Loss:2.869	translation_Loss:2.498	token_training_loss:0.0	distillation_Loss:0.371                                                   	MRR:21.02	Hits@10:37.69	Best:21.02
2025-01-07 16:26:26,493: Snapshot:3	Epoch:1	Loss:1.783	translation_Loss:1.262	token_training_loss:0.0	distillation_Loss:0.521                                                   	MRR:21.42	Hits@10:37.67	Best:21.42
2025-01-07 16:26:30,838: Snapshot:3	Epoch:2	Loss:1.371	translation_Loss:0.946	token_training_loss:0.0	distillation_Loss:0.425                                                   	MRR:21.47	Hits@10:37.63	Best:21.47
2025-01-07 16:26:34,735: Snapshot:3	Epoch:3	Loss:1.128	translation_Loss:0.744	token_training_loss:0.0	distillation_Loss:0.384                                                   	MRR:21.6	Hits@10:38.01	Best:21.6
2025-01-07 16:26:38,532: Snapshot:3	Epoch:4	Loss:1.049	translation_Loss:0.696	token_training_loss:0.0	distillation_Loss:0.353                                                   	MRR:21.54	Hits@10:38.11	Best:21.6
2025-01-07 16:26:42,263: Snapshot:3	Epoch:5	Loss:1.014	translation_Loss:0.662	token_training_loss:0.0	distillation_Loss:0.351                                                   	MRR:21.55	Hits@10:37.96	Best:21.6
2025-01-07 16:26:46,144: Snapshot:3	Epoch:6	Loss:0.996	translation_Loss:0.648	token_training_loss:0.0	distillation_Loss:0.348                                                   	MRR:21.61	Hits@10:37.87	Best:21.61
2025-01-07 16:26:50,453: Snapshot:3	Epoch:7	Loss:0.981	translation_Loss:0.631	token_training_loss:0.0	distillation_Loss:0.35                                                   	MRR:21.42	Hits@10:37.83	Best:21.61
2025-01-07 16:26:54,247: Snapshot:3	Epoch:8	Loss:0.984	translation_Loss:0.635	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:21.37	Hits@10:37.67	Best:21.61
2025-01-07 16:26:58,025: Early Stopping! Snapshot: 3 Epoch: 9 Best Results: 21.61
2025-01-07 16:26:58,025: Start to training tokens! Snapshot: 3 Epoch: 9 Loss:0.977 MRR:21.34 Best Results: 21.61
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:26:58,026: Snapshot:3	Epoch:9	Loss:0.977	translation_Loss:0.628	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:21.34	Hits@10:37.71	Best:21.61
2025-01-07 16:27:01,697: Snapshot:3	Epoch:10	Loss:19.759	translation_Loss:6.442	token_training_loss:13.318	distillation_Loss:0.0                                                   	MRR:21.34	Hits@10:37.71	Best:21.61
2025-01-07 16:27:05,354: End of token training: 3 Epoch: 11 Loss:7.758 MRR:21.34 Best Results: 21.61
2025-01-07 16:27:05,354: Snapshot:3	Epoch:11	Loss:7.758	translation_Loss:6.444	token_training_loss:1.313	distillation_Loss:0.0                                                           	MRR:21.34	Hits@10:37.71	Best:21.61
2025-01-07 16:27:05,740: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 16:27:18,355: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2506 | 0.1432 | 0.2957 | 0.3678 |  0.4596 |
|     1      | 0.1973 | 0.1116 | 0.2245 | 0.2843 |  0.368  |
|     2      | 0.2053 | 0.1205 | 0.2346 | 0.2951 |  0.3724 |
|     3      | 0.2156 | 0.1292 | 0.2517 | 0.3074 |  0.3787 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,911,600
Trainable params: 2,400
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:27:28,991: Snapshot:4	Epoch:0	Loss:1.969	translation_Loss:1.494	token_training_loss:0.0	distillation_Loss:0.475                                                   	MRR:23.42	Hits@10:39.64	Best:23.42
2025-01-07 16:27:32,755: Snapshot:4	Epoch:1	Loss:1.475	translation_Loss:1.022	token_training_loss:0.0	distillation_Loss:0.453                                                   	MRR:23.33	Hits@10:39.18	Best:23.42
2025-01-07 16:27:36,520: Snapshot:4	Epoch:2	Loss:1.356	translation_Loss:1.01	token_training_loss:0.0	distillation_Loss:0.346                                                   	MRR:23.31	Hits@10:39.33	Best:23.42
2025-01-07 16:27:40,340: Early Stopping! Snapshot: 4 Epoch: 3 Best Results: 23.42
2025-01-07 16:27:40,340: Start to training tokens! Snapshot: 4 Epoch: 3 Loss:1.232 MRR:23.37 Best Results: 23.42
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:27:40,341: Snapshot:4	Epoch:3	Loss:1.232	translation_Loss:0.895	token_training_loss:0.0	distillation_Loss:0.337                                                   	MRR:23.37	Hits@10:39.38	Best:23.42
2025-01-07 16:27:44,473: Snapshot:4	Epoch:4	Loss:20.71	translation_Loss:6.67	token_training_loss:14.04	distillation_Loss:0.0                                                   	MRR:23.37	Hits@10:39.38	Best:23.42
2025-01-07 16:27:48,176: End of token training: 4 Epoch: 5 Loss:8.296 MRR:23.37 Best Results: 23.42
2025-01-07 16:27:48,176: Snapshot:4	Epoch:5	Loss:8.296	translation_Loss:6.663	token_training_loss:1.633	distillation_Loss:0.0                                                           	MRR:23.37	Hits@10:39.38	Best:23.42
2025-01-07 16:27:48,423: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 16:28:02,796: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2482 | 0.1408 | 0.293  | 0.3659 |  0.4573 |
|     1      | 0.1987 | 0.1107 | 0.2267 | 0.287  |  0.3734 |
|     2      | 0.2053 | 0.1186 | 0.2338 | 0.2967 |  0.3781 |
|     3      | 0.2142 | 0.1245 | 0.2505 | 0.3099 |  0.3839 |
|     4      | 0.2315 | 0.1382 | 0.2707 | 0.3324 |  0.4112 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,400
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,400
Trainable params: 2,400
Non-trainable params: 3,003,000
=================================================================
2025-01-07 16:28:02,799: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1497 | 0.3034 | 0.376  |  0.4674 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1479 | 0.302  | 0.3746 |  0.4662 |
|     1      | 0.1893 | 0.1106 | 0.2115 | 0.2671 |  0.3509 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2542 | 0.1471 | 0.299  | 0.3716 |  0.464  |
|     1      | 0.1944 | 0.1124 | 0.218  | 0.2765 |  0.3605 |
|     2      | 0.202  | 0.1187 | 0.2327 | 0.2915 |  0.3655 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2506 | 0.1432 | 0.2957 | 0.3678 |  0.4596 |
|     1      | 0.1973 | 0.1116 | 0.2245 | 0.2843 |  0.368  |
|     2      | 0.2053 | 0.1205 | 0.2346 | 0.2951 |  0.3724 |
|     3      | 0.2156 | 0.1292 | 0.2517 | 0.3074 |  0.3787 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2482 | 0.1408 | 0.293  | 0.3659 |  0.4573 |
|     1      | 0.1987 | 0.1107 | 0.2267 | 0.287  |  0.3734 |
|     2      | 0.2053 | 0.1186 | 0.2338 | 0.2967 |  0.3781 |
|     3      | 0.2142 | 0.1245 | 0.2505 | 0.3099 |  0.3839 |
|     4      | 0.2315 | 0.1382 | 0.2707 | 0.3324 |  0.4112 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 16:28:02,799: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 178.16586637496948 |   0.258   |     0.15     |    0.303     |     0.467     |
|    1     | 113.99115300178528 |   0.246   |    0.143     |    0.289     |      0.45     |
|    2     | 62.343931436538696 |    0.24   |    0.139     |    0.281     |     0.439     |
|    3     | 51.433011293411255 |   0.236   |    0.136     |    0.276     |     0.431     |
|    4     | 27.68686270713806  |   0.234   |    0.134     |    0.274     |     0.429     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 16:28:02,799: Sum_Training_Time:433.6208248138428
2025-01-07 16:28:02,799: Every_Training_Time:[178.16586637496948, 113.99115300178528, 62.343931436538696, 51.433011293411255, 27.68686270713806]
2025-01-07 16:28:02,799: Forward transfer: 0.163575 Backward transfer: 0.0004999999999999935
2025-01-07 16:28:26,543: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107162806/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 16:28:50,685: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.19	Hits@10:34.62	Best:15.19
2025-01-07 16:29:09,978: Snapshot:0	Epoch:1	Loss:18.581	translation_Loss:18.581	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.76	Hits@10:45.2	Best:23.76
2025-01-07 16:29:29,296: Snapshot:0	Epoch:2	Loss:7.138	translation_Loss:7.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.51	Hits@10:46.76	Best:25.51
2025-01-07 16:29:48,690: Snapshot:0	Epoch:3	Loss:3.627	translation_Loss:3.627	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.74	Hits@10:46.84	Best:25.74
2025-01-07 16:30:08,379: Snapshot:0	Epoch:4	Loss:2.337	translation_Loss:2.337	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.72	Hits@10:46.5	Best:25.74
2025-01-07 16:30:27,605: Snapshot:0	Epoch:5	Loss:1.746	translation_Loss:1.746	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.44	Hits@10:46.43	Best:25.74
2025-01-07 16:30:46,904: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.74
2025-01-07 16:30:46,905: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.415 MRR:25.44 Best Results: 25.74
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:30:46,905: Snapshot:0	Epoch:6	Loss:1.415	translation_Loss:1.415	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.44	Hits@10:46.21	Best:25.74
2025-01-07 16:31:06,644: Snapshot:0	Epoch:7	Loss:52.747	translation_Loss:36.133	token_training_loss:16.614	distillation_Loss:0.0                                                   	MRR:25.44	Hits@10:46.21	Best:25.74
2025-01-07 16:31:25,754: End of token training: 0 Epoch: 8 Loss:36.144 MRR:25.44 Best Results: 25.74
2025-01-07 16:31:25,754: Snapshot:0	Epoch:8	Loss:36.144	translation_Loss:36.141	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.44	Hits@10:46.21	Best:25.74
2025-01-07 16:31:26,066: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 16:31:32,517: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1494 | 0.3026 | 0.3758 |  0.4665 |
+------------+-------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,598,600
Trainable params: 2,800
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:31:43,040: Snapshot:1	Epoch:0	Loss:6.564	translation_Loss:6.218	token_training_loss:0.0	distillation_Loss:0.346                                                   	MRR:10.15	Hits@10:19.7	Best:10.15
2025-01-07 16:31:46,614: Snapshot:1	Epoch:1	Loss:3.401	translation_Loss:3.057	token_training_loss:0.0	distillation_Loss:0.343                                                   	MRR:12.89	Hits@10:23.98	Best:12.89
2025-01-07 16:31:50,198: Snapshot:1	Epoch:2	Loss:1.769	translation_Loss:1.514	token_training_loss:0.0	distillation_Loss:0.255                                                   	MRR:14.59	Hits@10:27.1	Best:14.59
2025-01-07 16:31:54,281: Snapshot:1	Epoch:3	Loss:1.044	translation_Loss:0.869	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:15.7	Hits@10:29.21	Best:15.7
2025-01-07 16:31:57,875: Snapshot:1	Epoch:4	Loss:0.721	translation_Loss:0.599	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:16.48	Hits@10:30.81	Best:16.48
2025-01-07 16:32:01,526: Snapshot:1	Epoch:5	Loss:0.552	translation_Loss:0.458	token_training_loss:0.0	distillation_Loss:0.094                                                   	MRR:16.83	Hits@10:31.66	Best:16.83
2025-01-07 16:32:05,128: Snapshot:1	Epoch:6	Loss:0.463	translation_Loss:0.379	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:17.11	Hits@10:32.21	Best:17.11
2025-01-07 16:32:08,702: Snapshot:1	Epoch:7	Loss:0.406	translation_Loss:0.329	token_training_loss:0.0	distillation_Loss:0.077                                                   	MRR:17.21	Hits@10:32.72	Best:17.21
2025-01-07 16:32:12,689: Snapshot:1	Epoch:8	Loss:0.38	translation_Loss:0.306	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:17.46	Hits@10:33.17	Best:17.46
2025-01-07 16:32:16,230: Snapshot:1	Epoch:9	Loss:0.354	translation_Loss:0.282	token_training_loss:0.0	distillation_Loss:0.073                                                   	MRR:17.57	Hits@10:33.32	Best:17.57
2025-01-07 16:32:19,757: Snapshot:1	Epoch:10	Loss:0.328	translation_Loss:0.257	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:17.63	Hits@10:33.38	Best:17.63
2025-01-07 16:32:23,380: Snapshot:1	Epoch:11	Loss:0.312	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:17.82	Hits@10:33.58	Best:17.82
2025-01-07 16:32:26,913: Snapshot:1	Epoch:12	Loss:0.294	translation_Loss:0.227	token_training_loss:0.0	distillation_Loss:0.066                                                   	MRR:17.76	Hits@10:33.51	Best:17.82
2025-01-07 16:32:30,886: Snapshot:1	Epoch:13	Loss:0.29	translation_Loss:0.224	token_training_loss:0.0	distillation_Loss:0.066                                                   	MRR:17.8	Hits@10:33.41	Best:17.82
2025-01-07 16:32:34,440: Snapshot:1	Epoch:14	Loss:0.282	translation_Loss:0.217	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:17.86	Hits@10:33.63	Best:17.86
2025-01-07 16:32:38,014: Snapshot:1	Epoch:15	Loss:0.268	translation_Loss:0.204	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:17.89	Hits@10:33.87	Best:17.89
2025-01-07 16:32:41,504: Snapshot:1	Epoch:16	Loss:0.262	translation_Loss:0.199	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:17.85	Hits@10:33.55	Best:17.89
2025-01-07 16:32:44,987: Snapshot:1	Epoch:17	Loss:0.255	translation_Loss:0.19	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:17.87	Hits@10:33.77	Best:17.89
2025-01-07 16:32:48,943: Snapshot:1	Epoch:18	Loss:0.248	translation_Loss:0.186	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.91	Hits@10:33.95	Best:17.91
2025-01-07 16:32:52,479: Snapshot:1	Epoch:19	Loss:0.245	translation_Loss:0.184	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:18.07	Hits@10:34.05	Best:18.07
2025-01-07 16:32:56,008: Snapshot:1	Epoch:20	Loss:0.238	translation_Loss:0.175	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:18.18	Hits@10:34.13	Best:18.18
2025-01-07 16:32:59,511: Snapshot:1	Epoch:21	Loss:0.239	translation_Loss:0.177	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:18.03	Hits@10:33.95	Best:18.18
2025-01-07 16:33:02,987: Snapshot:1	Epoch:22	Loss:0.233	translation_Loss:0.171	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.95	Hits@10:33.92	Best:18.18
2025-01-07 16:33:07,019: Early Stopping! Snapshot: 1 Epoch: 23 Best Results: 18.18
2025-01-07 16:33:07,019: Start to training tokens! Snapshot: 1 Epoch: 23 Loss:0.228 MRR:18.0 Best Results: 18.18
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:33:07,020: Snapshot:1	Epoch:23	Loss:0.228	translation_Loss:0.166	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:18.0	Hits@10:33.84	Best:18.18
2025-01-07 16:33:10,490: Snapshot:1	Epoch:24	Loss:20.647	translation_Loss:6.171	token_training_loss:14.477	distillation_Loss:0.0                                                   	MRR:18.0	Hits@10:33.84	Best:18.18
2025-01-07 16:33:13,966: End of token training: 1 Epoch: 25 Loss:7.984 MRR:18.0 Best Results: 18.18
2025-01-07 16:33:13,966: Snapshot:1	Epoch:25	Loss:7.984	translation_Loss:6.174	token_training_loss:1.81	distillation_Loss:0.0                                                           	MRR:18.0	Hits@10:33.84	Best:18.18
2025-01-07 16:33:14,336: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:33:22,558: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2551 | 0.1469 | 0.3011 | 0.3736 |  0.466  |
|     1      | 0.1871 | 0.1083 | 0.2097 | 0.2668 |  0.3479 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,349,200
Trainable params: 2,800
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:33:33,222: Snapshot:2	Epoch:0	Loss:4.438	translation_Loss:4.054	token_training_loss:0.0	distillation_Loss:0.384                                                   	MRR:18.35	Hits@10:33.96	Best:18.35
2025-01-07 16:33:36,930: Snapshot:2	Epoch:1	Loss:2.392	translation_Loss:1.917	token_training_loss:0.0	distillation_Loss:0.475                                                   	MRR:19.49	Hits@10:35.03	Best:19.49
2025-01-07 16:33:40,682: Snapshot:2	Epoch:2	Loss:1.523	translation_Loss:1.163	token_training_loss:0.0	distillation_Loss:0.36                                                   	MRR:19.69	Hits@10:35.29	Best:19.69
2025-01-07 16:33:44,369: Snapshot:2	Epoch:3	Loss:1.127	translation_Loss:0.832	token_training_loss:0.0	distillation_Loss:0.296                                                   	MRR:19.88	Hits@10:35.77	Best:19.88
2025-01-07 16:33:48,467: Snapshot:2	Epoch:4	Loss:0.991	translation_Loss:0.733	token_training_loss:0.0	distillation_Loss:0.258                                                   	MRR:19.86	Hits@10:36.17	Best:19.88
2025-01-07 16:33:52,178: Snapshot:2	Epoch:5	Loss:0.913	translation_Loss:0.659	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:19.99	Hits@10:36.08	Best:19.99
2025-01-07 16:33:56,005: Snapshot:2	Epoch:6	Loss:0.88	translation_Loss:0.634	token_training_loss:0.0	distillation_Loss:0.245                                                   	MRR:20.05	Hits@10:36.34	Best:20.05
2025-01-07 16:33:59,777: Snapshot:2	Epoch:7	Loss:0.848	translation_Loss:0.604	token_training_loss:0.0	distillation_Loss:0.244                                                   	MRR:19.85	Hits@10:36.13	Best:20.05
2025-01-07 16:34:03,550: Snapshot:2	Epoch:8	Loss:0.836	translation_Loss:0.592	token_training_loss:0.0	distillation_Loss:0.244                                                   	MRR:19.85	Hits@10:36.09	Best:20.05
2025-01-07 16:34:07,748: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 20.05
2025-01-07 16:34:07,748: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:0.813 MRR:19.8 Best Results: 20.05
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:34:07,748: Snapshot:2	Epoch:9	Loss:0.813	translation_Loss:0.568	token_training_loss:0.0	distillation_Loss:0.245                                                   	MRR:19.8	Hits@10:36.07	Best:20.05
2025-01-07 16:34:11,344: Snapshot:2	Epoch:10	Loss:21.266	translation_Loss:6.412	token_training_loss:14.854	distillation_Loss:0.0                                                   	MRR:19.8	Hits@10:36.07	Best:20.05
2025-01-07 16:34:14,933: End of token training: 2 Epoch: 11 Loss:8.31 MRR:19.8 Best Results: 20.05
2025-01-07 16:34:14,933: Snapshot:2	Epoch:11	Loss:8.31	translation_Loss:6.425	token_training_loss:1.886	distillation_Loss:0.0                                                           	MRR:19.8	Hits@10:36.07	Best:20.05
2025-01-07 16:34:15,210: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:34:25,409: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.254  | 0.1466 | 0.2992 | 0.3721 |  0.4631 |
|     1      | 0.1903 | 0.1084 | 0.2129 | 0.2747 |  0.3572 |
|     2      | 0.2022 | 0.1212 | 0.2309 | 0.2878 |  0.3593 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,724,400
Trainable params: 2,800
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:34:36,448: Snapshot:3	Epoch:0	Loss:2.924	translation_Loss:2.525	token_training_loss:0.0	distillation_Loss:0.399                                                   	MRR:20.89	Hits@10:37.18	Best:20.89
2025-01-07 16:34:40,366: Snapshot:3	Epoch:1	Loss:1.833	translation_Loss:1.302	token_training_loss:0.0	distillation_Loss:0.531                                                   	MRR:21.55	Hits@10:37.57	Best:21.55
2025-01-07 16:34:44,219: Snapshot:3	Epoch:2	Loss:1.429	translation_Loss:1.01	token_training_loss:0.0	distillation_Loss:0.419                                                   	MRR:21.58	Hits@10:37.5	Best:21.58
2025-01-07 16:34:48,024: Snapshot:3	Epoch:3	Loss:1.18	translation_Loss:0.8	token_training_loss:0.0	distillation_Loss:0.38                                                   	MRR:21.44	Hits@10:37.65	Best:21.58
2025-01-07 16:34:52,298: Snapshot:3	Epoch:4	Loss:1.118	translation_Loss:0.764	token_training_loss:0.0	distillation_Loss:0.354                                                   	MRR:21.37	Hits@10:37.72	Best:21.58
2025-01-07 16:34:56,167: Early Stopping! Snapshot: 3 Epoch: 5 Best Results: 21.58
2025-01-07 16:34:56,167: Start to training tokens! Snapshot: 3 Epoch: 5 Loss:1.075 MRR:21.49 Best Results: 21.58
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:34:56,167: Snapshot:3	Epoch:5	Loss:1.075	translation_Loss:0.718	token_training_loss:0.0	distillation_Loss:0.357                                                   	MRR:21.49	Hits@10:37.68	Best:21.58
2025-01-07 16:34:59,889: Snapshot:3	Epoch:6	Loss:20.152	translation_Loss:6.51	token_training_loss:13.642	distillation_Loss:0.0                                                   	MRR:21.49	Hits@10:37.68	Best:21.58
2025-01-07 16:35:03,588: End of token training: 3 Epoch: 7 Loss:8.017 MRR:21.49 Best Results: 21.58
2025-01-07 16:35:03,588: Snapshot:3	Epoch:7	Loss:8.017	translation_Loss:6.531	token_training_loss:1.485	distillation_Loss:0.0                                                           	MRR:21.49	Hits@10:37.68	Best:21.58
2025-01-07 16:35:03,834: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 16:35:16,102: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2523 | 0.145  | 0.2974 | 0.3699 |   0.46  |
|     1      | 0.1923 | 0.1071 | 0.2184 | 0.2766 |  0.3666 |
|     2      | 0.2027 | 0.1192 | 0.2317 | 0.2907 |  0.3671 |
|     3      | 0.2124 | 0.1285 | 0.2472 | 0.3006 |  0.3709 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,912,000
Trainable params: 2,800
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:35:27,092: Snapshot:4	Epoch:0	Loss:2.071	translation_Loss:1.553	token_training_loss:0.0	distillation_Loss:0.517                                                   	MRR:23.03	Hits@10:38.83	Best:23.03
2025-01-07 16:35:30,973: Snapshot:4	Epoch:1	Loss:1.574	translation_Loss:1.118	token_training_loss:0.0	distillation_Loss:0.456                                                   	MRR:22.99	Hits@10:38.44	Best:23.03
2025-01-07 16:35:34,919: Snapshot:4	Epoch:2	Loss:1.447	translation_Loss:1.098	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:23.12	Hits@10:38.77	Best:23.12
2025-01-07 16:35:39,208: Snapshot:4	Epoch:3	Loss:1.32	translation_Loss:0.976	token_training_loss:0.0	distillation_Loss:0.344                                                   	MRR:22.86	Hits@10:38.71	Best:23.12
2025-01-07 16:35:42,999: Snapshot:4	Epoch:4	Loss:1.292	translation_Loss:0.958	token_training_loss:0.0	distillation_Loss:0.334                                                   	MRR:22.99	Hits@10:38.58	Best:23.12
2025-01-07 16:35:46,782: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 23.12
2025-01-07 16:35:46,782: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:1.278 MRR:23.04 Best Results: 23.12
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:35:46,783: Snapshot:4	Epoch:5	Loss:1.278	translation_Loss:0.942	token_training_loss:0.0	distillation_Loss:0.336                                                   	MRR:23.04	Hits@10:38.86	Best:23.12
2025-01-07 16:35:50,522: Snapshot:4	Epoch:6	Loss:21.234	translation_Loss:6.713	token_training_loss:14.521	distillation_Loss:0.0                                                   	MRR:23.04	Hits@10:38.86	Best:23.12
2025-01-07 16:35:54,255: End of token training: 4 Epoch: 7 Loss:8.579 MRR:23.04 Best Results: 23.12
2025-01-07 16:35:54,255: Snapshot:4	Epoch:7	Loss:8.579	translation_Loss:6.716	token_training_loss:1.863	distillation_Loss:0.0                                                           	MRR:23.04	Hits@10:38.86	Best:23.12
2025-01-07 16:35:54,502: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 16:36:09,173: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1435 | 0.2975 | 0.3677 |  0.4577 |
|     1      | 0.1956 | 0.1099 | 0.2201 | 0.2812 |  0.3692 |
|     2      | 0.2048 | 0.1203 | 0.2315 | 0.2933 |  0.3735 |
|     3      | 0.2162 | 0.1306 | 0.2506 | 0.3071 |  0.3784 |
|     4      | 0.2351 | 0.1429 | 0.2729 | 0.3346 |  0.4109 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,800
Trainable params: 2,800
Non-trainable params: 3,003,000
=================================================================
2025-01-07 16:36:09,176: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1494 | 0.3026 | 0.3758 |  0.4665 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2551 | 0.1469 | 0.3011 | 0.3736 |  0.466  |
|     1      | 0.1871 | 0.1083 | 0.2097 | 0.2668 |  0.3479 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.254  | 0.1466 | 0.2992 | 0.3721 |  0.4631 |
|     1      | 0.1903 | 0.1084 | 0.2129 | 0.2747 |  0.3572 |
|     2      | 0.2022 | 0.1212 | 0.2309 | 0.2878 |  0.3593 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2523 | 0.145  | 0.2974 | 0.3699 |   0.46  |
|     1      | 0.1923 | 0.1071 | 0.2184 | 0.2766 |  0.3666 |
|     2      | 0.2027 | 0.1192 | 0.2317 | 0.2907 |  0.3671 |
|     3      | 0.2124 | 0.1285 | 0.2472 | 0.3006 |  0.3709 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1435 | 0.2975 | 0.3677 |  0.4577 |
|     1      | 0.1956 | 0.1099 | 0.2201 | 0.2812 |  0.3692 |
|     2      | 0.2048 | 0.1203 | 0.2315 | 0.2933 |  0.3735 |
|     3      | 0.2162 | 0.1306 | 0.2506 | 0.3071 |  0.3784 |
|     4      | 0.2351 | 0.1429 | 0.2729 | 0.3346 |  0.4109 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 16:36:09,177: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 179.21042323112488 |   0.257   |    0.149     |    0.303     |     0.467     |
|    1     | 99.27029466629028  |   0.245   |    0.141     |    0.288     |     0.449     |
|    2     | 50.43311023712158  |    0.24   |    0.139     |     0.28     |     0.437     |
|    3     | 36.13091707229614  |   0.236   |    0.136     |    0.276     |     0.429     |
|    4     | 35.69754123687744  |   0.236   |    0.136     |    0.276     |     0.428     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 16:36:09,177: Sum_Training_Time:400.7422864437103
2025-01-07 16:36:09,177: Every_Training_Time:[179.21042323112488, 99.27029466629028, 50.43311023712158, 36.13091707229614, 35.69754123687744]
2025-01-07 16:36:09,177: Forward transfer: 0.163425 Backward transfer: 0.002250000000000002
2025-01-07 16:36:33,106: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107163613/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=8, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 16:36:57,082: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.19	Hits@10:34.63	Best:15.19
2025-01-07 16:37:16,317: Snapshot:0	Epoch:1	Loss:18.582	translation_Loss:18.582	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.77	Hits@10:45.2	Best:23.77
2025-01-07 16:37:35,483: Snapshot:0	Epoch:2	Loss:7.138	translation_Loss:7.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.56	Hits@10:46.87	Best:25.56
2025-01-07 16:37:54,858: Snapshot:0	Epoch:3	Loss:3.627	translation_Loss:3.627	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.81	Hits@10:47.03	Best:25.81
2025-01-07 16:38:14,558: Snapshot:0	Epoch:4	Loss:2.339	translation_Loss:2.339	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.71	Hits@10:46.56	Best:25.81
2025-01-07 16:38:33,858: Snapshot:0	Epoch:5	Loss:1.744	translation_Loss:1.744	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.47	Hits@10:46.28	Best:25.81
2025-01-07 16:38:53,060: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.81
2025-01-07 16:38:53,061: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.418 MRR:25.38 Best Results: 25.81
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:38:53,061: Snapshot:0	Epoch:6	Loss:1.418	translation_Loss:1.418	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.38	Hits@10:46.13	Best:25.81
2025-01-07 16:39:12,935: Snapshot:0	Epoch:7	Loss:53.364	translation_Loss:36.157	token_training_loss:17.208	distillation_Loss:0.0                                                   	MRR:25.38	Hits@10:46.13	Best:25.81
2025-01-07 16:39:32,232: End of token training: 0 Epoch: 8 Loss:36.167 MRR:25.38 Best Results: 25.81
2025-01-07 16:39:32,233: Snapshot:0	Epoch:8	Loss:36.167	translation_Loss:36.164	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.38	Hits@10:46.13	Best:25.81
2025-01-07 16:39:32,542: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 16:39:38,953: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1494 | 0.3041 | 0.376  |  0.4671 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,599,000
Trainable params: 3,200
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:39:49,349: Snapshot:1	Epoch:0	Loss:6.582	translation_Loss:6.222	token_training_loss:0.0	distillation_Loss:0.36                                                   	MRR:10.07	Hits@10:19.55	Best:10.07
2025-01-07 16:39:52,895: Snapshot:1	Epoch:1	Loss:3.42	translation_Loss:3.087	token_training_loss:0.0	distillation_Loss:0.334                                                   	MRR:12.89	Hits@10:23.7	Best:12.89
2025-01-07 16:39:56,392: Snapshot:1	Epoch:2	Loss:1.772	translation_Loss:1.528	token_training_loss:0.0	distillation_Loss:0.244                                                   	MRR:14.62	Hits@10:26.93	Best:14.62
2025-01-07 16:40:00,519: Snapshot:1	Epoch:3	Loss:1.043	translation_Loss:0.878	token_training_loss:0.0	distillation_Loss:0.166                                                   	MRR:15.7	Hits@10:29.42	Best:15.7
2025-01-07 16:40:04,215: Snapshot:1	Epoch:4	Loss:0.719	translation_Loss:0.604	token_training_loss:0.0	distillation_Loss:0.115                                                   	MRR:16.31	Hits@10:31.0	Best:16.31
2025-01-07 16:40:08,062: Snapshot:1	Epoch:5	Loss:0.552	translation_Loss:0.462	token_training_loss:0.0	distillation_Loss:0.089                                                   	MRR:16.85	Hits@10:31.85	Best:16.85
2025-01-07 16:40:11,735: Snapshot:1	Epoch:6	Loss:0.463	translation_Loss:0.383	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:17.15	Hits@10:32.22	Best:17.15
2025-01-07 16:40:15,330: Snapshot:1	Epoch:7	Loss:0.407	translation_Loss:0.332	token_training_loss:0.0	distillation_Loss:0.074                                                   	MRR:17.27	Hits@10:32.9	Best:17.27
2025-01-07 16:40:19,414: Snapshot:1	Epoch:8	Loss:0.38	translation_Loss:0.309	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:17.53	Hits@10:33.06	Best:17.53
2025-01-07 16:40:22,995: Snapshot:1	Epoch:9	Loss:0.355	translation_Loss:0.284	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:17.68	Hits@10:33.34	Best:17.68
2025-01-07 16:40:26,571: Snapshot:1	Epoch:10	Loss:0.328	translation_Loss:0.26	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:17.72	Hits@10:33.69	Best:17.72
2025-01-07 16:40:30,175: Snapshot:1	Epoch:11	Loss:0.313	translation_Loss:0.246	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.78	Hits@10:33.59	Best:17.78
2025-01-07 16:40:33,702: Snapshot:1	Epoch:12	Loss:0.294	translation_Loss:0.229	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:17.75	Hits@10:33.52	Best:17.78
2025-01-07 16:40:37,676: Snapshot:1	Epoch:13	Loss:0.29	translation_Loss:0.225	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:17.8	Hits@10:33.39	Best:17.8
2025-01-07 16:40:41,286: Snapshot:1	Epoch:14	Loss:0.283	translation_Loss:0.219	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:17.73	Hits@10:33.47	Best:17.8
2025-01-07 16:40:44,838: Snapshot:1	Epoch:15	Loss:0.268	translation_Loss:0.205	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:17.77	Hits@10:33.64	Best:17.8
2025-01-07 16:40:48,383: Snapshot:1	Epoch:16	Loss:0.262	translation_Loss:0.2	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.94	Hits@10:33.76	Best:17.94
2025-01-07 16:40:51,901: Snapshot:1	Epoch:17	Loss:0.254	translation_Loss:0.192	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.96	Hits@10:34.02	Best:17.96
2025-01-07 16:40:55,842: Snapshot:1	Epoch:18	Loss:0.249	translation_Loss:0.188	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:17.98	Hits@10:34.09	Best:17.98
2025-01-07 16:40:59,304: Snapshot:1	Epoch:19	Loss:0.245	translation_Loss:0.185	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:17.95	Hits@10:33.92	Best:17.98
2025-01-07 16:41:02,884: Snapshot:1	Epoch:20	Loss:0.238	translation_Loss:0.177	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:18.09	Hits@10:34.03	Best:18.09
2025-01-07 16:41:06,393: Snapshot:1	Epoch:21	Loss:0.239	translation_Loss:0.178	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:18.04	Hits@10:34.26	Best:18.09
2025-01-07 16:41:09,884: Snapshot:1	Epoch:22	Loss:0.233	translation_Loss:0.172	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:18.0	Hits@10:34.15	Best:18.09
2025-01-07 16:41:13,823: Early Stopping! Snapshot: 1 Epoch: 23 Best Results: 18.09
2025-01-07 16:41:13,823: Start to training tokens! Snapshot: 1 Epoch: 23 Loss:0.228 MRR:18.09 Best Results: 18.09
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:41:13,823: Snapshot:1	Epoch:23	Loss:0.228	translation_Loss:0.168	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.09	Hits@10:34.01	Best:18.09
2025-01-07 16:41:17,255: Snapshot:1	Epoch:24	Loss:20.989	translation_Loss:6.182	token_training_loss:14.807	distillation_Loss:0.0                                                   	MRR:18.09	Hits@10:34.01	Best:18.09
2025-01-07 16:41:20,688: End of token training: 1 Epoch: 25 Loss:8.126 MRR:18.09 Best Results: 18.09
2025-01-07 16:41:20,688: Snapshot:1	Epoch:25	Loss:8.126	translation_Loss:6.184	token_training_loss:1.942	distillation_Loss:0.0                                                           	MRR:18.09	Hits@10:34.01	Best:18.09
2025-01-07 16:41:21,054: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:41:29,270: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2559 | 0.1481 | 0.3016 | 0.3752 |  0.4661 |
|     1      | 0.1887 | 0.1102 | 0.2117 | 0.2661 |  0.3505 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,349,600
Trainable params: 3,200
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:41:40,183: Snapshot:2	Epoch:0	Loss:4.458	translation_Loss:4.057	token_training_loss:0.0	distillation_Loss:0.401                                                   	MRR:18.35	Hits@10:33.88	Best:18.35
2025-01-07 16:41:44,011: Snapshot:2	Epoch:1	Loss:2.421	translation_Loss:1.955	token_training_loss:0.0	distillation_Loss:0.466                                                   	MRR:19.32	Hits@10:34.48	Best:19.32
2025-01-07 16:41:47,785: Snapshot:2	Epoch:2	Loss:1.544	translation_Loss:1.198	token_training_loss:0.0	distillation_Loss:0.346                                                   	MRR:19.55	Hits@10:35.16	Best:19.55
2025-01-07 16:41:51,616: Snapshot:2	Epoch:3	Loss:1.147	translation_Loss:0.861	token_training_loss:0.0	distillation_Loss:0.287                                                   	MRR:19.75	Hits@10:35.26	Best:19.75
2025-01-07 16:41:55,859: Snapshot:2	Epoch:4	Loss:1.012	translation_Loss:0.761	token_training_loss:0.0	distillation_Loss:0.251                                                   	MRR:19.83	Hits@10:35.49	Best:19.83
2025-01-07 16:41:59,696: Snapshot:2	Epoch:5	Loss:0.93	translation_Loss:0.685	token_training_loss:0.0	distillation_Loss:0.246                                                   	MRR:20.04	Hits@10:35.71	Best:20.04
2025-01-07 16:42:03,400: Snapshot:2	Epoch:6	Loss:0.897	translation_Loss:0.66	token_training_loss:0.0	distillation_Loss:0.237                                                   	MRR:19.9	Hits@10:35.74	Best:20.04
2025-01-07 16:42:07,062: Snapshot:2	Epoch:7	Loss:0.869	translation_Loss:0.628	token_training_loss:0.0	distillation_Loss:0.241                                                   	MRR:19.79	Hits@10:35.71	Best:20.04
2025-01-07 16:42:10,722: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 20.04
2025-01-07 16:42:10,723: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:0.857 MRR:19.76 Best Results: 20.04
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:42:10,723: Snapshot:2	Epoch:8	Loss:0.857	translation_Loss:0.617	token_training_loss:0.0	distillation_Loss:0.239                                                   	MRR:19.76	Hits@10:35.92	Best:20.04
2025-01-07 16:42:14,758: Snapshot:2	Epoch:9	Loss:21.692	translation_Loss:6.439	token_training_loss:15.253	distillation_Loss:0.0                                                   	MRR:19.76	Hits@10:35.92	Best:20.04
2025-01-07 16:42:18,342: End of token training: 2 Epoch: 10 Loss:8.496 MRR:19.76 Best Results: 20.04
2025-01-07 16:42:18,342: Snapshot:2	Epoch:10	Loss:8.496	translation_Loss:6.441	token_training_loss:2.055	distillation_Loss:0.0                                                           	MRR:19.76	Hits@10:35.92	Best:20.04
2025-01-07 16:42:18,606: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:42:28,841: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2553 | 0.1473 | 0.3014 | 0.3744 |  0.4642 |
|     1      | 0.1918 | 0.1103 | 0.2171 | 0.2712 |  0.3557 |
|     2      | 0.2019 | 0.1203 | 0.2305 | 0.2884 |  0.3616 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,724,800
Trainable params: 3,200
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:42:39,402: Snapshot:3	Epoch:0	Loss:2.955	translation_Loss:2.534	token_training_loss:0.0	distillation_Loss:0.42                                                   	MRR:20.77	Hits@10:36.98	Best:20.77
2025-01-07 16:42:43,226: Snapshot:3	Epoch:1	Loss:1.877	translation_Loss:1.352	token_training_loss:0.0	distillation_Loss:0.525                                                   	MRR:21.21	Hits@10:37.11	Best:21.21
2025-01-07 16:42:47,455: Snapshot:3	Epoch:2	Loss:1.476	translation_Loss:1.068	token_training_loss:0.0	distillation_Loss:0.408                                                   	MRR:21.22	Hits@10:37.74	Best:21.22
2025-01-07 16:42:51,265: Snapshot:3	Epoch:3	Loss:1.227	translation_Loss:0.855	token_training_loss:0.0	distillation_Loss:0.372                                                   	MRR:21.22	Hits@10:37.56	Best:21.22
2025-01-07 16:42:55,061: Snapshot:3	Epoch:4	Loss:1.149	translation_Loss:0.8	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:21.16	Hits@10:37.6	Best:21.22
2025-01-07 16:42:58,889: Snapshot:3	Epoch:5	Loss:1.109	translation_Loss:0.761	token_training_loss:0.0	distillation_Loss:0.348                                                   	MRR:21.23	Hits@10:37.74	Best:21.23
2025-01-07 16:43:02,740: Snapshot:3	Epoch:6	Loss:1.102	translation_Loss:0.757	token_training_loss:0.0	distillation_Loss:0.345                                                   	MRR:21.33	Hits@10:37.59	Best:21.33
2025-01-07 16:43:06,961: Snapshot:3	Epoch:7	Loss:1.073	translation_Loss:0.73	token_training_loss:0.0	distillation_Loss:0.343                                                   	MRR:21.13	Hits@10:37.78	Best:21.33
2025-01-07 16:43:10,731: Snapshot:3	Epoch:8	Loss:1.076	translation_Loss:0.732	token_training_loss:0.0	distillation_Loss:0.344                                                   	MRR:21.26	Hits@10:37.57	Best:21.33
2025-01-07 16:43:14,544: Snapshot:3	Epoch:9	Loss:1.065	translation_Loss:0.721	token_training_loss:0.0	distillation_Loss:0.344                                                   	MRR:21.38	Hits@10:37.63	Best:21.38
2025-01-07 16:43:18,312: Snapshot:3	Epoch:10	Loss:1.057	translation_Loss:0.716	token_training_loss:0.0	distillation_Loss:0.341                                                   	MRR:21.2	Hits@10:37.67	Best:21.38
2025-01-07 16:43:22,083: Snapshot:3	Epoch:11	Loss:1.06	translation_Loss:0.715	token_training_loss:0.0	distillation_Loss:0.345                                                   	MRR:21.33	Hits@10:37.57	Best:21.38
2025-01-07 16:43:26,356: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 21.38
2025-01-07 16:43:26,357: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:1.063 MRR:21.17 Best Results: 21.38
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:43:26,357: Snapshot:3	Epoch:12	Loss:1.063	translation_Loss:0.721	token_training_loss:0.0	distillation_Loss:0.342                                                   	MRR:21.17	Hits@10:37.75	Best:21.38
2025-01-07 16:43:30,043: Snapshot:3	Epoch:13	Loss:20.36	translation_Loss:6.507	token_training_loss:13.853	distillation_Loss:0.0                                                   	MRR:21.17	Hits@10:37.75	Best:21.38
2025-01-07 16:43:33,742: End of token training: 3 Epoch: 14 Loss:8.076 MRR:21.17 Best Results: 21.38
2025-01-07 16:43:33,742: Snapshot:3	Epoch:14	Loss:8.076	translation_Loss:6.51	token_training_loss:1.566	distillation_Loss:0.0                                                           	MRR:21.17	Hits@10:37.75	Best:21.38
2025-01-07 16:43:34,107: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 16:43:46,315: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2526 | 0.1452 | 0.2977 | 0.3704 |  0.4612 |
|     1      | 0.196  | 0.1122 | 0.2213 | 0.2774 |  0.3651 |
|     2      | 0.2033 | 0.1199 | 0.2326 | 0.2917 |  0.3669 |
|     3      | 0.2146 | 0.1308 | 0.2469 | 0.3014 |  0.3747 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,912,400
Trainable params: 3,200
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:43:57,338: Snapshot:4	Epoch:0	Loss:2.081	translation_Loss:1.541	token_training_loss:0.0	distillation_Loss:0.541                                                   	MRR:23.17	Hits@10:38.91	Best:23.17
2025-01-07 16:44:01,240: Snapshot:4	Epoch:1	Loss:1.594	translation_Loss:1.156	token_training_loss:0.0	distillation_Loss:0.438                                                   	MRR:23.27	Hits@10:39.08	Best:23.27
2025-01-07 16:44:05,136: Snapshot:4	Epoch:2	Loss:1.434	translation_Loss:1.101	token_training_loss:0.0	distillation_Loss:0.333                                                   	MRR:23.25	Hits@10:38.92	Best:23.27
2025-01-07 16:44:09,004: Snapshot:4	Epoch:3	Loss:1.333	translation_Loss:1.001	token_training_loss:0.0	distillation_Loss:0.331                                                   	MRR:23.1	Hits@10:38.73	Best:23.27
2025-01-07 16:44:13,316: Snapshot:4	Epoch:4	Loss:1.287	translation_Loss:0.962	token_training_loss:0.0	distillation_Loss:0.325                                                   	MRR:23.39	Hits@10:39.12	Best:23.39
2025-01-07 16:44:17,188: Snapshot:4	Epoch:5	Loss:1.282	translation_Loss:0.965	token_training_loss:0.0	distillation_Loss:0.318                                                   	MRR:23.15	Hits@10:39.04	Best:23.39
2025-01-07 16:44:21,087: Snapshot:4	Epoch:6	Loss:1.274	translation_Loss:0.959	token_training_loss:0.0	distillation_Loss:0.315                                                   	MRR:23.41	Hits@10:38.96	Best:23.41
2025-01-07 16:44:24,925: Snapshot:4	Epoch:7	Loss:1.272	translation_Loss:0.949	token_training_loss:0.0	distillation_Loss:0.324                                                   	MRR:23.22	Hits@10:39.07	Best:23.41
2025-01-07 16:44:29,190: Snapshot:4	Epoch:8	Loss:1.265	translation_Loss:0.944	token_training_loss:0.0	distillation_Loss:0.32                                                   	MRR:23.21	Hits@10:39.06	Best:23.41
2025-01-07 16:44:33,217: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 23.41
2025-01-07 16:44:33,217: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:1.272 MRR:23.33 Best Results: 23.41
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:44:33,217: Snapshot:4	Epoch:9	Loss:1.272	translation_Loss:0.947	token_training_loss:0.0	distillation_Loss:0.325                                                   	MRR:23.33	Hits@10:39.14	Best:23.41
2025-01-07 16:44:36,947: Snapshot:4	Epoch:10	Loss:21.701	translation_Loss:6.679	token_training_loss:15.022	distillation_Loss:0.0                                                   	MRR:23.33	Hits@10:39.14	Best:23.41
2025-01-07 16:44:40,683: End of token training: 4 Epoch: 11 Loss:8.739 MRR:23.33 Best Results: 23.41
2025-01-07 16:44:40,683: Snapshot:4	Epoch:11	Loss:8.739	translation_Loss:6.685	token_training_loss:2.054	distillation_Loss:0.0                                                           	MRR:23.33	Hits@10:39.14	Best:23.41
2025-01-07 16:44:41,043: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 16:44:55,653: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.252  | 0.1447 | 0.2969 | 0.3697 |  0.4604 |
|     1      | 0.1982 | 0.1132 | 0.2259 | 0.282  |  0.367  |
|     2      | 0.2066 | 0.1239 | 0.2335 | 0.2944 |  0.3701 |
|     3      | 0.217  | 0.1316 | 0.2494 | 0.3067 |  0.3805 |
|     4      | 0.2365 | 0.1445 | 0.2743 | 0.3354 |  0.4107 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,200
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,006,200
Trainable params: 3,200
Non-trainable params: 3,003,000
=================================================================
2025-01-07 16:44:55,656: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1494 | 0.3041 | 0.376  |  0.4671 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2559 | 0.1481 | 0.3016 | 0.3752 |  0.4661 |
|     1      | 0.1887 | 0.1102 | 0.2117 | 0.2661 |  0.3505 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2553 | 0.1473 | 0.3014 | 0.3744 |  0.4642 |
|     1      | 0.1918 | 0.1103 | 0.2171 | 0.2712 |  0.3557 |
|     2      | 0.2019 | 0.1203 | 0.2305 | 0.2884 |  0.3616 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2526 | 0.1452 | 0.2977 | 0.3704 |  0.4612 |
|     1      | 0.196  | 0.1122 | 0.2213 | 0.2774 |  0.3651 |
|     2      | 0.2033 | 0.1199 | 0.2326 | 0.2917 |  0.3669 |
|     3      | 0.2146 | 0.1308 | 0.2469 | 0.3014 |  0.3747 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.252  | 0.1447 | 0.2969 | 0.3697 |  0.4604 |
|     1      | 0.1982 | 0.1132 | 0.2259 | 0.282  |  0.367  |
|     2      | 0.2066 | 0.1239 | 0.2335 | 0.2944 |  0.3701 |
|     3      | 0.217  | 0.1316 | 0.2494 | 0.3067 |  0.3805 |
|     4      | 0.2365 | 0.1445 | 0.2743 | 0.3354 |  0.4107 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 16:44:55,657: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 179.12633728981018 |   0.258   |    0.149     |    0.304     |     0.467     |
|    1     | 99.56552076339722  |   0.246   |    0.143     |    0.289     |      0.45     |
|    2     | 47.11521601676941  |   0.241   |    0.139     |    0.282     |     0.438     |
|    3     | 62.82571339607239  |   0.237   |    0.137     |    0.276     |      0.43     |
|    4     | 52.27161526679993  |   0.237   |    0.138     |    0.276     |     0.429     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 16:44:55,657: Sum_Training_Time:440.9044027328491
2025-01-07 16:44:55,657: Every_Training_Time:[179.12633728981018, 99.56552076339722, 47.11521601676941, 62.82571339607239, 52.27161526679993]
2025-01-07 16:44:55,657: Forward transfer: 0.1638 Backward transfer: 0.0027749999999999928
2025-01-07 16:45:19,278: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107164459/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=9, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 16:45:43,370: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.2	Hits@10:34.63	Best:15.2
2025-01-07 16:46:02,644: Snapshot:0	Epoch:1	Loss:18.581	translation_Loss:18.581	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.77	Hits@10:45.21	Best:23.77
2025-01-07 16:46:21,870: Snapshot:0	Epoch:2	Loss:7.138	translation_Loss:7.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.49	Hits@10:46.83	Best:25.49
2025-01-07 16:46:41,169: Snapshot:0	Epoch:3	Loss:3.626	translation_Loss:3.626	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.81	Hits@10:47.02	Best:25.81
2025-01-07 16:47:00,745: Snapshot:0	Epoch:4	Loss:2.339	translation_Loss:2.339	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.71	Hits@10:46.62	Best:25.81
2025-01-07 16:47:19,816: Snapshot:0	Epoch:5	Loss:1.744	translation_Loss:1.744	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.48	Hits@10:46.45	Best:25.81
2025-01-07 16:47:38,486: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.81
2025-01-07 16:47:38,486: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.416 MRR:25.42 Best Results: 25.81
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:47:38,486: Snapshot:0	Epoch:6	Loss:1.416	translation_Loss:1.416	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.42	Hits@10:46.17	Best:25.81
2025-01-07 16:47:57,800: Snapshot:0	Epoch:7	Loss:53.973	translation_Loss:36.15	token_training_loss:17.823	distillation_Loss:0.0                                                   	MRR:25.42	Hits@10:46.17	Best:25.81
2025-01-07 16:48:17,081: End of token training: 0 Epoch: 8 Loss:36.161 MRR:25.42 Best Results: 25.81
2025-01-07 16:48:17,082: Snapshot:0	Epoch:8	Loss:36.161	translation_Loss:36.158	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.42	Hits@10:46.17	Best:25.81
2025-01-07 16:48:17,392: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 16:48:23,783: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.258 | 0.1504 | 0.3043 | 0.3766 |  0.468  |
+------------+-------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,599,400
Trainable params: 3,600
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:48:34,075: Snapshot:1	Epoch:0	Loss:6.6	translation_Loss:6.227	token_training_loss:0.0	distillation_Loss:0.373                                                   	MRR:10.0	Hits@10:19.41	Best:10.0
2025-01-07 16:48:37,535: Snapshot:1	Epoch:1	Loss:3.436	translation_Loss:3.11	token_training_loss:0.0	distillation_Loss:0.326                                                   	MRR:12.84	Hits@10:23.77	Best:12.84
2025-01-07 16:48:41,015: Snapshot:1	Epoch:2	Loss:1.774	translation_Loss:1.537	token_training_loss:0.0	distillation_Loss:0.237                                                   	MRR:14.62	Hits@10:26.94	Best:14.62
2025-01-07 16:48:45,056: Snapshot:1	Epoch:3	Loss:1.043	translation_Loss:0.883	token_training_loss:0.0	distillation_Loss:0.16                                                   	MRR:15.67	Hits@10:29.25	Best:15.67
2025-01-07 16:48:48,622: Snapshot:1	Epoch:4	Loss:0.718	translation_Loss:0.608	token_training_loss:0.0	distillation_Loss:0.11                                                   	MRR:16.32	Hits@10:30.93	Best:16.32
2025-01-07 16:48:52,206: Snapshot:1	Epoch:5	Loss:0.551	translation_Loss:0.465	token_training_loss:0.0	distillation_Loss:0.086                                                   	MRR:16.87	Hits@10:31.87	Best:16.87
2025-01-07 16:48:55,809: Snapshot:1	Epoch:6	Loss:0.462	translation_Loss:0.385	token_training_loss:0.0	distillation_Loss:0.077                                                   	MRR:17.09	Hits@10:32.43	Best:17.09
2025-01-07 16:48:59,345: Snapshot:1	Epoch:7	Loss:0.406	translation_Loss:0.334	token_training_loss:0.0	distillation_Loss:0.072                                                   	MRR:17.22	Hits@10:32.83	Best:17.22
2025-01-07 16:49:03,321: Snapshot:1	Epoch:8	Loss:0.38	translation_Loss:0.31	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:17.54	Hits@10:33.16	Best:17.54
2025-01-07 16:49:06,826: Snapshot:1	Epoch:9	Loss:0.353	translation_Loss:0.285	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:17.6	Hits@10:33.38	Best:17.6
2025-01-07 16:49:10,308: Snapshot:1	Epoch:10	Loss:0.327	translation_Loss:0.261	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.7	Hits@10:33.4	Best:17.7
2025-01-07 16:49:13,759: Snapshot:1	Epoch:11	Loss:0.312	translation_Loss:0.248	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:17.81	Hits@10:33.64	Best:17.81
2025-01-07 16:49:17,235: Snapshot:1	Epoch:12	Loss:0.293	translation_Loss:0.231	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:17.87	Hits@10:33.49	Best:17.87
2025-01-07 16:49:21,106: Snapshot:1	Epoch:13	Loss:0.291	translation_Loss:0.227	token_training_loss:0.0	distillation_Loss:0.064                                                   	MRR:17.84	Hits@10:33.45	Best:17.87
2025-01-07 16:49:24,577: Snapshot:1	Epoch:14	Loss:0.284	translation_Loss:0.221	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:17.89	Hits@10:33.76	Best:17.89
2025-01-07 16:49:27,990: Snapshot:1	Epoch:15	Loss:0.267	translation_Loss:0.205	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.75	Hits@10:33.76	Best:17.89
2025-01-07 16:49:31,577: Snapshot:1	Epoch:16	Loss:0.262	translation_Loss:0.202	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:17.86	Hits@10:33.78	Best:17.89
2025-01-07 16:49:35,227: Snapshot:1	Epoch:17	Loss:0.256	translation_Loss:0.194	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:18.01	Hits@10:33.93	Best:18.01
2025-01-07 16:49:39,330: Snapshot:1	Epoch:18	Loss:0.249	translation_Loss:0.189	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.16	Hits@10:34.04	Best:18.16
2025-01-07 16:49:42,946: Snapshot:1	Epoch:19	Loss:0.244	translation_Loss:0.186	token_training_loss:0.0	distillation_Loss:0.058                                                   	MRR:18.14	Hits@10:34.18	Best:18.16
2025-01-07 16:49:46,591: Snapshot:1	Epoch:20	Loss:0.237	translation_Loss:0.178	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:18.21	Hits@10:34.28	Best:18.21
2025-01-07 16:49:50,214: Snapshot:1	Epoch:21	Loss:0.24	translation_Loss:0.181	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.05	Hits@10:34.11	Best:18.21
2025-01-07 16:49:53,828: Snapshot:1	Epoch:22	Loss:0.233	translation_Loss:0.172	token_training_loss:0.0	distillation_Loss:0.061                                                   	MRR:18.14	Hits@10:34.22	Best:18.21
2025-01-07 16:49:57,900: Snapshot:1	Epoch:23	Loss:0.228	translation_Loss:0.17	token_training_loss:0.0	distillation_Loss:0.058                                                   	MRR:18.22	Hits@10:34.22	Best:18.22
2025-01-07 16:50:01,501: Snapshot:1	Epoch:24	Loss:0.229	translation_Loss:0.171	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:18.25	Hits@10:34.08	Best:18.25
2025-01-07 16:50:05,114: Snapshot:1	Epoch:25	Loss:0.226	translation_Loss:0.167	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:18.24	Hits@10:34.21	Best:18.25
2025-01-07 16:50:08,798: Snapshot:1	Epoch:26	Loss:0.225	translation_Loss:0.164	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.23	Hits@10:34.2	Best:18.25
2025-01-07 16:50:12,451: Snapshot:1	Epoch:27	Loss:0.224	translation_Loss:0.164	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.28	Hits@10:34.6	Best:18.28
2025-01-07 16:50:16,599: Snapshot:1	Epoch:28	Loss:0.221	translation_Loss:0.161	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.22	Hits@10:34.7	Best:18.28
2025-01-07 16:50:20,178: Snapshot:1	Epoch:29	Loss:0.221	translation_Loss:0.161	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.33	Hits@10:34.53	Best:18.33
2025-01-07 16:50:23,732: Snapshot:1	Epoch:30	Loss:0.219	translation_Loss:0.159	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:18.3	Hits@10:34.38	Best:18.33
2025-01-07 16:50:27,211: Snapshot:1	Epoch:31	Loss:0.215	translation_Loss:0.156	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:18.3	Hits@10:34.42	Best:18.33
2025-01-07 16:50:30,761: Early Stopping! Snapshot: 1 Epoch: 32 Best Results: 18.33
2025-01-07 16:50:30,761: Start to training tokens! Snapshot: 1 Epoch: 32 Loss:0.211 MRR:18.28 Best Results: 18.33
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:50:30,761: Snapshot:1	Epoch:32	Loss:0.211	translation_Loss:0.152	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:18.28	Hits@10:34.53	Best:18.33
2025-01-07 16:50:34,630: Snapshot:1	Epoch:33	Loss:21.268	translation_Loss:6.145	token_training_loss:15.123	distillation_Loss:0.0                                                   	MRR:18.28	Hits@10:34.53	Best:18.33
2025-01-07 16:50:38,053: End of token training: 1 Epoch: 34 Loss:8.242 MRR:18.28 Best Results: 18.33
2025-01-07 16:50:38,053: Snapshot:1	Epoch:34	Loss:8.242	translation_Loss:6.141	token_training_loss:2.101	distillation_Loss:0.0                                                           	MRR:18.28	Hits@10:34.53	Best:18.33
2025-01-07 16:50:38,371: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:50:47,101: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2566 | 0.1485 | 0.3025 | 0.3752 |  0.467  |
|     1      | 0.1891 | 0.1088 | 0.2142 | 0.2707 |  0.3522 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,350,000
Trainable params: 3,600
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:50:57,457: Snapshot:2	Epoch:0	Loss:4.458	translation_Loss:4.047	token_training_loss:0.0	distillation_Loss:0.411                                                   	MRR:18.48	Hits@10:33.79	Best:18.48
2025-01-07 16:51:01,603: Snapshot:2	Epoch:1	Loss:2.429	translation_Loss:1.989	token_training_loss:0.0	distillation_Loss:0.44                                                   	MRR:19.39	Hits@10:34.89	Best:19.39
2025-01-07 16:51:05,316: Snapshot:2	Epoch:2	Loss:1.553	translation_Loss:1.233	token_training_loss:0.0	distillation_Loss:0.32                                                   	MRR:19.79	Hits@10:35.46	Best:19.79
2025-01-07 16:51:09,038: Snapshot:2	Epoch:3	Loss:1.16	translation_Loss:0.891	token_training_loss:0.0	distillation_Loss:0.269                                                   	MRR:19.89	Hits@10:36.05	Best:19.89
2025-01-07 16:51:12,766: Snapshot:2	Epoch:4	Loss:1.017	translation_Loss:0.778	token_training_loss:0.0	distillation_Loss:0.239                                                   	MRR:20.0	Hits@10:36.0	Best:20.0
2025-01-07 16:51:16,436: Snapshot:2	Epoch:5	Loss:0.934	translation_Loss:0.704	token_training_loss:0.0	distillation_Loss:0.23                                                   	MRR:20.0	Hits@10:36.23	Best:20.0
2025-01-07 16:51:20,644: Snapshot:2	Epoch:6	Loss:0.896	translation_Loss:0.674	token_training_loss:0.0	distillation_Loss:0.222                                                   	MRR:20.02	Hits@10:36.09	Best:20.02
2025-01-07 16:51:24,354: Snapshot:2	Epoch:7	Loss:0.88	translation_Loss:0.654	token_training_loss:0.0	distillation_Loss:0.226                                                   	MRR:20.07	Hits@10:36.12	Best:20.07
2025-01-07 16:51:28,018: Snapshot:2	Epoch:8	Loss:0.86	translation_Loss:0.637	token_training_loss:0.0	distillation_Loss:0.223                                                   	MRR:19.99	Hits@10:35.88	Best:20.07
2025-01-07 16:51:31,676: Snapshot:2	Epoch:9	Loss:0.842	translation_Loss:0.62	token_training_loss:0.0	distillation_Loss:0.222                                                   	MRR:19.95	Hits@10:36.09	Best:20.07
2025-01-07 16:51:35,287: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 20.07
2025-01-07 16:51:35,287: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.84 MRR:20.03 Best Results: 20.07
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:51:35,287: Snapshot:2	Epoch:10	Loss:0.84	translation_Loss:0.615	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:20.03	Hits@10:35.92	Best:20.07
2025-01-07 16:51:39,330: Snapshot:2	Epoch:11	Loss:22.011	translation_Loss:6.422	token_training_loss:15.589	distillation_Loss:0.0                                                   	MRR:20.03	Hits@10:35.92	Best:20.07
2025-01-07 16:51:42,902: End of token training: 2 Epoch: 12 Loss:8.691 MRR:20.03 Best Results: 20.07
2025-01-07 16:51:42,902: Snapshot:2	Epoch:12	Loss:8.691	translation_Loss:6.416	token_training_loss:2.275	distillation_Loss:0.0                                                           	MRR:20.03	Hits@10:35.92	Best:20.07
2025-01-07 16:51:43,216: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:51:53,436: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2565 | 0.1491 | 0.3021 | 0.3748 |  0.4644 |
|     1      | 0.194  | 0.1112 | 0.2207 | 0.2776 |  0.3597 |
|     2      | 0.2019 | 0.1203 | 0.231  | 0.2881 |  0.3622 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,725,200
Trainable params: 3,600
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:52:04,061: Snapshot:3	Epoch:0	Loss:2.94	translation_Loss:2.509	token_training_loss:0.0	distillation_Loss:0.432                                                   	MRR:20.9	Hits@10:37.26	Best:20.9
2025-01-07 16:52:07,871: Snapshot:3	Epoch:1	Loss:1.892	translation_Loss:1.393	token_training_loss:0.0	distillation_Loss:0.499                                                   	MRR:21.32	Hits@10:37.22	Best:21.32
2025-01-07 16:52:11,678: Snapshot:3	Epoch:2	Loss:1.471	translation_Loss:1.095	token_training_loss:0.0	distillation_Loss:0.377                                                   	MRR:21.48	Hits@10:37.56	Best:21.48
2025-01-07 16:52:15,957: Snapshot:3	Epoch:3	Loss:1.231	translation_Loss:0.886	token_training_loss:0.0	distillation_Loss:0.345                                                   	MRR:21.39	Hits@10:37.61	Best:21.48
2025-01-07 16:52:19,747: Snapshot:3	Epoch:4	Loss:1.164	translation_Loss:0.837	token_training_loss:0.0	distillation_Loss:0.328                                                   	MRR:21.39	Hits@10:37.62	Best:21.48
2025-01-07 16:52:23,627: Early Stopping! Snapshot: 3 Epoch: 5 Best Results: 21.48
2025-01-07 16:52:23,627: Start to training tokens! Snapshot: 3 Epoch: 5 Loss:1.117 MRR:21.36 Best Results: 21.48
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:52:23,627: Snapshot:3	Epoch:5	Loss:1.117	translation_Loss:0.79	token_training_loss:0.0	distillation_Loss:0.327                                                   	MRR:21.36	Hits@10:37.87	Best:21.48
2025-01-07 16:52:27,329: Snapshot:3	Epoch:6	Loss:21.004	translation_Loss:6.561	token_training_loss:14.443	distillation_Loss:0.0                                                   	MRR:21.36	Hits@10:37.87	Best:21.48
2025-01-07 16:52:31,021: End of token training: 3 Epoch: 7 Loss:8.359 MRR:21.36 Best Results: 21.48
2025-01-07 16:52:31,021: Snapshot:3	Epoch:7	Loss:8.359	translation_Loss:6.545	token_training_loss:1.814	distillation_Loss:0.0                                                           	MRR:21.36	Hits@10:37.87	Best:21.48
2025-01-07 16:52:31,384: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 16:52:44,272: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1491 | 0.3009 | 0.3733 |  0.4628 |
|     1      | 0.1976 | 0.1133 | 0.2255 | 0.2824 |  0.3648 |
|     2      | 0.2043 | 0.1212 | 0.2311 | 0.2927 |  0.3709 |
|     3      | 0.2137 | 0.1295 | 0.2485 | 0.3047 |  0.372  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,912,800
Trainable params: 3,600
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:52:54,976: Snapshot:4	Epoch:0	Loss:2.132	translation_Loss:1.567	token_training_loss:0.0	distillation_Loss:0.565                                                   	MRR:22.99	Hits@10:39.07	Best:22.99
2025-01-07 16:52:58,839: Snapshot:4	Epoch:1	Loss:1.668	translation_Loss:1.246	token_training_loss:0.0	distillation_Loss:0.421                                                   	MRR:23.09	Hits@10:38.9	Best:23.09
2025-01-07 16:53:02,822: Snapshot:4	Epoch:2	Loss:1.471	translation_Loss:1.152	token_training_loss:0.0	distillation_Loss:0.319                                                   	MRR:23.37	Hits@10:38.97	Best:23.37
2025-01-07 16:53:07,195: Snapshot:4	Epoch:3	Loss:1.362	translation_Loss:1.048	token_training_loss:0.0	distillation_Loss:0.314                                                   	MRR:23.34	Hits@10:38.98	Best:23.37
2025-01-07 16:53:11,111: Snapshot:4	Epoch:4	Loss:1.332	translation_Loss:1.022	token_training_loss:0.0	distillation_Loss:0.31                                                   	MRR:23.15	Hits@10:38.93	Best:23.37
2025-01-07 16:53:15,005: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 23.37
2025-01-07 16:53:15,006: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:1.326 MRR:23.09 Best Results: 23.37
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:53:15,006: Snapshot:4	Epoch:5	Loss:1.326	translation_Loss:1.024	token_training_loss:0.0	distillation_Loss:0.303                                                   	MRR:23.09	Hits@10:38.96	Best:23.37
2025-01-07 16:53:18,746: Snapshot:4	Epoch:6	Loss:22.053	translation_Loss:6.75	token_training_loss:15.302	distillation_Loss:0.0                                                   	MRR:23.09	Hits@10:38.96	Best:23.37
2025-01-07 16:53:22,469: End of token training: 4 Epoch: 7 Loss:8.931 MRR:23.09 Best Results: 23.37
2025-01-07 16:53:22,469: Snapshot:4	Epoch:7	Loss:8.931	translation_Loss:6.757	token_training_loss:2.174	distillation_Loss:0.0                                                           	MRR:23.09	Hits@10:38.96	Best:23.37
2025-01-07 16:53:22,714: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 16:53:37,458: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2548 | 0.1483 | 0.2997 | 0.3713 |  0.4617 |
|     1      | 0.2003 | 0.1156 | 0.2296 | 0.2847 |  0.3712 |
|     2      | 0.2057 | 0.1206 | 0.2359 | 0.2945 |  0.3743 |
|     3      | 0.215  | 0.129  | 0.2497 | 0.3075 |  0.3761 |
|     4      | 0.2359 | 0.144  | 0.2762 | 0.3315 |  0.4105 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   3,600
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,006,600
Trainable params: 3,600
Non-trainable params: 3,003,000
=================================================================
2025-01-07 16:53:37,461: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.258 | 0.1504 | 0.3043 | 0.3766 |  0.468  |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2566 | 0.1485 | 0.3025 | 0.3752 |  0.467  |
|     1      | 0.1891 | 0.1088 | 0.2142 | 0.2707 |  0.3522 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2565 | 0.1491 | 0.3021 | 0.3748 |  0.4644 |
|     1      | 0.194  | 0.1112 | 0.2207 | 0.2776 |  0.3597 |
|     2      | 0.2019 | 0.1203 | 0.231  | 0.2881 |  0.3622 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1491 | 0.3009 | 0.3733 |  0.4628 |
|     1      | 0.1976 | 0.1133 | 0.2255 | 0.2824 |  0.3648 |
|     2      | 0.2043 | 0.1212 | 0.2311 | 0.2927 |  0.3709 |
|     3      | 0.2137 | 0.1295 | 0.2485 | 0.3047 |  0.372  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2548 | 0.1483 | 0.2997 | 0.3713 |  0.4617 |
|     1      | 0.2003 | 0.1156 | 0.2296 | 0.2847 |  0.3712 |
|     2      | 0.2057 | 0.1206 | 0.2359 | 0.2945 |  0.3743 |
|     3      | 0.215  | 0.129  | 0.2497 | 0.3075 |  0.3761 |
|     4      | 0.2359 | 0.144  | 0.2762 | 0.3315 |  0.4105 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 16:53:37,461: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 177.8026463985443  |   0.258   |     0.15     |    0.304     |     0.468     |
|    1     | 132.13737440109253 |   0.247   |    0.143     |     0.29     |     0.451     |
|    2     | 53.827550172805786 |   0.242   |    0.141     |    0.283     |     0.439     |
|    3     | 35.49680757522583  |   0.239   |     0.14     |    0.279     |     0.432     |
|    4     | 36.074145555496216 |   0.239   |     0.14     |    0.279     |      0.43     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 16:53:37,462: Sum_Training_Time:435.3385241031647
2025-01-07 16:53:37,462: Every_Training_Time:[177.8026463985443, 132.13737440109253, 53.827550172805786, 35.49680757522583, 36.074145555496216]
2025-01-07 16:53:37,462: Forward transfer: 0.1639 Backward transfer: 0.003275000000000007
2025-01-07 16:54:01,091: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107165341/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 16:54:25,107: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.2	Hits@10:34.63	Best:15.2
2025-01-07 16:54:44,389: Snapshot:0	Epoch:1	Loss:18.582	translation_Loss:18.582	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.77	Hits@10:45.26	Best:23.77
2025-01-07 16:55:03,611: Snapshot:0	Epoch:2	Loss:7.139	translation_Loss:7.139	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.51	Hits@10:46.75	Best:25.51
2025-01-07 16:55:22,935: Snapshot:0	Epoch:3	Loss:3.627	translation_Loss:3.627	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.81	Hits@10:47.01	Best:25.81
2025-01-07 16:55:42,565: Snapshot:0	Epoch:4	Loss:2.339	translation_Loss:2.339	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.72	Hits@10:46.51	Best:25.81
2025-01-07 16:56:01,774: Snapshot:0	Epoch:5	Loss:1.745	translation_Loss:1.745	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.5	Hits@10:46.38	Best:25.81
2025-01-07 16:56:21,037: Early Stopping! Snapshot: 0 Epoch: 6 Best Results: 25.81
2025-01-07 16:56:21,038: Start to training tokens! Snapshot: 0 Epoch: 6 Loss:1.416 MRR:25.5 Best Results: 25.81
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:56:21,038: Snapshot:0	Epoch:6	Loss:1.416	translation_Loss:1.416	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.5	Hits@10:46.3	Best:25.81
2025-01-07 16:56:40,812: Snapshot:0	Epoch:7	Loss:54.209	translation_Loss:36.142	token_training_loss:18.068	distillation_Loss:0.0                                                   	MRR:25.5	Hits@10:46.3	Best:25.81
2025-01-07 16:57:00,081: End of token training: 0 Epoch: 8 Loss:36.152 MRR:25.5 Best Results: 25.81
2025-01-07 16:57:00,082: Snapshot:0	Epoch:8	Loss:36.152	translation_Loss:36.148	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.5	Hits@10:46.3	Best:25.81
2025-01-07 16:57:00,392: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 16:57:07,197: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.1495 | 0.3042 | 0.3757 |  0.4679 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,599,800
Trainable params: 4,000
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:57:17,672: Snapshot:1	Epoch:0	Loss:6.615	translation_Loss:6.23	token_training_loss:0.0	distillation_Loss:0.385                                                   	MRR:10.04	Hits@10:19.52	Best:10.04
2025-01-07 16:57:21,209: Snapshot:1	Epoch:1	Loss:3.452	translation_Loss:3.131	token_training_loss:0.0	distillation_Loss:0.321                                                   	MRR:12.84	Hits@10:23.68	Best:12.84
2025-01-07 16:57:24,846: Snapshot:1	Epoch:2	Loss:1.778	translation_Loss:1.547	token_training_loss:0.0	distillation_Loss:0.231                                                   	MRR:14.56	Hits@10:27.0	Best:14.56
2025-01-07 16:57:28,914: Snapshot:1	Epoch:3	Loss:1.044	translation_Loss:0.891	token_training_loss:0.0	distillation_Loss:0.153                                                   	MRR:15.67	Hits@10:29.31	Best:15.67
2025-01-07 16:57:32,816: Snapshot:1	Epoch:4	Loss:0.719	translation_Loss:0.612	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:16.47	Hits@10:30.8	Best:16.47
2025-01-07 16:57:36,450: Snapshot:1	Epoch:5	Loss:0.552	translation_Loss:0.469	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:16.98	Hits@10:31.73	Best:16.98
2025-01-07 16:57:40,058: Snapshot:1	Epoch:6	Loss:0.462	translation_Loss:0.386	token_training_loss:0.0	distillation_Loss:0.075                                                   	MRR:17.14	Hits@10:32.39	Best:17.14
2025-01-07 16:57:43,582: Snapshot:1	Epoch:7	Loss:0.406	translation_Loss:0.336	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:17.32	Hits@10:32.67	Best:17.32
2025-01-07 16:57:47,634: Snapshot:1	Epoch:8	Loss:0.379	translation_Loss:0.312	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.59	Hits@10:32.97	Best:17.59
2025-01-07 16:57:51,206: Snapshot:1	Epoch:9	Loss:0.353	translation_Loss:0.287	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:17.66	Hits@10:33.3	Best:17.66
2025-01-07 16:57:54,741: Snapshot:1	Epoch:10	Loss:0.328	translation_Loss:0.263	token_training_loss:0.0	distillation_Loss:0.065                                                   	MRR:17.71	Hits@10:33.36	Best:17.71
2025-01-07 16:57:58,358: Snapshot:1	Epoch:11	Loss:0.312	translation_Loss:0.249	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:17.84	Hits@10:33.62	Best:17.84
2025-01-07 16:58:01,886: Snapshot:1	Epoch:12	Loss:0.296	translation_Loss:0.234	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.81	Hits@10:33.67	Best:17.84
2025-01-07 16:58:05,828: Snapshot:1	Epoch:13	Loss:0.292	translation_Loss:0.229	token_training_loss:0.0	distillation_Loss:0.063                                                   	MRR:17.83	Hits@10:33.59	Best:17.84
2025-01-07 16:58:09,303: Early Stopping! Snapshot: 1 Epoch: 14 Best Results: 17.84
2025-01-07 16:58:09,303: Start to training tokens! Snapshot: 1 Epoch: 14 Loss:0.285 MRR:17.8 Best Results: 17.84
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:58:09,304: Snapshot:1	Epoch:14	Loss:0.285	translation_Loss:0.223	token_training_loss:0.0	distillation_Loss:0.062                                                   	MRR:17.8	Hits@10:33.72	Best:17.84
2025-01-07 16:58:12,726: Snapshot:1	Epoch:15	Loss:21.758	translation_Loss:6.256	token_training_loss:15.502	distillation_Loss:0.0                                                   	MRR:17.8	Hits@10:33.72	Best:17.84
2025-01-07 16:58:16,141: End of token training: 1 Epoch: 16 Loss:8.52 MRR:17.8 Best Results: 17.84
2025-01-07 16:58:16,141: Snapshot:1	Epoch:16	Loss:8.52	translation_Loss:6.25	token_training_loss:2.27	distillation_Loss:0.0                                                           	MRR:17.8	Hits@10:33.72	Best:17.84
2025-01-07 16:58:16,456: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 16:58:24,604: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1496 | 0.3039 | 0.3765 |  0.4669 |
|     1      | 0.1852 | 0.1077 | 0.2086 | 0.2613 |  0.3433 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,350,400
Trainable params: 4,000
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:58:35,309: Snapshot:2	Epoch:0	Loss:4.531	translation_Loss:4.094	token_training_loss:0.0	distillation_Loss:0.437                                                   	MRR:18.16	Hits@10:33.37	Best:18.16
2025-01-07 16:58:38,997: Snapshot:2	Epoch:1	Loss:2.487	translation_Loss:2.021	token_training_loss:0.0	distillation_Loss:0.465                                                   	MRR:19.04	Hits@10:34.32	Best:19.04
2025-01-07 16:58:42,699: Snapshot:2	Epoch:2	Loss:1.604	translation_Loss:1.265	token_training_loss:0.0	distillation_Loss:0.339                                                   	MRR:19.28	Hits@10:34.78	Best:19.28
2025-01-07 16:58:46,373: Snapshot:2	Epoch:3	Loss:1.218	translation_Loss:0.931	token_training_loss:0.0	distillation_Loss:0.287                                                   	MRR:19.45	Hits@10:35.22	Best:19.45
2025-01-07 16:58:50,515: Snapshot:2	Epoch:4	Loss:1.065	translation_Loss:0.81	token_training_loss:0.0	distillation_Loss:0.255                                                   	MRR:19.45	Hits@10:35.13	Best:19.45
2025-01-07 16:58:54,198: Snapshot:2	Epoch:5	Loss:0.987	translation_Loss:0.739	token_training_loss:0.0	distillation_Loss:0.248                                                   	MRR:19.56	Hits@10:35.35	Best:19.56
2025-01-07 16:58:57,902: Snapshot:2	Epoch:6	Loss:0.95	translation_Loss:0.707	token_training_loss:0.0	distillation_Loss:0.243                                                   	MRR:19.62	Hits@10:35.25	Best:19.62
2025-01-07 16:59:01,661: Snapshot:2	Epoch:7	Loss:0.929	translation_Loss:0.687	token_training_loss:0.0	distillation_Loss:0.242                                                   	MRR:19.71	Hits@10:35.49	Best:19.71
2025-01-07 16:59:05,431: Snapshot:2	Epoch:8	Loss:0.905	translation_Loss:0.663	token_training_loss:0.0	distillation_Loss:0.242                                                   	MRR:19.56	Hits@10:35.34	Best:19.71
2025-01-07 16:59:09,692: Snapshot:2	Epoch:9	Loss:0.886	translation_Loss:0.644	token_training_loss:0.0	distillation_Loss:0.242                                                   	MRR:19.64	Hits@10:35.53	Best:19.71
2025-01-07 16:59:13,406: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 19.71
2025-01-07 16:59:13,406: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.885 MRR:19.57 Best Results: 19.71
Token added to optimizer, embeddings excluded successfully.
2025-01-07 16:59:13,407: Snapshot:2	Epoch:10	Loss:0.885	translation_Loss:0.65	token_training_loss:0.0	distillation_Loss:0.236                                                   	MRR:19.57	Hits@10:35.38	Best:19.71
2025-01-07 16:59:16,988: Snapshot:2	Epoch:11	Loss:22.332	translation_Loss:6.484	token_training_loss:15.848	distillation_Loss:0.0                                                   	MRR:19.57	Hits@10:35.38	Best:19.71
2025-01-07 16:59:20,578: End of token training: 2 Epoch: 12 Loss:8.914 MRR:19.57 Best Results: 19.71
2025-01-07 16:59:20,578: Snapshot:2	Epoch:12	Loss:8.914	translation_Loss:6.464	token_training_loss:2.45	distillation_Loss:0.0                                                           	MRR:19.57	Hits@10:35.38	Best:19.71
2025-01-07 16:59:20,896: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 16:59:31,189: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2559 | 0.148  | 0.3024 | 0.3747 |  0.465  |
|     1      | 0.1896 | 0.1102 | 0.2136 | 0.2665 |  0.3509 |
|     2      | 0.1985 | 0.1184 | 0.2262 | 0.2821 |  0.3556 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,725,600
Trainable params: 4,000
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 16:59:42,132: Snapshot:3	Epoch:0	Loss:3.033	translation_Loss:2.569	token_training_loss:0.0	distillation_Loss:0.464                                                   	MRR:20.74	Hits@10:36.75	Best:20.74
2025-01-07 16:59:45,939: Snapshot:3	Epoch:1	Loss:1.954	translation_Loss:1.428	token_training_loss:0.0	distillation_Loss:0.527                                                   	MRR:21.03	Hits@10:36.95	Best:21.03
2025-01-07 16:59:49,830: Snapshot:3	Epoch:2	Loss:1.546	translation_Loss:1.149	token_training_loss:0.0	distillation_Loss:0.398                                                   	MRR:21.16	Hits@10:36.81	Best:21.16
2025-01-07 16:59:54,207: Snapshot:3	Epoch:3	Loss:1.3	translation_Loss:0.935	token_training_loss:0.0	distillation_Loss:0.365                                                   	MRR:21.27	Hits@10:37.23	Best:21.27
2025-01-07 16:59:58,035: Snapshot:3	Epoch:4	Loss:1.216	translation_Loss:0.869	token_training_loss:0.0	distillation_Loss:0.347                                                   	MRR:21.19	Hits@10:37.04	Best:21.27
2025-01-07 17:00:01,879: Snapshot:3	Epoch:5	Loss:1.176	translation_Loss:0.828	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:21.13	Hits@10:37.18	Best:21.27
2025-01-07 17:00:05,816: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 21.27
2025-01-07 17:00:05,816: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:1.162 MRR:21.15 Best Results: 21.27
Token added to optimizer, embeddings excluded successfully.
2025-01-07 17:00:05,817: Snapshot:3	Epoch:6	Loss:1.162	translation_Loss:0.823	token_training_loss:0.0	distillation_Loss:0.34                                                   	MRR:21.15	Hits@10:37.24	Best:21.27
2025-01-07 17:00:09,601: Snapshot:3	Epoch:7	Loss:21.454	translation_Loss:6.582	token_training_loss:14.872	distillation_Loss:0.0                                                   	MRR:21.15	Hits@10:37.24	Best:21.27
2025-01-07 17:00:13,811: End of token training: 3 Epoch: 8 Loss:8.611 MRR:21.15 Best Results: 21.27
2025-01-07 17:00:13,811: Snapshot:3	Epoch:8	Loss:8.611	translation_Loss:6.597	token_training_loss:2.014	distillation_Loss:0.0                                                           	MRR:21.15	Hits@10:37.24	Best:21.27
2025-01-07 17:00:14,081: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 17:00:26,910: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2544 | 0.1468 | 0.3002 | 0.3728 |  0.4627 |
|     1      | 0.1936 | 0.1125 | 0.2183 | 0.2727 |  0.3574 |
|     2      | 0.2007 | 0.1192 | 0.2289 | 0.2857 |  0.3632 |
|     3      | 0.2127 | 0.1295 | 0.2463 | 0.3009 |  0.3697 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,913,200
Trainable params: 4,000
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 17:00:37,929: Snapshot:4	Epoch:0	Loss:2.211	translation_Loss:1.6	token_training_loss:0.0	distillation_Loss:0.611                                                   	MRR:22.8	Hits@10:38.35	Best:22.8
2025-01-07 17:00:41,783: Snapshot:4	Epoch:1	Loss:1.731	translation_Loss:1.286	token_training_loss:0.0	distillation_Loss:0.445                                                   	MRR:22.84	Hits@10:38.12	Best:22.84
2025-01-07 17:00:45,640: Snapshot:4	Epoch:2	Loss:1.509	translation_Loss:1.172	token_training_loss:0.0	distillation_Loss:0.338                                                   	MRR:22.86	Hits@10:38.19	Best:22.86
2025-01-07 17:00:50,028: Snapshot:4	Epoch:3	Loss:1.409	translation_Loss:1.081	token_training_loss:0.0	distillation_Loss:0.329                                                   	MRR:22.91	Hits@10:38.3	Best:22.91
2025-01-07 17:00:53,963: Snapshot:4	Epoch:4	Loss:1.375	translation_Loss:1.057	token_training_loss:0.0	distillation_Loss:0.318                                                   	MRR:22.85	Hits@10:38.22	Best:22.91
2025-01-07 17:00:57,885: Snapshot:4	Epoch:5	Loss:1.359	translation_Loss:1.04	token_training_loss:0.0	distillation_Loss:0.319                                                   	MRR:22.95	Hits@10:38.14	Best:22.95
2025-01-07 17:01:01,719: Snapshot:4	Epoch:6	Loss:1.356	translation_Loss:1.039	token_training_loss:0.0	distillation_Loss:0.318                                                   	MRR:22.95	Hits@10:38.36	Best:22.95
2025-01-07 17:01:05,733: Snapshot:4	Epoch:7	Loss:1.359	translation_Loss:1.039	token_training_loss:0.0	distillation_Loss:0.32                                                   	MRR:22.97	Hits@10:38.17	Best:22.97
2025-01-07 17:01:10,117: Snapshot:4	Epoch:8	Loss:1.354	translation_Loss:1.032	token_training_loss:0.0	distillation_Loss:0.322                                                   	MRR:22.91	Hits@10:38.23	Best:22.97
2025-01-07 17:01:14,037: Snapshot:4	Epoch:9	Loss:1.363	translation_Loss:1.042	token_training_loss:0.0	distillation_Loss:0.321                                                   	MRR:22.94	Hits@10:38.27	Best:22.97
2025-01-07 17:01:17,973: Snapshot:4	Epoch:10	Loss:1.355	translation_Loss:1.03	token_training_loss:0.0	distillation_Loss:0.324                                                   	MRR:23.12	Hits@10:38.28	Best:23.12
2025-01-07 17:01:21,803: Snapshot:4	Epoch:11	Loss:1.355	translation_Loss:1.031	token_training_loss:0.0	distillation_Loss:0.324                                                   	MRR:22.85	Hits@10:38.11	Best:23.12
2025-01-07 17:01:25,609: Snapshot:4	Epoch:12	Loss:1.347	translation_Loss:1.023	token_training_loss:0.0	distillation_Loss:0.323                                                   	MRR:22.89	Hits@10:38.15	Best:23.12
2025-01-07 17:01:29,995: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 23.12
2025-01-07 17:01:29,995: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:1.357 MRR:22.9 Best Results: 23.12
Token added to optimizer, embeddings excluded successfully.
2025-01-07 17:01:29,995: Snapshot:4	Epoch:13	Loss:1.357	translation_Loss:1.033	token_training_loss:0.0	distillation_Loss:0.324                                                   	MRR:22.9	Hits@10:38.06	Best:23.12
2025-01-07 17:01:33,756: Snapshot:4	Epoch:14	Loss:22.306	translation_Loss:6.752	token_training_loss:15.555	distillation_Loss:0.0                                                   	MRR:22.9	Hits@10:38.06	Best:23.12
2025-01-07 17:01:37,526: End of token training: 4 Epoch: 15 Loss:9.081 MRR:22.9 Best Results: 23.12
2025-01-07 17:01:37,526: Snapshot:4	Epoch:15	Loss:9.081	translation_Loss:6.756	token_training_loss:2.325	distillation_Loss:0.0                                                           	MRR:22.9	Hits@10:38.06	Best:23.12
2025-01-07 17:01:37,845: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 17:01:52,724: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2538 | 0.1461 | 0.3012 | 0.3718 |  0.4614 |
|     1      | 0.1945 | 0.1119 | 0.2194 | 0.2758 |  0.3634 |
|     2      | 0.2011 | 0.1174 |  0.23  | 0.2895 |  0.3673 |
|     3      | 0.2142 | 0.129  | 0.2504 | 0.3044 |  0.3731 |
|     4      | 0.2363 | 0.1446 | 0.2758 | 0.3361 |  0.4103 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   4,000
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,007,000
Trainable params: 4,000
Non-trainable params: 3,003,000
=================================================================
2025-01-07 17:01:52,727: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.1495 | 0.3042 | 0.3757 |  0.4679 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1496 | 0.3039 | 0.3765 |  0.4669 |
|     1      | 0.1852 | 0.1077 | 0.2086 | 0.2613 |  0.3433 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2559 | 0.148  | 0.3024 | 0.3747 |  0.465  |
|     1      | 0.1896 | 0.1102 | 0.2136 | 0.2665 |  0.3509 |
|     2      | 0.1985 | 0.1184 | 0.2262 | 0.2821 |  0.3556 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2544 | 0.1468 | 0.3002 | 0.3728 |  0.4627 |
|     1      | 0.1936 | 0.1125 | 0.2183 | 0.2727 |  0.3574 |
|     2      | 0.2007 | 0.1192 | 0.2289 | 0.2857 |  0.3632 |
|     3      | 0.2127 | 0.1295 | 0.2463 | 0.3009 |  0.3697 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2538 | 0.1461 | 0.3012 | 0.3718 |  0.4614 |
|     1      | 0.1945 | 0.1119 | 0.2194 | 0.2758 |  0.3634 |
|     2      | 0.2011 | 0.1174 |  0.23  | 0.2895 |  0.3673 |
|     3      | 0.2142 | 0.129  | 0.2504 | 0.3044 |  0.3731 |
|     4      | 0.2363 | 0.1446 | 0.2758 | 0.3361 |  0.4103 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 17:01:52,727: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 178.98963165283203 |   0.258   |    0.149     |    0.304     |     0.468     |
|    1     | 66.77627348899841  |   0.247   |    0.144     |     0.29     |     0.449     |
|    2     | 53.676153898239136 |    0.24   |     0.14     |    0.282     |     0.437     |
|    3     | 40.23149657249451  |   0.237   |    0.138     |    0.277     |      0.43     |
|    4     | 68.18036270141602  |   0.237   |    0.138     |    0.278     |     0.428     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 17:01:52,727: Sum_Training_Time:407.8539183139801
2025-01-07 17:01:52,727: Every_Training_Time:[178.98963165283203, 66.77627348899841, 53.676153898239136, 40.23149657249451, 68.18036270141602]
2025-01-07 17:01:52,727: Forward transfer: 0.16245 Backward transfer: 0.0024000000000000063
2025-01-07 17:02:16,474: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FB_CKGE/', dataset='FB_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107170156/FB_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FB_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 10000.0, 10000.0, 20000.0], token_num=15, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 17:02:40,868: Snapshot:0	Epoch:0	Loss:43.8	translation_Loss:43.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.2	Hits@10:34.63	Best:15.2
2025-01-07 17:03:00,391: Snapshot:0	Epoch:1	Loss:18.581	translation_Loss:18.581	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.8	Hits@10:45.21	Best:23.8
2025-01-07 17:03:19,952: Snapshot:0	Epoch:2	Loss:7.138	translation_Loss:7.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.54	Hits@10:46.82	Best:25.54
2025-01-07 17:03:39,472: Snapshot:0	Epoch:3	Loss:3.628	translation_Loss:3.628	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.77	Hits@10:46.99	Best:25.77
2025-01-07 17:03:59,376: Snapshot:0	Epoch:4	Loss:2.337	translation_Loss:2.337	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.83	Hits@10:46.74	Best:25.83
2025-01-07 17:04:18,565: Snapshot:0	Epoch:5	Loss:1.747	translation_Loss:1.747	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.57	Hits@10:46.49	Best:25.83
2025-01-07 17:04:38,182: Snapshot:0	Epoch:6	Loss:1.415	translation_Loss:1.415	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.48	Hits@10:46.27	Best:25.83
2025-01-07 17:04:57,594: Early Stopping! Snapshot: 0 Epoch: 7 Best Results: 25.83
2025-01-07 17:04:57,595: Start to training tokens! Snapshot: 0 Epoch: 7 Loss:1.211 MRR:25.37 Best Results: 25.83
Token added to optimizer, embeddings excluded successfully.
2025-01-07 17:04:57,595: Snapshot:0	Epoch:7	Loss:1.211	translation_Loss:1.211	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.37	Hits@10:46.1	Best:25.83
2025-01-07 17:05:17,946: Snapshot:0	Epoch:8	Loss:55.301	translation_Loss:36.215	token_training_loss:19.086	distillation_Loss:0.0                                                   	MRR:25.37	Hits@10:46.1	Best:25.83
2025-01-07 17:05:37,504: End of token training: 0 Epoch: 9 Loss:36.226 MRR:25.37 Best Results: 25.83
2025-01-07 17:05:37,504: Snapshot:0	Epoch:9	Loss:36.226	translation_Loss:36.222	token_training_loss:0.003	distillation_Loss:0.0                                                           	MRR:25.37	Hits@10:46.1	Best:25.83
2025-01-07 17:05:37,834: => loading checkpoint './checkpoint/FB_CKGE/0model_best.tar'
2025-01-07 17:05:44,628: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2566 | 0.1492 | 0.3018 | 0.3743 |  0.4662 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   6,000
├─Embedding: 1-1                         (1,501,000)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 1,601,800
Trainable params: 6,000
Non-trainable params: 1,595,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 17:05:54,805: Snapshot:1	Epoch:0	Loss:6.653	translation_Loss:6.209	token_training_loss:0.0	distillation_Loss:0.444                                                   	MRR:9.87	Hits@10:19.17	Best:9.87
2025-01-07 17:05:58,533: Snapshot:1	Epoch:1	Loss:3.393	translation_Loss:3.081	token_training_loss:0.0	distillation_Loss:0.311                                                   	MRR:12.72	Hits@10:23.34	Best:12.72
2025-01-07 17:06:02,261: Snapshot:1	Epoch:2	Loss:1.657	translation_Loss:1.45	token_training_loss:0.0	distillation_Loss:0.207                                                   	MRR:14.38	Hits@10:26.52	Best:14.38
2025-01-07 17:06:06,392: Snapshot:1	Epoch:3	Loss:0.917	translation_Loss:0.79	token_training_loss:0.0	distillation_Loss:0.127                                                   	MRR:15.52	Hits@10:28.81	Best:15.52
2025-01-07 17:06:10,106: Snapshot:1	Epoch:4	Loss:0.61	translation_Loss:0.524	token_training_loss:0.0	distillation_Loss:0.086                                                   	MRR:16.26	Hits@10:30.26	Best:16.26
2025-01-07 17:06:13,818: Snapshot:1	Epoch:5	Loss:0.467	translation_Loss:0.398	token_training_loss:0.0	distillation_Loss:0.069                                                   	MRR:16.66	Hits@10:31.22	Best:16.66
2025-01-07 17:06:17,548: Snapshot:1	Epoch:6	Loss:0.392	translation_Loss:0.332	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:16.92	Hits@10:31.56	Best:16.92
2025-01-07 17:06:21,246: Snapshot:1	Epoch:7	Loss:0.342	translation_Loss:0.286	token_training_loss:0.0	distillation_Loss:0.057                                                   	MRR:17.13	Hits@10:32.18	Best:17.13
2025-01-07 17:06:25,327: Snapshot:1	Epoch:8	Loss:0.312	translation_Loss:0.258	token_training_loss:0.0	distillation_Loss:0.054                                                   	MRR:17.27	Hits@10:32.59	Best:17.27
2025-01-07 17:06:28,966: Snapshot:1	Epoch:9	Loss:0.287	translation_Loss:0.234	token_training_loss:0.0	distillation_Loss:0.053                                                   	MRR:17.47	Hits@10:32.68	Best:17.47
2025-01-07 17:06:32,719: Snapshot:1	Epoch:10	Loss:0.273	translation_Loss:0.222	token_training_loss:0.0	distillation_Loss:0.051                                                   	MRR:17.61	Hits@10:32.78	Best:17.61
2025-01-07 17:06:36,437: Snapshot:1	Epoch:11	Loss:0.251	translation_Loss:0.202	token_training_loss:0.0	distillation_Loss:0.05                                                   	MRR:17.74	Hits@10:33.2	Best:17.74
2025-01-07 17:06:40,114: Snapshot:1	Epoch:12	Loss:0.244	translation_Loss:0.195	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:17.68	Hits@10:33.14	Best:17.74
2025-01-07 17:06:44,177: Snapshot:1	Epoch:13	Loss:0.238	translation_Loss:0.189	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:17.71	Hits@10:33.03	Best:17.74
2025-01-07 17:06:47,849: Snapshot:1	Epoch:14	Loss:0.229	translation_Loss:0.18	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:17.82	Hits@10:33.18	Best:17.82
2025-01-07 17:06:51,568: Snapshot:1	Epoch:15	Loss:0.222	translation_Loss:0.174	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:17.84	Hits@10:33.67	Best:17.84
2025-01-07 17:06:55,287: Snapshot:1	Epoch:16	Loss:0.217	translation_Loss:0.167	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:17.99	Hits@10:33.41	Best:17.99
2025-01-07 17:06:58,948: Snapshot:1	Epoch:17	Loss:0.211	translation_Loss:0.162	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:17.92	Hits@10:33.5	Best:17.99
2025-01-07 17:07:03,067: Snapshot:1	Epoch:18	Loss:0.207	translation_Loss:0.159	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:17.97	Hits@10:33.55	Best:17.99
2025-01-07 17:07:06,779: Snapshot:1	Epoch:19	Loss:0.203	translation_Loss:0.157	token_training_loss:0.0	distillation_Loss:0.046                                                   	MRR:18.07	Hits@10:33.77	Best:18.07
2025-01-07 17:07:10,448: Snapshot:1	Epoch:20	Loss:0.2	translation_Loss:0.151	token_training_loss:0.0	distillation_Loss:0.048                                                   	MRR:18.06	Hits@10:34.05	Best:18.07
2025-01-07 17:07:14,158: Snapshot:1	Epoch:21	Loss:0.201	translation_Loss:0.151	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:18.09	Hits@10:33.96	Best:18.09
2025-01-07 17:07:17,841: Snapshot:1	Epoch:22	Loss:0.198	translation_Loss:0.148	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:18.0	Hits@10:34.12	Best:18.09
2025-01-07 17:07:21,899: Snapshot:1	Epoch:23	Loss:0.19	translation_Loss:0.141	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:18.12	Hits@10:33.75	Best:18.12
2025-01-07 17:07:25,615: Snapshot:1	Epoch:24	Loss:0.189	translation_Loss:0.142	token_training_loss:0.0	distillation_Loss:0.047                                                   	MRR:18.15	Hits@10:33.93	Best:18.15
2025-01-07 17:07:29,350: Snapshot:1	Epoch:25	Loss:0.194	translation_Loss:0.145	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:18.17	Hits@10:34.24	Best:18.17
2025-01-07 17:07:33,046: Snapshot:1	Epoch:26	Loss:0.188	translation_Loss:0.138	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:18.17	Hits@10:34.09	Best:18.17
2025-01-07 17:07:36,648: Snapshot:1	Epoch:27	Loss:0.183	translation_Loss:0.135	token_training_loss:0.0	distillation_Loss:0.048                                                   	MRR:18.15	Hits@10:34.1	Best:18.17
2025-01-07 17:07:40,715: Early Stopping! Snapshot: 1 Epoch: 28 Best Results: 18.17
2025-01-07 17:07:40,715: Start to training tokens! Snapshot: 1 Epoch: 28 Loss:0.182 MRR:18.01 Best Results: 18.17
Token added to optimizer, embeddings excluded successfully.
2025-01-07 17:07:40,716: Snapshot:1	Epoch:28	Loss:0.182	translation_Loss:0.135	token_training_loss:0.0	distillation_Loss:0.047                                                   	MRR:18.01	Hits@10:34.0	Best:18.17
2025-01-07 17:07:44,190: Snapshot:1	Epoch:29	Loss:22.37	translation_Loss:6.155	token_training_loss:16.215	distillation_Loss:0.0                                                   	MRR:18.01	Hits@10:34.0	Best:18.17
2025-01-07 17:07:47,667: End of token training: 1 Epoch: 30 Loss:8.829 MRR:18.01 Best Results: 18.17
2025-01-07 17:07:47,668: Snapshot:1	Epoch:30	Loss:8.829	translation_Loss:6.156	token_training_loss:2.672	distillation_Loss:0.0                                                           	MRR:18.01	Hits@10:34.0	Best:18.17
2025-01-07 17:07:47,964: => loading checkpoint './checkpoint/FB_CKGE/1model_best.tar'
2025-01-07 17:07:56,100: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2565 | 0.1492 | 0.3015 | 0.3735 |  0.4662 |
|     1      | 0.1887 | 0.1093 | 0.2118 | 0.2681 |  0.3479 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   6,000
├─Embedding: 1-1                         (2,251,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,352,400
Trainable params: 6,000
Non-trainable params: 2,346,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 17:08:06,811: Snapshot:2	Epoch:0	Loss:4.457	translation_Loss:3.97	token_training_loss:0.0	distillation_Loss:0.487                                                   	MRR:18.42	Hits@10:33.39	Best:18.42
2025-01-07 17:08:10,679: Snapshot:2	Epoch:1	Loss:2.401	translation_Loss:2.008	token_training_loss:0.0	distillation_Loss:0.393                                                   	MRR:19.05	Hits@10:33.94	Best:19.05
2025-01-07 17:08:14,459: Snapshot:2	Epoch:2	Loss:1.449	translation_Loss:1.182	token_training_loss:0.0	distillation_Loss:0.267                                                   	MRR:19.57	Hits@10:34.52	Best:19.57
2025-01-07 17:08:18,279: Snapshot:2	Epoch:3	Loss:1.084	translation_Loss:0.861	token_training_loss:0.0	distillation_Loss:0.223                                                   	MRR:19.63	Hits@10:34.73	Best:19.63
2025-01-07 17:08:22,074: Snapshot:2	Epoch:4	Loss:0.935	translation_Loss:0.736	token_training_loss:0.0	distillation_Loss:0.199                                                   	MRR:19.66	Hits@10:34.95	Best:19.66
2025-01-07 17:08:26,339: Snapshot:2	Epoch:5	Loss:0.873	translation_Loss:0.683	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:19.84	Hits@10:35.06	Best:19.84
2025-01-07 17:08:30,124: Snapshot:2	Epoch:6	Loss:0.835	translation_Loss:0.647	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:19.76	Hits@10:35.18	Best:19.84
2025-01-07 17:08:33,940: Snapshot:2	Epoch:7	Loss:0.81	translation_Loss:0.621	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:19.73	Hits@10:35.17	Best:19.84
2025-01-07 17:08:37,705: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 19.84
2025-01-07 17:08:37,705: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:0.796 MRR:19.79 Best Results: 19.84
Token added to optimizer, embeddings excluded successfully.
2025-01-07 17:08:37,705: Snapshot:2	Epoch:8	Loss:0.796	translation_Loss:0.606	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:19.79	Hits@10:35.2	Best:19.84
2025-01-07 17:08:41,354: Snapshot:2	Epoch:9	Loss:22.737	translation_Loss:6.473	token_training_loss:16.264	distillation_Loss:0.0                                                   	MRR:19.79	Hits@10:35.2	Best:19.84
2025-01-07 17:08:45,418: End of token training: 2 Epoch: 10 Loss:9.284 MRR:19.79 Best Results: 19.84
2025-01-07 17:08:45,419: Snapshot:2	Epoch:10	Loss:9.284	translation_Loss:6.459	token_training_loss:2.825	distillation_Loss:0.0                                                           	MRR:19.79	Hits@10:35.2	Best:19.84
2025-01-07 17:08:45,662: => loading checkpoint './checkpoint/FB_CKGE/2model_best.tar'
2025-01-07 17:08:55,704: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2564 | 0.149  | 0.3021 | 0.3731 |  0.4658 |
|     1      | 0.192  | 0.111  | 0.2169 | 0.2741 |  0.3531 |
|     2      | 0.1987 | 0.1187 | 0.2259 | 0.2808 |  0.3541 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   6,000
├─Embedding: 1-1                         (2,626,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,727,600
Trainable params: 6,000
Non-trainable params: 2,721,600
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 17:09:06,190: Snapshot:3	Epoch:0	Loss:2.924	translation_Loss:2.41	token_training_loss:0.0	distillation_Loss:0.514                                                   	MRR:20.72	Hits@10:36.33	Best:20.72
2025-01-07 17:09:10,060: Snapshot:3	Epoch:1	Loss:1.908	translation_Loss:1.471	token_training_loss:0.0	distillation_Loss:0.437                                                   	MRR:21.09	Hits@10:36.35	Best:21.09
2025-01-07 17:09:14,475: Snapshot:3	Epoch:2	Loss:1.435	translation_Loss:1.112	token_training_loss:0.0	distillation_Loss:0.323                                                   	MRR:21.2	Hits@10:36.66	Best:21.2
2025-01-07 17:09:18,447: Snapshot:3	Epoch:3	Loss:1.205	translation_Loss:0.906	token_training_loss:0.0	distillation_Loss:0.299                                                   	MRR:21.26	Hits@10:36.76	Best:21.26
2025-01-07 17:09:22,316: Snapshot:3	Epoch:4	Loss:1.126	translation_Loss:0.842	token_training_loss:0.0	distillation_Loss:0.284                                                   	MRR:21.14	Hits@10:36.54	Best:21.26
2025-01-07 17:09:26,152: Snapshot:3	Epoch:5	Loss:1.09	translation_Loss:0.808	token_training_loss:0.0	distillation_Loss:0.282                                                   	MRR:21.02	Hits@10:36.74	Best:21.26
2025-01-07 17:09:29,988: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 21.26
2025-01-07 17:09:29,988: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:1.073 MRR:21.22 Best Results: 21.26
Token added to optimizer, embeddings excluded successfully.
2025-01-07 17:09:29,989: Snapshot:3	Epoch:6	Loss:1.073	translation_Loss:0.794	token_training_loss:0.0	distillation_Loss:0.279                                                   	MRR:21.22	Hits@10:36.92	Best:21.26
2025-01-07 17:09:34,200: Snapshot:3	Epoch:7	Loss:22.664	translation_Loss:6.595	token_training_loss:16.069	distillation_Loss:0.0                                                   	MRR:21.22	Hits@10:36.92	Best:21.26
2025-01-07 17:09:38,018: End of token training: 3 Epoch: 8 Loss:9.217 MRR:21.22 Best Results: 21.26
2025-01-07 17:09:38,019: Snapshot:3	Epoch:8	Loss:9.217	translation_Loss:6.596	token_training_loss:2.621	distillation_Loss:0.0                                                           	MRR:21.22	Hits@10:36.92	Best:21.26
2025-01-07 17:09:38,273: => loading checkpoint './checkpoint/FB_CKGE/3model_best.tar'
2025-01-07 17:09:50,410: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1486 | 0.3011 | 0.3719 |  0.4641 |
|     1      | 0.1945 | 0.1113 | 0.2197 | 0.2808 |  0.3617 |
|     2      | 0.2018 | 0.1208 | 0.2285 | 0.2865 |  0.3602 |
|     3      | 0.2127 | 0.1312 | 0.2444 | 0.2975 |  0.367  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   6,000
├─Embedding: 1-1                         (2,814,400)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,915,200
Trainable params: 6,000
Non-trainable params: 2,909,200
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 17:10:01,089: Snapshot:4	Epoch:0	Loss:2.167	translation_Loss:1.466	token_training_loss:0.0	distillation_Loss:0.701                                                   	MRR:22.45	Hits@10:37.85	Best:22.45
2025-01-07 17:10:05,202: Snapshot:4	Epoch:1	Loss:1.723	translation_Loss:1.313	token_training_loss:0.0	distillation_Loss:0.41                                                   	MRR:22.65	Hits@10:38.02	Best:22.65
2025-01-07 17:10:09,312: Snapshot:4	Epoch:2	Loss:1.408	translation_Loss:1.109	token_training_loss:0.0	distillation_Loss:0.299                                                   	MRR:22.66	Hits@10:38.11	Best:22.66
2025-01-07 17:10:13,829: Snapshot:4	Epoch:3	Loss:1.304	translation_Loss:1.04	token_training_loss:0.0	distillation_Loss:0.264                                                   	MRR:22.69	Hits@10:38.12	Best:22.69
2025-01-07 17:10:17,840: Snapshot:4	Epoch:4	Loss:1.267	translation_Loss:1.013	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:22.65	Hits@10:38.06	Best:22.69
2025-01-07 17:10:21,838: Snapshot:4	Epoch:5	Loss:1.257	translation_Loss:1.006	token_training_loss:0.0	distillation_Loss:0.251                                                   	MRR:22.66	Hits@10:38.08	Best:22.69
2025-01-07 17:10:25,902: Snapshot:4	Epoch:6	Loss:1.25	translation_Loss:0.998	token_training_loss:0.0	distillation_Loss:0.253                                                   	MRR:22.76	Hits@10:38.2	Best:22.76
2025-01-07 17:10:29,877: Snapshot:4	Epoch:7	Loss:1.253	translation_Loss:0.995	token_training_loss:0.0	distillation_Loss:0.258                                                   	MRR:22.64	Hits@10:38.0	Best:22.76
2025-01-07 17:10:34,345: Snapshot:4	Epoch:8	Loss:1.253	translation_Loss:0.998	token_training_loss:0.0	distillation_Loss:0.255                                                   	MRR:22.74	Hits@10:38.08	Best:22.76
2025-01-07 17:10:38,296: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 22.76
2025-01-07 17:10:38,296: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:1.243 MRR:22.69 Best Results: 22.76
Token added to optimizer, embeddings excluded successfully.
2025-01-07 17:10:38,296: Snapshot:4	Epoch:9	Loss:1.243	translation_Loss:0.987	token_training_loss:0.0	distillation_Loss:0.256                                                   	MRR:22.69	Hits@10:38.15	Best:22.76
2025-01-07 17:10:42,084: Snapshot:4	Epoch:10	Loss:22.987	translation_Loss:6.768	token_training_loss:16.219	distillation_Loss:0.0                                                   	MRR:22.69	Hits@10:38.15	Best:22.76
2025-01-07 17:10:45,872: End of token training: 4 Epoch: 11 Loss:9.452 MRR:22.69 Best Results: 22.76
2025-01-07 17:10:45,872: Snapshot:4	Epoch:11	Loss:9.452	translation_Loss:6.768	token_training_loss:2.685	distillation_Loss:0.0                                                           	MRR:22.69	Hits@10:38.15	Best:22.76
2025-01-07 17:10:46,117: => loading checkpoint './checkpoint/FB_CKGE/4model_best.tar'
2025-01-07 17:11:00,806: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1473 | 0.3008 | 0.3719 |  0.4637 |
|     1      | 0.1967 | 0.1133 | 0.2222 | 0.2821 |  0.3645 |
|     2      | 0.2039 | 0.1225 | 0.2292 | 0.2889 |  0.3626 |
|     3      | 0.2135 | 0.1307 | 0.2471 |  0.3   |  0.3697 |
|     4      | 0.2371 | 0.1448 | 0.274  | 0.3363 |  0.4127 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   6,000
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,009,000
Trainable params: 6,000
Non-trainable params: 3,003,000
=================================================================
2025-01-07 17:11:00,809: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2566 | 0.1492 | 0.3018 | 0.3743 |  0.4662 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2565 | 0.1492 | 0.3015 | 0.3735 |  0.4662 |
|     1      | 0.1887 | 0.1093 | 0.2118 | 0.2681 |  0.3479 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2564 | 0.149  | 0.3021 | 0.3731 |  0.4658 |
|     1      | 0.192  | 0.111  | 0.2169 | 0.2741 |  0.3531 |
|     2      | 0.1987 | 0.1187 | 0.2259 | 0.2808 |  0.3541 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1486 | 0.3011 | 0.3719 |  0.4641 |
|     1      | 0.1945 | 0.1113 | 0.2197 | 0.2808 |  0.3617 |
|     2      | 0.2018 | 0.1208 | 0.2285 | 0.2865 |  0.3602 |
|     3      | 0.2127 | 0.1312 | 0.2444 | 0.2975 |  0.367  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1473 | 0.3008 | 0.3719 |  0.4637 |
|     1      | 0.1967 | 0.1133 | 0.2222 | 0.2821 |  0.3645 |
|     2      | 0.2039 | 0.1225 | 0.2292 | 0.2889 |  0.3626 |
|     3      | 0.2135 | 0.1307 | 0.2471 |  0.3   |  0.3697 |
|     4      | 0.2371 | 0.1448 | 0.274  | 0.3363 |  0.4127 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 17:11:00,809: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 201.02975702285767 |   0.257   |    0.149     |    0.302     |     0.466     |
|    1     | 121.20970416069031 |   0.247   |    0.143     |    0.289     |     0.449     |
|    2     |  47.3866069316864  |   0.241   |     0.14     |    0.282     |     0.438     |
|    3     | 40.27440786361694  |   0.238   |    0.139     |    0.278     |      0.43     |
|    4     | 53.36003851890564  |   0.238   |     0.14     |    0.278     |     0.429     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 17:11:00,809: Sum_Training_Time:463.26051449775696
2025-01-07 17:11:00,810: Every_Training_Time:[201.02975702285767, 121.20970416069031, 47.3866069316864, 40.27440786361694, 53.36003851890564]
2025-01-07 17:11:00,810: Forward transfer: 0.163125 Backward transfer: 0.0030750000000000083
