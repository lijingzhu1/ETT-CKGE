2024-12-28 02:14:07,283: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241228021328/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 02:14:18,476: Snapshot:0	Epoch:0	Loss:102.979	translation_Loss:102.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.78	Hits@10:4.76	Best:2.78
2024-12-28 02:14:26,035: Snapshot:0	Epoch:1	Loss:93.519	translation_Loss:93.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:8.53	Best:4.54
2024-12-28 02:14:33,608: Snapshot:0	Epoch:2	Loss:84.912	translation_Loss:84.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.71	Hits@10:11.57	Best:5.71
2024-12-28 02:14:41,385: Snapshot:0	Epoch:3	Loss:77.12	translation_Loss:77.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.9	Hits@10:14.86	Best:6.9
2024-12-28 02:14:49,031: Snapshot:0	Epoch:4	Loss:69.774	translation_Loss:69.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.2	Hits@10:18.03	Best:8.2
2024-12-28 02:14:56,598: Snapshot:0	Epoch:5	Loss:62.903	translation_Loss:62.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.61	Hits@10:20.97	Best:9.61
2024-12-28 02:15:04,251: Snapshot:0	Epoch:6	Loss:56.536	translation_Loss:56.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.94	Hits@10:23.74	Best:10.94
2024-12-28 02:15:11,920: Snapshot:0	Epoch:7	Loss:50.671	translation_Loss:50.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.08	Hits@10:26.07	Best:12.08
2024-12-28 02:15:20,135: Snapshot:0	Epoch:8	Loss:45.219	translation_Loss:45.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.1	Hits@10:28.12	Best:13.1
2024-12-28 02:15:27,723: Snapshot:0	Epoch:9	Loss:40.27	translation_Loss:40.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.02	Hits@10:29.83	Best:14.02
2024-12-28 02:15:35,294: Snapshot:0	Epoch:10	Loss:35.786	translation_Loss:35.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.9	Hits@10:31.36	Best:14.9
2024-12-28 02:15:42,959: Snapshot:0	Epoch:11	Loss:31.679	translation_Loss:31.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.73	Hits@10:32.64	Best:15.73
2024-12-28 02:15:50,536: Snapshot:0	Epoch:12	Loss:28.083	translation_Loss:28.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.49	Hits@10:33.67	Best:16.49
2024-12-28 02:15:58,156: Snapshot:0	Epoch:13	Loss:24.798	translation_Loss:24.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.23	Hits@10:34.65	Best:17.23
2024-12-28 02:16:05,782: Snapshot:0	Epoch:14	Loss:21.947	translation_Loss:21.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.98	Hits@10:35.47	Best:17.98
2024-12-28 02:16:13,357: Snapshot:0	Epoch:15	Loss:19.373	translation_Loss:19.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.63	Hits@10:36.24	Best:18.63
2024-12-28 02:16:21,079: Snapshot:0	Epoch:16	Loss:17.061	translation_Loss:17.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.31	Hits@10:36.76	Best:19.31
2024-12-28 02:16:28,778: Snapshot:0	Epoch:17	Loss:15.006	translation_Loss:15.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.91	Hits@10:37.25	Best:19.91
2024-12-28 02:16:36,432: Snapshot:0	Epoch:18	Loss:13.198	translation_Loss:13.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.45	Hits@10:37.68	Best:20.45
2024-12-28 02:16:44,146: Snapshot:0	Epoch:19	Loss:11.564	translation_Loss:11.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.88	Hits@10:38.06	Best:20.88
2024-12-28 02:16:51,825: Snapshot:0	Epoch:20	Loss:10.132	translation_Loss:10.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.35	Hits@10:38.36	Best:21.35
2024-12-28 02:16:59,430: Snapshot:0	Epoch:21	Loss:8.908	translation_Loss:8.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.69	Hits@10:38.57	Best:21.69
2024-12-28 02:17:07,029: Snapshot:0	Epoch:22	Loss:7.843	translation_Loss:7.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.02	Hits@10:38.69	Best:22.02
2024-12-28 02:17:14,675: Snapshot:0	Epoch:23	Loss:6.873	translation_Loss:6.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.4	Hits@10:39.02	Best:22.4
2024-12-28 02:17:22,376: Snapshot:0	Epoch:24	Loss:6.073	translation_Loss:6.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.66	Hits@10:39.21	Best:22.66
2024-12-28 02:17:29,989: Snapshot:0	Epoch:25	Loss:5.408	translation_Loss:5.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.93	Hits@10:39.4	Best:22.93
2024-12-28 02:17:37,643: Snapshot:0	Epoch:26	Loss:4.762	translation_Loss:4.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.15	Hits@10:39.43	Best:23.15
2024-12-28 02:17:45,278: Snapshot:0	Epoch:27	Loss:4.277	translation_Loss:4.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.53	Best:23.36
2024-12-28 02:17:52,888: Snapshot:0	Epoch:28	Loss:3.816	translation_Loss:3.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.52	Hits@10:39.68	Best:23.52
2024-12-28 02:18:00,994: Snapshot:0	Epoch:29	Loss:3.412	translation_Loss:3.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:39.79	Best:23.66
2024-12-28 02:18:08,593: Snapshot:0	Epoch:30	Loss:3.093	translation_Loss:3.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.7	Hits@10:39.81	Best:23.7
2024-12-28 02:18:16,217: Snapshot:0	Epoch:31	Loss:2.835	translation_Loss:2.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.85	Hits@10:39.86	Best:23.85
2024-12-28 02:18:23,902: Snapshot:0	Epoch:32	Loss:2.578	translation_Loss:2.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:39.93	Best:23.95
2024-12-28 02:18:31,491: Snapshot:0	Epoch:33	Loss:2.344	translation_Loss:2.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:40.01	Best:24.05
2024-12-28 02:18:39,145: Snapshot:0	Epoch:34	Loss:2.139	translation_Loss:2.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:39.96	Best:24.05
2024-12-28 02:18:46,729: Snapshot:0	Epoch:35	Loss:2.009	translation_Loss:2.009	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:39.93	Best:24.12
2024-12-28 02:18:54,295: Snapshot:0	Epoch:36	Loss:1.837	translation_Loss:1.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:39.96	Best:24.12
2024-12-28 02:19:01,896: Snapshot:0	Epoch:37	Loss:1.735	translation_Loss:1.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.19	Hits@10:40.02	Best:24.19
2024-12-28 02:19:09,554: Snapshot:0	Epoch:38	Loss:1.598	translation_Loss:1.598	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:40.11	Best:24.22
2024-12-28 02:19:17,200: Snapshot:0	Epoch:39	Loss:1.525	translation_Loss:1.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.27	Hits@10:40.06	Best:24.27
2024-12-28 02:19:24,774: Snapshot:0	Epoch:40	Loss:1.437	translation_Loss:1.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:40.15	Best:24.27
2024-12-28 02:19:32,369: Snapshot:0	Epoch:41	Loss:1.362	translation_Loss:1.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.3	Hits@10:40.08	Best:24.3
2024-12-28 02:19:40,118: Snapshot:0	Epoch:42	Loss:1.275	translation_Loss:1.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:40.3	Best:24.37
2024-12-28 02:19:47,691: Snapshot:0	Epoch:43	Loss:1.23	translation_Loss:1.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:40.13	Best:24.37
2024-12-28 02:19:55,318: Snapshot:0	Epoch:44	Loss:1.173	translation_Loss:1.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:40.11	Best:24.42
2024-12-28 02:20:02,979: Snapshot:0	Epoch:45	Loss:1.13	translation_Loss:1.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:40.08	Best:24.42
2024-12-28 02:20:10,727: Snapshot:0	Epoch:46	Loss:1.072	translation_Loss:1.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:40.15	Best:24.43
2024-12-28 02:20:18,376: Snapshot:0	Epoch:47	Loss:1.033	translation_Loss:1.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.45	Hits@10:40.12	Best:24.45
2024-12-28 02:20:26,036: Snapshot:0	Epoch:48	Loss:1.002	translation_Loss:1.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.4	Hits@10:40.04	Best:24.45
2024-12-28 02:20:34,133: Snapshot:0	Epoch:49	Loss:0.949	translation_Loss:0.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:40.08	Best:24.46
2024-12-28 02:20:41,912: Snapshot:0	Epoch:50	Loss:0.929	translation_Loss:0.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:40.05	Best:24.49
2024-12-28 02:20:49,522: Snapshot:0	Epoch:51	Loss:0.895	translation_Loss:0.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.52	Hits@10:40.04	Best:24.52
2024-12-28 02:20:57,100: Snapshot:0	Epoch:52	Loss:0.87	translation_Loss:0.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:40.12	Best:24.52
2024-12-28 02:21:04,740: Snapshot:0	Epoch:53	Loss:0.834	translation_Loss:0.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.56	Hits@10:40.05	Best:24.56
2024-12-28 02:21:12,326: Snapshot:0	Epoch:54	Loss:0.818	translation_Loss:0.818	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:39.99	Best:24.56
2024-12-28 02:21:19,982: Snapshot:0	Epoch:55	Loss:0.779	translation_Loss:0.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:39.99	Best:24.56
2024-12-28 02:21:27,635: Early Stopping! Snapshot: 0 Epoch: 56 Best Results: 24.56
2024-12-28 02:21:27,636: Start to training tokens! Snapshot: 0 Epoch: 56 Loss:0.779 MRR:24.51 Best Results: 24.56
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:21:27,636: Snapshot:0	Epoch:56	Loss:0.779	translation_Loss:0.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:40.23	Best:24.56
2024-12-28 02:21:36,004: Snapshot:0	Epoch:57	Loss:196.966	translation_Loss:72.134	multi_layer_Loss:124.832	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:40.23	Best:24.56
2024-12-28 02:21:43,863: End of token training: 0 Epoch: 58 Loss:75.466 MRR:24.51 Best Results: 24.56
2024-12-28 02:21:43,863: Snapshot:0	Epoch:58	Loss:75.466	translation_Loss:72.124	multi_layer_Loss:3.342	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.51	Hits@10:40.23	Best:24.56
2024-12-28 02:21:44,213: => no checking found at './checkpoint/FACT/0model_best.tar'
2024-12-28 02:21:47,153: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2392 | 0.1551 |  0.28  | 0.3266 |  0.3913 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:22:14,862: Snapshot:1	Epoch:0	Loss:44.976	translation_Loss:43.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.021                                                   	MRR:18.52	Hits@10:31.56	Best:18.52
2024-12-28 02:22:23,478: Snapshot:1	Epoch:1	Loss:39.118	translation_Loss:36.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.799                                                   	MRR:19.21	Hits@10:32.68	Best:19.21
2024-12-28 02:22:32,064: Snapshot:1	Epoch:2	Loss:35.749	translation_Loss:31.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.281                                                   	MRR:19.6	Hits@10:33.51	Best:19.6
2024-12-28 02:22:40,700: Snapshot:1	Epoch:3	Loss:33.048	translation_Loss:27.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.269                                                   	MRR:20.11	Hits@10:33.96	Best:20.11
2024-12-28 02:22:49,276: Snapshot:1	Epoch:4	Loss:30.897	translation_Loss:25.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.897                                                   	MRR:20.42	Hits@10:34.33	Best:20.42
2024-12-28 02:22:57,874: Snapshot:1	Epoch:5	Loss:29.076	translation_Loss:22.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.282                                                   	MRR:20.68	Hits@10:34.65	Best:20.68
2024-12-28 02:23:06,472: Snapshot:1	Epoch:6	Loss:27.749	translation_Loss:21.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.526                                                   	MRR:20.88	Hits@10:34.89	Best:20.88
2024-12-28 02:23:15,110: Snapshot:1	Epoch:7	Loss:26.719	translation_Loss:20.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.665                                                   	MRR:21.01	Hits@10:35.24	Best:21.01
2024-12-28 02:23:23,796: Snapshot:1	Epoch:8	Loss:25.967	translation_Loss:19.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.74                                                   	MRR:21.19	Hits@10:35.34	Best:21.19
2024-12-28 02:23:32,351: Snapshot:1	Epoch:9	Loss:25.417	translation_Loss:18.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.77                                                   	MRR:21.31	Hits@10:35.65	Best:21.31
2024-12-28 02:23:41,032: Snapshot:1	Epoch:10	Loss:25.028	translation_Loss:18.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.773                                                   	MRR:21.39	Hits@10:35.75	Best:21.39
2024-12-28 02:23:49,635: Snapshot:1	Epoch:11	Loss:24.823	translation_Loss:18.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.777                                                   	MRR:21.48	Hits@10:35.74	Best:21.48
2024-12-28 02:23:58,167: Snapshot:1	Epoch:12	Loss:24.596	translation_Loss:17.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.764                                                   	MRR:21.48	Hits@10:35.79	Best:21.48
2024-12-28 02:24:06,764: Snapshot:1	Epoch:13	Loss:24.449	translation_Loss:17.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.76                                                   	MRR:21.58	Hits@10:35.91	Best:21.58
2024-12-28 02:24:15,356: Snapshot:1	Epoch:14	Loss:24.271	translation_Loss:17.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.761                                                   	MRR:21.57	Hits@10:35.89	Best:21.58
2024-12-28 02:24:23,970: Snapshot:1	Epoch:15	Loss:24.184	translation_Loss:17.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.748                                                   	MRR:21.54	Hits@10:35.95	Best:21.58
2024-12-28 02:24:32,512: Early Stopping! Snapshot: 1 Epoch: 16 Best Results: 21.58
2024-12-28 02:24:32,512: Start to training tokens! Snapshot: 1 Epoch: 16 Loss:24.108 MRR:21.57 Best Results: 21.58
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:24:32,513: Snapshot:1	Epoch:16	Loss:24.108	translation_Loss:17.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.74                                                   	MRR:21.57	Hits@10:35.88	Best:21.58
2024-12-28 02:24:41,054: Snapshot:1	Epoch:17	Loss:205.308	translation_Loss:85.046	multi_layer_Loss:120.262	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.57	Hits@10:35.88	Best:21.58
2024-12-28 02:24:49,426: End of token training: 1 Epoch: 18 Loss:87.53 MRR:21.57 Best Results: 21.58
2024-12-28 02:24:49,426: Snapshot:1	Epoch:18	Loss:87.53	translation_Loss:84.942	multi_layer_Loss:2.588	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.57	Hits@10:35.88	Best:21.58
2024-12-28 02:24:49,784: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-28 02:24:56,606: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2501 | 0.1623 | 0.291  | 0.3436 |  0.4096 |
|     1      | 0.2134 | 0.1317 | 0.2538 | 0.3009 |  0.3615 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:25:24,613: Snapshot:2	Epoch:0	Loss:32.929	translation_Loss:31.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.997                                                   	MRR:19.46	Hits@10:34.2	Best:19.46
2024-12-28 02:25:33,435: Snapshot:2	Epoch:1	Loss:27.407	translation_Loss:24.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.727                                                   	MRR:20.06	Hits@10:35.1	Best:20.06
2024-12-28 02:25:42,279: Snapshot:2	Epoch:2	Loss:24.81	translation_Loss:20.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.235                                                   	MRR:20.45	Hits@10:35.7	Best:20.45
2024-12-28 02:25:51,067: Snapshot:2	Epoch:3	Loss:23.167	translation_Loss:17.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.274                                                   	MRR:20.7	Hits@10:36.05	Best:20.7
2024-12-28 02:25:59,877: Snapshot:2	Epoch:4	Loss:22.02	translation_Loss:16.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.957                                                   	MRR:20.89	Hits@10:36.31	Best:20.89
2024-12-28 02:26:08,654: Snapshot:2	Epoch:5	Loss:21.281	translation_Loss:14.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.394                                                   	MRR:20.98	Hits@10:36.53	Best:20.98
2024-12-28 02:26:17,458: Snapshot:2	Epoch:6	Loss:20.828	translation_Loss:14.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.679                                                   	MRR:21.08	Hits@10:36.59	Best:21.08
2024-12-28 02:26:26,235: Snapshot:2	Epoch:7	Loss:20.432	translation_Loss:13.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.872                                                   	MRR:21.13	Hits@10:36.72	Best:21.13
2024-12-28 02:26:35,139: Snapshot:2	Epoch:8	Loss:20.205	translation_Loss:13.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.995                                                   	MRR:21.14	Hits@10:36.76	Best:21.14
2024-12-28 02:26:44,006: Snapshot:2	Epoch:9	Loss:20.061	translation_Loss:12.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.088                                                   	MRR:21.16	Hits@10:36.83	Best:21.16
2024-12-28 02:26:53,370: Snapshot:2	Epoch:10	Loss:19.982	translation_Loss:12.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.137                                                   	MRR:21.19	Hits@10:36.84	Best:21.19
2024-12-28 02:27:02,131: Snapshot:2	Epoch:11	Loss:19.871	translation_Loss:12.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.18                                                   	MRR:21.17	Hits@10:36.98	Best:21.19
2024-12-28 02:27:10,967: Snapshot:2	Epoch:12	Loss:19.823	translation_Loss:12.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.21                                                   	MRR:21.19	Hits@10:36.81	Best:21.19
2024-12-28 02:27:19,873: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.19
2024-12-28 02:27:19,873: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:19.814 MRR:21.18 Best Results: 21.19
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:27:19,873: Snapshot:2	Epoch:13	Loss:19.814	translation_Loss:12.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.236                                                   	MRR:21.18	Hits@10:36.92	Best:21.19
2024-12-28 02:27:28,388: Snapshot:2	Epoch:14	Loss:209.271	translation_Loss:83.607	multi_layer_Loss:125.664	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.18	Hits@10:36.92	Best:21.19
2024-12-28 02:27:36,966: End of token training: 2 Epoch: 15 Loss:86.918 MRR:21.18 Best Results: 21.19
2024-12-28 02:27:36,967: Snapshot:2	Epoch:15	Loss:86.918	translation_Loss:83.572	multi_layer_Loss:3.345	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.18	Hits@10:36.92	Best:21.19
2024-12-28 02:27:37,260: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-28 02:27:47,210: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2505 | 0.1612 | 0.2927 | 0.3459 |  0.4142 |
|     1      | 0.2223 | 0.1386 | 0.2606 | 0.3104 |  0.3798 |
|     2      | 0.2118 | 0.1277 | 0.2506 | 0.3021 |  0.369  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:28:14,956: Snapshot:3	Epoch:0	Loss:20.117	translation_Loss:19.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.922                                                   	MRR:18.71	Hits@10:35.75	Best:18.71
2024-12-28 02:28:23,810: Snapshot:3	Epoch:1	Loss:15.459	translation_Loss:13.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.258                                                   	MRR:19.27	Hits@10:36.67	Best:19.27
2024-12-28 02:28:32,683: Snapshot:3	Epoch:2	Loss:13.658	translation_Loss:10.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.27                                                   	MRR:19.5	Hits@10:37.15	Best:19.5
2024-12-28 02:28:42,014: Snapshot:3	Epoch:3	Loss:12.771	translation_Loss:8.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.925                                                   	MRR:19.69	Hits@10:37.51	Best:19.69
2024-12-28 02:28:50,948: Snapshot:3	Epoch:4	Loss:12.241	translation_Loss:7.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.333                                                   	MRR:19.79	Hits@10:37.6	Best:19.79
2024-12-28 02:28:59,849: Snapshot:3	Epoch:5	Loss:11.99	translation_Loss:7.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.583                                                   	MRR:19.83	Hits@10:37.66	Best:19.83
2024-12-28 02:29:08,921: Snapshot:3	Epoch:6	Loss:11.796	translation_Loss:7.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.744                                                   	MRR:19.86	Hits@10:37.72	Best:19.86
2024-12-28 02:29:17,770: Snapshot:3	Epoch:7	Loss:11.745	translation_Loss:6.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.859                                                   	MRR:19.84	Hits@10:37.64	Best:19.86
2024-12-28 02:29:26,645: Snapshot:3	Epoch:8	Loss:11.684	translation_Loss:6.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.927                                                   	MRR:19.93	Hits@10:37.72	Best:19.93
2024-12-28 02:29:35,510: Snapshot:3	Epoch:9	Loss:11.669	translation_Loss:6.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.985                                                   	MRR:19.89	Hits@10:37.77	Best:19.93
2024-12-28 02:29:44,661: Snapshot:3	Epoch:10	Loss:11.651	translation_Loss:6.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.021                                                   	MRR:19.94	Hits@10:37.79	Best:19.94
2024-12-28 02:29:53,586: Snapshot:3	Epoch:11	Loss:11.598	translation_Loss:6.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.061                                                   	MRR:19.99	Hits@10:37.91	Best:19.99
2024-12-28 02:30:02,549: Snapshot:3	Epoch:12	Loss:11.608	translation_Loss:6.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.066                                                   	MRR:20.05	Hits@10:37.97	Best:20.05
2024-12-28 02:30:11,592: Snapshot:3	Epoch:13	Loss:11.64	translation_Loss:6.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.104                                                   	MRR:20.01	Hits@10:37.84	Best:20.05
2024-12-28 02:30:20,430: Snapshot:3	Epoch:14	Loss:11.637	translation_Loss:6.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.124                                                   	MRR:19.98	Hits@10:37.92	Best:20.05
2024-12-28 02:30:29,253: Early Stopping! Snapshot: 3 Epoch: 15 Best Results: 20.05
2024-12-28 02:30:29,253: Start to training tokens! Snapshot: 3 Epoch: 15 Loss:11.589 MRR:20.01 Best Results: 20.05
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:30:29,254: Snapshot:3	Epoch:15	Loss:11.589	translation_Loss:6.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.136                                                   	MRR:20.01	Hits@10:37.89	Best:20.05
2024-12-28 02:30:37,920: Snapshot:3	Epoch:16	Loss:199.48	translation_Loss:79.68	multi_layer_Loss:119.8	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.01	Hits@10:37.89	Best:20.05
2024-12-28 02:30:46,518: End of token training: 3 Epoch: 17 Loss:82.263 MRR:20.01 Best Results: 20.05
2024-12-28 02:30:46,519: Snapshot:3	Epoch:17	Loss:82.263	translation_Loss:79.608	multi_layer_Loss:2.655	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.01	Hits@10:37.89	Best:20.05
2024-12-28 02:30:46,876: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-28 02:31:00,698: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2423 | 0.1559 | 0.2786 | 0.3322 |  0.4052 |
|     1      | 0.2188 | 0.1362 | 0.2535 | 0.3053 |  0.3751 |
|     2      | 0.2127 | 0.1267 | 0.248  | 0.304  |  0.3799 |
|     3      | 0.2023 | 0.1115 | 0.236  | 0.2998 |  0.3826 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:31:28,907: Snapshot:4	Epoch:0	Loss:11.1	translation_Loss:10.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.663                                                   	MRR:19.84	Hits@10:43.78	Best:19.84
2024-12-28 02:31:37,917: Snapshot:4	Epoch:1	Loss:7.404	translation_Loss:6.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.193                                                   	MRR:20.58	Hits@10:45.62	Best:20.58
2024-12-28 02:31:46,949: Snapshot:4	Epoch:2	Loss:5.767	translation_Loss:4.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.466                                                   	MRR:20.93	Hits@10:46.09	Best:20.93
2024-12-28 02:31:55,888: Snapshot:4	Epoch:3	Loss:4.748	translation_Loss:3.145	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.603                                                   	MRR:21.32	Hits@10:46.21	Best:21.32
2024-12-28 02:32:04,804: Snapshot:4	Epoch:4	Loss:4.122	translation_Loss:2.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.665                                                   	MRR:21.43	Hits@10:46.4	Best:21.43
2024-12-28 02:32:13,876: Snapshot:4	Epoch:5	Loss:3.816	translation_Loss:2.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.683                                                   	MRR:21.77	Hits@10:46.74	Best:21.77
2024-12-28 02:32:22,890: Snapshot:4	Epoch:6	Loss:3.696	translation_Loss:1.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.715                                                   	MRR:21.93	Hits@10:46.64	Best:21.93
2024-12-28 02:32:31,756: Snapshot:4	Epoch:7	Loss:3.582	translation_Loss:1.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.723                                                   	MRR:21.8	Hits@10:46.49	Best:21.93
2024-12-28 02:32:40,665: Snapshot:4	Epoch:8	Loss:3.503	translation_Loss:1.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.717                                                   	MRR:21.84	Hits@10:46.56	Best:21.93
2024-12-28 02:32:49,538: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 21.93
2024-12-28 02:32:49,538: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:3.501 MRR:21.85 Best Results: 21.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:32:49,539: Snapshot:4	Epoch:9	Loss:3.501	translation_Loss:1.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.732                                                   	MRR:21.85	Hits@10:46.37	Best:21.93
2024-12-28 02:32:58,147: Snapshot:4	Epoch:10	Loss:187.007	translation_Loss:67.406	multi_layer_Loss:119.601	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.85	Hits@10:46.37	Best:21.93
2024-12-28 02:33:06,782: End of token training: 4 Epoch: 11 Loss:69.367 MRR:21.85 Best Results: 21.93
2024-12-28 02:33:06,783: Snapshot:4	Epoch:11	Loss:69.367	translation_Loss:67.406	multi_layer_Loss:1.961	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.85	Hits@10:46.37	Best:21.93
2024-12-28 02:33:07,138: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-28 02:33:24,708: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2325 | 0.1484 | 0.2662 | 0.319  |  0.3918 |
|     1      | 0.2089 | 0.1296 | 0.239  | 0.2915 |  0.362  |
|     2      | 0.2005 | 0.1164 | 0.2317 | 0.2887 |  0.367  |
|     3      | 0.1922 | 0.1014 | 0.2207 | 0.2868 |  0.3781 |
|     4      | 0.2212 | 0.1029 | 0.2604 |  0.35  |  0.4632 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 02:33:24,730: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2392 | 0.1551 |  0.28  | 0.3266 |  0.3913 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2501 | 0.1623 | 0.291  | 0.3436 |  0.4096 |
|     1      | 0.2134 | 0.1317 | 0.2538 | 0.3009 |  0.3615 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2505 | 0.1612 | 0.2927 | 0.3459 |  0.4142 |
|     1      | 0.2223 | 0.1386 | 0.2606 | 0.3104 |  0.3798 |
|     2      | 0.2118 | 0.1277 | 0.2506 | 0.3021 |  0.369  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2423 | 0.1559 | 0.2786 | 0.3322 |  0.4052 |
|     1      | 0.2188 | 0.1362 | 0.2535 | 0.3053 |  0.3751 |
|     2      | 0.2127 | 0.1267 | 0.248  | 0.304  |  0.3799 |
|     3      | 0.2023 | 0.1115 | 0.236  | 0.2998 |  0.3826 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2325 | 0.1484 | 0.2662 | 0.319  |  0.3918 |
|     1      | 0.2089 | 0.1296 | 0.239  | 0.2915 |  0.362  |
|     2      | 0.2005 | 0.1164 | 0.2317 | 0.2887 |  0.367  |
|     3      | 0.1922 | 0.1014 | 0.2207 | 0.2868 |  0.3781 |
|     4      | 0.2212 | 0.1029 | 0.2604 |  0.35  |  0.4632 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 02:33:24,730: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 456.57922554016113 |   0.239   |    0.155     |     0.28     |     0.391     |
|    1     | 178.9290735721588  |   0.232   |    0.147     |    0.272     |     0.386     |
|    2     | 156.81947422027588 |   0.228   |    0.143     |    0.268     |     0.388     |
|    3     | 175.68571829795837 |   0.219   |    0.133     |    0.254     |     0.386     |
|    4     | 122.34644675254822 |   0.211   |     0.12     |    0.244     |     0.392     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 02:33:24,730: Sum_Training_Time:1090.3599383831024
2024-12-28 02:33:24,731: Every_Training_Time:[456.57922554016113, 178.9290735721588, 156.81947422027588, 175.68571829795837, 122.34644675254822]
2024-12-28 02:33:24,731: Forward transfer: 0.16970000000000002 Backward transfer: -0.00814999999999999
2024-12-28 02:34:04,265: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241228023330/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=50000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 02:34:15,206: Snapshot:0	Epoch:0	Loss:102.979	translation_Loss:102.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.78	Hits@10:4.76	Best:2.78
2024-12-28 02:34:22,678: Snapshot:0	Epoch:1	Loss:93.519	translation_Loss:93.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:8.53	Best:4.54
2024-12-28 02:34:30,110: Snapshot:0	Epoch:2	Loss:84.912	translation_Loss:84.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.71	Hits@10:11.57	Best:5.71
2024-12-28 02:34:38,153: Snapshot:0	Epoch:3	Loss:77.12	translation_Loss:77.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.9	Hits@10:14.85	Best:6.9
2024-12-28 02:34:46,203: Snapshot:0	Epoch:4	Loss:69.774	translation_Loss:69.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.2	Hits@10:18.03	Best:8.2
2024-12-28 02:34:54,177: Snapshot:0	Epoch:5	Loss:62.903	translation_Loss:62.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.61	Hits@10:20.98	Best:9.61
2024-12-28 02:35:02,174: Snapshot:0	Epoch:6	Loss:56.537	translation_Loss:56.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.94	Hits@10:23.73	Best:10.94
2024-12-28 02:35:10,194: Snapshot:0	Epoch:7	Loss:50.671	translation_Loss:50.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.08	Hits@10:26.07	Best:12.08
2024-12-28 02:35:18,651: Snapshot:0	Epoch:8	Loss:45.219	translation_Loss:45.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.1	Hits@10:28.13	Best:13.1
2024-12-28 02:35:26,641: Snapshot:0	Epoch:9	Loss:40.27	translation_Loss:40.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.02	Hits@10:29.9	Best:14.02
2024-12-28 02:35:34,642: Snapshot:0	Epoch:10	Loss:35.785	translation_Loss:35.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.9	Hits@10:31.35	Best:14.9
2024-12-28 02:35:42,669: Snapshot:0	Epoch:11	Loss:31.68	translation_Loss:31.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.71	Hits@10:32.65	Best:15.71
2024-12-28 02:35:50,677: Snapshot:0	Epoch:12	Loss:28.083	translation_Loss:28.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.49	Hits@10:33.7	Best:16.49
2024-12-28 02:35:58,789: Snapshot:0	Epoch:13	Loss:24.796	translation_Loss:24.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.23	Hits@10:34.63	Best:17.23
2024-12-28 02:36:06,893: Snapshot:0	Epoch:14	Loss:21.945	translation_Loss:21.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.99	Hits@10:35.45	Best:17.99
2024-12-28 02:36:14,963: Snapshot:0	Epoch:15	Loss:19.373	translation_Loss:19.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.65	Hits@10:36.28	Best:18.65
2024-12-28 02:36:22,982: Snapshot:0	Epoch:16	Loss:17.058	translation_Loss:17.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.32	Hits@10:36.66	Best:19.32
2024-12-28 02:36:30,991: Snapshot:0	Epoch:17	Loss:15.008	translation_Loss:15.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.89	Hits@10:37.3	Best:19.89
2024-12-28 02:36:39,141: Snapshot:0	Epoch:18	Loss:13.199	translation_Loss:13.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.45	Hits@10:37.67	Best:20.45
2024-12-28 02:36:47,176: Snapshot:0	Epoch:19	Loss:11.564	translation_Loss:11.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.88	Hits@10:38.08	Best:20.88
2024-12-28 02:36:54,695: Snapshot:0	Epoch:20	Loss:10.13	translation_Loss:10.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.36	Hits@10:38.32	Best:21.36
2024-12-28 02:37:02,158: Snapshot:0	Epoch:21	Loss:8.911	translation_Loss:8.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.65	Hits@10:38.61	Best:21.65
2024-12-28 02:37:10,201: Snapshot:0	Epoch:22	Loss:7.847	translation_Loss:7.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.02	Hits@10:38.87	Best:22.02
2024-12-28 02:37:18,202: Snapshot:0	Epoch:23	Loss:6.876	translation_Loss:6.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.39	Hits@10:38.95	Best:22.39
2024-12-28 02:37:26,230: Snapshot:0	Epoch:24	Loss:6.078	translation_Loss:6.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.7	Hits@10:39.17	Best:22.7
2024-12-28 02:37:34,249: Snapshot:0	Epoch:25	Loss:5.409	translation_Loss:5.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.91	Hits@10:39.31	Best:22.91
2024-12-28 02:37:41,789: Snapshot:0	Epoch:26	Loss:4.763	translation_Loss:4.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.12	Hits@10:39.36	Best:23.12
2024-12-28 02:37:49,834: Snapshot:0	Epoch:27	Loss:4.282	translation_Loss:4.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.59	Best:23.36
2024-12-28 02:37:57,901: Snapshot:0	Epoch:28	Loss:3.817	translation_Loss:3.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.48	Hits@10:39.66	Best:23.48
2024-12-28 02:38:05,968: Snapshot:0	Epoch:29	Loss:3.417	translation_Loss:3.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:39.81	Best:23.66
2024-12-28 02:38:13,984: Snapshot:0	Epoch:30	Loss:3.097	translation_Loss:3.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.77	Hits@10:39.82	Best:23.77
2024-12-28 02:38:21,520: Snapshot:0	Epoch:31	Loss:2.835	translation_Loss:2.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.84	Hits@10:39.88	Best:23.84
2024-12-28 02:38:29,542: Snapshot:0	Epoch:32	Loss:2.571	translation_Loss:2.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:39.91	Best:23.99
2024-12-28 02:38:37,620: Snapshot:0	Epoch:33	Loss:2.349	translation_Loss:2.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.07	Hits@10:40.03	Best:24.07
2024-12-28 02:38:45,641: Snapshot:0	Epoch:34	Loss:2.142	translation_Loss:2.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:39.98	Best:24.07
2024-12-28 02:38:53,619: Snapshot:0	Epoch:35	Loss:2.008	translation_Loss:2.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:39.96	Best:24.11
2024-12-28 02:39:01,636: Snapshot:0	Epoch:36	Loss:1.841	translation_Loss:1.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.09	Hits@10:39.85	Best:24.11
2024-12-28 02:39:09,638: Snapshot:0	Epoch:37	Loss:1.731	translation_Loss:1.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:39.97	Best:24.2
2024-12-28 02:39:17,626: Snapshot:0	Epoch:38	Loss:1.602	translation_Loss:1.602	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:40.0	Best:24.2
2024-12-28 02:39:25,612: Snapshot:0	Epoch:39	Loss:1.527	translation_Loss:1.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:40.08	Best:24.24
2024-12-28 02:39:33,620: Snapshot:0	Epoch:40	Loss:1.44	translation_Loss:1.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:40.02	Best:24.24
2024-12-28 02:39:41,302: Snapshot:0	Epoch:41	Loss:1.364	translation_Loss:1.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.28	Hits@10:40.1	Best:24.28
2024-12-28 02:39:49,329: Snapshot:0	Epoch:42	Loss:1.278	translation_Loss:1.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:40.12	Best:24.41
2024-12-28 02:39:57,309: Snapshot:0	Epoch:43	Loss:1.231	translation_Loss:1.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:40.18	Best:24.41
2024-12-28 02:40:05,346: Snapshot:0	Epoch:44	Loss:1.176	translation_Loss:1.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:40.05	Best:24.43
2024-12-28 02:40:12,988: Snapshot:0	Epoch:45	Loss:1.127	translation_Loss:1.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.45	Hits@10:40.24	Best:24.45
2024-12-28 02:40:21,020: Snapshot:0	Epoch:46	Loss:1.07	translation_Loss:1.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:40.22	Best:24.49
2024-12-28 02:40:29,085: Snapshot:0	Epoch:47	Loss:1.026	translation_Loss:1.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:40.16	Best:24.49
2024-12-28 02:40:37,125: Snapshot:0	Epoch:48	Loss:1.005	translation_Loss:1.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.39	Hits@10:40.19	Best:24.49
2024-12-28 02:40:45,263: Early Stopping! Snapshot: 0 Epoch: 49 Best Results: 24.49
2024-12-28 02:40:45,264: Start to training tokens! Snapshot: 0 Epoch: 49 Loss:0.949 MRR:24.47 Best Results: 24.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:40:45,264: Snapshot:0	Epoch:49	Loss:0.949	translation_Loss:0.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.47	Hits@10:40.1	Best:24.49
2024-12-28 02:40:53,489: Snapshot:0	Epoch:50	Loss:196.93	translation_Loss:72.098	multi_layer_Loss:124.832	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.47	Hits@10:40.1	Best:24.49
2024-12-28 02:41:01,168: End of token training: 0 Epoch: 51 Loss:75.537 MRR:24.47 Best Results: 24.49
2024-12-28 02:41:01,168: Snapshot:0	Epoch:51	Loss:75.537	translation_Loss:72.195	multi_layer_Loss:3.342	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.47	Hits@10:40.1	Best:24.49
2024-12-28 02:41:01,486: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-28 02:41:04,689: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.237 | 0.1503 | 0.2806 | 0.3295 |  0.392  |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:41:31,841: Snapshot:1	Epoch:0	Loss:48.512	translation_Loss:46.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.857                                                   	MRR:18.09	Hits@10:30.89	Best:18.09
2024-12-28 02:41:40,239: Snapshot:1	Epoch:1	Loss:46.132	translation_Loss:43.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.942                                                   	MRR:18.4	Hits@10:31.4	Best:18.4
2024-12-28 02:41:48,690: Snapshot:1	Epoch:2	Loss:43.965	translation_Loss:40.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.187                                                   	MRR:18.66	Hits@10:31.76	Best:18.66
2024-12-28 02:41:57,081: Snapshot:1	Epoch:3	Loss:41.854	translation_Loss:38.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.272                                                   	MRR:18.94	Hits@10:32.1	Best:18.94
2024-12-28 02:42:05,511: Snapshot:1	Epoch:4	Loss:39.867	translation_Loss:36.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.302                                                   	MRR:19.22	Hits@10:32.45	Best:19.22
2024-12-28 02:42:13,934: Snapshot:1	Epoch:5	Loss:38.147	translation_Loss:34.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.316                                                   	MRR:19.41	Hits@10:32.69	Best:19.41
2024-12-28 02:42:22,438: Snapshot:1	Epoch:6	Loss:36.822	translation_Loss:33.508	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.314                                                   	MRR:19.6	Hits@10:32.89	Best:19.6
2024-12-28 02:42:30,885: Snapshot:1	Epoch:7	Loss:35.748	translation_Loss:32.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.3                                                   	MRR:19.74	Hits@10:33.11	Best:19.74
2024-12-28 02:42:39,367: Snapshot:1	Epoch:8	Loss:34.967	translation_Loss:31.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.281                                                   	MRR:19.84	Hits@10:33.29	Best:19.84
2024-12-28 02:42:47,788: Snapshot:1	Epoch:9	Loss:34.309	translation_Loss:31.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.263                                                   	MRR:19.96	Hits@10:33.44	Best:19.96
2024-12-28 02:42:56,235: Snapshot:1	Epoch:10	Loss:33.834	translation_Loss:30.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.238                                                   	MRR:20.07	Hits@10:33.47	Best:20.07
2024-12-28 02:43:04,705: Snapshot:1	Epoch:11	Loss:33.519	translation_Loss:30.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.219                                                   	MRR:20.09	Hits@10:33.58	Best:20.09
2024-12-28 02:43:13,229: Snapshot:1	Epoch:12	Loss:33.229	translation_Loss:30.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.206                                                   	MRR:20.12	Hits@10:33.62	Best:20.12
2024-12-28 02:43:21,744: Snapshot:1	Epoch:13	Loss:33.093	translation_Loss:29.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.193                                                   	MRR:20.25	Hits@10:33.66	Best:20.25
2024-12-28 02:43:30,735: Snapshot:1	Epoch:14	Loss:32.986	translation_Loss:29.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.188                                                   	MRR:20.26	Hits@10:33.71	Best:20.26
2024-12-28 02:43:39,238: Snapshot:1	Epoch:15	Loss:32.843	translation_Loss:29.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.178                                                   	MRR:20.24	Hits@10:33.71	Best:20.26
2024-12-28 02:43:47,697: Snapshot:1	Epoch:16	Loss:32.691	translation_Loss:29.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.165                                                   	MRR:20.25	Hits@10:33.74	Best:20.26
2024-12-28 02:43:56,139: Early Stopping! Snapshot: 1 Epoch: 17 Best Results: 20.26
2024-12-28 02:43:56,139: Start to training tokens! Snapshot: 1 Epoch: 17 Loss:32.618 MRR:20.26 Best Results: 20.26
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:43:56,139: Snapshot:1	Epoch:17	Loss:32.618	translation_Loss:29.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.169                                                   	MRR:20.26	Hits@10:33.78	Best:20.26
2024-12-28 02:44:04,388: Snapshot:1	Epoch:18	Loss:209.213	translation_Loss:88.951	multi_layer_Loss:120.262	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.26	Hits@10:33.78	Best:20.26
2024-12-28 02:44:12,629: End of token training: 1 Epoch: 19 Loss:91.478 MRR:20.26 Best Results: 20.26
2024-12-28 02:44:12,629: Snapshot:1	Epoch:19	Loss:91.478	translation_Loss:88.89	multi_layer_Loss:2.588	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.26	Hits@10:33.78	Best:20.26
2024-12-28 02:44:12,932: => no checking found at './checkpoint/FACT/1model_best.tar'
2024-12-28 02:44:19,245: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2417 | 0.1543 | 0.2847 | 0.3351 |  0.3977 |
|     1      | 0.2002 | 0.1209 | 0.2403 | 0.2858 |  0.3422 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:44:46,924: Snapshot:2	Epoch:0	Loss:41.102	translation_Loss:39.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.027                                                   	MRR:18.14	Hits@10:32.22	Best:18.14
2024-12-28 02:44:55,588: Snapshot:2	Epoch:1	Loss:39.407	translation_Loss:35.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.583                                                   	MRR:18.35	Hits@10:32.64	Best:18.35
2024-12-28 02:45:04,687: Snapshot:2	Epoch:2	Loss:38.414	translation_Loss:34.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.017                                                   	MRR:18.49	Hits@10:32.81	Best:18.49
2024-12-28 02:45:13,356: Snapshot:2	Epoch:3	Loss:37.496	translation_Loss:33.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.163                                                   	MRR:18.58	Hits@10:32.95	Best:18.58
2024-12-28 02:45:22,075: Snapshot:2	Epoch:4	Loss:36.642	translation_Loss:32.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.225                                                   	MRR:18.69	Hits@10:33.14	Best:18.69
2024-12-28 02:45:30,724: Snapshot:2	Epoch:5	Loss:35.947	translation_Loss:31.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.248                                                   	MRR:18.78	Hits@10:33.2	Best:18.78
2024-12-28 02:45:39,430: Snapshot:2	Epoch:6	Loss:35.425	translation_Loss:31.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.24                                                   	MRR:18.86	Hits@10:33.34	Best:18.86
2024-12-28 02:45:48,059: Snapshot:2	Epoch:7	Loss:35.075	translation_Loss:30.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.25                                                   	MRR:18.92	Hits@10:33.43	Best:18.92
2024-12-28 02:45:56,657: Snapshot:2	Epoch:8	Loss:34.873	translation_Loss:30.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.244                                                   	MRR:18.9	Hits@10:33.47	Best:18.92
2024-12-28 02:46:05,319: Snapshot:2	Epoch:9	Loss:34.637	translation_Loss:30.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.237                                                   	MRR:18.95	Hits@10:33.51	Best:18.95
2024-12-28 02:46:13,964: Snapshot:2	Epoch:10	Loss:34.519	translation_Loss:30.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.222                                                   	MRR:18.97	Hits@10:33.56	Best:18.97
2024-12-28 02:46:22,690: Snapshot:2	Epoch:11	Loss:34.409	translation_Loss:30.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.216                                                   	MRR:18.98	Hits@10:33.51	Best:18.98
2024-12-28 02:46:31,426: Snapshot:2	Epoch:12	Loss:34.336	translation_Loss:30.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.212                                                   	MRR:18.99	Hits@10:33.51	Best:18.99
2024-12-28 02:46:40,081: Snapshot:2	Epoch:13	Loss:34.319	translation_Loss:30.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.211                                                   	MRR:18.99	Hits@10:33.51	Best:18.99
2024-12-28 02:46:48,762: Snapshot:2	Epoch:14	Loss:34.252	translation_Loss:30.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.204                                                   	MRR:19.03	Hits@10:33.53	Best:19.03
2024-12-28 02:46:57,485: Snapshot:2	Epoch:15	Loss:34.237	translation_Loss:30.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.212                                                   	MRR:19.04	Hits@10:33.54	Best:19.04
2024-12-28 02:47:06,206: Snapshot:2	Epoch:16	Loss:34.139	translation_Loss:29.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.202                                                   	MRR:19.05	Hits@10:33.53	Best:19.05
2024-12-28 02:47:14,863: Snapshot:2	Epoch:17	Loss:34.144	translation_Loss:29.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.212                                                   	MRR:19.01	Hits@10:33.57	Best:19.05
2024-12-28 02:47:23,482: Snapshot:2	Epoch:18	Loss:34.173	translation_Loss:29.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.209                                                   	MRR:18.99	Hits@10:33.52	Best:19.05
2024-12-28 02:47:32,110: Early Stopping! Snapshot: 2 Epoch: 19 Best Results: 19.05
2024-12-28 02:47:32,111: Start to training tokens! Snapshot: 2 Epoch: 19 Loss:34.135 MRR:19.0 Best Results: 19.05
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:47:32,111: Snapshot:2	Epoch:19	Loss:34.135	translation_Loss:29.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.206                                                   	MRR:19.0	Hits@10:33.52	Best:19.05
2024-12-28 02:47:40,540: Snapshot:2	Epoch:20	Loss:215.68	translation_Loss:90.016	multi_layer_Loss:125.664	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.0	Hits@10:33.52	Best:19.05
2024-12-28 02:47:48,955: End of token training: 2 Epoch: 21 Loss:93.428 MRR:19.0 Best Results: 19.05
2024-12-28 02:47:48,956: Snapshot:2	Epoch:21	Loss:93.428	translation_Loss:90.082	multi_layer_Loss:3.345	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.0	Hits@10:33.52	Best:19.05
2024-12-28 02:47:49,334: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-28 02:47:59,532: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.1563 | 0.2869 | 0.3363 |  0.4022 |
|     1      | 0.2058 | 0.1253 | 0.2461 | 0.2923 |  0.3511 |
|     2      | 0.1888 | 0.1091 | 0.2284 | 0.274  |  0.3322 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:48:27,332: Snapshot:3	Epoch:0	Loss:33.96	translation_Loss:31.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.083                                                   	MRR:16.57	Hits@10:31.6	Best:16.57
2024-12-28 02:48:36,117: Snapshot:3	Epoch:1	Loss:32.114	translation_Loss:28.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.002                                                   	MRR:16.9	Hits@10:32.12	Best:16.9
2024-12-28 02:48:44,899: Snapshot:3	Epoch:2	Loss:31.527	translation_Loss:26.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.653                                                   	MRR:17.03	Hits@10:32.3	Best:17.03
2024-12-28 02:48:53,654: Snapshot:3	Epoch:3	Loss:31.046	translation_Loss:26.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.879                                                   	MRR:17.16	Hits@10:32.52	Best:17.16
2024-12-28 02:49:02,410: Snapshot:3	Epoch:4	Loss:30.614	translation_Loss:25.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.956                                                   	MRR:17.21	Hits@10:32.64	Best:17.21
2024-12-28 02:49:11,224: Snapshot:3	Epoch:5	Loss:30.449	translation_Loss:25.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.996                                                   	MRR:17.3	Hits@10:32.75	Best:17.3
2024-12-28 02:49:19,971: Snapshot:3	Epoch:6	Loss:30.221	translation_Loss:25.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.997                                                   	MRR:17.35	Hits@10:32.82	Best:17.35
2024-12-28 02:49:28,750: Snapshot:3	Epoch:7	Loss:30.184	translation_Loss:25.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.006                                                   	MRR:17.4	Hits@10:32.83	Best:17.4
2024-12-28 02:49:37,447: Snapshot:3	Epoch:8	Loss:30.084	translation_Loss:25.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.0                                                   	MRR:17.31	Hits@10:32.86	Best:17.4
2024-12-28 02:49:46,344: Snapshot:3	Epoch:9	Loss:30.015	translation_Loss:25.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.001                                                   	MRR:17.42	Hits@10:32.7	Best:17.42
2024-12-28 02:49:55,036: Snapshot:3	Epoch:10	Loss:29.984	translation_Loss:24.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.995                                                   	MRR:17.41	Hits@10:32.86	Best:17.42
2024-12-28 02:50:03,697: Snapshot:3	Epoch:11	Loss:29.943	translation_Loss:24.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.001                                                   	MRR:17.39	Hits@10:32.89	Best:17.42
2024-12-28 02:50:12,502: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 17.42
2024-12-28 02:50:12,502: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:29.985 MRR:17.37 Best Results: 17.42
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:50:12,502: Snapshot:3	Epoch:12	Loss:29.985	translation_Loss:24.98	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.004                                                   	MRR:17.37	Hits@10:32.81	Best:17.42
2024-12-28 02:50:21,451: Snapshot:3	Epoch:13	Loss:207.766	translation_Loss:87.966	multi_layer_Loss:119.8	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.37	Hits@10:32.81	Best:17.42
2024-12-28 02:50:29,951: End of token training: 3 Epoch: 14 Loss:90.64 MRR:17.37 Best Results: 17.42
2024-12-28 02:50:29,951: Snapshot:3	Epoch:14	Loss:90.64	translation_Loss:87.985	multi_layer_Loss:2.655	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.37	Hits@10:32.81	Best:17.42
2024-12-28 02:50:30,229: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-28 02:50:43,704: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1557 | 0.2838 | 0.3357 |  0.4016 |
|     1      | 0.2066 | 0.1255 | 0.2464 | 0.2936 |  0.3561 |
|     2      | 0.1942 | 0.1132 | 0.2332 | 0.2814 |  0.3443 |
|     3      | 0.176  | 0.0961 | 0.2071 | 0.2589 |  0.3313 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:51:11,454: Snapshot:4	Epoch:0	Loss:25.635	translation_Loss:23.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.047                                                   	MRR:17.38	Hits@10:37.69	Best:17.38
2024-12-28 02:51:20,468: Snapshot:4	Epoch:1	Loss:21.444	translation_Loss:17.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.282                                                   	MRR:18.21	Hits@10:39.51	Best:18.21
2024-12-28 02:51:29,287: Snapshot:4	Epoch:2	Loss:19.955	translation_Loss:14.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.105                                                   	MRR:18.55	Hits@10:40.08	Best:18.55
2024-12-28 02:51:38,181: Snapshot:4	Epoch:3	Loss:18.79	translation_Loss:13.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.376                                                   	MRR:18.89	Hits@10:40.33	Best:18.89
2024-12-28 02:51:47,025: Snapshot:4	Epoch:4	Loss:18.138	translation_Loss:12.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.471                                                   	MRR:19.11	Hits@10:40.54	Best:19.11
2024-12-28 02:51:55,895: Snapshot:4	Epoch:5	Loss:17.73	translation_Loss:12.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.515                                                   	MRR:19.33	Hits@10:40.46	Best:19.33
2024-12-28 02:52:04,666: Snapshot:4	Epoch:6	Loss:17.513	translation_Loss:11.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.541                                                   	MRR:19.49	Hits@10:40.59	Best:19.49
2024-12-28 02:52:13,476: Snapshot:4	Epoch:7	Loss:17.388	translation_Loss:11.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.538                                                   	MRR:19.5	Hits@10:40.59	Best:19.5
2024-12-28 02:52:22,281: Snapshot:4	Epoch:8	Loss:17.282	translation_Loss:11.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.518                                                   	MRR:19.58	Hits@10:40.72	Best:19.58
2024-12-28 02:52:31,515: Snapshot:4	Epoch:9	Loss:17.253	translation_Loss:11.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.53                                                   	MRR:19.53	Hits@10:40.62	Best:19.58
2024-12-28 02:52:40,226: Snapshot:4	Epoch:10	Loss:17.206	translation_Loss:11.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.542                                                   	MRR:19.56	Hits@10:40.72	Best:19.58
2024-12-28 02:52:49,118: Snapshot:4	Epoch:11	Loss:17.186	translation_Loss:11.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.534                                                   	MRR:19.59	Hits@10:40.58	Best:19.59
2024-12-28 02:52:57,984: Snapshot:4	Epoch:12	Loss:17.132	translation_Loss:11.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.533                                                   	MRR:19.6	Hits@10:40.75	Best:19.6
2024-12-28 02:53:06,740: Snapshot:4	Epoch:13	Loss:17.171	translation_Loss:11.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.546                                                   	MRR:19.62	Hits@10:40.89	Best:19.62
2024-12-28 02:53:15,549: Snapshot:4	Epoch:14	Loss:17.161	translation_Loss:11.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.549                                                   	MRR:19.68	Hits@10:40.83	Best:19.68
2024-12-28 02:53:24,370: Snapshot:4	Epoch:15	Loss:17.163	translation_Loss:11.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.556                                                   	MRR:19.63	Hits@10:40.78	Best:19.68
2024-12-28 02:53:33,121: Snapshot:4	Epoch:16	Loss:17.111	translation_Loss:11.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.535                                                   	MRR:19.59	Hits@10:40.73	Best:19.68
2024-12-28 02:53:41,898: Early Stopping! Snapshot: 4 Epoch: 17 Best Results: 19.68
2024-12-28 02:53:41,898: Start to training tokens! Snapshot: 4 Epoch: 17 Loss:17.11 MRR:19.54 Best Results: 19.68
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:53:41,899: Snapshot:4	Epoch:17	Loss:17.11	translation_Loss:11.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.538                                                   	MRR:19.54	Hits@10:40.76	Best:19.68
2024-12-28 02:53:50,395: Snapshot:4	Epoch:18	Loss:193.824	translation_Loss:74.223	multi_layer_Loss:119.601	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.54	Hits@10:40.76	Best:19.68
2024-12-28 02:53:58,883: End of token training: 4 Epoch: 19 Loss:76.187 MRR:19.54 Best Results: 19.68
2024-12-28 02:53:58,884: Snapshot:4	Epoch:19	Loss:76.187	translation_Loss:74.226	multi_layer_Loss:1.961	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.54	Hits@10:40.76	Best:19.68
2024-12-28 02:53:59,252: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-28 02:54:16,483: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2363 | 0.1494 | 0.278  | 0.3287 |  0.396  |
|     1      | 0.2019 | 0.1213 | 0.2401 | 0.2876 |  0.352  |
|     2      |  0.19  | 0.1096 | 0.225  | 0.2759 |  0.3432 |
|     3      | 0.1751 | 0.0924 | 0.2053 | 0.2604 |  0.3407 |
|     4      | 0.1977 | 0.095  | 0.2332 | 0.3087 |  0.4059 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 02:54:16,485: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.237 | 0.1503 | 0.2806 | 0.3295 |  0.392  |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2417 | 0.1543 | 0.2847 | 0.3351 |  0.3977 |
|     1      | 0.2002 | 0.1209 | 0.2403 | 0.2858 |  0.3422 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.1563 | 0.2869 | 0.3363 |  0.4022 |
|     1      | 0.2058 | 0.1253 | 0.2461 | 0.2923 |  0.3511 |
|     2      | 0.1888 | 0.1091 | 0.2284 | 0.274  |  0.3322 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1557 | 0.2838 | 0.3357 |  0.4016 |
|     1      | 0.2066 | 0.1255 | 0.2464 | 0.2936 |  0.3561 |
|     2      | 0.1942 | 0.1132 | 0.2332 | 0.2814 |  0.3443 |
|     3      | 0.176  | 0.0961 | 0.2071 | 0.2589 |  0.3313 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2363 | 0.1494 | 0.278  | 0.3287 |  0.396  |
|     1      | 0.2019 | 0.1213 | 0.2401 | 0.2876 |  0.352  |
|     2      |  0.19  | 0.1096 | 0.225  | 0.2759 |  0.3432 |
|     3      | 0.1751 | 0.0924 | 0.2053 | 0.2604 |  0.3407 |
|     4      | 0.1977 | 0.095  | 0.2332 | 0.3087 |  0.4059 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 02:54:16,486: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 416.9022936820984  |   0.237   |     0.15     |    0.281     |     0.392     |
|    1     | 184.59209609031677 |   0.221   |    0.138     |    0.263     |      0.37     |
|    2     | 206.17254400253296 |   0.213   |     0.13     |    0.254     |     0.362     |
|    3     | 146.82488656044006 |   0.205   |    0.123     |    0.243     |     0.358     |
|    4     | 191.52618050575256 |    0.2    |    0.114     |    0.236     |     0.368     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 02:54:16,486: Sum_Training_Time:1146.0180008411407
2024-12-28 02:54:16,486: Every_Training_Time:[416.9022936820984, 184.59209609031677, 206.17254400253296, 146.82488656044006, 191.52618050575256]
2024-12-28 02:54:16,486: Forward transfer: 0.15562499999999999 Backward transfer: 0.0003250000000000128
2024-12-28 02:54:55,873: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241228025421/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 02:55:06,540: Snapshot:0	Epoch:0	Loss:17.871	translation_Loss:17.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-28 02:55:14,169: Snapshot:0	Epoch:1	Loss:17.114	translation_Loss:17.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.51	Hits@10:1.66	Best:1.51
2024-12-28 02:55:21,447: Snapshot:0	Epoch:2	Loss:16.452	translation_Loss:16.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.96	Hits@10:3.03	Best:1.96
2024-12-28 02:55:29,084: Snapshot:0	Epoch:3	Loss:15.815	translation_Loss:15.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.85	Hits@10:5.09	Best:2.85
2024-12-28 02:55:36,261: Snapshot:0	Epoch:4	Loss:15.191	translation_Loss:15.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.68	Hits@10:6.85	Best:3.68
2024-12-28 02:55:43,479: Snapshot:0	Epoch:5	Loss:14.574	translation_Loss:14.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.35	Hits@10:8.36	Best:4.35
2024-12-28 02:55:51,045: Snapshot:0	Epoch:6	Loss:13.981	translation_Loss:13.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.94	Hits@10:9.89	Best:4.94
2024-12-28 02:55:58,216: Snapshot:0	Epoch:7	Loss:13.402	translation_Loss:13.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:11.27	Best:5.49
2024-12-28 02:56:05,939: Snapshot:0	Epoch:8	Loss:12.837	translation_Loss:12.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.74	Best:6.03
2024-12-28 02:56:13,138: Snapshot:0	Epoch:9	Loss:12.295	translation_Loss:12.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.55	Hits@10:14.16	Best:6.55
2024-12-28 02:56:20,426: Snapshot:0	Epoch:10	Loss:11.76	translation_Loss:11.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.38	Best:7.06
2024-12-28 02:56:28,044: Snapshot:0	Epoch:11	Loss:11.237	translation_Loss:11.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.51	Hits@10:16.59	Best:7.51
2024-12-28 02:56:35,224: Snapshot:0	Epoch:12	Loss:10.735	translation_Loss:10.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.88	Best:7.94
2024-12-28 02:56:42,864: Snapshot:0	Epoch:13	Loss:10.246	translation_Loss:10.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.38	Hits@10:19.02	Best:8.38
2024-12-28 02:56:50,035: Snapshot:0	Epoch:14	Loss:9.775	translation_Loss:9.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.81	Hits@10:20.31	Best:8.81
2024-12-28 02:56:57,207: Snapshot:0	Epoch:15	Loss:9.328	translation_Loss:9.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.2	Hits@10:21.48	Best:9.2
2024-12-28 02:57:04,789: Snapshot:0	Epoch:16	Loss:8.889	translation_Loss:8.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.62	Hits@10:22.59	Best:9.62
2024-12-28 02:57:11,977: Snapshot:0	Epoch:17	Loss:8.466	translation_Loss:8.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.03	Hits@10:23.64	Best:10.03
2024-12-28 02:57:19,677: Snapshot:0	Epoch:18	Loss:8.065	translation_Loss:8.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.41	Hits@10:24.68	Best:10.41
2024-12-28 02:57:26,837: Snapshot:0	Epoch:19	Loss:7.676	translation_Loss:7.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:25.59	Best:10.83
2024-12-28 02:57:34,415: Snapshot:0	Epoch:20	Loss:7.299	translation_Loss:7.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.23	Hits@10:26.47	Best:11.23
2024-12-28 02:57:41,665: Snapshot:0	Epoch:21	Loss:6.949	translation_Loss:6.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.64	Hits@10:27.3	Best:11.64
2024-12-28 02:57:48,851: Snapshot:0	Epoch:22	Loss:6.616	translation_Loss:6.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.04	Hits@10:28.08	Best:12.04
2024-12-28 02:57:56,445: Snapshot:0	Epoch:23	Loss:6.284	translation_Loss:6.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.45	Hits@10:28.72	Best:12.45
2024-12-28 02:58:03,659: Snapshot:0	Epoch:24	Loss:5.975	translation_Loss:5.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.81	Hits@10:29.37	Best:12.81
2024-12-28 02:58:11,342: Snapshot:0	Epoch:25	Loss:5.688	translation_Loss:5.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.15	Hits@10:30.0	Best:13.15
2024-12-28 02:58:18,563: Snapshot:0	Epoch:26	Loss:5.402	translation_Loss:5.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.51	Hits@10:30.64	Best:13.51
2024-12-28 02:58:25,741: Snapshot:0	Epoch:27	Loss:5.131	translation_Loss:5.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.9	Hits@10:31.14	Best:13.9
2024-12-28 02:58:33,398: Snapshot:0	Epoch:28	Loss:4.875	translation_Loss:4.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.26	Hits@10:31.64	Best:14.26
2024-12-28 02:58:40,606: Snapshot:0	Epoch:29	Loss:4.623	translation_Loss:4.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.62	Hits@10:32.17	Best:14.62
2024-12-28 02:58:47,816: Snapshot:0	Epoch:30	Loss:4.397	translation_Loss:4.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.97	Hits@10:32.55	Best:14.97
2024-12-28 02:58:55,379: Snapshot:0	Epoch:31	Loss:4.177	translation_Loss:4.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.29	Hits@10:33.11	Best:15.29
2024-12-28 02:59:02,624: Snapshot:0	Epoch:32	Loss:3.969	translation_Loss:3.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.62	Hits@10:33.47	Best:15.62
2024-12-28 02:59:10,221: Snapshot:0	Epoch:33	Loss:3.752	translation_Loss:3.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.93	Hits@10:33.9	Best:15.93
2024-12-28 02:59:17,431: Snapshot:0	Epoch:34	Loss:3.561	translation_Loss:3.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.27	Hits@10:34.29	Best:16.27
2024-12-28 02:59:24,679: Snapshot:0	Epoch:35	Loss:3.375	translation_Loss:3.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.54	Hits@10:34.63	Best:16.54
2024-12-28 02:59:32,353: Snapshot:0	Epoch:36	Loss:3.197	translation_Loss:3.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.88	Hits@10:34.9	Best:16.88
2024-12-28 02:59:39,661: Snapshot:0	Epoch:37	Loss:3.037	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.2	Hits@10:35.34	Best:17.2
2024-12-28 02:59:47,313: Snapshot:0	Epoch:38	Loss:2.874	translation_Loss:2.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.51	Hits@10:35.53	Best:17.51
2024-12-28 02:59:54,629: Snapshot:0	Epoch:39	Loss:2.719	translation_Loss:2.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.85	Hits@10:35.89	Best:17.85
2024-12-28 03:00:01,966: Snapshot:0	Epoch:40	Loss:2.579	translation_Loss:2.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.12	Hits@10:36.13	Best:18.12
2024-12-28 03:00:09,809: Snapshot:0	Epoch:41	Loss:2.44	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.4	Hits@10:36.44	Best:18.4
2024-12-28 03:00:17,033: Snapshot:0	Epoch:42	Loss:2.309	translation_Loss:2.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.62	Hits@10:36.72	Best:18.62
2024-12-28 03:00:24,656: Snapshot:0	Epoch:43	Loss:2.179	translation_Loss:2.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.88	Hits@10:36.9	Best:18.88
2024-12-28 03:00:31,884: Snapshot:0	Epoch:44	Loss:2.061	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.11	Hits@10:37.06	Best:19.11
2024-12-28 03:00:39,592: Snapshot:0	Epoch:45	Loss:1.948	translation_Loss:1.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.31	Hits@10:37.28	Best:19.31
2024-12-28 03:00:46,797: Snapshot:0	Epoch:46	Loss:1.835	translation_Loss:1.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.57	Hits@10:37.43	Best:19.57
2024-12-28 03:00:53,989: Snapshot:0	Epoch:47	Loss:1.738	translation_Loss:1.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.78	Hits@10:37.61	Best:19.78
2024-12-28 03:01:01,604: Snapshot:0	Epoch:48	Loss:1.647	translation_Loss:1.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.97	Hits@10:37.77	Best:19.97
2024-12-28 03:01:08,806: Snapshot:0	Epoch:49	Loss:1.553	translation_Loss:1.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.1	Hits@10:37.91	Best:20.1
2024-12-28 03:01:16,514: Snapshot:0	Epoch:50	Loss:1.463	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.32	Hits@10:38.03	Best:20.32
2024-12-28 03:01:23,745: Snapshot:0	Epoch:51	Loss:1.383	translation_Loss:1.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.49	Hits@10:38.12	Best:20.49
2024-12-28 03:01:30,996: Snapshot:0	Epoch:52	Loss:1.312	translation_Loss:1.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.68	Hits@10:38.24	Best:20.68
2024-12-28 03:01:38,668: Snapshot:0	Epoch:53	Loss:1.24	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.83	Hits@10:38.3	Best:20.83
2024-12-28 03:01:45,839: Snapshot:0	Epoch:54	Loss:1.17	translation_Loss:1.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.01	Hits@10:38.5	Best:21.01
2024-12-28 03:01:53,014: Snapshot:0	Epoch:55	Loss:1.108	translation_Loss:1.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.14	Hits@10:38.53	Best:21.14
2024-12-28 03:02:00,687: Snapshot:0	Epoch:56	Loss:1.05	translation_Loss:1.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.27	Hits@10:38.63	Best:21.27
2024-12-28 03:02:07,878: Snapshot:0	Epoch:57	Loss:0.995	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.39	Hits@10:38.76	Best:21.39
2024-12-28 03:02:15,450: Snapshot:0	Epoch:58	Loss:0.944	translation_Loss:0.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.54	Hits@10:38.75	Best:21.54
2024-12-28 03:02:22,695: Snapshot:0	Epoch:59	Loss:0.893	translation_Loss:0.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.67	Hits@10:38.88	Best:21.67
2024-12-28 03:02:29,957: Snapshot:0	Epoch:60	Loss:0.841	translation_Loss:0.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.77	Hits@10:38.86	Best:21.77
2024-12-28 03:02:37,580: Snapshot:0	Epoch:61	Loss:0.807	translation_Loss:0.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.88	Hits@10:38.98	Best:21.88
2024-12-28 03:02:44,814: Snapshot:0	Epoch:62	Loss:0.763	translation_Loss:0.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.02	Hits@10:38.99	Best:22.02
2024-12-28 03:02:52,478: Snapshot:0	Epoch:63	Loss:0.723	translation_Loss:0.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.11	Hits@10:39.04	Best:22.11
2024-12-28 03:02:59,697: Snapshot:0	Epoch:64	Loss:0.684	translation_Loss:0.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.15	Hits@10:39.05	Best:22.15
2024-12-28 03:03:06,895: Snapshot:0	Epoch:65	Loss:0.652	translation_Loss:0.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.28	Hits@10:39.06	Best:22.28
2024-12-28 03:03:14,494: Snapshot:0	Epoch:66	Loss:0.626	translation_Loss:0.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.32	Hits@10:39.07	Best:22.32
2024-12-28 03:03:21,744: Snapshot:0	Epoch:67	Loss:0.603	translation_Loss:0.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.46	Hits@10:39.15	Best:22.46
2024-12-28 03:03:29,412: Snapshot:0	Epoch:68	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.52	Hits@10:39.23	Best:22.52
2024-12-28 03:03:36,606: Snapshot:0	Epoch:69	Loss:0.538	translation_Loss:0.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.6	Hits@10:39.26	Best:22.6
2024-12-28 03:03:44,308: Snapshot:0	Epoch:70	Loss:0.521	translation_Loss:0.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.66	Hits@10:39.26	Best:22.66
2024-12-28 03:03:51,494: Snapshot:0	Epoch:71	Loss:0.492	translation_Loss:0.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.72	Hits@10:39.25	Best:22.72
2024-12-28 03:03:58,719: Snapshot:0	Epoch:72	Loss:0.471	translation_Loss:0.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.81	Hits@10:39.31	Best:22.81
2024-12-28 03:04:06,408: Snapshot:0	Epoch:73	Loss:0.45	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.85	Hits@10:39.33	Best:22.85
2024-12-28 03:04:13,687: Snapshot:0	Epoch:74	Loss:0.431	translation_Loss:0.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.91	Hits@10:39.42	Best:22.91
2024-12-28 03:04:21,303: Snapshot:0	Epoch:75	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.92	Hits@10:39.46	Best:22.92
2024-12-28 03:04:28,512: Snapshot:0	Epoch:76	Loss:0.397	translation_Loss:0.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.99	Hits@10:39.39	Best:22.99
2024-12-28 03:04:35,709: Snapshot:0	Epoch:77	Loss:0.384	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.03	Hits@10:39.49	Best:23.03
2024-12-28 03:04:43,509: Snapshot:0	Epoch:78	Loss:0.364	translation_Loss:0.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.05	Hits@10:39.62	Best:23.05
2024-12-28 03:04:50,760: Snapshot:0	Epoch:79	Loss:0.351	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:39.59	Best:23.1
2024-12-28 03:04:57,952: Snapshot:0	Epoch:80	Loss:0.342	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.16	Hits@10:39.72	Best:23.16
2024-12-28 03:05:05,720: Snapshot:0	Epoch:81	Loss:0.33	translation_Loss:0.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.19	Hits@10:39.65	Best:23.19
2024-12-28 03:05:12,875: Snapshot:0	Epoch:82	Loss:0.317	translation_Loss:0.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.18	Hits@10:39.7	Best:23.19
2024-12-28 03:05:20,516: Snapshot:0	Epoch:83	Loss:0.302	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.21	Hits@10:39.76	Best:23.21
2024-12-28 03:05:27,755: Snapshot:0	Epoch:84	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.27	Hits@10:39.77	Best:23.27
2024-12-28 03:05:35,014: Snapshot:0	Epoch:85	Loss:0.285	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.29	Hits@10:39.72	Best:23.29
2024-12-28 03:05:42,670: Snapshot:0	Epoch:86	Loss:0.273	translation_Loss:0.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.32	Hits@10:39.71	Best:23.32
2024-12-28 03:05:49,858: Snapshot:0	Epoch:87	Loss:0.269	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.34	Hits@10:39.72	Best:23.34
2024-12-28 03:05:57,521: Snapshot:0	Epoch:88	Loss:0.256	translation_Loss:0.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.76	Best:23.36
2024-12-28 03:06:04,757: Snapshot:0	Epoch:89	Loss:0.25	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.79	Best:23.36
2024-12-28 03:06:11,961: Snapshot:0	Epoch:90	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.37	Hits@10:39.85	Best:23.37
2024-12-28 03:06:19,742: Snapshot:0	Epoch:91	Loss:0.233	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:39.8	Best:23.43
2024-12-28 03:06:26,961: Snapshot:0	Epoch:92	Loss:0.227	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:39.75	Best:23.45
2024-12-28 03:06:34,704: Snapshot:0	Epoch:93	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.48	Hits@10:39.72	Best:23.48
2024-12-28 03:06:42,057: Snapshot:0	Epoch:94	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.48	Hits@10:39.72	Best:23.48
2024-12-28 03:06:49,681: Snapshot:0	Epoch:95	Loss:0.209	translation_Loss:0.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.5	Hits@10:39.76	Best:23.5
2024-12-28 03:06:56,939: Snapshot:0	Epoch:96	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.52	Hits@10:39.81	Best:23.52
2024-12-28 03:07:04,223: Snapshot:0	Epoch:97	Loss:0.194	translation_Loss:0.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.54	Hits@10:39.87	Best:23.54
2024-12-28 03:07:11,869: Snapshot:0	Epoch:98	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.54	Hits@10:39.82	Best:23.54
2024-12-28 03:07:19,114: Snapshot:0	Epoch:99	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.6	Hits@10:39.85	Best:23.6
2024-12-28 03:07:26,851: Snapshot:0	Epoch:100	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.6	Hits@10:39.81	Best:23.6
2024-12-28 03:07:34,157: Snapshot:0	Epoch:101	Loss:0.178	translation_Loss:0.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.82	Best:23.63
2024-12-28 03:07:41,399: Snapshot:0	Epoch:102	Loss:0.173	translation_Loss:0.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.62	Hits@10:39.8	Best:23.63
2024-12-28 03:07:49,000: Snapshot:0	Epoch:103	Loss:0.174	translation_Loss:0.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.83	Best:23.64
2024-12-28 03:07:56,205: Snapshot:0	Epoch:104	Loss:0.164	translation_Loss:0.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.9	Best:23.67
2024-12-28 03:08:03,414: Snapshot:0	Epoch:105	Loss:0.161	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.96	Best:23.67
2024-12-28 03:08:11,074: Snapshot:0	Epoch:106	Loss:0.157	translation_Loss:0.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:39.92	Best:23.67
2024-12-28 03:08:18,317: Snapshot:0	Epoch:107	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.89	Best:23.69
2024-12-28 03:08:26,010: Snapshot:0	Epoch:108	Loss:0.152	translation_Loss:0.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.72	Hits@10:39.94	Best:23.72
2024-12-28 03:08:33,295: Snapshot:0	Epoch:109	Loss:0.146	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.92	Best:23.76
2024-12-28 03:08:40,513: Snapshot:0	Epoch:110	Loss:0.143	translation_Loss:0.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:39.94	Best:23.76
2024-12-28 03:08:48,081: Snapshot:0	Epoch:111	Loss:0.147	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:40.01	Best:23.76
2024-12-28 03:08:55,276: Early Stopping! Snapshot: 0 Epoch: 112 Best Results: 23.76
2024-12-28 03:08:55,276: Start to training tokens! Snapshot: 0 Epoch: 112 Loss:0.14 MRR:23.74 Best Results: 23.76
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 03:08:55,277: Snapshot:0	Epoch:112	Loss:0.14	translation_Loss:0.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:40.03	Best:23.76
2024-12-28 03:09:03,403: Snapshot:0	Epoch:113	Loss:57.284	translation_Loss:12.179	multi_layer_Loss:45.105	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:40.03	Best:23.76
2024-12-28 03:09:10,664: End of token training: 0 Epoch: 114 Loss:45.66 MRR:23.74 Best Results: 23.76
2024-12-28 03:09:10,665: Snapshot:0	Epoch:114	Loss:45.66	translation_Loss:12.198	multi_layer_Loss:33.462	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.74	Hits@10:40.03	Best:23.76
2024-12-28 03:09:10,927: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-28 03:09:13,892: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2312 | 0.1418 | 0.2792 | 0.327  |  0.3886 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 03:09:40,805: Snapshot:1	Epoch:0	Loss:8.724	translation_Loss:8.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.059                                                   	MRR:17.29	Hits@10:30.22	Best:17.29
2024-12-28 03:09:48,642: Snapshot:1	Epoch:1	Loss:7.831	translation_Loss:7.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:17.91	Hits@10:31.17	Best:17.91
2024-12-28 03:09:56,459: Snapshot:1	Epoch:2	Loss:7.307	translation_Loss:6.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:18.33	Hits@10:31.78	Best:18.33
2024-12-28 03:10:04,381: Snapshot:1	Epoch:3	Loss:6.94	translation_Loss:6.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.488                                                   	MRR:18.61	Hits@10:32.29	Best:18.61
2024-12-28 03:10:12,330: Snapshot:1	Epoch:4	Loss:6.653	translation_Loss:6.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.627                                                   	MRR:18.81	Hits@10:32.67	Best:18.81
2024-12-28 03:10:20,208: Snapshot:1	Epoch:5	Loss:6.407	translation_Loss:5.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.746                                                   	MRR:19.03	Hits@10:32.97	Best:19.03
2024-12-28 03:10:28,477: Snapshot:1	Epoch:6	Loss:6.188	translation_Loss:5.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.845                                                   	MRR:19.2	Hits@10:33.32	Best:19.2
2024-12-28 03:10:36,298: Snapshot:1	Epoch:7	Loss:5.989	translation_Loss:5.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.929                                                   	MRR:19.4	Hits@10:33.65	Best:19.4
2024-12-28 03:10:44,179: Snapshot:1	Epoch:8	Loss:5.805	translation_Loss:4.805	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.999                                                   	MRR:19.6	Hits@10:33.86	Best:19.6
2024-12-28 03:10:52,440: Snapshot:1	Epoch:9	Loss:5.649	translation_Loss:4.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.059                                                   	MRR:19.73	Hits@10:34.03	Best:19.73
2024-12-28 03:11:00,264: Snapshot:1	Epoch:10	Loss:5.498	translation_Loss:4.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.107                                                   	MRR:19.85	Hits@10:34.22	Best:19.85
2024-12-28 03:11:08,571: Snapshot:1	Epoch:11	Loss:5.348	translation_Loss:4.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.147                                                   	MRR:19.97	Hits@10:34.37	Best:19.97
2024-12-28 03:11:16,380: Snapshot:1	Epoch:12	Loss:5.216	translation_Loss:4.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.181                                                   	MRR:20.08	Hits@10:34.62	Best:20.08
2024-12-28 03:11:24,264: Snapshot:1	Epoch:13	Loss:5.107	translation_Loss:3.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.209                                                   	MRR:20.16	Hits@10:34.81	Best:20.16
2024-12-28 03:11:32,600: Snapshot:1	Epoch:14	Loss:5.016	translation_Loss:3.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.232                                                   	MRR:20.29	Hits@10:34.84	Best:20.29
2024-12-28 03:11:40,435: Snapshot:1	Epoch:15	Loss:4.926	translation_Loss:3.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.251                                                   	MRR:20.36	Hits@10:34.98	Best:20.36
2024-12-28 03:11:48,709: Snapshot:1	Epoch:16	Loss:4.845	translation_Loss:3.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.266                                                   	MRR:20.43	Hits@10:35.09	Best:20.43
2024-12-28 03:11:56,517: Snapshot:1	Epoch:17	Loss:4.766	translation_Loss:3.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.277                                                   	MRR:20.53	Hits@10:35.21	Best:20.53
2024-12-28 03:12:04,529: Snapshot:1	Epoch:18	Loss:4.706	translation_Loss:3.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.286                                                   	MRR:20.64	Hits@10:35.34	Best:20.64
2024-12-28 03:12:12,804: Snapshot:1	Epoch:19	Loss:4.664	translation_Loss:3.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.294                                                   	MRR:20.67	Hits@10:35.37	Best:20.67
2024-12-28 03:12:20,691: Snapshot:1	Epoch:20	Loss:4.612	translation_Loss:3.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.3                                                   	MRR:20.75	Hits@10:35.4	Best:20.75
2024-12-28 03:12:28,506: Snapshot:1	Epoch:21	Loss:4.584	translation_Loss:3.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.304                                                   	MRR:20.8	Hits@10:35.51	Best:20.8
2024-12-28 03:12:36,784: Snapshot:1	Epoch:22	Loss:4.55	translation_Loss:3.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.307                                                   	MRR:20.8	Hits@10:35.6	Best:20.8
2024-12-28 03:12:44,653: Snapshot:1	Epoch:23	Loss:4.524	translation_Loss:3.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.309                                                   	MRR:20.82	Hits@10:35.67	Best:20.82
2024-12-28 03:12:52,866: Snapshot:1	Epoch:24	Loss:4.502	translation_Loss:3.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.311                                                   	MRR:20.88	Hits@10:35.76	Best:20.88
2024-12-28 03:13:00,664: Snapshot:1	Epoch:25	Loss:4.482	translation_Loss:3.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.312                                                   	MRR:20.91	Hits@10:35.8	Best:20.91
2024-12-28 03:13:08,423: Snapshot:1	Epoch:26	Loss:4.465	translation_Loss:3.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.313                                                   	MRR:20.9	Hits@10:35.8	Best:20.91
2024-12-28 03:13:16,654: Snapshot:1	Epoch:27	Loss:4.434	translation_Loss:3.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.314                                                   	MRR:20.95	Hits@10:35.83	Best:20.95
2024-12-28 03:13:24,518: Snapshot:1	Epoch:28	Loss:4.429	translation_Loss:3.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.314                                                   	MRR:21.02	Hits@10:35.86	Best:21.02
2024-12-28 03:13:32,836: Snapshot:1	Epoch:29	Loss:4.418	translation_Loss:3.104	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.314                                                   	MRR:20.97	Hits@10:35.93	Best:21.02
2024-12-28 03:13:40,812: Snapshot:1	Epoch:30	Loss:4.404	translation_Loss:3.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.315                                                   	MRR:21.0	Hits@10:35.82	Best:21.02
2024-12-28 03:13:48,712: Early Stopping! Snapshot: 1 Epoch: 31 Best Results: 21.02
2024-12-28 03:13:48,713: Start to training tokens! Snapshot: 1 Epoch: 31 Loss:4.385 MRR:21.02 Best Results: 21.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 03:13:48,713: Snapshot:1	Epoch:31	Loss:4.385	translation_Loss:3.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.314                                                   	MRR:21.02	Hits@10:35.87	Best:21.02
2024-12-28 03:13:56,890: Snapshot:1	Epoch:32	Loss:58.921	translation_Loss:14.395	multi_layer_Loss:44.526	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.02	Hits@10:35.87	Best:21.02
2024-12-28 03:14:04,602: End of token training: 1 Epoch: 33 Loss:47.114 MRR:21.02 Best Results: 21.02
2024-12-28 03:14:04,602: Snapshot:1	Epoch:33	Loss:47.114	translation_Loss:14.398	multi_layer_Loss:32.716	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.02	Hits@10:35.87	Best:21.02
2024-12-28 03:14:04,869: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-28 03:14:11,716: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2435 | 0.1526 | 0.288  | 0.341  |  0.4099 |
|     1      | 0.2083 | 0.1225 | 0.2538 | 0.3016 |  0.3601 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 03:14:38,408: Snapshot:2	Epoch:0	Loss:6.545	translation_Loss:6.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.06                                                   	MRR:18.59	Hits@10:33.38	Best:18.59
2024-12-28 03:14:46,609: Snapshot:2	Epoch:1	Loss:5.657	translation_Loss:5.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:19.04	Hits@10:34.14	Best:19.04
2024-12-28 03:14:54,726: Snapshot:2	Epoch:2	Loss:5.155	translation_Loss:4.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.321                                                   	MRR:19.39	Hits@10:34.72	Best:19.39
2024-12-28 03:15:02,785: Snapshot:2	Epoch:3	Loss:4.815	translation_Loss:4.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.471                                                   	MRR:19.68	Hits@10:35.01	Best:19.68
2024-12-28 03:15:10,813: Snapshot:2	Epoch:4	Loss:4.577	translation_Loss:3.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.608                                                   	MRR:19.9	Hits@10:35.41	Best:19.9
2024-12-28 03:15:19,265: Snapshot:2	Epoch:5	Loss:4.397	translation_Loss:3.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.728                                                   	MRR:20.05	Hits@10:35.63	Best:20.05
2024-12-28 03:15:27,286: Snapshot:2	Epoch:6	Loss:4.244	translation_Loss:3.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.832                                                   	MRR:20.18	Hits@10:35.83	Best:20.18
2024-12-28 03:15:35,704: Snapshot:2	Epoch:7	Loss:4.119	translation_Loss:3.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.918                                                   	MRR:20.29	Hits@10:36.05	Best:20.29
2024-12-28 03:15:43,807: Snapshot:2	Epoch:8	Loss:4.015	translation_Loss:3.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.991                                                   	MRR:20.36	Hits@10:36.15	Best:20.36
2024-12-28 03:15:51,855: Snapshot:2	Epoch:9	Loss:3.908	translation_Loss:2.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.052                                                   	MRR:20.46	Hits@10:36.29	Best:20.46
2024-12-28 03:16:00,407: Snapshot:2	Epoch:10	Loss:3.842	translation_Loss:2.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.102                                                   	MRR:20.51	Hits@10:36.36	Best:20.51
2024-12-28 03:16:08,454: Snapshot:2	Epoch:11	Loss:3.769	translation_Loss:2.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.144                                                   	MRR:20.55	Hits@10:36.48	Best:20.55
2024-12-28 03:16:16,501: Snapshot:2	Epoch:12	Loss:3.71	translation_Loss:2.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.179                                                   	MRR:20.62	Hits@10:36.55	Best:20.62
2024-12-28 03:16:24,896: Snapshot:2	Epoch:13	Loss:3.665	translation_Loss:2.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.208                                                   	MRR:20.63	Hits@10:36.68	Best:20.63
2024-12-28 03:16:32,936: Snapshot:2	Epoch:14	Loss:3.63	translation_Loss:2.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.233                                                   	MRR:20.71	Hits@10:36.79	Best:20.71
2024-12-28 03:16:41,464: Snapshot:2	Epoch:15	Loss:3.593	translation_Loss:2.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.254                                                   	MRR:20.75	Hits@10:36.87	Best:20.75
2024-12-28 03:16:49,400: Snapshot:2	Epoch:16	Loss:3.562	translation_Loss:2.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.271                                                   	MRR:20.74	Hits@10:36.91	Best:20.75
2024-12-28 03:16:57,434: Snapshot:2	Epoch:17	Loss:3.537	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.284                                                   	MRR:20.79	Hits@10:36.92	Best:20.79
2024-12-28 03:17:05,978: Snapshot:2	Epoch:18	Loss:3.513	translation_Loss:2.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.296                                                   	MRR:20.8	Hits@10:36.96	Best:20.8
2024-12-28 03:17:13,972: Snapshot:2	Epoch:19	Loss:3.513	translation_Loss:2.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.307                                                   	MRR:20.79	Hits@10:36.94	Best:20.8
2024-12-28 03:17:22,429: Snapshot:2	Epoch:20	Loss:3.499	translation_Loss:2.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.315                                                   	MRR:20.81	Hits@10:36.99	Best:20.81
2024-12-28 03:17:30,535: Snapshot:2	Epoch:21	Loss:3.478	translation_Loss:2.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.322                                                   	MRR:20.82	Hits@10:36.96	Best:20.82
2024-12-28 03:17:38,521: Snapshot:2	Epoch:22	Loss:3.471	translation_Loss:2.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.328                                                   	MRR:20.82	Hits@10:36.98	Best:20.82
2024-12-28 03:17:46,948: Snapshot:2	Epoch:23	Loss:3.463	translation_Loss:2.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.334                                                   	MRR:20.83	Hits@10:36.92	Best:20.83
2024-12-28 03:17:54,962: Snapshot:2	Epoch:24	Loss:3.464	translation_Loss:2.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.339                                                   	MRR:20.83	Hits@10:37.03	Best:20.83
2024-12-28 03:18:03,398: Snapshot:2	Epoch:25	Loss:3.454	translation_Loss:2.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:20.87	Hits@10:37.1	Best:20.87
2024-12-28 03:18:11,470: Snapshot:2	Epoch:26	Loss:3.445	translation_Loss:2.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.346                                                   	MRR:20.87	Hits@10:37.11	Best:20.87
2024-12-28 03:18:20,025: Snapshot:2	Epoch:27	Loss:3.437	translation_Loss:2.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.35                                                   	MRR:20.9	Hits@10:37.08	Best:20.9
2024-12-28 03:18:28,011: Snapshot:2	Epoch:28	Loss:3.439	translation_Loss:2.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.354                                                   	MRR:20.87	Hits@10:37.09	Best:20.9
2024-12-28 03:18:36,087: Snapshot:2	Epoch:29	Loss:3.435	translation_Loss:2.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.357                                                   	MRR:20.92	Hits@10:37.09	Best:20.92
2024-12-28 03:18:44,494: Snapshot:2	Epoch:30	Loss:3.439	translation_Loss:2.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.359                                                   	MRR:20.91	Hits@10:37.15	Best:20.92
2024-12-28 03:18:52,476: Snapshot:2	Epoch:31	Loss:3.443	translation_Loss:2.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.361                                                   	MRR:20.91	Hits@10:37.11	Best:20.92
2024-12-28 03:19:00,828: Early Stopping! Snapshot: 2 Epoch: 32 Best Results: 20.92
2024-12-28 03:19:00,828: Start to training tokens! Snapshot: 2 Epoch: 32 Loss:3.422 MRR:20.87 Best Results: 20.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 03:19:00,828: Snapshot:2	Epoch:32	Loss:3.422	translation_Loss:2.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.363                                                   	MRR:20.87	Hits@10:37.05	Best:20.92
2024-12-28 03:19:08,653: Snapshot:2	Epoch:33	Loss:59.661	translation_Loss:14.058	multi_layer_Loss:45.603	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.87	Hits@10:37.05	Best:20.92
2024-12-28 03:19:16,549: End of token training: 2 Epoch: 34 Loss:47.995 MRR:20.87 Best Results: 20.92
2024-12-28 03:19:16,550: Snapshot:2	Epoch:34	Loss:47.995	translation_Loss:14.064	multi_layer_Loss:33.931	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.87	Hits@10:37.05	Best:20.92
2024-12-28 03:19:16,881: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-28 03:19:26,967: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1522 | 0.2857 | 0.3405 |  0.4102 |
|     1      | 0.2167 | 0.1303 | 0.2573 | 0.3082 |  0.3778 |
|     2      | 0.2105 | 0.1241 |  0.25  | 0.3026 |  0.3712 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 03:19:54,006: Snapshot:3	Epoch:0	Loss:4.071	translation_Loss:4.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.061                                                   	MRR:18.27	Hits@10:34.84	Best:18.27
2024-12-28 03:20:02,168: Snapshot:3	Epoch:1	Loss:3.265	translation_Loss:3.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:18.8	Hits@10:35.86	Best:18.8
2024-12-28 03:20:10,590: Snapshot:3	Epoch:2	Loss:2.842	translation_Loss:2.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.274                                                   	MRR:19.09	Hits@10:36.54	Best:19.09
2024-12-28 03:20:18,679: Snapshot:3	Epoch:3	Loss:2.594	translation_Loss:2.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.379                                                   	MRR:19.33	Hits@10:37.07	Best:19.33
2024-12-28 03:20:26,743: Snapshot:3	Epoch:4	Loss:2.433	translation_Loss:1.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.469                                                   	MRR:19.49	Hits@10:37.3	Best:19.49
2024-12-28 03:20:34,833: Snapshot:3	Epoch:5	Loss:2.317	translation_Loss:1.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.544                                                   	MRR:19.63	Hits@10:37.51	Best:19.63
2024-12-28 03:20:43,060: Snapshot:3	Epoch:6	Loss:2.219	translation_Loss:1.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.605                                                   	MRR:19.66	Hits@10:37.75	Best:19.66
2024-12-28 03:20:51,648: Snapshot:3	Epoch:7	Loss:2.161	translation_Loss:1.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.655                                                   	MRR:19.76	Hits@10:37.85	Best:19.76
2024-12-28 03:20:59,728: Snapshot:3	Epoch:8	Loss:2.099	translation_Loss:1.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.694                                                   	MRR:19.79	Hits@10:37.88	Best:19.79
2024-12-28 03:21:08,221: Snapshot:3	Epoch:9	Loss:2.053	translation_Loss:1.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.727                                                   	MRR:19.85	Hits@10:37.95	Best:19.85
2024-12-28 03:21:16,333: Snapshot:3	Epoch:10	Loss:2.019	translation_Loss:1.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.753                                                   	MRR:19.89	Hits@10:38.02	Best:19.89
2024-12-28 03:21:24,447: Snapshot:3	Epoch:11	Loss:1.995	translation_Loss:1.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.775                                                   	MRR:19.93	Hits@10:37.98	Best:19.93
2024-12-28 03:21:33,026: Snapshot:3	Epoch:12	Loss:1.974	translation_Loss:1.182	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.792                                                   	MRR:19.98	Hits@10:38.12	Best:19.98
2024-12-28 03:21:41,265: Snapshot:3	Epoch:13	Loss:1.955	translation_Loss:1.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.808                                                   	MRR:19.99	Hits@10:38.15	Best:19.99
2024-12-28 03:21:49,824: Snapshot:3	Epoch:14	Loss:1.942	translation_Loss:1.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.818                                                   	MRR:20.06	Hits@10:38.25	Best:20.06
2024-12-28 03:21:57,892: Snapshot:3	Epoch:15	Loss:1.917	translation_Loss:1.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.828                                                   	MRR:20.06	Hits@10:38.12	Best:20.06
2024-12-28 03:22:06,421: Snapshot:3	Epoch:16	Loss:1.923	translation_Loss:1.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.835                                                   	MRR:20.04	Hits@10:38.3	Best:20.06
2024-12-28 03:22:14,461: Early Stopping! Snapshot: 3 Epoch: 17 Best Results: 20.06
2024-12-28 03:22:14,461: Start to training tokens! Snapshot: 3 Epoch: 17 Loss:1.913 MRR:20.02 Best Results: 20.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 03:22:14,461: Snapshot:3	Epoch:17	Loss:1.913	translation_Loss:1.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.842                                                   	MRR:20.02	Hits@10:38.31	Best:20.06
2024-12-28 03:22:22,479: Snapshot:3	Epoch:18	Loss:58.23	translation_Loss:13.382	multi_layer_Loss:44.849	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.02	Hits@10:38.31	Best:20.06
2024-12-28 03:22:30,763: End of token training: 3 Epoch: 19 Loss:46.18 MRR:20.02 Best Results: 20.06
2024-12-28 03:22:30,764: Snapshot:3	Epoch:19	Loss:46.18	translation_Loss:13.389	multi_layer_Loss:32.79	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.02	Hits@10:38.31	Best:20.06
2024-12-28 03:22:31,054: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-28 03:22:44,549: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2333 | 0.1455 | 0.2705 | 0.3252 |  0.399  |
|     1      | 0.2125 | 0.1273 | 0.2507 | 0.3025 |  0.3747 |
|     2      | 0.208  | 0.1188 | 0.2447 | 0.3038 |  0.3807 |
|     3      | 0.2015 | 0.1073 | 0.2388 | 0.3029 |  0.3858 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 03:23:11,436: Snapshot:4	Epoch:0	Loss:2.391	translation_Loss:2.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.054                                                   	MRR:18.5	Hits@10:41.2	Best:18.5
2024-12-28 03:23:19,572: Snapshot:4	Epoch:1	Loss:1.724	translation_Loss:1.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.119                                                   	MRR:19.89	Hits@10:44.13	Best:19.89
2024-12-28 03:23:27,732: Snapshot:4	Epoch:2	Loss:1.431	translation_Loss:1.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:20.61	Hits@10:45.76	Best:20.61
2024-12-28 03:23:35,837: Snapshot:4	Epoch:3	Loss:1.25	translation_Loss:1.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:20.99	Hits@10:46.47	Best:20.99
2024-12-28 03:23:44,076: Snapshot:4	Epoch:4	Loss:1.125	translation_Loss:0.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:21.16	Hits@10:46.9	Best:21.16
2024-12-28 03:23:52,182: Snapshot:4	Epoch:5	Loss:1.007	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.233                                                   	MRR:21.36	Hits@10:47.02	Best:21.36
2024-12-28 03:24:00,812: Snapshot:4	Epoch:6	Loss:0.912	translation_Loss:0.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:21.44	Hits@10:47.34	Best:21.44
2024-12-28 03:24:08,921: Snapshot:4	Epoch:7	Loss:0.83	translation_Loss:0.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:21.54	Hits@10:47.41	Best:21.54
2024-12-28 03:24:17,560: Snapshot:4	Epoch:8	Loss:0.766	translation_Loss:0.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.265                                                   	MRR:21.67	Hits@10:47.45	Best:21.67
2024-12-28 03:24:25,671: Snapshot:4	Epoch:9	Loss:0.705	translation_Loss:0.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.27                                                   	MRR:21.78	Hits@10:47.45	Best:21.78
2024-12-28 03:24:33,754: Snapshot:4	Epoch:10	Loss:0.671	translation_Loss:0.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.273                                                   	MRR:21.95	Hits@10:47.58	Best:21.95
2024-12-28 03:24:42,457: Snapshot:4	Epoch:11	Loss:0.641	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.275                                                   	MRR:22.11	Hits@10:47.67	Best:22.11
2024-12-28 03:24:50,690: Snapshot:4	Epoch:12	Loss:0.614	translation_Loss:0.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.277                                                   	MRR:22.07	Hits@10:47.64	Best:22.11
2024-12-28 03:24:59,270: Snapshot:4	Epoch:13	Loss:0.6	translation_Loss:0.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.278                                                   	MRR:22.17	Hits@10:47.73	Best:22.17
2024-12-28 03:25:07,584: Snapshot:4	Epoch:14	Loss:0.586	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.279                                                   	MRR:22.28	Hits@10:47.59	Best:22.28
2024-12-28 03:25:15,724: Snapshot:4	Epoch:15	Loss:0.581	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.279                                                   	MRR:22.29	Hits@10:47.75	Best:22.29
2024-12-28 03:25:24,224: Snapshot:4	Epoch:16	Loss:0.57	translation_Loss:0.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.28                                                   	MRR:22.25	Hits@10:47.74	Best:22.29
2024-12-28 03:25:32,441: Snapshot:4	Epoch:17	Loss:0.565	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.279                                                   	MRR:22.25	Hits@10:47.92	Best:22.29
2024-12-28 03:25:40,960: Snapshot:4	Epoch:18	Loss:0.56	translation_Loss:0.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:22.39	Hits@10:47.88	Best:22.39
2024-12-28 03:25:49,015: Snapshot:4	Epoch:19	Loss:0.559	translation_Loss:0.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.28                                                   	MRR:22.27	Hits@10:47.76	Best:22.39
2024-12-28 03:25:57,126: Snapshot:4	Epoch:20	Loss:0.552	translation_Loss:0.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.28                                                   	MRR:22.29	Hits@10:47.83	Best:22.39
2024-12-28 03:26:05,740: Snapshot:4	Epoch:21	Loss:0.549	translation_Loss:0.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:22.42	Hits@10:47.99	Best:22.42
2024-12-28 03:26:13,870: Snapshot:4	Epoch:22	Loss:0.549	translation_Loss:0.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:22.6	Hits@10:48.03	Best:22.6
2024-12-28 03:26:22,433: Snapshot:4	Epoch:23	Loss:0.548	translation_Loss:0.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:22.63	Hits@10:48.19	Best:22.63
2024-12-28 03:26:30,501: Snapshot:4	Epoch:24	Loss:0.548	translation_Loss:0.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:22.46	Hits@10:47.86	Best:22.63
2024-12-28 03:26:38,649: Snapshot:4	Epoch:25	Loss:0.548	translation_Loss:0.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:22.5	Hits@10:48.13	Best:22.63
2024-12-28 03:26:47,159: Early Stopping! Snapshot: 4 Epoch: 26 Best Results: 22.63
2024-12-28 03:26:47,159: Start to training tokens! Snapshot: 4 Epoch: 26 Loss:0.544 MRR:22.6 Best Results: 22.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 03:26:47,159: Snapshot:4	Epoch:26	Loss:0.544	translation_Loss:0.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.282                                                   	MRR:22.6	Hits@10:48.09	Best:22.63
2024-12-28 03:26:55,110: Snapshot:4	Epoch:27	Loss:56.321	translation_Loss:11.23	multi_layer_Loss:45.091	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.6	Hits@10:48.09	Best:22.63
2024-12-28 03:27:03,515: End of token training: 4 Epoch: 28 Loss:44.215 MRR:22.6 Best Results: 22.63
2024-12-28 03:27:03,515: Snapshot:4	Epoch:28	Loss:44.215	translation_Loss:11.233	multi_layer_Loss:32.982	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.6	Hits@10:48.09	Best:22.63
2024-12-28 03:27:03,805: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-28 03:27:20,830: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2192 | 0.1333 | 0.2534 | 0.3078 |  0.3838 |
|     1      | 0.1985 | 0.1166 | 0.2309 | 0.2842 |  0.3561 |
|     2      | 0.1941 | 0.1086 | 0.2246 | 0.2829 |  0.3657 |
|     3      | 0.1881 | 0.0941 | 0.2198 | 0.2846 |  0.3799 |
|     4      | 0.2274 | 0.1064 | 0.2682 | 0.3594 |  0.4802 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 03:27:20,832: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2312 | 0.1418 | 0.2792 | 0.327  |  0.3886 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2435 | 0.1526 | 0.288  | 0.341  |  0.4099 |
|     1      | 0.2083 | 0.1225 | 0.2538 | 0.3016 |  0.3601 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1522 | 0.2857 | 0.3405 |  0.4102 |
|     1      | 0.2167 | 0.1303 | 0.2573 | 0.3082 |  0.3778 |
|     2      | 0.2105 | 0.1241 |  0.25  | 0.3026 |  0.3712 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2333 | 0.1455 | 0.2705 | 0.3252 |  0.399  |
|     1      | 0.2125 | 0.1273 | 0.2507 | 0.3025 |  0.3747 |
|     2      | 0.208  | 0.1188 | 0.2447 | 0.3038 |  0.3807 |
|     3      | 0.2015 | 0.1073 | 0.2388 | 0.3029 |  0.3858 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2192 | 0.1333 | 0.2534 | 0.3078 |  0.3838 |
|     1      | 0.1985 | 0.1166 | 0.2309 | 0.2842 |  0.3561 |
|     2      | 0.1941 | 0.1086 | 0.2246 | 0.2829 |  0.3657 |
|     3      | 0.1881 | 0.0941 | 0.2198 | 0.2846 |  0.3799 |
|     4      | 0.2274 | 0.1064 | 0.2682 | 0.3594 |  0.4802 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 03:27:20,833: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 854.7905385494232  |   0.231   |    0.142     |    0.279     |     0.389     |
|    1     | 287.3886013031006  |   0.226   |    0.138     |    0.271     |     0.385     |
|    2     | 301.59942507743835 |   0.223   |    0.136     |    0.264     |     0.386     |
|    3     | 180.20708012580872 |   0.214   |    0.125     |    0.251     |     0.385     |
|    4     | 255.64638566970825 |   0.205   |    0.112     |    0.239     |     0.393     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 03:27:20,833: Sum_Training_Time:1879.6320307254791
2024-12-28 03:27:20,833: Every_Training_Time:[854.7905385494232, 287.3886013031006, 301.59942507743835, 180.20708012580872, 255.64638566970825]
2024-12-28 03:27:20,833: Forward transfer: 0.166475 Backward transfer: -0.012900000000000002
