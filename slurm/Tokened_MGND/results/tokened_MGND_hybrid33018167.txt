2024-12-29 03:50:28,743: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229034958/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 03:50:38,915: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.0	Best:7.3
2024-12-29 03:50:44,625: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.05	Best:12.4
2024-12-29 03:50:51,072: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.69	Best:18.93
2024-12-29 03:50:57,073: Snapshot:0	Epoch:3	Loss:4.124	translation_Loss:4.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:43.52	Best:22.67
2024-12-29 03:51:03,528: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:45.39	Best:24.24
2024-12-29 03:51:09,498: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:46.01	Best:24.95
2024-12-29 03:51:15,967: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.32	Hits@10:46.18	Best:25.32
2024-12-29 03:51:22,155: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.4	Best:25.65
2024-12-29 03:51:28,648: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.42	Best:25.67
2024-12-29 03:51:34,676: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:46.36	Best:25.67
2024-12-29 03:51:41,009: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.39	Best:25.67
2024-12-29 03:51:46,958: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.31	Best:25.74
2024-12-29 03:51:53,433: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.33	Best:25.74
2024-12-29 03:51:59,049: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.14	Best:25.78
2024-12-29 03:52:06,025: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.24	Best:25.78
2024-12-29 03:52:11,791: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:45.84	Best:25.78
2024-12-29 03:52:18,561: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.13	Best:25.78
2024-12-29 03:52:24,290: Snapshot:0	Epoch:17	Loss:0.238	translation_Loss:0.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.08	Best:25.78
2024-12-29 03:52:31,092: Early Stopping! Snapshot: 0 Epoch: 18 Best Results: 25.78
2024-12-29 03:52:31,092: Start to training tokens! Snapshot: 0 Epoch: 18 Loss:0.222 MRR:25.74 Best Results: 25.78
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:52:31,093: Snapshot:0	Epoch:18	Loss:0.222	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.97	Best:25.78
2024-12-29 03:52:37,200: Snapshot:0	Epoch:19	Loss:26.527	translation_Loss:11.529	multi_layer_Loss:14.999	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.97	Best:25.78
2024-12-29 03:52:43,991: End of token training: 0 Epoch: 20 Loss:11.895 MRR:25.74 Best Results: 25.78
2024-12-29 03:52:43,992: Snapshot:0	Epoch:20	Loss:11.895	translation_Loss:11.522	multi_layer_Loss:0.373	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.74	Hits@10:45.97	Best:25.78
2024-12-29 03:52:44,224: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-29 03:52:46,713: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1499 | 0.3141 | 0.3778 |  0.4536 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:52:57,496: Snapshot:1	Epoch:0	Loss:4.914	translation_Loss:4.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.162                                                   	MRR:9.78	Hits@10:17.13	Best:9.78
2024-12-29 03:52:59,576: Snapshot:1	Epoch:1	Loss:2.934	translation_Loss:2.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.344                                                   	MRR:16.84	Hits@10:30.9	Best:16.84
2024-12-29 03:53:01,644: Snapshot:1	Epoch:2	Loss:1.852	translation_Loss:1.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.46                                                   	MRR:21.04	Hits@10:37.11	Best:21.04
2024-12-29 03:53:04,137: Snapshot:1	Epoch:3	Loss:1.299	translation_Loss:0.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.458                                                   	MRR:23.91	Hits@10:40.77	Best:23.91
2024-12-29 03:53:06,359: Snapshot:1	Epoch:4	Loss:0.989	translation_Loss:0.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:25.04	Hits@10:43.15	Best:25.04
2024-12-29 03:53:08,427: Snapshot:1	Epoch:5	Loss:0.809	translation_Loss:0.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.337                                                   	MRR:25.77	Hits@10:45.05	Best:25.77
2024-12-29 03:53:10,531: Snapshot:1	Epoch:6	Loss:0.701	translation_Loss:0.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.294                                                   	MRR:26.65	Hits@10:46.83	Best:26.65
2024-12-29 03:53:13,011: Snapshot:1	Epoch:7	Loss:0.63	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.268                                                   	MRR:27.31	Hits@10:48.04	Best:27.31
2024-12-29 03:53:15,263: Snapshot:1	Epoch:8	Loss:0.587	translation_Loss:0.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:27.97	Hits@10:48.61	Best:27.97
2024-12-29 03:53:17,734: Snapshot:1	Epoch:9	Loss:0.556	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:28.22	Hits@10:49.28	Best:28.22
2024-12-29 03:53:19,802: Snapshot:1	Epoch:10	Loss:0.522	translation_Loss:0.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:28.71	Hits@10:49.56	Best:28.71
2024-12-29 03:53:21,928: Snapshot:1	Epoch:11	Loss:0.495	translation_Loss:0.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.223                                                   	MRR:28.86	Hits@10:49.78	Best:28.86
2024-12-29 03:53:24,406: Snapshot:1	Epoch:12	Loss:0.48	translation_Loss:0.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.216                                                   	MRR:29.14	Hits@10:50.01	Best:29.14
2024-12-29 03:53:26,898: Snapshot:1	Epoch:13	Loss:0.464	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:29.43	Hits@10:50.18	Best:29.43
2024-12-29 03:53:29,164: Snapshot:1	Epoch:14	Loss:0.452	translation_Loss:0.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.206                                                   	MRR:29.57	Hits@10:50.53	Best:29.57
2024-12-29 03:53:31,598: Snapshot:1	Epoch:15	Loss:0.434	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:29.79	Hits@10:50.93	Best:29.79
2024-12-29 03:53:34,088: Snapshot:1	Epoch:16	Loss:0.429	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:29.89	Hits@10:50.68	Best:29.89
2024-12-29 03:53:36,417: Snapshot:1	Epoch:17	Loss:0.417	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:30.1	Hits@10:51.03	Best:30.1
2024-12-29 03:53:38,521: Snapshot:1	Epoch:18	Loss:0.416	translation_Loss:0.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:29.99	Hits@10:51.22	Best:30.1
2024-12-29 03:53:41,002: Snapshot:1	Epoch:19	Loss:0.402	translation_Loss:0.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:30.09	Hits@10:51.13	Best:30.1
2024-12-29 03:53:43,439: Snapshot:1	Epoch:20	Loss:0.399	translation_Loss:0.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:30.15	Hits@10:50.89	Best:30.15
2024-12-29 03:53:45,612: Snapshot:1	Epoch:21	Loss:0.393	translation_Loss:0.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.185                                                   	MRR:30.08	Hits@10:50.86	Best:30.15
2024-12-29 03:53:48,367: Snapshot:1	Epoch:22	Loss:0.386	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:29.86	Hits@10:50.99	Best:30.15
2024-12-29 03:53:50,523: Snapshot:1	Epoch:23	Loss:0.381	translation_Loss:0.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.181                                                   	MRR:29.9	Hits@10:50.97	Best:30.15
2024-12-29 03:53:52,963: Snapshot:1	Epoch:24	Loss:0.377	translation_Loss:0.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:30.0	Hits@10:50.99	Best:30.15
2024-12-29 03:53:55,187: Early Stopping! Snapshot: 1 Epoch: 25 Best Results: 30.15
2024-12-29 03:53:55,187: Start to training tokens! Snapshot: 1 Epoch: 25 Loss:0.374 MRR:30.04 Best Results: 30.15
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:53:55,187: Snapshot:1	Epoch:25	Loss:0.374	translation_Loss:0.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:30.04	Hits@10:51.19	Best:30.15
2024-12-29 03:53:57,277: Snapshot:1	Epoch:26	Loss:16.152	translation_Loss:4.363	multi_layer_Loss:11.789	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.04	Hits@10:51.19	Best:30.15
2024-12-29 03:53:59,331: End of token training: 1 Epoch: 27 Loss:6.999 MRR:30.04 Best Results: 30.15
2024-12-29 03:53:59,331: Snapshot:1	Epoch:27	Loss:6.999	translation_Loss:4.361	multi_layer_Loss:2.638	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.04	Hits@10:51.19	Best:30.15
2024-12-29 03:53:59,534: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-29 03:54:03,219: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1496 | 0.3123 |  0.38  |  0.4556 |
|     1      | 0.2971 | 0.1907 | 0.3442 | 0.4149 |  0.5061 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:54:33,545: Snapshot:2	Epoch:0	Loss:15.993	translation_Loss:14.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.211                                                   	MRR:14.06	Hits@10:28.69	Best:14.06
2024-12-29 03:54:42,585: Snapshot:2	Epoch:1	Loss:7.411	translation_Loss:5.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.445                                                   	MRR:19.0	Hits@10:35.94	Best:19.0
2024-12-29 03:54:51,910: Snapshot:2	Epoch:2	Loss:4.895	translation_Loss:3.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.164                                                   	MRR:20.38	Hits@10:37.28	Best:20.38
2024-12-29 03:55:01,316: Snapshot:2	Epoch:3	Loss:4.05	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.013                                                   	MRR:21.0	Hits@10:37.7	Best:21.0
2024-12-29 03:55:11,859: Snapshot:2	Epoch:4	Loss:3.692	translation_Loss:2.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.949                                                   	MRR:21.19	Hits@10:38.01	Best:21.19
2024-12-29 03:55:21,381: Snapshot:2	Epoch:5	Loss:3.534	translation_Loss:2.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.921                                                   	MRR:21.28	Hits@10:38.03	Best:21.28
2024-12-29 03:55:32,202: Snapshot:2	Epoch:6	Loss:3.433	translation_Loss:2.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.909                                                   	MRR:21.33	Hits@10:38.13	Best:21.33
2024-12-29 03:55:42,633: Snapshot:2	Epoch:7	Loss:3.383	translation_Loss:2.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.9                                                   	MRR:21.39	Hits@10:38.15	Best:21.39
2024-12-29 03:55:53,339: Snapshot:2	Epoch:8	Loss:3.344	translation_Loss:2.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.896                                                   	MRR:21.33	Hits@10:38.12	Best:21.39
2024-12-29 03:56:02,752: Snapshot:2	Epoch:9	Loss:3.314	translation_Loss:2.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.895                                                   	MRR:21.44	Hits@10:38.1	Best:21.44
2024-12-29 03:56:11,713: Snapshot:2	Epoch:10	Loss:3.285	translation_Loss:2.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.52	Hits@10:38.04	Best:21.52
2024-12-29 03:56:21,005: Snapshot:2	Epoch:11	Loss:3.269	translation_Loss:2.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.53	Hits@10:38.24	Best:21.53
2024-12-29 03:56:31,875: Snapshot:2	Epoch:12	Loss:3.252	translation_Loss:2.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.895                                                   	MRR:21.32	Hits@10:38.08	Best:21.53
2024-12-29 03:56:41,022: Snapshot:2	Epoch:13	Loss:3.242	translation_Loss:2.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.896                                                   	MRR:21.37	Hits@10:38.03	Best:21.53
2024-12-29 03:56:50,284: Snapshot:2	Epoch:14	Loss:3.225	translation_Loss:2.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.89                                                   	MRR:21.21	Hits@10:37.87	Best:21.53
2024-12-29 03:57:01,087: Snapshot:2	Epoch:15	Loss:3.226	translation_Loss:2.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.37	Hits@10:38.13	Best:21.53
2024-12-29 03:57:10,446: Snapshot:2	Epoch:16	Loss:3.209	translation_Loss:2.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.55	Hits@10:38.18	Best:21.55
2024-12-29 03:57:19,324: Snapshot:2	Epoch:17	Loss:3.208	translation_Loss:2.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.29	Hits@10:37.88	Best:21.55
2024-12-29 03:57:28,584: Snapshot:2	Epoch:18	Loss:3.194	translation_Loss:2.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.892                                                   	MRR:21.24	Hits@10:38.1	Best:21.55
2024-12-29 03:57:37,888: Snapshot:2	Epoch:19	Loss:3.186	translation_Loss:2.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.2	Hits@10:38.08	Best:21.55
2024-12-29 03:57:46,969: Snapshot:2	Epoch:20	Loss:3.187	translation_Loss:2.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.38	Hits@10:38.18	Best:21.55
2024-12-29 03:57:56,197: Early Stopping! Snapshot: 2 Epoch: 21 Best Results: 21.55
2024-12-29 03:57:56,197: Start to training tokens! Snapshot: 2 Epoch: 21 Loss:3.175 MRR:21.18 Best Results: 21.55
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:57:56,198: Snapshot:2	Epoch:21	Loss:3.175	translation_Loss:2.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.18	Hits@10:38.1	Best:21.55
2024-12-29 03:58:05,438: Snapshot:2	Epoch:22	Loss:32.632	translation_Loss:18.194	multi_layer_Loss:14.438	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.18	Hits@10:38.1	Best:21.55
2024-12-29 03:58:14,368: End of token training: 2 Epoch: 23 Loss:18.328 MRR:21.18 Best Results: 21.55
2024-12-29 03:58:14,368: Snapshot:2	Epoch:23	Loss:18.328	translation_Loss:18.201	multi_layer_Loss:0.127	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.18	Hits@10:38.1	Best:21.55
2024-12-29 03:58:14,600: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-29 03:58:22,208: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1365 | 0.3073 | 0.378  |  0.4591 |
|     1      | 0.2868 | 0.1823 | 0.3321 | 0.398  |  0.4883 |
|     2      | 0.2148 | 0.1287 | 0.2468 | 0.3045 |  0.3817 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:58:57,780: Snapshot:3	Epoch:0	Loss:15.432	translation_Loss:13.724	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:15.39	Hits@10:31.18	Best:15.39
2024-12-29 03:59:10,396: Snapshot:3	Epoch:1	Loss:7.309	translation_Loss:5.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.969                                                   	MRR:19.51	Hits@10:36.75	Best:19.51
2024-12-29 03:59:22,935: Snapshot:3	Epoch:2	Loss:5.385	translation_Loss:3.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.62                                                   	MRR:20.45	Hits@10:37.4	Best:20.45
2024-12-29 03:59:35,877: Snapshot:3	Epoch:3	Loss:4.735	translation_Loss:3.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.457                                                   	MRR:20.71	Hits@10:37.54	Best:20.71
2024-12-29 03:59:48,742: Snapshot:3	Epoch:4	Loss:4.457	translation_Loss:3.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.376                                                   	MRR:20.9	Hits@10:37.69	Best:20.9
2024-12-29 04:00:00,191: Snapshot:3	Epoch:5	Loss:4.328	translation_Loss:2.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.345                                                   	MRR:20.91	Hits@10:37.57	Best:20.91
2024-12-29 04:00:11,659: Snapshot:3	Epoch:6	Loss:4.261	translation_Loss:2.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.332                                                   	MRR:20.95	Hits@10:37.71	Best:20.95
2024-12-29 04:00:22,876: Snapshot:3	Epoch:7	Loss:4.191	translation_Loss:2.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.319                                                   	MRR:20.98	Hits@10:37.65	Best:20.98
2024-12-29 04:00:33,747: Snapshot:3	Epoch:8	Loss:4.171	translation_Loss:2.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.315                                                   	MRR:20.86	Hits@10:37.52	Best:20.98
2024-12-29 04:00:46,625: Snapshot:3	Epoch:9	Loss:4.154	translation_Loss:2.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.32                                                   	MRR:20.91	Hits@10:37.74	Best:20.98
2024-12-29 04:00:57,863: Snapshot:3	Epoch:10	Loss:4.121	translation_Loss:2.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.309                                                   	MRR:20.84	Hits@10:37.79	Best:20.98
2024-12-29 04:01:10,820: Snapshot:3	Epoch:11	Loss:4.117	translation_Loss:2.803	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.313                                                   	MRR:20.82	Hits@10:37.65	Best:20.98
2024-12-29 04:01:23,779: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 20.98
2024-12-29 04:01:23,780: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:4.112 MRR:20.73 Best Results: 20.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:01:23,780: Snapshot:3	Epoch:12	Loss:4.112	translation_Loss:2.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.317                                                   	MRR:20.73	Hits@10:37.57	Best:20.98
2024-12-29 04:01:35,050: Snapshot:3	Epoch:13	Loss:35.013	translation_Loss:19.665	multi_layer_Loss:15.348	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.73	Hits@10:37.57	Best:20.98
2024-12-29 04:01:45,795: End of token training: 3 Epoch: 14 Loss:19.758 MRR:20.73 Best Results: 20.98
2024-12-29 04:01:45,796: Snapshot:3	Epoch:14	Loss:19.758	translation_Loss:19.687	multi_layer_Loss:0.071	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.73	Hits@10:37.57	Best:20.98
2024-12-29 04:01:46,028: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2024-12-29 04:01:58,410: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1528 | 0.3185 | 0.383  |  0.4622 |
|     1      | 0.2816 | 0.176  | 0.3243 | 0.3904 |  0.4856 |
|     2      | 0.2152 | 0.1293 | 0.2454 | 0.3042 |  0.3835 |
|     3      | 0.2098 | 0.1216 | 0.2428 | 0.3014 |  0.3787 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:02:15,888: Snapshot:4	Epoch:0	Loss:6.638	translation_Loss:6.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.408                                                   	MRR:9.39	Hits@10:20.1	Best:9.39
2024-12-29 04:02:20,613: Snapshot:4	Epoch:1	Loss:4.131	translation_Loss:3.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:15.73	Hits@10:33.19	Best:15.73
2024-12-29 04:02:25,209: Snapshot:4	Epoch:2	Loss:3.045	translation_Loss:2.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.954                                                   	MRR:18.96	Hits@10:34.74	Best:18.96
2024-12-29 04:02:30,468: Snapshot:4	Epoch:3	Loss:2.508	translation_Loss:1.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.954                                                   	MRR:20.65	Hits@10:36.01	Best:20.65
2024-12-29 04:02:35,523: Snapshot:4	Epoch:4	Loss:2.191	translation_Loss:1.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.916                                                   	MRR:21.4	Hits@10:36.72	Best:21.4
2024-12-29 04:02:40,859: Snapshot:4	Epoch:5	Loss:1.985	translation_Loss:1.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.874                                                   	MRR:22.05	Hits@10:36.72	Best:22.05
2024-12-29 04:02:45,457: Snapshot:4	Epoch:6	Loss:1.842	translation_Loss:1.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.837                                                   	MRR:22.19	Hits@10:37.1	Best:22.19
2024-12-29 04:02:50,921: Snapshot:4	Epoch:7	Loss:1.746	translation_Loss:0.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.806                                                   	MRR:22.11	Hits@10:36.98	Best:22.19
2024-12-29 04:02:55,498: Snapshot:4	Epoch:8	Loss:1.688	translation_Loss:0.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.782                                                   	MRR:22.08	Hits@10:36.95	Best:22.19
2024-12-29 04:03:00,003: Snapshot:4	Epoch:9	Loss:1.644	translation_Loss:0.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.768                                                   	MRR:22.43	Hits@10:37.09	Best:22.43
2024-12-29 04:03:04,765: Snapshot:4	Epoch:10	Loss:1.628	translation_Loss:0.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.757                                                   	MRR:22.23	Hits@10:36.98	Best:22.43
2024-12-29 04:03:09,973: Snapshot:4	Epoch:11	Loss:1.6	translation_Loss:0.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.745                                                   	MRR:22.58	Hits@10:37.16	Best:22.58
2024-12-29 04:03:14,550: Snapshot:4	Epoch:12	Loss:1.582	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.736                                                   	MRR:22.14	Hits@10:37.27	Best:22.58
2024-12-29 04:03:18,926: Snapshot:4	Epoch:13	Loss:1.583	translation_Loss:0.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.734                                                   	MRR:21.95	Hits@10:36.31	Best:22.58
2024-12-29 04:03:23,645: Snapshot:4	Epoch:14	Loss:1.57	translation_Loss:0.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.73                                                   	MRR:22.1	Hits@10:36.93	Best:22.58
2024-12-29 04:03:28,037: Snapshot:4	Epoch:15	Loss:1.565	translation_Loss:0.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.726                                                   	MRR:22.27	Hits@10:37.0	Best:22.58
2024-12-29 04:03:33,151: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 22.58
2024-12-29 04:03:33,151: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:1.553 MRR:22.07 Best Results: 22.58
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:03:33,151: Snapshot:4	Epoch:16	Loss:1.553	translation_Loss:0.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.723                                                   	MRR:22.07	Hits@10:36.67	Best:22.58
2024-12-29 04:03:38,116: Snapshot:4	Epoch:17	Loss:24.146	translation_Loss:9.083	multi_layer_Loss:15.063	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.07	Hits@10:36.67	Best:22.58
2024-12-29 04:03:42,506: End of token training: 4 Epoch: 18 Loss:9.997 MRR:22.07 Best Results: 22.58
2024-12-29 04:03:42,507: Snapshot:4	Epoch:18	Loss:9.997	translation_Loss:9.081	multi_layer_Loss:0.916	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.07	Hits@10:36.67	Best:22.58
2024-12-29 04:03:42,741: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2024-12-29 04:03:57,275: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2272 | 0.1246 | 0.2766 | 0.3397 |  0.4135 |
|     1      | 0.2693 | 0.1679 | 0.3073 | 0.3737 |  0.4666 |
|     2      | 0.2036 | 0.1207 | 0.2297 | 0.2872 |  0.3664 |
|     3      | 0.1862 | 0.1005 | 0.2146 | 0.2732 |  0.3507 |
|     4      | 0.227  | 0.1509 | 0.2587 | 0.3076 |  0.3702 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:03:57,277: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1499 | 0.3141 | 0.3778 |  0.4536 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1496 | 0.3123 |  0.38  |  0.4556 |
|     1      | 0.2971 | 0.1907 | 0.3442 | 0.4149 |  0.5061 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1365 | 0.3073 | 0.378  |  0.4591 |
|     1      | 0.2868 | 0.1823 | 0.3321 | 0.398  |  0.4883 |
|     2      | 0.2148 | 0.1287 | 0.2468 | 0.3045 |  0.3817 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1528 | 0.3185 | 0.383  |  0.4622 |
|     1      | 0.2816 | 0.176  | 0.3243 | 0.3904 |  0.4856 |
|     2      | 0.2152 | 0.1293 | 0.2454 | 0.3042 |  0.3835 |
|     3      | 0.2098 | 0.1216 | 0.2428 | 0.3014 |  0.3787 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2272 | 0.1246 | 0.2766 | 0.3397 |  0.4135 |
|     1      | 0.2693 | 0.1679 | 0.3073 | 0.3737 |  0.4666 |
|     2      | 0.2036 | 0.1207 | 0.2297 | 0.2872 |  0.3664 |
|     3      | 0.1862 | 0.1005 | 0.2146 | 0.2732 |  0.3507 |
|     4      | 0.227  | 0.1509 | 0.2587 | 0.3076 |  0.3702 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:03:57,278: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 135.24804186820984 |   0.257   |     0.15     |    0.314     |     0.454     |
|    1     | 71.39151120185852  |   0.268   |    0.161     |    0.321     |     0.469     |
|    2     | 247.16854310035706 |   0.236   |    0.138     |    0.278     |     0.422     |
|    3     | 198.90277004241943 |   0.228   |    0.135     |    0.266     |     0.406     |
|    4     | 101.80217432975769 |    0.21   |    0.122     |    0.242     |     0.377     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:03:57,278: Sum_Training_Time:754.5130405426025
2024-12-29 04:03:57,278: Every_Training_Time:[135.24804186820984, 71.39151120185852, 247.16854310035706, 198.90277004241943, 101.80217432975769]
2024-12-29 04:03:57,278: Forward transfer: 0.042825 Backward transfer: -0.02319999999999999
2024-12-29 04:04:29,472: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229040402/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:04:39,443: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2024-12-29 04:04:44,952: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2024-12-29 04:04:51,321: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.69	Best:18.94
2024-12-29 04:04:57,477: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.56	Best:22.69
2024-12-29 04:05:03,867: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:45.27	Best:24.21
2024-12-29 04:05:09,864: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.96	Hits@10:45.86	Best:24.96
2024-12-29 04:05:16,235: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.28	Hits@10:46.15	Best:25.28
2024-12-29 04:05:22,068: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.62	Best:25.56
2024-12-29 04:05:28,530: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.68	Best:25.62
2024-12-29 04:05:34,520: Snapshot:0	Epoch:9	Loss:0.536	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.61	Best:25.65
2024-12-29 04:05:40,870: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.59	Hits@10:46.42	Best:25.65
2024-12-29 04:05:46,772: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.35	Best:25.82
2024-12-29 04:05:53,152: Snapshot:0	Epoch:12	Loss:0.355	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.38	Best:25.82
2024-12-29 04:05:58,664: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.08	Best:25.82
2024-12-29 04:06:05,413: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.87	Hits@10:46.21	Best:25.87
2024-12-29 04:06:10,882: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.98	Best:25.87
2024-12-29 04:06:17,582: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.11	Best:25.87
2024-12-29 04:06:23,037: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.17	Best:25.87
2024-12-29 04:06:28,707: Snapshot:0	Epoch:18	Loss:0.222	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.02	Best:25.87
2024-12-29 04:06:35,061: Early Stopping! Snapshot: 0 Epoch: 19 Best Results: 25.87
2024-12-29 04:06:35,061: Start to training tokens! Snapshot: 0 Epoch: 19 Loss:0.219 MRR:25.39 Best Results: 25.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:06:35,061: Snapshot:0	Epoch:19	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.39	Hits@10:45.89	Best:25.87
2024-12-29 04:06:42,170: Snapshot:0	Epoch:20	Loss:26.5	translation_Loss:11.502	multi_layer_Loss:14.999	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.39	Hits@10:45.89	Best:25.87
2024-12-29 04:06:47,626: End of token training: 0 Epoch: 21 Loss:11.861 MRR:25.39 Best Results: 25.87
2024-12-29 04:06:47,626: Snapshot:0	Epoch:21	Loss:11.861	translation_Loss:11.489	multi_layer_Loss:0.373	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.39	Hits@10:45.89	Best:25.87
2024-12-29 04:06:47,860: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-29 04:06:50,098: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1511 | 0.3147 | 0.3801 |  0.4529 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:07:00,608: Snapshot:1	Epoch:0	Loss:4.972	translation_Loss:4.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.218                                                   	MRR:9.56	Hits@10:16.8	Best:9.56
2024-12-29 04:07:02,888: Snapshot:1	Epoch:1	Loss:3.07	translation_Loss:2.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.399                                                   	MRR:16.33	Hits@10:30.0	Best:16.33
2024-12-29 04:07:05,334: Snapshot:1	Epoch:2	Loss:1.977	translation_Loss:1.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.45                                                   	MRR:20.65	Hits@10:36.19	Best:20.65
2024-12-29 04:07:07,518: Snapshot:1	Epoch:3	Loss:1.418	translation_Loss:1.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.412                                                   	MRR:23.1	Hits@10:40.02	Best:23.1
2024-12-29 04:07:09,947: Snapshot:1	Epoch:4	Loss:1.093	translation_Loss:0.747	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:24.48	Hits@10:43.02	Best:24.48
2024-12-29 04:07:12,086: Snapshot:1	Epoch:5	Loss:0.912	translation_Loss:0.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.293                                                   	MRR:25.66	Hits@10:45.34	Best:25.66
2024-12-29 04:07:14,546: Snapshot:1	Epoch:6	Loss:0.799	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.263                                                   	MRR:26.5	Hits@10:46.97	Best:26.5
2024-12-29 04:07:16,973: Snapshot:1	Epoch:7	Loss:0.73	translation_Loss:0.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:27.04	Hits@10:47.99	Best:27.04
2024-12-29 04:07:19,386: Snapshot:1	Epoch:8	Loss:0.678	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.228                                                   	MRR:27.56	Hits@10:48.66	Best:27.56
2024-12-29 04:07:21,574: Snapshot:1	Epoch:9	Loss:0.636	translation_Loss:0.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.219                                                   	MRR:28.06	Hits@10:48.73	Best:28.06
2024-12-29 04:07:24,450: Snapshot:1	Epoch:10	Loss:0.605	translation_Loss:0.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:28.49	Hits@10:49.31	Best:28.49
2024-12-29 04:07:26,664: Snapshot:1	Epoch:11	Loss:0.582	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:28.72	Hits@10:49.74	Best:28.72
2024-12-29 04:07:29,146: Snapshot:1	Epoch:12	Loss:0.555	translation_Loss:0.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.196                                                   	MRR:28.92	Hits@10:49.64	Best:28.92
2024-12-29 04:07:31,314: Snapshot:1	Epoch:13	Loss:0.539	translation_Loss:0.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.192                                                   	MRR:29.03	Hits@10:49.95	Best:29.03
2024-12-29 04:07:33,798: Snapshot:1	Epoch:14	Loss:0.526	translation_Loss:0.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:29.07	Hits@10:49.77	Best:29.07
2024-12-29 04:07:35,993: Snapshot:1	Epoch:15	Loss:0.51	translation_Loss:0.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:29.1	Hits@10:49.88	Best:29.1
2024-12-29 04:07:38,820: Snapshot:1	Epoch:16	Loss:0.5	translation_Loss:0.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:29.37	Hits@10:50.14	Best:29.37
2024-12-29 04:07:41,023: Snapshot:1	Epoch:17	Loss:0.488	translation_Loss:0.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:29.44	Hits@10:50.07	Best:29.44
2024-12-29 04:07:43,036: Snapshot:1	Epoch:18	Loss:0.479	translation_Loss:0.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:29.4	Hits@10:50.12	Best:29.44
2024-12-29 04:07:45,470: Snapshot:1	Epoch:19	Loss:0.469	translation_Loss:0.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:29.32	Hits@10:50.36	Best:29.44
2024-12-29 04:07:47,614: Snapshot:1	Epoch:20	Loss:0.465	translation_Loss:0.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:29.66	Hits@10:50.53	Best:29.66
2024-12-29 04:07:49,731: Snapshot:1	Epoch:21	Loss:0.464	translation_Loss:0.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:29.74	Hits@10:50.61	Best:29.74
2024-12-29 04:07:52,144: Snapshot:1	Epoch:22	Loss:0.45	translation_Loss:0.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.8	Hits@10:50.59	Best:29.8
2024-12-29 04:07:54,241: Snapshot:1	Epoch:23	Loss:0.448	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.83	Hits@10:50.61	Best:29.83
2024-12-29 04:07:56,279: Snapshot:1	Epoch:24	Loss:0.443	translation_Loss:0.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:29.86	Hits@10:50.56	Best:29.86
2024-12-29 04:07:58,775: Snapshot:1	Epoch:25	Loss:0.442	translation_Loss:0.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.162                                                   	MRR:29.9	Hits@10:50.65	Best:29.9
2024-12-29 04:08:00,990: Snapshot:1	Epoch:26	Loss:0.439	translation_Loss:0.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.95	Hits@10:50.77	Best:29.95
2024-12-29 04:08:03,043: Snapshot:1	Epoch:27	Loss:0.431	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.16                                                   	MRR:29.78	Hits@10:50.35	Best:29.95
2024-12-29 04:08:05,379: Snapshot:1	Epoch:28	Loss:0.434	translation_Loss:0.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:29.72	Hits@10:50.54	Best:29.95
2024-12-29 04:08:07,456: Snapshot:1	Epoch:29	Loss:0.428	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.159                                                   	MRR:30.0	Hits@10:50.55	Best:30.0
2024-12-29 04:08:09,949: Snapshot:1	Epoch:30	Loss:0.426	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:30.02	Hits@10:50.6	Best:30.02
2024-12-29 04:08:12,106: Snapshot:1	Epoch:31	Loss:0.428	translation_Loss:0.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:30.0	Hits@10:50.5	Best:30.02
2024-12-29 04:08:14,149: Snapshot:1	Epoch:32	Loss:0.423	translation_Loss:0.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:29.8	Hits@10:50.8	Best:30.02
2024-12-29 04:08:16,560: Snapshot:1	Epoch:33	Loss:0.423	translation_Loss:0.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:29.84	Hits@10:50.36	Best:30.02
2024-12-29 04:08:19,072: Snapshot:1	Epoch:34	Loss:0.424	translation_Loss:0.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:29.98	Hits@10:50.23	Best:30.02
2024-12-29 04:08:21,135: Snapshot:1	Epoch:35	Loss:0.421	translation_Loss:0.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:30.05	Hits@10:50.44	Best:30.05
2024-12-29 04:08:23,177: Snapshot:1	Epoch:36	Loss:0.417	translation_Loss:0.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:29.9	Hits@10:50.68	Best:30.05
2024-12-29 04:08:25,210: Snapshot:1	Epoch:37	Loss:0.417	translation_Loss:0.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:29.86	Hits@10:50.66	Best:30.05
2024-12-29 04:08:27,237: Snapshot:1	Epoch:38	Loss:0.413	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:29.85	Hits@10:50.69	Best:30.05
2024-12-29 04:08:29,263: Snapshot:1	Epoch:39	Loss:0.414	translation_Loss:0.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:30.04	Hits@10:50.91	Best:30.05
2024-12-29 04:08:31,982: Early Stopping! Snapshot: 1 Epoch: 40 Best Results: 30.05
2024-12-29 04:08:31,982: Start to training tokens! Snapshot: 1 Epoch: 40 Loss:0.413 MRR:29.95 Best Results: 30.05
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:08:31,983: Snapshot:1	Epoch:40	Loss:0.413	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:29.95	Hits@10:50.63	Best:30.05
2024-12-29 04:08:34,144: Snapshot:1	Epoch:41	Loss:16.256	translation_Loss:4.468	multi_layer_Loss:11.789	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.95	Hits@10:50.63	Best:30.05
2024-12-29 04:08:36,167: End of token training: 1 Epoch: 42 Loss:7.101 MRR:29.95 Best Results: 30.05
2024-12-29 04:08:36,167: Snapshot:1	Epoch:42	Loss:7.101	translation_Loss:4.462	multi_layer_Loss:2.638	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.95	Hits@10:50.63	Best:30.05
2024-12-29 04:08:36,402: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-29 04:08:39,770: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2578 | 0.1508 | 0.3153 |  0.38  |  0.4545 |
|     1      | 0.2944 | 0.1897 | 0.3406 | 0.4103 |  0.503  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:09:11,929: Snapshot:2	Epoch:0	Loss:16.232	translation_Loss:14.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.362                                                   	MRR:13.63	Hits@10:27.61	Best:13.63
2024-12-29 04:09:20,824: Snapshot:2	Epoch:1	Loss:7.907	translation_Loss:6.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.376                                                   	MRR:18.39	Hits@10:34.99	Best:18.39
2024-12-29 04:09:29,696: Snapshot:2	Epoch:2	Loss:5.302	translation_Loss:4.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.09                                                   	MRR:19.88	Hits@10:36.41	Best:19.88
2024-12-29 04:09:40,172: Snapshot:2	Epoch:3	Loss:4.398	translation_Loss:3.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.948                                                   	MRR:20.45	Hits@10:36.86	Best:20.45
2024-12-29 04:09:49,102: Snapshot:2	Epoch:4	Loss:4.056	translation_Loss:3.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.891                                                   	MRR:20.66	Hits@10:36.95	Best:20.66
2024-12-29 04:09:59,943: Snapshot:2	Epoch:5	Loss:3.881	translation_Loss:3.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.865                                                   	MRR:20.76	Hits@10:37.09	Best:20.76
2024-12-29 04:10:09,024: Snapshot:2	Epoch:6	Loss:3.794	translation_Loss:2.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.858                                                   	MRR:20.89	Hits@10:37.15	Best:20.89
2024-12-29 04:10:19,706: Snapshot:2	Epoch:7	Loss:3.72	translation_Loss:2.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.847                                                   	MRR:20.84	Hits@10:37.05	Best:20.89
2024-12-29 04:10:29,017: Snapshot:2	Epoch:8	Loss:3.685	translation_Loss:2.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:20.92	Hits@10:37.02	Best:20.92
2024-12-29 04:10:39,241: Snapshot:2	Epoch:9	Loss:3.666	translation_Loss:2.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.85                                                   	MRR:20.89	Hits@10:37.01	Best:20.92
2024-12-29 04:10:48,427: Snapshot:2	Epoch:10	Loss:3.636	translation_Loss:2.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.848                                                   	MRR:20.87	Hits@10:37.08	Best:20.92
2024-12-29 04:10:57,609: Snapshot:2	Epoch:11	Loss:3.61	translation_Loss:2.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.848                                                   	MRR:20.76	Hits@10:37.14	Best:20.92
2024-12-29 04:11:06,414: Snapshot:2	Epoch:12	Loss:3.594	translation_Loss:2.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.844                                                   	MRR:20.93	Hits@10:37.23	Best:20.93
2024-12-29 04:11:17,093: Snapshot:2	Epoch:13	Loss:3.583	translation_Loss:2.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.847                                                   	MRR:20.92	Hits@10:37.23	Best:20.93
2024-12-29 04:11:27,718: Snapshot:2	Epoch:14	Loss:3.571	translation_Loss:2.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.845                                                   	MRR:20.84	Hits@10:37.33	Best:20.93
2024-12-29 04:11:37,125: Snapshot:2	Epoch:15	Loss:3.566	translation_Loss:2.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.848                                                   	MRR:20.79	Hits@10:37.38	Best:20.93
2024-12-29 04:11:47,463: Snapshot:2	Epoch:16	Loss:3.553	translation_Loss:2.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.849                                                   	MRR:20.85	Hits@10:37.33	Best:20.93
2024-12-29 04:11:58,012: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 20.93
2024-12-29 04:11:58,013: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:3.526 MRR:20.74 Best Results: 20.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:11:58,013: Snapshot:2	Epoch:17	Loss:3.526	translation_Loss:2.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.847                                                   	MRR:20.74	Hits@10:37.35	Best:20.93
2024-12-29 04:12:07,311: Snapshot:2	Epoch:18	Loss:33.007	translation_Loss:18.569	multi_layer_Loss:14.438	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.74	Hits@10:37.35	Best:20.93
2024-12-29 04:12:16,052: End of token training: 2 Epoch: 19 Loss:18.684 MRR:20.74 Best Results: 20.93
2024-12-29 04:12:16,052: Snapshot:2	Epoch:19	Loss:18.684	translation_Loss:18.557	multi_layer_Loss:0.127	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.74	Hits@10:37.35	Best:20.93
2024-12-29 04:12:16,299: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-29 04:12:23,699: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2556 | 0.1456 | 0.3153 | 0.3804 |  0.4577 |
|     1      | 0.2893 | 0.1841 | 0.335  | 0.4032 |  0.5019 |
|     2      | 0.2075 | 0.1227 | 0.2371 | 0.2955 |  0.3726 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:13:01,071: Snapshot:3	Epoch:0	Loss:16.057	translation_Loss:14.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.938                                                   	MRR:14.74	Hits@10:30.12	Best:14.74
2024-12-29 04:13:13,549: Snapshot:3	Epoch:1	Loss:8.265	translation_Loss:6.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.977                                                   	MRR:18.98	Hits@10:35.8	Best:18.98
2024-12-29 04:13:24,956: Snapshot:3	Epoch:2	Loss:6.196	translation_Loss:4.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.661                                                   	MRR:19.9	Hits@10:36.37	Best:19.9
2024-12-29 04:13:35,718: Snapshot:3	Epoch:3	Loss:5.501	translation_Loss:4.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.496                                                   	MRR:20.25	Hits@10:36.53	Best:20.25
2024-12-29 04:13:46,802: Snapshot:3	Epoch:4	Loss:5.225	translation_Loss:3.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.423                                                   	MRR:20.37	Hits@10:36.58	Best:20.37
2024-12-29 04:13:59,761: Snapshot:3	Epoch:5	Loss:5.091	translation_Loss:3.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.396                                                   	MRR:20.34	Hits@10:36.36	Best:20.37
2024-12-29 04:14:11,019: Snapshot:3	Epoch:6	Loss:5.022	translation_Loss:3.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.384                                                   	MRR:20.48	Hits@10:36.46	Best:20.48
2024-12-29 04:14:22,134: Snapshot:3	Epoch:7	Loss:4.963	translation_Loss:3.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.379                                                   	MRR:20.32	Hits@10:36.46	Best:20.48
2024-12-29 04:14:32,815: Snapshot:3	Epoch:8	Loss:4.949	translation_Loss:3.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.379                                                   	MRR:20.37	Hits@10:36.36	Best:20.48
2024-12-29 04:14:43,810: Snapshot:3	Epoch:9	Loss:4.917	translation_Loss:3.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.375                                                   	MRR:20.35	Hits@10:36.36	Best:20.48
2024-12-29 04:14:54,825: Snapshot:3	Epoch:10	Loss:4.893	translation_Loss:3.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.379                                                   	MRR:20.37	Hits@10:36.62	Best:20.48
2024-12-29 04:15:05,900: Early Stopping! Snapshot: 3 Epoch: 11 Best Results: 20.48
2024-12-29 04:15:05,900: Start to training tokens! Snapshot: 3 Epoch: 11 Loss:4.878 MRR:20.24 Best Results: 20.48
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:15:05,901: Snapshot:3	Epoch:11	Loss:4.878	translation_Loss:3.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.374                                                   	MRR:20.24	Hits@10:36.29	Best:20.48
2024-12-29 04:15:17,006: Snapshot:3	Epoch:12	Loss:35.55	translation_Loss:20.203	multi_layer_Loss:15.348	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.24	Hits@10:36.29	Best:20.48
2024-12-29 04:15:28,074: End of token training: 3 Epoch: 13 Loss:20.279 MRR:20.24 Best Results: 20.48
2024-12-29 04:15:28,074: Snapshot:3	Epoch:13	Loss:20.279	translation_Loss:20.208	multi_layer_Loss:0.071	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.24	Hits@10:36.29	Best:20.48
2024-12-29 04:15:28,310: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2024-12-29 04:15:40,282: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2637 | 0.1578 | 0.3176 | 0.3825 |   0.46  |
|     1      | 0.2886 | 0.1816 | 0.3351 | 0.4035 |  0.5024 |
|     2      | 0.2088 | 0.1232 | 0.2386 | 0.2983 |  0.3772 |
|     3      | 0.2035 | 0.1175 | 0.2364 | 0.2925 |  0.3653 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:15:56,985: Snapshot:4	Epoch:0	Loss:6.901	translation_Loss:6.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.546                                                   	MRR:8.97	Hits@10:19.81	Best:8.97
2024-12-29 04:16:01,754: Snapshot:4	Epoch:1	Loss:4.666	translation_Loss:3.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.903                                                   	MRR:14.85	Hits@10:32.02	Best:14.85
2024-12-29 04:16:06,268: Snapshot:4	Epoch:2	Loss:3.661	translation_Loss:2.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.935                                                   	MRR:18.5	Hits@10:34.21	Best:18.5
2024-12-29 04:16:11,400: Snapshot:4	Epoch:3	Loss:3.108	translation_Loss:2.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:20.07	Hits@10:35.4	Best:20.07
2024-12-29 04:16:16,293: Snapshot:4	Epoch:4	Loss:2.752	translation_Loss:1.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.848                                                   	MRR:21.09	Hits@10:35.51	Best:21.09
2024-12-29 04:16:20,758: Snapshot:4	Epoch:5	Loss:2.514	translation_Loss:1.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.815                                                   	MRR:21.67	Hits@10:36.02	Best:21.67
2024-12-29 04:16:25,324: Snapshot:4	Epoch:6	Loss:2.364	translation_Loss:1.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.784                                                   	MRR:21.72	Hits@10:35.72	Best:21.72
2024-12-29 04:16:29,783: Snapshot:4	Epoch:7	Loss:2.266	translation_Loss:1.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.759                                                   	MRR:21.5	Hits@10:35.8	Best:21.72
2024-12-29 04:16:34,685: Snapshot:4	Epoch:8	Loss:2.21	translation_Loss:1.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.742                                                   	MRR:21.88	Hits@10:35.88	Best:21.88
2024-12-29 04:16:39,136: Snapshot:4	Epoch:9	Loss:2.168	translation_Loss:1.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.732                                                   	MRR:21.72	Hits@10:35.85	Best:21.88
2024-12-29 04:16:44,286: Snapshot:4	Epoch:10	Loss:2.135	translation_Loss:1.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.715                                                   	MRR:21.53	Hits@10:35.23	Best:21.88
2024-12-29 04:16:49,201: Snapshot:4	Epoch:11	Loss:2.118	translation_Loss:1.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.712                                                   	MRR:21.75	Hits@10:35.63	Best:21.88
2024-12-29 04:16:53,696: Snapshot:4	Epoch:12	Loss:2.104	translation_Loss:1.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.706                                                   	MRR:21.7	Hits@10:35.85	Best:21.88
2024-12-29 04:16:58,169: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 21.88
2024-12-29 04:16:58,169: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:2.094 MRR:21.69 Best Results: 21.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:16:58,169: Snapshot:4	Epoch:13	Loss:2.094	translation_Loss:1.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.705                                                   	MRR:21.69	Hits@10:35.65	Best:21.88
2024-12-29 04:17:02,882: Snapshot:4	Epoch:14	Loss:24.59	translation_Loss:9.527	multi_layer_Loss:15.063	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.69	Hits@10:35.65	Best:21.88
2024-12-29 04:17:07,950: End of token training: 4 Epoch: 15 Loss:10.43 MRR:21.69 Best Results: 21.88
2024-12-29 04:17:07,950: Snapshot:4	Epoch:15	Loss:10.43	translation_Loss:9.514	multi_layer_Loss:0.916	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.69	Hits@10:35.65	Best:21.88
2024-12-29 04:17:08,189: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2024-12-29 04:17:24,146: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2318 | 0.1292 | 0.2832 | 0.3459 |  0.4188 |
|     1      | 0.2773 | 0.1726 | 0.325  | 0.3873 |  0.4826 |
|     2      | 0.1969 | 0.1133 | 0.2234 | 0.2827 |  0.362  |
|     3      | 0.1871 | 0.1034 | 0.2162 | 0.2729 |  0.3461 |
|     4      | 0.2203 | 0.1488 | 0.2469 | 0.2972 |  0.3598 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:17:24,148: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1511 | 0.3147 | 0.3801 |  0.4529 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2578 | 0.1508 | 0.3153 |  0.38  |  0.4545 |
|     1      | 0.2944 | 0.1897 | 0.3406 | 0.4103 |  0.503  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2556 | 0.1456 | 0.3153 | 0.3804 |  0.4577 |
|     1      | 0.2893 | 0.1841 | 0.335  | 0.4032 |  0.5019 |
|     2      | 0.2075 | 0.1227 | 0.2371 | 0.2955 |  0.3726 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2637 | 0.1578 | 0.3176 | 0.3825 |   0.46  |
|     1      | 0.2886 | 0.1816 | 0.3351 | 0.4035 |  0.5024 |
|     2      | 0.2088 | 0.1232 | 0.2386 | 0.2983 |  0.3772 |
|     3      | 0.2035 | 0.1175 | 0.2364 | 0.2925 |  0.3653 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2318 | 0.1292 | 0.2832 | 0.3459 |  0.4188 |
|     1      | 0.2773 | 0.1726 | 0.325  | 0.3873 |  0.4826 |
|     2      | 0.1969 | 0.1133 | 0.2234 | 0.2827 |  0.362  |
|     3      | 0.1871 | 0.1034 | 0.2162 | 0.2729 |  0.3461 |
|     4      | 0.2203 | 0.1488 | 0.2469 | 0.2972 |  0.3598 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:17:24,149: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 138.1532473564148  |   0.258   |    0.151     |    0.315     |     0.453     |
|    1     | 104.67188239097595 |   0.268   |    0.161     |    0.322     |     0.467     |
|    2     | 212.32604098320007 |   0.234   |    0.138     |    0.276     |     0.418     |
|    3     | 179.41726565361023 |   0.225   |    0.133     |    0.262     |      0.4      |
|    4     | 85.17352771759033  |   0.209   |    0.122     |    0.242     |     0.375     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:17:24,149: Sum_Training_Time:719.7419641017914
2024-12-29 04:17:24,149: Every_Training_Time:[138.1532473564148, 104.67188239097595, 212.32604098320007, 179.41726565361023, 85.17352771759033]
2024-12-29 04:17:24,149: Forward transfer: 0.040825 Backward transfer: -0.017599999999999998
2024-12-29 04:17:56,067: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229041729/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:18:06,090: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.0	Best:7.3
2024-12-29 04:18:11,844: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.05	Best:12.4
2024-12-29 04:18:18,257: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.7	Best:18.93
2024-12-29 04:18:24,277: Snapshot:0	Epoch:3	Loss:4.124	translation_Loss:4.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:43.51	Best:22.67
2024-12-29 04:18:30,818: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.29	Hits@10:45.41	Best:24.29
2024-12-29 04:18:36,923: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:45.98	Best:25.0
2024-12-29 04:18:43,425: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.32	Best:25.35
2024-12-29 04:18:49,488: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.51	Best:25.66
2024-12-29 04:18:56,025: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.53	Best:25.74
2024-12-29 04:19:02,040: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.46	Best:25.74
2024-12-29 04:19:08,476: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.29	Best:25.74
2024-12-29 04:19:14,353: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.3	Best:25.78
2024-12-29 04:19:20,754: Snapshot:0	Epoch:12	Loss:0.355	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.24	Best:25.8
2024-12-29 04:19:26,355: Snapshot:0	Epoch:13	Loss:0.32	translation_Loss:0.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.17	Best:25.8
2024-12-29 04:19:33,084: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.28	Best:25.8
2024-12-29 04:19:38,741: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:45.83	Best:25.8
2024-12-29 04:19:45,514: Snapshot:0	Epoch:16	Loss:0.251	translation_Loss:0.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:45.76	Best:25.8
2024-12-29 04:19:51,053: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 25.8
2024-12-29 04:19:51,053: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.237 MRR:25.59 Best Results: 25.8
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:19:51,053: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.59	Hits@10:45.88	Best:25.8
2024-12-29 04:19:58,363: Snapshot:0	Epoch:18	Loss:26.503	translation_Loss:11.504	multi_layer_Loss:14.999	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.59	Hits@10:45.88	Best:25.8
2024-12-29 04:20:04,920: End of token training: 0 Epoch: 19 Loss:11.883 MRR:25.59 Best Results: 25.8
2024-12-29 04:20:04,921: Snapshot:0	Epoch:19	Loss:11.883	translation_Loss:11.51	multi_layer_Loss:0.373	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.59	Hits@10:45.88	Best:25.8
2024-12-29 04:20:05,152: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-29 04:20:08,040: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1507 | 0.3159 | 0.3795 |  0.4525 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:20:19,189: Snapshot:1	Epoch:0	Loss:4.809	translation_Loss:4.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.067                                                   	MRR:9.94	Hits@10:17.69	Best:9.94
2024-12-29 04:20:21,287: Snapshot:1	Epoch:1	Loss:2.67	translation_Loss:2.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:17.15	Hits@10:31.86	Best:17.15
2024-12-29 04:20:23,814: Snapshot:1	Epoch:2	Loss:1.479	translation_Loss:1.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.263                                                   	MRR:21.73	Hits@10:38.84	Best:21.73
2024-12-29 04:20:26,326: Snapshot:1	Epoch:3	Loss:0.949	translation_Loss:0.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.311                                                   	MRR:24.65	Hits@10:42.48	Best:24.65
2024-12-29 04:20:28,823: Snapshot:1	Epoch:4	Loss:0.692	translation_Loss:0.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.323                                                   	MRR:25.48	Hits@10:43.86	Best:25.48
2024-12-29 04:20:31,323: Snapshot:1	Epoch:5	Loss:0.543	translation_Loss:0.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.308                                                   	MRR:25.85	Hits@10:44.42	Best:25.85
2024-12-29 04:20:33,593: Snapshot:1	Epoch:6	Loss:0.446	translation_Loss:0.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:26.17	Hits@10:44.72	Best:26.17
2024-12-29 04:20:35,730: Snapshot:1	Epoch:7	Loss:0.39	translation_Loss:0.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:26.68	Hits@10:45.23	Best:26.68
2024-12-29 04:20:37,860: Snapshot:1	Epoch:8	Loss:0.353	translation_Loss:0.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:26.98	Hits@10:45.76	Best:26.98
2024-12-29 04:20:40,384: Snapshot:1	Epoch:9	Loss:0.323	translation_Loss:0.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.215                                                   	MRR:27.13	Hits@10:46.6	Best:27.13
2024-12-29 04:20:43,142: Snapshot:1	Epoch:10	Loss:0.302	translation_Loss:0.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.203                                                   	MRR:27.54	Hits@10:47.22	Best:27.54
2024-12-29 04:20:45,355: Snapshot:1	Epoch:11	Loss:0.286	translation_Loss:0.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.194                                                   	MRR:27.78	Hits@10:47.56	Best:27.78
2024-12-29 04:20:47,433: Snapshot:1	Epoch:12	Loss:0.277	translation_Loss:0.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:28.21	Hits@10:48.18	Best:28.21
2024-12-29 04:20:49,619: Snapshot:1	Epoch:13	Loss:0.265	translation_Loss:0.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:28.33	Hits@10:48.79	Best:28.33
2024-12-29 04:20:51,764: Snapshot:1	Epoch:14	Loss:0.256	translation_Loss:0.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:28.61	Hits@10:49.09	Best:28.61
2024-12-29 04:20:53,900: Snapshot:1	Epoch:15	Loss:0.248	translation_Loss:0.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:28.83	Hits@10:49.44	Best:28.83
2024-12-29 04:20:56,329: Snapshot:1	Epoch:16	Loss:0.241	translation_Loss:0.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.166                                                   	MRR:29.12	Hits@10:49.83	Best:29.12
2024-12-29 04:20:58,482: Snapshot:1	Epoch:17	Loss:0.234	translation_Loss:0.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.45	Hits@10:50.16	Best:29.45
2024-12-29 04:21:01,080: Snapshot:1	Epoch:18	Loss:0.228	translation_Loss:0.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:29.71	Hits@10:50.57	Best:29.71
2024-12-29 04:21:03,285: Snapshot:1	Epoch:19	Loss:0.224	translation_Loss:0.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:29.92	Hits@10:50.89	Best:29.92
2024-12-29 04:21:05,383: Snapshot:1	Epoch:20	Loss:0.222	translation_Loss:0.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:29.93	Hits@10:50.97	Best:29.93
2024-12-29 04:21:07,461: Snapshot:1	Epoch:21	Loss:0.217	translation_Loss:0.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.15                                                   	MRR:30.17	Hits@10:50.77	Best:30.17
2024-12-29 04:21:09,953: Snapshot:1	Epoch:22	Loss:0.213	translation_Loss:0.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.148                                                   	MRR:30.41	Hits@10:50.85	Best:30.41
2024-12-29 04:21:12,454: Snapshot:1	Epoch:23	Loss:0.209	translation_Loss:0.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.146                                                   	MRR:30.45	Hits@10:51.2	Best:30.45
2024-12-29 04:21:14,763: Snapshot:1	Epoch:24	Loss:0.208	translation_Loss:0.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:30.36	Hits@10:50.85	Best:30.45
2024-12-29 04:21:17,275: Snapshot:1	Epoch:25	Loss:0.205	translation_Loss:0.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.141                                                   	MRR:30.44	Hits@10:50.92	Best:30.45
2024-12-29 04:21:19,731: Snapshot:1	Epoch:26	Loss:0.204	translation_Loss:0.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.139                                                   	MRR:30.46	Hits@10:51.35	Best:30.46
2024-12-29 04:21:22,190: Snapshot:1	Epoch:27	Loss:0.198	translation_Loss:0.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.137                                                   	MRR:30.5	Hits@10:51.28	Best:30.5
2024-12-29 04:21:24,713: Snapshot:1	Epoch:28	Loss:0.196	translation_Loss:0.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.135                                                   	MRR:30.46	Hits@10:51.69	Best:30.5
2024-12-29 04:21:26,794: Snapshot:1	Epoch:29	Loss:0.199	translation_Loss:0.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.135                                                   	MRR:30.58	Hits@10:51.37	Best:30.58
2024-12-29 04:21:28,898: Snapshot:1	Epoch:30	Loss:0.195	translation_Loss:0.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.135                                                   	MRR:30.42	Hits@10:51.09	Best:30.58
2024-12-29 04:21:30,930: Snapshot:1	Epoch:31	Loss:0.195	translation_Loss:0.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.134                                                   	MRR:30.56	Hits@10:51.65	Best:30.58
2024-12-29 04:21:33,414: Snapshot:1	Epoch:32	Loss:0.193	translation_Loss:0.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.132                                                   	MRR:30.7	Hits@10:51.64	Best:30.7
2024-12-29 04:21:35,659: Snapshot:1	Epoch:33	Loss:0.19	translation_Loss:0.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.131                                                   	MRR:30.75	Hits@10:51.62	Best:30.75
2024-12-29 04:21:38,110: Snapshot:1	Epoch:34	Loss:0.189	translation_Loss:0.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:30.91	Hits@10:51.59	Best:30.91
2024-12-29 04:21:40,601: Snapshot:1	Epoch:35	Loss:0.189	translation_Loss:0.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:30.91	Hits@10:51.58	Best:30.91
2024-12-29 04:21:42,633: Snapshot:1	Epoch:36	Loss:0.188	translation_Loss:0.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:30.8	Hits@10:51.75	Best:30.91
2024-12-29 04:21:45,075: Snapshot:1	Epoch:37	Loss:0.185	translation_Loss:0.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:30.81	Hits@10:51.77	Best:30.91
2024-12-29 04:21:47,364: Snapshot:1	Epoch:38	Loss:0.185	translation_Loss:0.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.128                                                   	MRR:30.89	Hits@10:51.76	Best:30.91
2024-12-29 04:21:49,803: Early Stopping! Snapshot: 1 Epoch: 39 Best Results: 30.91
2024-12-29 04:21:49,803: Start to training tokens! Snapshot: 1 Epoch: 39 Loss:0.184 MRR:30.86 Best Results: 30.91
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:21:49,803: Snapshot:1	Epoch:39	Loss:0.184	translation_Loss:0.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.127                                                   	MRR:30.86	Hits@10:51.62	Best:30.91
2024-12-29 04:21:52,031: Snapshot:1	Epoch:40	Loss:15.923	translation_Loss:4.134	multi_layer_Loss:11.789	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.86	Hits@10:51.62	Best:30.91
2024-12-29 04:21:54,443: End of token training: 1 Epoch: 41 Loss:6.774 MRR:30.86 Best Results: 30.91
2024-12-29 04:21:54,443: Snapshot:1	Epoch:41	Loss:6.774	translation_Loss:4.136	multi_layer_Loss:2.638	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.86	Hits@10:51.62	Best:30.91
2024-12-29 04:21:54,682: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-29 04:21:58,110: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2504 | 0.1434 | 0.3056 | 0.3727 |  0.4475 |
|     1      | 0.304  | 0.1964 | 0.3532 | 0.4265 |  0.5144 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:22:30,072: Snapshot:2	Epoch:0	Loss:15.23	translation_Loss:14.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.643                                                   	MRR:14.74	Hits@10:29.63	Best:14.74
2024-12-29 04:22:39,209: Snapshot:2	Epoch:1	Loss:5.893	translation_Loss:4.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.191                                                   	MRR:19.49	Hits@10:36.54	Best:19.49
2024-12-29 04:22:48,491: Snapshot:2	Epoch:2	Loss:3.499	translation_Loss:2.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.227                                                   	MRR:20.63	Hits@10:37.82	Best:20.63
2024-12-29 04:22:57,867: Snapshot:2	Epoch:3	Loss:2.734	translation_Loss:1.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.129                                                   	MRR:21.43	Hits@10:38.78	Best:21.43
2024-12-29 04:23:08,403: Snapshot:2	Epoch:4	Loss:2.431	translation_Loss:1.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.07                                                   	MRR:21.59	Hits@10:38.62	Best:21.59
2024-12-29 04:23:19,186: Snapshot:2	Epoch:5	Loss:2.29	translation_Loss:1.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.036                                                   	MRR:21.7	Hits@10:38.67	Best:21.7
2024-12-29 04:23:30,142: Snapshot:2	Epoch:6	Loss:2.197	translation_Loss:1.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.016                                                   	MRR:21.77	Hits@10:38.85	Best:21.77
2024-12-29 04:23:39,232: Snapshot:2	Epoch:7	Loss:2.148	translation_Loss:1.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.0                                                   	MRR:21.91	Hits@10:38.75	Best:21.91
2024-12-29 04:23:48,474: Snapshot:2	Epoch:8	Loss:2.107	translation_Loss:1.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.992                                                   	MRR:21.91	Hits@10:39.1	Best:21.91
2024-12-29 04:23:57,877: Snapshot:2	Epoch:9	Loss:2.085	translation_Loss:1.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.987                                                   	MRR:22.08	Hits@10:39.22	Best:22.08
2024-12-29 04:24:06,928: Snapshot:2	Epoch:10	Loss:2.067	translation_Loss:1.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.982                                                   	MRR:22.0	Hits@10:39.28	Best:22.08
2024-12-29 04:24:16,206: Snapshot:2	Epoch:11	Loss:2.041	translation_Loss:1.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.983                                                   	MRR:21.94	Hits@10:38.98	Best:22.08
2024-12-29 04:24:26,923: Snapshot:2	Epoch:12	Loss:2.038	translation_Loss:1.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.976                                                   	MRR:21.86	Hits@10:39.05	Best:22.08
2024-12-29 04:24:36,307: Snapshot:2	Epoch:13	Loss:2.024	translation_Loss:1.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.978                                                   	MRR:22.04	Hits@10:39.18	Best:22.08
2024-12-29 04:24:45,287: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 22.08
2024-12-29 04:24:45,288: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:2.015 MRR:21.98 Best Results: 22.08
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:24:45,288: Snapshot:2	Epoch:14	Loss:2.015	translation_Loss:1.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.974                                                   	MRR:21.98	Hits@10:39.12	Best:22.08
2024-12-29 04:24:56,028: Snapshot:2	Epoch:15	Loss:31.489	translation_Loss:17.051	multi_layer_Loss:14.438	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.98	Hits@10:39.12	Best:22.08
2024-12-29 04:25:05,560: End of token training: 2 Epoch: 16 Loss:17.173 MRR:21.98 Best Results: 22.08
2024-12-29 04:25:05,561: Snapshot:2	Epoch:16	Loss:17.173	translation_Loss:17.045	multi_layer_Loss:0.127	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.98	Hits@10:39.12	Best:22.08
2024-12-29 04:25:05,820: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-29 04:25:13,036: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2466 | 0.137  | 0.2998 | 0.3688 |  0.4569 |
|     1      | 0.2727 | 0.1705 | 0.3099 | 0.3832 |  0.4752 |
|     2      | 0.2217 | 0.1339 | 0.2522 | 0.3122 |  0.3951 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:25:48,976: Snapshot:3	Epoch:0	Loss:13.815	translation_Loss:12.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.889                                                   	MRR:16.04	Hits@10:32.38	Best:16.04
2024-12-29 04:26:01,573: Snapshot:3	Epoch:1	Loss:4.867	translation_Loss:3.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.434                                                   	MRR:20.22	Hits@10:37.82	Best:20.22
2024-12-29 04:26:12,631: Snapshot:3	Epoch:2	Loss:3.185	translation_Loss:1.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.317                                                   	MRR:20.89	Hits@10:38.68	Best:20.89
2024-12-29 04:26:25,322: Snapshot:3	Epoch:3	Loss:2.674	translation_Loss:1.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.217                                                   	MRR:21.25	Hits@10:39.26	Best:21.25
2024-12-29 04:26:36,348: Snapshot:3	Epoch:4	Loss:2.476	translation_Loss:1.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.165                                                   	MRR:21.54	Hits@10:39.21	Best:21.54
2024-12-29 04:26:47,762: Snapshot:3	Epoch:5	Loss:2.373	translation_Loss:1.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.136                                                   	MRR:21.65	Hits@10:39.61	Best:21.65
2024-12-29 04:26:59,021: Snapshot:3	Epoch:6	Loss:2.312	translation_Loss:1.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.122                                                   	MRR:21.64	Hits@10:39.49	Best:21.65
2024-12-29 04:27:11,977: Snapshot:3	Epoch:7	Loss:2.282	translation_Loss:1.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.114                                                   	MRR:21.93	Hits@10:39.9	Best:21.93
2024-12-29 04:27:25,033: Snapshot:3	Epoch:8	Loss:2.243	translation_Loss:1.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.103                                                   	MRR:21.9	Hits@10:39.92	Best:21.93
2024-12-29 04:27:37,630: Snapshot:3	Epoch:9	Loss:2.226	translation_Loss:1.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.099                                                   	MRR:21.68	Hits@10:39.37	Best:21.93
2024-12-29 04:27:48,878: Snapshot:3	Epoch:10	Loss:2.215	translation_Loss:1.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.098                                                   	MRR:21.76	Hits@10:39.52	Best:21.93
2024-12-29 04:28:01,877: Snapshot:3	Epoch:11	Loss:2.203	translation_Loss:1.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.096                                                   	MRR:21.73	Hits@10:39.52	Best:21.93
2024-12-29 04:28:14,727: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 21.93
2024-12-29 04:28:14,727: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:2.207 MRR:21.62 Best Results: 21.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:28:14,728: Snapshot:3	Epoch:12	Loss:2.207	translation_Loss:1.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.101                                                   	MRR:21.62	Hits@10:39.44	Best:21.93
2024-12-29 04:28:26,053: Snapshot:3	Epoch:13	Loss:33.428	translation_Loss:18.081	multi_layer_Loss:15.348	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.62	Hits@10:39.44	Best:21.93
2024-12-29 04:28:37,226: End of token training: 3 Epoch: 14 Loss:18.142 MRR:21.62 Best Results: 21.93
2024-12-29 04:28:37,226: Snapshot:3	Epoch:14	Loss:18.142	translation_Loss:18.071	multi_layer_Loss:0.071	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.62	Hits@10:39.44	Best:21.93
2024-12-29 04:28:37,458: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2024-12-29 04:28:49,304: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.255  | 0.1508 | 0.3044 | 0.3695 |  0.4494 |
|     1      | 0.2717 | 0.1734 | 0.3078 | 0.3695 |  0.463  |
|     2      | 0.2117 | 0.1233 | 0.2414 | 0.3014 |  0.3854 |
|     3      | 0.218  | 0.1235 | 0.2549 | 0.316  |  0.397  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:29:06,208: Snapshot:4	Epoch:0	Loss:6.05	translation_Loss:5.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:9.39	Hits@10:20.8	Best:9.39
2024-12-29 04:29:11,462: Snapshot:4	Epoch:1	Loss:3.044	translation_Loss:2.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.405                                                   	MRR:16.04	Hits@10:33.87	Best:16.04
2024-12-29 04:29:16,370: Snapshot:4	Epoch:2	Loss:1.759	translation_Loss:1.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.566                                                   	MRR:18.58	Hits@10:35.58	Best:18.58
2024-12-29 04:29:20,840: Snapshot:4	Epoch:3	Loss:1.216	translation_Loss:0.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.629                                                   	MRR:19.62	Hits@10:36.17	Best:19.62
2024-12-29 04:29:26,028: Snapshot:4	Epoch:4	Loss:0.973	translation_Loss:0.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.609                                                   	MRR:20.92	Hits@10:37.01	Best:20.92
2024-12-29 04:29:31,057: Snapshot:4	Epoch:5	Loss:0.839	translation_Loss:0.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.564                                                   	MRR:22.03	Hits@10:37.42	Best:22.03
2024-12-29 04:29:35,573: Snapshot:4	Epoch:6	Loss:0.768	translation_Loss:0.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.528                                                   	MRR:22.58	Hits@10:37.95	Best:22.58
2024-12-29 04:29:39,995: Snapshot:4	Epoch:7	Loss:0.717	translation_Loss:0.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.501                                                   	MRR:22.4	Hits@10:37.86	Best:22.58
2024-12-29 04:29:44,727: Snapshot:4	Epoch:8	Loss:0.684	translation_Loss:0.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.479                                                   	MRR:22.55	Hits@10:38.02	Best:22.58
2024-12-29 04:29:49,202: Snapshot:4	Epoch:9	Loss:0.661	translation_Loss:0.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:22.58	Hits@10:37.84	Best:22.58
2024-12-29 04:29:53,750: Snapshot:4	Epoch:10	Loss:0.641	translation_Loss:0.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.448                                                   	MRR:22.83	Hits@10:38.49	Best:22.83
2024-12-29 04:29:58,200: Snapshot:4	Epoch:11	Loss:0.631	translation_Loss:0.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.436                                                   	MRR:22.71	Hits@10:38.28	Best:22.83
2024-12-29 04:30:03,005: Snapshot:4	Epoch:12	Loss:0.623	translation_Loss:0.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:22.92	Hits@10:38.35	Best:22.92
2024-12-29 04:30:08,515: Snapshot:4	Epoch:13	Loss:0.617	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.426                                                   	MRR:23.12	Hits@10:38.66	Best:23.12
2024-12-29 04:30:13,145: Snapshot:4	Epoch:14	Loss:0.604	translation_Loss:0.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.419                                                   	MRR:22.93	Hits@10:38.8	Best:23.12
2024-12-29 04:30:17,913: Snapshot:4	Epoch:15	Loss:0.604	translation_Loss:0.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.413                                                   	MRR:22.91	Hits@10:38.69	Best:23.12
2024-12-29 04:30:23,033: Snapshot:4	Epoch:16	Loss:0.6	translation_Loss:0.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.412                                                   	MRR:22.88	Hits@10:38.8	Best:23.12
2024-12-29 04:30:27,589: Snapshot:4	Epoch:17	Loss:0.594	translation_Loss:0.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.409                                                   	MRR:22.84	Hits@10:38.78	Best:23.12
2024-12-29 04:30:32,413: Early Stopping! Snapshot: 4 Epoch: 18 Best Results: 23.12
2024-12-29 04:30:32,414: Start to training tokens! Snapshot: 4 Epoch: 18 Loss:0.592 MRR:22.55 Best Results: 23.12
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:30:32,414: Snapshot:4	Epoch:18	Loss:0.592	translation_Loss:0.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.404                                                   	MRR:22.55	Hits@10:38.14	Best:23.12
2024-12-29 04:30:36,837: Snapshot:4	Epoch:19	Loss:23.218	translation_Loss:8.155	multi_layer_Loss:15.063	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.55	Hits@10:38.14	Best:23.12
2024-12-29 04:30:41,989: End of token training: 4 Epoch: 20 Loss:9.072 MRR:22.55 Best Results: 23.12
2024-12-29 04:30:41,989: Snapshot:4	Epoch:20	Loss:9.072	translation_Loss:8.156	multi_layer_Loss:0.916	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.55	Hits@10:38.14	Best:23.12
2024-12-29 04:30:42,256: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2024-12-29 04:30:57,975: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2049 | 0.1111 | 0.2455 | 0.3046 |  0.3807 |
|     1      | 0.2464 | 0.1534 | 0.2757 | 0.3402 |  0.4331 |
|     2      | 0.1904 |  0.11  | 0.2148 | 0.2672 |  0.3478 |
|     3      | 0.1815 | 0.0966 | 0.2092 | 0.2637 |  0.3439 |
|     4      | 0.2295 | 0.1488 | 0.2594 | 0.312  |  0.3831 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:30:57,978: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1507 | 0.3159 | 0.3795 |  0.4525 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2504 | 0.1434 | 0.3056 | 0.3727 |  0.4475 |
|     1      | 0.304  | 0.1964 | 0.3532 | 0.4265 |  0.5144 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2466 | 0.137  | 0.2998 | 0.3688 |  0.4569 |
|     1      | 0.2727 | 0.1705 | 0.3099 | 0.3832 |  0.4752 |
|     2      | 0.2217 | 0.1339 | 0.2522 | 0.3122 |  0.3951 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.255  | 0.1508 | 0.3044 | 0.3695 |  0.4494 |
|     1      | 0.2717 | 0.1734 | 0.3078 | 0.3695 |  0.463  |
|     2      | 0.2117 | 0.1233 | 0.2414 | 0.3014 |  0.3854 |
|     3      | 0.218  | 0.1235 | 0.2549 | 0.316  |  0.397  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2049 | 0.1111 | 0.2455 | 0.3046 |  0.3807 |
|     1      | 0.2464 | 0.1534 | 0.2757 | 0.3402 |  0.4331 |
|     2      | 0.1904 |  0.11  | 0.2148 | 0.2672 |  0.3478 |
|     3      | 0.1815 | 0.0966 | 0.2092 | 0.2637 |  0.3439 |
|     4      | 0.2295 | 0.1488 | 0.2594 | 0.312  |  0.3831 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:30:57,978: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 128.8522868156433  |   0.258   |    0.151     |    0.316     |     0.452     |
|    1     | 104.79613304138184 |   0.265   |    0.158     |    0.318     |     0.465     |
|    2     | 183.72733354568481 |   0.237   |     0.14     |    0.276     |     0.427     |
|    3     | 199.3058090209961  |   0.228   |    0.133     |    0.265     |      0.41     |
|    4     | 110.19494318962097 |   0.199   |    0.114     |    0.229     |     0.363     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:30:57,978: Sum_Training_Time:726.876505613327
2024-12-29 04:30:57,978: Every_Training_Time:[128.8522868156433, 104.79613304138184, 183.72733354568481, 199.3058090209961, 110.19494318962097]
2024-12-29 04:30:57,978: Forward transfer: 0.0433 Backward transfer: -0.044649999999999995
2024-12-29 04:31:29,804: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229043102/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=50000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:31:39,019: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2024-12-29 04:31:45,386: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2024-12-29 04:31:50,972: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.68	Best:18.93
2024-12-29 04:31:57,783: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.53	Best:22.69
2024-12-29 04:32:03,299: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:45.24	Best:24.2
2024-12-29 04:32:10,013: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:45.97	Best:24.98
2024-12-29 04:32:15,601: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:46.2	Best:25.33
2024-12-29 04:32:22,392: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.46	Best:25.6
2024-12-29 04:32:28,115: Snapshot:0	Epoch:8	Loss:0.63	translation_Loss:0.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.61	Hits@10:46.55	Best:25.61
2024-12-29 04:32:34,948: Snapshot:0	Epoch:9	Loss:0.534	translation_Loss:0.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.51	Best:25.7
2024-12-29 04:32:40,462: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.33	Best:25.77
2024-12-29 04:32:47,197: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.89	Hits@10:46.32	Best:25.89
2024-12-29 04:32:52,685: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:46.3	Best:25.89
2024-12-29 04:32:59,166: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.14	Best:25.89
2024-12-29 04:33:05,094: Snapshot:0	Epoch:14	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.1	Best:25.89
2024-12-29 04:33:11,461: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.06	Best:25.89
2024-12-29 04:33:17,280: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 25.89
2024-12-29 04:33:17,281: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.252 MRR:25.64 Best Results: 25.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:33:17,281: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:45.97	Best:25.89
2024-12-29 04:33:24,204: Snapshot:0	Epoch:17	Loss:26.566	translation_Loss:11.567	multi_layer_Loss:14.999	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:45.97	Best:25.89
2024-12-29 04:33:30,130: End of token training: 0 Epoch: 18 Loss:11.92 MRR:25.64 Best Results: 25.89
2024-12-29 04:33:30,130: Snapshot:0	Epoch:18	Loss:11.92	translation_Loss:11.548	multi_layer_Loss:0.373	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.64	Hits@10:45.97	Best:25.89
2024-12-29 04:33:30,387: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-29 04:33:32,706: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2568 | 0.1484 | 0.3139 | 0.3775 |  0.454  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:33:42,890: Snapshot:1	Epoch:0	Loss:5.29	translation_Loss:4.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.483                                                   	MRR:9.35	Hits@10:16.05	Best:9.35
2024-12-29 04:33:44,986: Snapshot:1	Epoch:1	Loss:3.515	translation_Loss:3.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:14.53	Hits@10:26.43	Best:14.53
2024-12-29 04:33:47,074: Snapshot:1	Epoch:2	Loss:2.362	translation_Loss:2.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:18.29	Hits@10:32.24	Best:18.29
2024-12-29 04:33:49,209: Snapshot:1	Epoch:3	Loss:1.75	translation_Loss:1.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.275                                                   	MRR:21.04	Hits@10:37.45	Best:21.04
2024-12-29 04:33:51,300: Snapshot:1	Epoch:4	Loss:1.415	translation_Loss:1.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:22.85	Hits@10:41.13	Best:22.85
2024-12-29 04:33:53,396: Snapshot:1	Epoch:5	Loss:1.206	translation_Loss:1.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.203                                                   	MRR:24.28	Hits@10:43.41	Best:24.28
2024-12-29 04:33:55,885: Snapshot:1	Epoch:6	Loss:1.079	translation_Loss:0.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:25.37	Hits@10:45.29	Best:25.37
2024-12-29 04:33:58,128: Snapshot:1	Epoch:7	Loss:0.995	translation_Loss:0.818	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:26.28	Hits@10:46.47	Best:26.28
2024-12-29 04:34:00,538: Snapshot:1	Epoch:8	Loss:0.931	translation_Loss:0.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:26.83	Hits@10:46.8	Best:26.83
2024-12-29 04:34:02,652: Snapshot:1	Epoch:9	Loss:0.871	translation_Loss:0.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.159                                                   	MRR:27.39	Hits@10:47.57	Best:27.39
2024-12-29 04:34:04,772: Snapshot:1	Epoch:10	Loss:0.827	translation_Loss:0.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:27.89	Hits@10:48.01	Best:27.89
2024-12-29 04:34:07,266: Snapshot:1	Epoch:11	Loss:0.791	translation_Loss:0.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.15                                                   	MRR:28.25	Hits@10:48.42	Best:28.25
2024-12-29 04:34:09,513: Snapshot:1	Epoch:12	Loss:0.766	translation_Loss:0.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.144                                                   	MRR:28.62	Hits@10:48.74	Best:28.62
2024-12-29 04:34:11,629: Snapshot:1	Epoch:13	Loss:0.739	translation_Loss:0.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.142                                                   	MRR:28.91	Hits@10:49.08	Best:28.91
2024-12-29 04:34:14,057: Snapshot:1	Epoch:14	Loss:0.722	translation_Loss:0.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.139                                                   	MRR:28.95	Hits@10:49.29	Best:28.95
2024-12-29 04:34:16,108: Snapshot:1	Epoch:15	Loss:0.708	translation_Loss:0.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.136                                                   	MRR:28.9	Hits@10:49.44	Best:28.95
2024-12-29 04:34:18,168: Snapshot:1	Epoch:16	Loss:0.695	translation_Loss:0.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.135                                                   	MRR:28.79	Hits@10:49.71	Best:28.95
2024-12-29 04:34:20,248: Snapshot:1	Epoch:17	Loss:0.681	translation_Loss:0.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.133                                                   	MRR:29.09	Hits@10:49.64	Best:29.09
2024-12-29 04:34:22,295: Snapshot:1	Epoch:18	Loss:0.668	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.132                                                   	MRR:29.05	Hits@10:49.74	Best:29.09
2024-12-29 04:34:24,339: Snapshot:1	Epoch:19	Loss:0.661	translation_Loss:0.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:29.25	Hits@10:49.65	Best:29.25
2024-12-29 04:34:26,705: Snapshot:1	Epoch:20	Loss:0.656	translation_Loss:0.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.131                                                   	MRR:29.18	Hits@10:49.86	Best:29.25
2024-12-29 04:34:29,202: Snapshot:1	Epoch:21	Loss:0.647	translation_Loss:0.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.131                                                   	MRR:29.32	Hits@10:49.93	Best:29.32
2024-12-29 04:34:31,461: Snapshot:1	Epoch:22	Loss:0.645	translation_Loss:0.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:29.49	Hits@10:50.18	Best:29.49
2024-12-29 04:34:33,900: Snapshot:1	Epoch:23	Loss:0.635	translation_Loss:0.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.128                                                   	MRR:29.47	Hits@10:50.16	Best:29.49
2024-12-29 04:34:36,040: Snapshot:1	Epoch:24	Loss:0.629	translation_Loss:0.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.127                                                   	MRR:29.41	Hits@10:50.22	Best:29.49
2024-12-29 04:34:38,070: Snapshot:1	Epoch:25	Loss:0.627	translation_Loss:0.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.128                                                   	MRR:29.4	Hits@10:50.2	Best:29.49
2024-12-29 04:34:40,397: Snapshot:1	Epoch:26	Loss:0.623	translation_Loss:0.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.127                                                   	MRR:29.4	Hits@10:50.13	Best:29.49
2024-12-29 04:34:42,474: Snapshot:1	Epoch:27	Loss:0.622	translation_Loss:0.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.128                                                   	MRR:29.63	Hits@10:50.26	Best:29.63
2024-12-29 04:34:44,528: Snapshot:1	Epoch:28	Loss:0.617	translation_Loss:0.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.126                                                   	MRR:29.39	Hits@10:50.19	Best:29.63
2024-12-29 04:34:46,603: Snapshot:1	Epoch:29	Loss:0.616	translation_Loss:0.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.127                                                   	MRR:29.36	Hits@10:50.26	Best:29.63
2024-12-29 04:34:48,708: Snapshot:1	Epoch:30	Loss:0.615	translation_Loss:0.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.128                                                   	MRR:29.48	Hits@10:50.18	Best:29.63
2024-12-29 04:34:50,772: Snapshot:1	Epoch:31	Loss:0.615	translation_Loss:0.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.128                                                   	MRR:29.6	Hits@10:50.2	Best:29.63
2024-12-29 04:34:52,807: Early Stopping! Snapshot: 1 Epoch: 32 Best Results: 29.63
2024-12-29 04:34:52,807: Start to training tokens! Snapshot: 1 Epoch: 32 Loss:0.609 MRR:29.39 Best Results: 29.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:34:52,808: Snapshot:1	Epoch:32	Loss:0.609	translation_Loss:0.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.127                                                   	MRR:29.39	Hits@10:50.18	Best:29.63
2024-12-29 04:34:55,647: Snapshot:1	Epoch:33	Loss:16.391	translation_Loss:4.603	multi_layer_Loss:11.789	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.39	Hits@10:50.18	Best:29.63
2024-12-29 04:34:58,076: End of token training: 1 Epoch: 34 Loss:7.239 MRR:29.39 Best Results: 29.63
2024-12-29 04:34:58,076: Snapshot:1	Epoch:34	Loss:7.239	translation_Loss:4.601	multi_layer_Loss:2.638	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.39	Hits@10:50.18	Best:29.63
2024-12-29 04:34:58,338: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-29 04:35:02,056: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.258  | 0.1498 | 0.3163 | 0.3786 |  0.455  |
|     1      | 0.2875 | 0.1822 | 0.3364 | 0.408  |  0.4984 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:35:34,002: Snapshot:2	Epoch:0	Loss:17.333	translation_Loss:15.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.592                                                   	MRR:12.14	Hits@10:26.05	Best:12.14
2024-12-29 04:35:43,237: Snapshot:2	Epoch:1	Loss:8.786	translation_Loss:7.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.182                                                   	MRR:17.51	Hits@10:33.52	Best:17.51
2024-12-29 04:35:53,762: Snapshot:2	Epoch:2	Loss:6.004	translation_Loss:5.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.988                                                   	MRR:19.43	Hits@10:35.43	Best:19.43
2024-12-29 04:36:03,131: Snapshot:2	Epoch:3	Loss:5.025	translation_Loss:4.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.89                                                   	MRR:20.1	Hits@10:35.93	Best:20.1
2024-12-29 04:36:12,425: Snapshot:2	Epoch:4	Loss:4.629	translation_Loss:3.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.847                                                   	MRR:20.13	Hits@10:36.13	Best:20.13
2024-12-29 04:36:23,174: Snapshot:2	Epoch:5	Loss:4.45	translation_Loss:3.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.828                                                   	MRR:20.47	Hits@10:36.12	Best:20.47
2024-12-29 04:36:32,159: Snapshot:2	Epoch:6	Loss:4.354	translation_Loss:3.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.819                                                   	MRR:20.33	Hits@10:36.18	Best:20.47
2024-12-29 04:36:41,363: Snapshot:2	Epoch:7	Loss:4.289	translation_Loss:3.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.811                                                   	MRR:20.57	Hits@10:36.17	Best:20.57
2024-12-29 04:36:52,165: Snapshot:2	Epoch:8	Loss:4.236	translation_Loss:3.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.806                                                   	MRR:20.26	Hits@10:36.15	Best:20.57
2024-12-29 04:37:01,139: Snapshot:2	Epoch:9	Loss:4.208	translation_Loss:3.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.806                                                   	MRR:20.29	Hits@10:36.18	Best:20.57
2024-12-29 04:37:10,380: Snapshot:2	Epoch:10	Loss:4.176	translation_Loss:3.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.802                                                   	MRR:20.31	Hits@10:36.21	Best:20.57
2024-12-29 04:37:21,191: Snapshot:2	Epoch:11	Loss:4.143	translation_Loss:3.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.796                                                   	MRR:20.42	Hits@10:36.09	Best:20.57
2024-12-29 04:37:31,523: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 20.57
2024-12-29 04:37:31,524: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:4.123 MRR:20.46 Best Results: 20.57
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:37:31,524: Snapshot:2	Epoch:12	Loss:4.123	translation_Loss:3.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.795                                                   	MRR:20.46	Hits@10:36.15	Best:20.57
2024-12-29 04:37:40,822: Snapshot:2	Epoch:13	Loss:33.549	translation_Loss:19.111	multi_layer_Loss:14.438	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.46	Hits@10:36.15	Best:20.57
2024-12-29 04:37:51,498: End of token training: 2 Epoch: 14 Loss:19.241 MRR:20.46 Best Results: 20.57
2024-12-29 04:37:51,498: Snapshot:2	Epoch:14	Loss:19.241	translation_Loss:19.114	multi_layer_Loss:0.127	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.46	Hits@10:36.15	Best:20.57
2024-12-29 04:37:51,734: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-29 04:37:59,827: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1474 | 0.3164 | 0.3796 |  0.4547 |
|     1      | 0.2845 | 0.1788 | 0.3322 | 0.4024 |  0.4988 |
|     2      | 0.2037 | 0.1221 | 0.2318 | 0.2881 |  0.3642 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:38:37,460: Snapshot:3	Epoch:0	Loss:17.81	translation_Loss:15.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.104                                                   	MRR:13.19	Hits@10:27.73	Best:13.19
2024-12-29 04:38:48,287: Snapshot:3	Epoch:1	Loss:9.892	translation_Loss:8.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.587                                                   	MRR:18.02	Hits@10:33.79	Best:18.02
2024-12-29 04:39:00,993: Snapshot:3	Epoch:2	Loss:7.573	translation_Loss:6.159	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.414                                                   	MRR:18.98	Hits@10:34.74	Best:18.98
2024-12-29 04:39:13,862: Snapshot:3	Epoch:3	Loss:6.783	translation_Loss:5.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.324                                                   	MRR:19.35	Hits@10:34.87	Best:19.35
2024-12-29 04:39:24,789: Snapshot:3	Epoch:4	Loss:6.46	translation_Loss:5.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.282                                                   	MRR:19.57	Hits@10:34.98	Best:19.57
2024-12-29 04:39:35,962: Snapshot:3	Epoch:5	Loss:6.311	translation_Loss:5.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.271                                                   	MRR:19.65	Hits@10:35.0	Best:19.65
2024-12-29 04:39:47,183: Snapshot:3	Epoch:6	Loss:6.232	translation_Loss:4.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.258                                                   	MRR:19.68	Hits@10:35.11	Best:19.68
2024-12-29 04:39:58,341: Snapshot:3	Epoch:7	Loss:6.191	translation_Loss:4.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.258                                                   	MRR:19.51	Hits@10:34.94	Best:19.68
2024-12-29 04:40:09,538: Snapshot:3	Epoch:8	Loss:6.134	translation_Loss:4.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.253                                                   	MRR:19.65	Hits@10:35.14	Best:19.68
2024-12-29 04:40:22,050: Snapshot:3	Epoch:9	Loss:6.104	translation_Loss:4.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.248                                                   	MRR:19.57	Hits@10:35.09	Best:19.68
2024-12-29 04:40:34,893: Snapshot:3	Epoch:10	Loss:6.083	translation_Loss:4.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.249                                                   	MRR:19.61	Hits@10:35.2	Best:19.68
2024-12-29 04:40:46,273: Early Stopping! Snapshot: 3 Epoch: 11 Best Results: 19.68
2024-12-29 04:40:46,273: Start to training tokens! Snapshot: 3 Epoch: 11 Loss:6.047 MRR:19.61 Best Results: 19.68
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:40:46,273: Snapshot:3	Epoch:11	Loss:6.047	translation_Loss:4.803	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.244                                                   	MRR:19.61	Hits@10:35.23	Best:19.68
2024-12-29 04:40:57,524: Snapshot:3	Epoch:12	Loss:36.617	translation_Loss:21.27	multi_layer_Loss:15.348	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.61	Hits@10:35.23	Best:19.68
2024-12-29 04:41:08,584: End of token training: 3 Epoch: 13 Loss:21.338 MRR:19.61 Best Results: 19.68
2024-12-29 04:41:08,585: Snapshot:3	Epoch:13	Loss:21.338	translation_Loss:21.268	multi_layer_Loss:0.071	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.61	Hits@10:35.23	Best:19.68
2024-12-29 04:41:08,822: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2024-12-29 04:41:21,080: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.1488 | 0.3164 | 0.3789 |  0.4548 |
|     1      | 0.2868 | 0.181  | 0.3338 | 0.4049 |  0.4992 |
|     2      | 0.2053 | 0.1238 | 0.2325 | 0.2888 |  0.3661 |
|     3      | 0.195  | 0.1112 | 0.2302 | 0.2837 |  0.3517 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:41:38,830: Snapshot:4	Epoch:0	Loss:7.678	translation_Loss:6.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.909                                                   	MRR:7.88	Hits@10:17.43	Best:7.88
2024-12-29 04:41:43,457: Snapshot:4	Epoch:1	Loss:6.083	translation_Loss:5.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.866                                                   	MRR:13.17	Hits@10:28.32	Best:13.17
2024-12-29 04:41:48,679: Snapshot:4	Epoch:2	Loss:4.973	translation_Loss:4.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.769                                                   	MRR:16.97	Hits@10:31.26	Best:16.97
2024-12-29 04:41:53,873: Snapshot:4	Epoch:3	Loss:4.367	translation_Loss:3.659	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.708                                                   	MRR:19.03	Hits@10:32.52	Best:19.03
2024-12-29 04:41:58,479: Snapshot:4	Epoch:4	Loss:3.95	translation_Loss:3.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.664                                                   	MRR:19.84	Hits@10:32.54	Best:19.84
2024-12-29 04:42:02,934: Snapshot:4	Epoch:5	Loss:3.665	translation_Loss:3.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.626                                                   	MRR:20.21	Hits@10:32.66	Best:20.21
2024-12-29 04:42:07,405: Snapshot:4	Epoch:6	Loss:3.484	translation_Loss:2.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.598                                                   	MRR:20.51	Hits@10:32.92	Best:20.51
2024-12-29 04:42:12,923: Snapshot:4	Epoch:7	Loss:3.383	translation_Loss:2.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.578                                                   	MRR:20.34	Hits@10:32.7	Best:20.51
2024-12-29 04:42:17,492: Snapshot:4	Epoch:8	Loss:3.314	translation_Loss:2.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.563                                                   	MRR:20.53	Hits@10:32.77	Best:20.53
2024-12-29 04:42:22,656: Snapshot:4	Epoch:9	Loss:3.274	translation_Loss:2.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.554                                                   	MRR:20.59	Hits@10:32.96	Best:20.59
2024-12-29 04:42:27,252: Snapshot:4	Epoch:10	Loss:3.252	translation_Loss:2.707	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.545                                                   	MRR:20.3	Hits@10:32.72	Best:20.59
2024-12-29 04:42:31,944: Snapshot:4	Epoch:11	Loss:3.236	translation_Loss:2.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.544                                                   	MRR:20.46	Hits@10:32.7	Best:20.59
2024-12-29 04:42:36,363: Snapshot:4	Epoch:12	Loss:3.22	translation_Loss:2.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.537                                                   	MRR:20.37	Hits@10:32.7	Best:20.59
2024-12-29 04:42:40,787: Snapshot:4	Epoch:13	Loss:3.221	translation_Loss:2.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.54                                                   	MRR:20.58	Hits@10:32.54	Best:20.59
2024-12-29 04:42:45,531: Early Stopping! Snapshot: 4 Epoch: 14 Best Results: 20.59
2024-12-29 04:42:45,531: Start to training tokens! Snapshot: 4 Epoch: 14 Loss:3.221 MRR:20.29 Best Results: 20.59
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:42:45,531: Snapshot:4	Epoch:14	Loss:3.221	translation_Loss:2.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.54                                                   	MRR:20.29	Hits@10:32.74	Best:20.59
2024-12-29 04:42:49,915: Snapshot:4	Epoch:15	Loss:25.322	translation_Loss:10.259	multi_layer_Loss:15.063	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.29	Hits@10:32.74	Best:20.59
2024-12-29 04:42:54,305: End of token training: 4 Epoch: 16 Loss:11.188 MRR:20.29 Best Results: 20.59
2024-12-29 04:42:54,306: Snapshot:4	Epoch:16	Loss:11.188	translation_Loss:10.273	multi_layer_Loss:0.916	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.29	Hits@10:32.74	Best:20.59
2024-12-29 04:42:54,606: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2024-12-29 04:43:09,244: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2364 | 0.129  | 0.2899 | 0.3531 |  0.4325 |
|     1      | 0.2807 | 0.1752 | 0.3271 | 0.3965 |  0.4929 |
|     2      |  0.2   | 0.1201 | 0.2251 | 0.2799 |  0.3588 |
|     3      | 0.1879 | 0.1035 | 0.2224 | 0.2764 |  0.3466 |
|     4      | 0.2074 | 0.1449 | 0.2295 | 0.2734 |  0.328  |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:43:09,246: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2568 | 0.1484 | 0.3139 | 0.3775 |  0.454  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.258  | 0.1498 | 0.3163 | 0.3786 |  0.455  |
|     1      | 0.2875 | 0.1822 | 0.3364 | 0.408  |  0.4984 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1474 | 0.3164 | 0.3796 |  0.4547 |
|     1      | 0.2845 | 0.1788 | 0.3322 | 0.4024 |  0.4988 |
|     2      | 0.2037 | 0.1221 | 0.2318 | 0.2881 |  0.3642 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.1488 | 0.3164 | 0.3789 |  0.4548 |
|     1      | 0.2868 | 0.181  | 0.3338 | 0.4049 |  0.4992 |
|     2      | 0.2053 | 0.1238 | 0.2325 | 0.2888 |  0.3661 |
|     3      | 0.195  | 0.1112 | 0.2302 | 0.2837 |  0.3517 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2364 | 0.129  | 0.2899 | 0.3531 |  0.4325 |
|     1      | 0.2807 | 0.1752 | 0.3271 | 0.3965 |  0.4929 |
|     2      |  0.2   | 0.1201 | 0.2251 | 0.2799 |  0.3588 |
|     3      | 0.1879 | 0.1035 | 0.2224 | 0.2764 |  0.3466 |
|     4      | 0.2074 | 0.1449 | 0.2295 | 0.2734 |  0.328  |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:43:09,247: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 120.32516622543335 |   0.257   |    0.148     |    0.314     |     0.454     |
|    1     | 84.20761919021606  |   0.266   |    0.158     |    0.322     |     0.467     |
|    2     | 165.11383628845215 |   0.232   |    0.138     |    0.274     |     0.412     |
|    3     | 183.68884587287903 |   0.219   |    0.129     |    0.257     |      0.39     |
|    4     | 90.95802450180054  |   0.209   |    0.123     |    0.244     |     0.373     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:43:09,247: Sum_Training_Time:644.2934920787811
2024-12-29 04:43:09,247: Every_Training_Time:[120.32516622543335, 84.20761919021606, 165.11383628845215, 183.68884587287903, 90.95802450180054]
2024-12-29 04:43:09,247: Forward transfer: 0.038775000000000004 Backward transfer: -0.00949999999999998
