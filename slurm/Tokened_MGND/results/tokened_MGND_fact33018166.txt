2024-12-29 03:48:35,479: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229034756/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 03:48:44,751: Snapshot:0	Epoch:0	Loss:17.024	translation_Loss:17.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.26	Best:6.23
2024-12-29 03:48:51,168: Snapshot:0	Epoch:1	Loss:11.963	translation_Loss:11.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.69	Hits@10:22.61	Best:9.69
2024-12-29 03:48:57,225: Snapshot:0	Epoch:2	Loss:8.603	translation_Loss:8.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.9	Hits@10:28.89	Best:12.9
2024-12-29 03:49:03,645: Snapshot:0	Epoch:3	Loss:6.158	translation_Loss:6.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.77	Hits@10:33.12	Best:15.77
2024-12-29 03:49:09,662: Snapshot:0	Epoch:4	Loss:4.364	translation_Loss:4.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.35	Hits@10:35.96	Best:18.35
2024-12-29 03:49:15,755: Snapshot:0	Epoch:5	Loss:3.085	translation_Loss:3.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.46	Hits@10:37.73	Best:20.46
2024-12-29 03:49:22,154: Snapshot:0	Epoch:6	Loss:2.176	translation_Loss:2.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.95	Hits@10:38.73	Best:21.95
2024-12-29 03:49:28,208: Snapshot:0	Epoch:7	Loss:1.542	translation_Loss:1.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.02	Hits@10:39.44	Best:23.02
2024-12-29 03:49:34,559: Snapshot:0	Epoch:8	Loss:1.096	translation_Loss:1.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.85	Best:23.69
2024-12-29 03:49:40,704: Snapshot:0	Epoch:9	Loss:0.816	translation_Loss:0.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:39.96	Best:24.18
2024-12-29 03:49:46,783: Snapshot:0	Epoch:10	Loss:0.626	translation_Loss:0.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:40.23	Best:24.31
2024-12-29 03:49:53,190: Snapshot:0	Epoch:11	Loss:0.496	translation_Loss:0.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:40.22	Best:24.43
2024-12-29 03:49:59,193: Snapshot:0	Epoch:12	Loss:0.411	translation_Loss:0.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:40.26	Best:24.48
2024-12-29 03:50:05,740: Snapshot:0	Epoch:13	Loss:0.352	translation_Loss:0.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.73	Hits@10:40.43	Best:24.73
2024-12-29 03:50:11,829: Snapshot:0	Epoch:14	Loss:0.31	translation_Loss:0.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.65	Hits@10:40.33	Best:24.73
2024-12-29 03:50:17,838: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.7	Hits@10:40.31	Best:24.73
2024-12-29 03:50:24,279: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 24.73
2024-12-29 03:50:24,280: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.243 MRR:24.66 Best Results: 24.73
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:50:24,280: Snapshot:0	Epoch:16	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:40.23	Best:24.73
2024-12-29 03:50:30,646: Snapshot:0	Epoch:17	Loss:27.654	translation_Loss:12.324	multi_layer_Loss:15.33	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:40.23	Best:24.73
2024-12-29 03:50:36,898: End of token training: 0 Epoch: 18 Loss:12.631 MRR:24.66 Best Results: 24.73
2024-12-29 03:50:36,898: Snapshot:0	Epoch:18	Loss:12.631	translation_Loss:12.322	multi_layer_Loss:0.309	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.66	Hits@10:40.23	Best:24.73
2024-12-29 03:50:37,144: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 03:50:39,617: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2398 | 0.1543 | 0.2824 | 0.3312 |  0.3938 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:51:02,545: Snapshot:1	Epoch:0	Loss:8.557	translation_Loss:7.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.009                                                   	MRR:20.25	Hits@10:34.11	Best:20.25
2024-12-29 03:51:08,999: Snapshot:1	Epoch:1	Loss:6.218	translation_Loss:4.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.523                                                   	MRR:21.02	Hits@10:35.45	Best:21.02
2024-12-29 03:51:15,538: Snapshot:1	Epoch:2	Loss:5.555	translation_Loss:4.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.48                                                   	MRR:21.57	Hits@10:36.11	Best:21.57
2024-12-29 03:51:22,053: Snapshot:1	Epoch:3	Loss:5.219	translation_Loss:3.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.487                                                   	MRR:21.69	Hits@10:36.31	Best:21.69
2024-12-29 03:51:28,436: Snapshot:1	Epoch:4	Loss:5.103	translation_Loss:3.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.468                                                   	MRR:21.75	Hits@10:36.37	Best:21.75
2024-12-29 03:51:34,884: Snapshot:1	Epoch:5	Loss:5.031	translation_Loss:3.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:21.78	Hits@10:36.45	Best:21.78
2024-12-29 03:51:41,207: Snapshot:1	Epoch:6	Loss:5.016	translation_Loss:3.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.46                                                   	MRR:21.75	Hits@10:36.52	Best:21.78
2024-12-29 03:51:48,050: Snapshot:1	Epoch:7	Loss:4.98	translation_Loss:3.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.462                                                   	MRR:21.77	Hits@10:36.28	Best:21.78
2024-12-29 03:51:54,409: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 21.78
2024-12-29 03:51:54,409: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:4.971 MRR:21.76 Best Results: 21.78
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:51:54,409: Snapshot:1	Epoch:8	Loss:4.971	translation_Loss:3.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.462                                                   	MRR:21.76	Hits@10:36.45	Best:21.78
2024-12-29 03:52:01,037: Snapshot:1	Epoch:9	Loss:29.206	translation_Loss:14.407	multi_layer_Loss:14.799	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.76	Hits@10:36.45	Best:21.78
2024-12-29 03:52:07,433: End of token training: 1 Epoch: 10 Loss:14.716 MRR:21.76 Best Results: 21.78
2024-12-29 03:52:07,434: Snapshot:1	Epoch:10	Loss:14.716	translation_Loss:14.407	multi_layer_Loss:0.308	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.76	Hits@10:36.45	Best:21.78
2024-12-29 03:52:07,684: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 03:52:13,010: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2506 | 0.1624 | 0.2929 | 0.3447 |  0.4116 |
|     1      | 0.2154 | 0.1333 | 0.2567 | 0.3025 |  0.3642 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:52:36,243: Snapshot:2	Epoch:0	Loss:6.333	translation_Loss:5.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.99                                                   	MRR:20.51	Hits@10:36.22	Best:20.51
2024-12-29 03:52:42,874: Snapshot:2	Epoch:1	Loss:4.5	translation_Loss:2.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.56                                                   	MRR:21.01	Hits@10:36.61	Best:21.01
2024-12-29 03:52:49,564: Snapshot:2	Epoch:2	Loss:4.259	translation_Loss:2.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.526                                                   	MRR:21.25	Hits@10:37.11	Best:21.25
2024-12-29 03:52:56,219: Snapshot:2	Epoch:3	Loss:4.138	translation_Loss:2.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.559                                                   	MRR:21.28	Hits@10:37.12	Best:21.28
2024-12-29 03:53:02,879: Snapshot:2	Epoch:4	Loss:4.143	translation_Loss:2.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.557                                                   	MRR:21.35	Hits@10:37.19	Best:21.35
2024-12-29 03:53:09,435: Snapshot:2	Epoch:5	Loss:4.111	translation_Loss:2.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.569                                                   	MRR:21.35	Hits@10:36.99	Best:21.35
2024-12-29 03:53:16,365: Snapshot:2	Epoch:6	Loss:4.122	translation_Loss:2.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.576                                                   	MRR:21.38	Hits@10:37.15	Best:21.38
2024-12-29 03:53:22,986: Snapshot:2	Epoch:7	Loss:4.11	translation_Loss:2.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.576                                                   	MRR:21.3	Hits@10:37.09	Best:21.38
2024-12-29 03:53:29,887: Snapshot:2	Epoch:8	Loss:4.131	translation_Loss:2.55	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.58                                                   	MRR:21.34	Hits@10:36.98	Best:21.38
2024-12-29 03:53:36,419: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 21.38
2024-12-29 03:53:36,419: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:4.108 MRR:21.28 Best Results: 21.38
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:53:36,420: Snapshot:2	Epoch:9	Loss:4.108	translation_Loss:2.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.587                                                   	MRR:21.28	Hits@10:37.07	Best:21.38
2024-12-29 03:53:42,928: Snapshot:2	Epoch:10	Loss:29.663	translation_Loss:14.177	multi_layer_Loss:15.486	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.28	Hits@10:37.07	Best:21.38
2024-12-29 03:53:49,908: End of token training: 2 Epoch: 11 Loss:14.501 MRR:21.28 Best Results: 21.38
2024-12-29 03:53:49,908: Snapshot:2	Epoch:11	Loss:14.501	translation_Loss:14.175	multi_layer_Loss:0.326	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.28	Hits@10:37.07	Best:21.38
2024-12-29 03:53:50,163: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 03:53:58,467: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2472 | 0.1583 | 0.2886 | 0.3422 |  0.4131 |
|     1      | 0.223  | 0.1403 | 0.2583 | 0.3118 |  0.3803 |
|     2      | 0.2138 | 0.1295 | 0.2513 | 0.3034 |  0.3735 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:54:21,709: Snapshot:3	Epoch:0	Loss:3.897	translation_Loss:3.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.856                                                   	MRR:20.02	Hits@10:37.95	Best:20.02
2024-12-29 03:54:28,533: Snapshot:3	Epoch:1	Loss:2.628	translation_Loss:1.426	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.203                                                   	MRR:20.09	Hits@10:37.98	Best:20.09
2024-12-29 03:54:35,370: Snapshot:3	Epoch:2	Loss:2.56	translation_Loss:1.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.117                                                   	MRR:20.1	Hits@10:38.34	Best:20.1
2024-12-29 03:54:42,074: Snapshot:3	Epoch:3	Loss:2.487	translation_Loss:1.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.146                                                   	MRR:20.01	Hits@10:38.25	Best:20.1
2024-12-29 03:54:48,850: Snapshot:3	Epoch:4	Loss:2.516	translation_Loss:1.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.157                                                   	MRR:20.04	Hits@10:38.25	Best:20.1
2024-12-29 03:54:55,449: Early Stopping! Snapshot: 3 Epoch: 5 Best Results: 20.1
2024-12-29 03:54:55,449: Start to training tokens! Snapshot: 3 Epoch: 5 Loss:2.514 MRR:20.05 Best Results: 20.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:54:55,449: Snapshot:3	Epoch:5	Loss:2.514	translation_Loss:1.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.165                                                   	MRR:20.05	Hits@10:38.3	Best:20.1
2024-12-29 03:55:02,038: Snapshot:3	Epoch:6	Loss:28.096	translation_Loss:13.412	multi_layer_Loss:14.684	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.05	Hits@10:38.3	Best:20.1
2024-12-29 03:55:09,056: End of token training: 3 Epoch: 7 Loss:13.727 MRR:20.05 Best Results: 20.1
2024-12-29 03:55:09,057: Snapshot:3	Epoch:7	Loss:13.727	translation_Loss:13.416	multi_layer_Loss:0.31	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.05	Hits@10:38.3	Best:20.1
2024-12-29 03:55:09,307: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 03:55:20,511: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2358 | 0.1497 | 0.2738 | 0.3237 |  0.3972 |
|     1      | 0.2158 | 0.1342 | 0.2479 | 0.3012 |  0.374  |
|     2      | 0.2107 | 0.1254 | 0.2428 | 0.3025 |  0.3792 |
|     3      | 0.205  | 0.1136 | 0.2376 | 0.3011 |  0.3852 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:55:43,447: Snapshot:4	Epoch:0	Loss:1.928	translation_Loss:1.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.517                                                   	MRR:21.47	Hits@10:46.94	Best:21.47
2024-12-29 03:55:50,042: Snapshot:4	Epoch:1	Loss:0.994	translation_Loss:0.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.528                                                   	MRR:21.66	Hits@10:45.73	Best:21.66
2024-12-29 03:55:57,117: Snapshot:4	Epoch:2	Loss:0.871	translation_Loss:0.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.457                                                   	MRR:21.87	Hits@10:45.9	Best:21.87
2024-12-29 03:56:03,778: Snapshot:4	Epoch:3	Loss:0.822	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.46                                                   	MRR:21.96	Hits@10:45.88	Best:21.96
2024-12-29 03:56:10,834: Snapshot:4	Epoch:4	Loss:0.84	translation_Loss:0.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:21.91	Hits@10:45.84	Best:21.96
2024-12-29 03:56:17,515: Snapshot:4	Epoch:5	Loss:0.825	translation_Loss:0.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.468                                                   	MRR:21.83	Hits@10:46.03	Best:21.96
2024-12-29 03:56:24,028: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 21.96
2024-12-29 03:56:24,028: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:0.834 MRR:21.61 Best Results: 21.96
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:56:24,029: Snapshot:4	Epoch:6	Loss:0.834	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.469                                                   	MRR:21.61	Hits@10:45.93	Best:21.96
2024-12-29 03:56:30,864: Snapshot:4	Epoch:7	Loss:25.948	translation_Loss:11.178	multi_layer_Loss:14.77	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.61	Hits@10:45.93	Best:21.96
2024-12-29 03:56:37,417: End of token training: 4 Epoch: 8 Loss:11.478 MRR:21.61 Best Results: 21.96
2024-12-29 03:56:37,418: Snapshot:4	Epoch:8	Loss:11.478	translation_Loss:11.161	multi_layer_Loss:0.317	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.61	Hits@10:45.93	Best:21.96
2024-12-29 03:56:37,669: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 03:56:52,182: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2239 | 0.1406 | 0.2574 | 0.3079 |  0.3812 |
|     1      | 0.203  | 0.1247 | 0.2304 | 0.2846 |  0.3571 |
|     2      | 0.1975 | 0.1152 | 0.2249 | 0.2821 |  0.3628 |
|     3      | 0.1933 | 0.104  | 0.2191 | 0.2867 |  0.3781 |
|     4      | 0.2221 | 0.1065 |  0.26  | 0.3461 |  0.4594 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 03:56:52,185: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2398 | 0.1543 | 0.2824 | 0.3312 |  0.3938 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2506 | 0.1624 | 0.2929 | 0.3447 |  0.4116 |
|     1      | 0.2154 | 0.1333 | 0.2567 | 0.3025 |  0.3642 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2472 | 0.1583 | 0.2886 | 0.3422 |  0.4131 |
|     1      | 0.223  | 0.1403 | 0.2583 | 0.3118 |  0.3803 |
|     2      | 0.2138 | 0.1295 | 0.2513 | 0.3034 |  0.3735 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2358 | 0.1497 | 0.2738 | 0.3237 |  0.3972 |
|     1      | 0.2158 | 0.1342 | 0.2479 | 0.3012 |  0.374  |
|     2      | 0.2107 | 0.1254 | 0.2428 | 0.3025 |  0.3792 |
|     3      | 0.205  | 0.1136 | 0.2376 | 0.3011 |  0.3852 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2239 | 0.1406 | 0.2574 | 0.3079 |  0.3812 |
|     1      | 0.203  | 0.1247 | 0.2304 | 0.2846 |  0.3571 |
|     2      | 0.1975 | 0.1152 | 0.2249 | 0.2821 |  0.3628 |
|     3      | 0.1933 | 0.104  | 0.2191 | 0.2867 |  0.3781 |
|     4      | 0.2221 | 0.1065 |  0.26  | 0.3461 |  0.4594 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 03:56:52,185: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 121.41785860061646 |    0.24   |    0.154     |    0.282     |     0.394     |
|    1     | 85.00993418693542  |   0.233   |    0.148     |    0.275     |     0.388     |
|    2     |  93.9544517993927  |   0.228   |    0.143     |    0.266     |     0.389     |
|    3     | 67.57855820655823  |   0.217   |    0.131     |    0.251     |     0.384     |
|    4     | 73.87540769577026  |   0.208   |    0.118     |    0.238     |     0.388     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 03:56:52,185: Sum_Training_Time:441.83621048927307
2024-12-29 03:56:52,185: Every_Training_Time:[121.41785860061646, 85.00993418693542, 93.9544517993927, 67.57855820655823, 73.87540769577026]
2024-12-29 03:56:52,185: Forward transfer: 0.17322500000000002 Backward transfer: -0.014074999999999997
2024-12-29 03:57:29,005: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229035656/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=3000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 03:57:38,094: Snapshot:0	Epoch:0	Loss:17.024	translation_Loss:17.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.26	Best:6.23
2024-12-29 03:57:44,337: Snapshot:0	Epoch:1	Loss:11.963	translation_Loss:11.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:22.6	Best:9.68
2024-12-29 03:57:50,221: Snapshot:0	Epoch:2	Loss:8.603	translation_Loss:8.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.9	Hits@10:28.9	Best:12.9
2024-12-29 03:57:56,453: Snapshot:0	Epoch:3	Loss:6.158	translation_Loss:6.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.76	Hits@10:33.11	Best:15.76
2024-12-29 03:58:02,351: Snapshot:0	Epoch:4	Loss:4.364	translation_Loss:4.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.36	Hits@10:36.01	Best:18.36
2024-12-29 03:58:08,270: Snapshot:0	Epoch:5	Loss:3.084	translation_Loss:3.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.43	Hits@10:37.68	Best:20.43
2024-12-29 03:58:14,587: Snapshot:0	Epoch:6	Loss:2.175	translation_Loss:2.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.93	Hits@10:38.66	Best:21.93
2024-12-29 03:58:20,408: Snapshot:0	Epoch:7	Loss:1.54	translation_Loss:1.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.01	Hits@10:39.58	Best:23.01
2024-12-29 03:58:26,684: Snapshot:0	Epoch:8	Loss:1.096	translation_Loss:1.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.84	Best:23.67
2024-12-29 03:58:32,533: Snapshot:0	Epoch:9	Loss:0.816	translation_Loss:0.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.08	Best:24.11
2024-12-29 03:58:38,438: Snapshot:0	Epoch:10	Loss:0.629	translation_Loss:0.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.27	Hits@10:40.12	Best:24.27
2024-12-29 03:58:44,626: Snapshot:0	Epoch:11	Loss:0.496	translation_Loss:0.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:40.33	Best:24.33
2024-12-29 03:58:50,397: Snapshot:0	Epoch:12	Loss:0.412	translation_Loss:0.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:40.31	Best:24.42
2024-12-29 03:58:56,667: Snapshot:0	Epoch:13	Loss:0.352	translation_Loss:0.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.65	Hits@10:40.29	Best:24.65
2024-12-29 03:59:02,545: Snapshot:0	Epoch:14	Loss:0.309	translation_Loss:0.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:40.44	Best:24.71
2024-12-29 03:59:08,424: Snapshot:0	Epoch:15	Loss:0.275	translation_Loss:0.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:40.44	Best:24.72
2024-12-29 03:59:14,755: Snapshot:0	Epoch:16	Loss:0.244	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:40.32	Best:24.75
2024-12-29 03:59:20,672: Snapshot:0	Epoch:17	Loss:0.224	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:40.5	Best:24.8
2024-12-29 03:59:26,927: Snapshot:0	Epoch:18	Loss:0.207	translation_Loss:0.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.27	Best:24.8
2024-12-29 03:59:32,745: Snapshot:0	Epoch:19	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:40.25	Best:24.8
2024-12-29 03:59:38,981: Early Stopping! Snapshot: 0 Epoch: 20 Best Results: 24.8
2024-12-29 03:59:38,981: Start to training tokens! Snapshot: 0 Epoch: 20 Loss:0.18 MRR:24.63 Best Results: 24.8
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:59:38,982: Snapshot:0	Epoch:20	Loss:0.18	translation_Loss:0.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:40.33	Best:24.8
2024-12-29 03:59:45,222: Snapshot:0	Epoch:21	Loss:27.64	translation_Loss:12.31	multi_layer_Loss:15.33	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:40.33	Best:24.8
2024-12-29 03:59:50,952: End of token training: 0 Epoch: 22 Loss:12.607 MRR:24.63 Best Results: 24.8
2024-12-29 03:59:50,953: Snapshot:0	Epoch:22	Loss:12.607	translation_Loss:12.299	multi_layer_Loss:0.309	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.63	Hits@10:40.33	Best:24.8
2024-12-29 03:59:51,198: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 03:59:53,617: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2408 | 0.1561 | 0.2823 | 0.3308 |  0.3934 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:00:16,827: Snapshot:1	Epoch:0	Loss:7.927	translation_Loss:7.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.799                                                   	MRR:20.42	Hits@10:34.31	Best:20.42
2024-12-29 04:00:23,158: Snapshot:1	Epoch:1	Loss:5.268	translation_Loss:3.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:21.54	Hits@10:36.25	Best:21.54
2024-12-29 04:00:29,381: Snapshot:1	Epoch:2	Loss:4.542	translation_Loss:3.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.536                                                   	MRR:22.14	Hits@10:36.88	Best:22.14
2024-12-29 04:00:35,665: Snapshot:1	Epoch:3	Loss:4.197	translation_Loss:2.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.576                                                   	MRR:22.35	Hits@10:37.13	Best:22.35
2024-12-29 04:00:41,920: Snapshot:1	Epoch:4	Loss:4.114	translation_Loss:2.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.574                                                   	MRR:22.23	Hits@10:37.21	Best:22.35
2024-12-29 04:00:48,151: Snapshot:1	Epoch:5	Loss:4.055	translation_Loss:2.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.581                                                   	MRR:22.31	Hits@10:37.39	Best:22.35
2024-12-29 04:00:54,774: Early Stopping! Snapshot: 1 Epoch: 6 Best Results: 22.35
2024-12-29 04:00:54,774: Start to training tokens! Snapshot: 1 Epoch: 6 Loss:4.032 MRR:22.28 Best Results: 22.35
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:00:54,774: Snapshot:1	Epoch:6	Loss:4.032	translation_Loss:2.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.578                                                   	MRR:22.28	Hits@10:37.38	Best:22.35
2024-12-29 04:01:01,015: Snapshot:1	Epoch:7	Loss:28.896	translation_Loss:14.097	multi_layer_Loss:14.799	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.28	Hits@10:37.38	Best:22.35
2024-12-29 04:01:07,173: End of token training: 1 Epoch: 8 Loss:14.416 MRR:22.28 Best Results: 22.35
2024-12-29 04:01:07,173: Snapshot:1	Epoch:8	Loss:14.416	translation_Loss:14.108	multi_layer_Loss:0.308	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.28	Hits@10:37.38	Best:22.35
2024-12-29 04:01:07,366: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 04:01:12,881: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2523 | 0.1633 | 0.2946 | 0.347  |  0.4143 |
|     1      | 0.2221 | 0.1408 | 0.2603 | 0.3091 |  0.3716 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:01:35,811: Snapshot:2	Epoch:0	Loss:5.51	translation_Loss:4.75	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:20.88	Hits@10:36.67	Best:20.88
2024-12-29 04:01:42,154: Snapshot:2	Epoch:1	Loss:3.461	translation_Loss:2.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:21.63	Hits@10:37.74	Best:21.63
2024-12-29 04:01:48,631: Snapshot:2	Epoch:2	Loss:3.108	translation_Loss:1.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.364                                                   	MRR:21.78	Hits@10:38.05	Best:21.78
2024-12-29 04:01:55,082: Snapshot:2	Epoch:3	Loss:2.995	translation_Loss:1.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.408                                                   	MRR:21.88	Hits@10:37.88	Best:21.88
2024-12-29 04:02:01,524: Snapshot:2	Epoch:4	Loss:3.007	translation_Loss:1.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.426                                                   	MRR:21.88	Hits@10:37.92	Best:21.88
2024-12-29 04:02:08,086: Snapshot:2	Epoch:5	Loss:2.988	translation_Loss:1.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.451                                                   	MRR:21.75	Hits@10:37.79	Best:21.88
2024-12-29 04:02:14,481: Early Stopping! Snapshot: 2 Epoch: 6 Best Results: 21.88
2024-12-29 04:02:14,482: Start to training tokens! Snapshot: 2 Epoch: 6 Loss:3.01 MRR:21.82 Best Results: 21.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:02:14,482: Snapshot:2	Epoch:6	Loss:3.01	translation_Loss:1.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.452                                                   	MRR:21.82	Hits@10:37.9	Best:21.88
2024-12-29 04:02:21,117: Snapshot:2	Epoch:7	Loss:29.274	translation_Loss:13.788	multi_layer_Loss:15.486	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.82	Hits@10:37.9	Best:21.88
2024-12-29 04:02:27,372: End of token training: 2 Epoch: 8 Loss:14.115 MRR:21.82 Best Results: 21.88
2024-12-29 04:02:27,372: Snapshot:2	Epoch:8	Loss:14.115	translation_Loss:13.789	multi_layer_Loss:0.326	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.82	Hits@10:37.9	Best:21.88
2024-12-29 04:02:27,611: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 04:02:36,044: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2485 | 0.1611 | 0.2867 | 0.341  |  0.4138 |
|     1      | 0.2285 | 0.1454 | 0.2657 | 0.3156 |  0.3857 |
|     2      | 0.2202 | 0.1348 | 0.2567 | 0.3109 |  0.3816 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:02:58,885: Snapshot:3	Epoch:0	Loss:3.04	translation_Loss:2.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.631                                                   	MRR:20.27	Hits@10:38.66	Best:20.27
2024-12-29 04:03:05,535: Snapshot:3	Epoch:1	Loss:1.813	translation_Loss:0.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.922                                                   	MRR:20.32	Hits@10:38.68	Best:20.32
2024-12-29 04:03:12,013: Snapshot:3	Epoch:2	Loss:1.668	translation_Loss:0.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.828                                                   	MRR:20.35	Hits@10:38.93	Best:20.35
2024-12-29 04:03:18,472: Snapshot:3	Epoch:3	Loss:1.637	translation_Loss:0.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.86                                                   	MRR:20.35	Hits@10:38.64	Best:20.35
2024-12-29 04:03:25,158: Snapshot:3	Epoch:4	Loss:1.668	translation_Loss:0.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.875                                                   	MRR:20.22	Hits@10:38.62	Best:20.35
2024-12-29 04:03:31,724: Snapshot:3	Epoch:5	Loss:1.661	translation_Loss:0.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.888                                                   	MRR:20.38	Hits@10:38.78	Best:20.38
2024-12-29 04:03:38,239: Snapshot:3	Epoch:6	Loss:1.679	translation_Loss:0.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:20.51	Hits@10:38.63	Best:20.51
2024-12-29 04:03:45,238: Snapshot:3	Epoch:7	Loss:1.677	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.903                                                   	MRR:20.36	Hits@10:38.64	Best:20.51
2024-12-29 04:03:51,691: Snapshot:3	Epoch:8	Loss:1.685	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.911                                                   	MRR:20.16	Hits@10:38.81	Best:20.51
2024-12-29 04:03:58,458: Early Stopping! Snapshot: 3 Epoch: 9 Best Results: 20.51
2024-12-29 04:03:58,458: Start to training tokens! Snapshot: 3 Epoch: 9 Loss:1.682 MRR:20.38 Best Results: 20.51
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:03:58,458: Snapshot:3	Epoch:9	Loss:1.682	translation_Loss:0.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.916                                                   	MRR:20.38	Hits@10:38.73	Best:20.51
2024-12-29 04:04:04,841: Snapshot:3	Epoch:10	Loss:27.748	translation_Loss:13.064	multi_layer_Loss:14.684	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.38	Hits@10:38.73	Best:20.51
2024-12-29 04:04:11,273: End of token training: 3 Epoch: 11 Loss:13.404 MRR:20.38 Best Results: 20.51
2024-12-29 04:04:11,273: Snapshot:3	Epoch:11	Loss:13.404	translation_Loss:13.093	multi_layer_Loss:0.31	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.38	Hits@10:38.73	Best:20.51
2024-12-29 04:04:11,527: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 04:04:22,941: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2341 | 0.1486 | 0.2681 | 0.3217 |  0.3962 |
|     1      | 0.2172 | 0.1359 | 0.2492 | 0.2992 |  0.375  |
|     2      | 0.2127 | 0.1274 | 0.2439 | 0.3021 |  0.3805 |
|     3      | 0.206  | 0.1157 | 0.2374 | 0.2992 |  0.3875 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:04:45,825: Snapshot:4	Epoch:0	Loss:1.519	translation_Loss:1.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.368                                                   	MRR:21.81	Hits@10:46.78	Best:21.81
2024-12-29 04:04:52,377: Snapshot:4	Epoch:1	Loss:0.7	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.385                                                   	MRR:21.93	Hits@10:45.56	Best:21.93
2024-12-29 04:04:58,886: Snapshot:4	Epoch:2	Loss:0.555	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.311                                                   	MRR:21.76	Hits@10:45.97	Best:21.93
2024-12-29 04:05:05,527: Snapshot:4	Epoch:3	Loss:0.525	translation_Loss:0.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.316                                                   	MRR:22.07	Hits@10:46.2	Best:22.07
2024-12-29 04:05:12,592: Snapshot:4	Epoch:4	Loss:0.53	translation_Loss:0.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.317                                                   	MRR:22.08	Hits@10:46.04	Best:22.08
2024-12-29 04:05:19,140: Snapshot:4	Epoch:5	Loss:0.539	translation_Loss:0.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:22.04	Hits@10:46.13	Best:22.08
2024-12-29 04:05:25,568: Snapshot:4	Epoch:6	Loss:0.536	translation_Loss:0.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:22.04	Hits@10:46.07	Best:22.08
2024-12-29 04:05:32,496: Snapshot:4	Epoch:7	Loss:0.533	translation_Loss:0.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:22.24	Hits@10:46.3	Best:22.24
2024-12-29 04:05:39,182: Snapshot:4	Epoch:8	Loss:0.532	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.331                                                   	MRR:22.06	Hits@10:46.49	Best:22.24
2024-12-29 04:05:46,012: Snapshot:4	Epoch:9	Loss:0.535	translation_Loss:0.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:21.9	Hits@10:45.75	Best:22.24
2024-12-29 04:05:52,496: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 22.24
2024-12-29 04:05:52,499: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:0.538 MRR:21.93 Best Results: 22.24
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:05:52,499: Snapshot:4	Epoch:10	Loss:0.538	translation_Loss:0.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.335                                                   	MRR:21.93	Hits@10:45.9	Best:22.24
2024-12-29 04:05:59,000: Snapshot:4	Epoch:11	Loss:25.683	translation_Loss:10.914	multi_layer_Loss:14.77	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.93	Hits@10:45.9	Best:22.24
2024-12-29 04:06:05,972: End of token training: 4 Epoch: 12 Loss:11.235 MRR:21.93 Best Results: 22.24
2024-12-29 04:06:05,972: Snapshot:4	Epoch:12	Loss:11.235	translation_Loss:10.918	multi_layer_Loss:0.317	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.93	Hits@10:45.9	Best:22.24
2024-12-29 04:06:06,224: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 04:06:20,263: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2214 | 0.1388 | 0.2519 | 0.3062 |  0.3801 |
|     1      | 0.2017 | 0.1229 |  0.23  | 0.2811 |  0.3565 |
|     2      | 0.1958 | 0.1137 | 0.2235 | 0.279  |  0.3587 |
|     3      | 0.1899 | 0.1012 | 0.2142 | 0.2797 |  0.3742 |
|     4      | 0.222  | 0.1053 | 0.2622 | 0.3465 |  0.4609 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:06:20,265: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2408 | 0.1561 | 0.2823 | 0.3308 |  0.3934 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2523 | 0.1633 | 0.2946 | 0.347  |  0.4143 |
|     1      | 0.2221 | 0.1408 | 0.2603 | 0.3091 |  0.3716 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2485 | 0.1611 | 0.2867 | 0.341  |  0.4138 |
|     1      | 0.2285 | 0.1454 | 0.2657 | 0.3156 |  0.3857 |
|     2      | 0.2202 | 0.1348 | 0.2567 | 0.3109 |  0.3816 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2341 | 0.1486 | 0.2681 | 0.3217 |  0.3962 |
|     1      | 0.2172 | 0.1359 | 0.2492 | 0.2992 |  0.375  |
|     2      | 0.2127 | 0.1274 | 0.2439 | 0.3021 |  0.3805 |
|     3      | 0.206  | 0.1157 | 0.2374 | 0.2992 |  0.3875 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2214 | 0.1388 | 0.2519 | 0.3062 |  0.3801 |
|     1      | 0.2017 | 0.1229 |  0.23  | 0.2811 |  0.3565 |
|     2      | 0.1958 | 0.1137 | 0.2235 | 0.279  |  0.3587 |
|     3      | 0.1899 | 0.1012 | 0.2142 | 0.2797 |  0.3742 |
|     4      | 0.222  | 0.1053 | 0.2622 | 0.3465 |  0.4609 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:06:20,266: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 141.94698905944824 |   0.241   |    0.156     |    0.282     |     0.393     |
|    1     | 70.42778539657593  |   0.237   |    0.152     |    0.277     |     0.393     |
|    2     | 71.51078295707703  |   0.232   |    0.147     |     0.27     |     0.394     |
|    3     | 92.22785019874573  |   0.217   |    0.132     |     0.25     |     0.385     |
|    4     |  99.9845540523529  |   0.206   |    0.116     |    0.236     |     0.386     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:06:20,266: Sum_Training_Time:476.09796166419983
2024-12-29 04:06:20,266: Every_Training_Time:[141.94698905944824, 70.42778539657593, 71.51078295707703, 92.22785019874573, 99.9845540523529]
2024-12-29 04:06:20,266: Forward transfer: 0.17535 Backward transfer: -0.02007499999999999
2024-12-29 04:06:57,176: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229040624/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=500.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:07:06,284: Snapshot:0	Epoch:0	Loss:17.024	translation_Loss:17.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.26	Best:6.23
2024-12-29 04:07:12,557: Snapshot:0	Epoch:1	Loss:11.963	translation_Loss:11.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:22.6	Best:9.68
2024-12-29 04:07:18,555: Snapshot:0	Epoch:2	Loss:8.603	translation_Loss:8.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.9	Hits@10:28.89	Best:12.9
2024-12-29 04:07:24,883: Snapshot:0	Epoch:3	Loss:6.158	translation_Loss:6.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.76	Hits@10:33.13	Best:15.76
2024-12-29 04:07:30,748: Snapshot:0	Epoch:4	Loss:4.364	translation_Loss:4.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.35	Hits@10:35.96	Best:18.35
2024-12-29 04:07:36,804: Snapshot:0	Epoch:5	Loss:3.084	translation_Loss:3.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.45	Hits@10:37.67	Best:20.45
2024-12-29 04:07:43,345: Snapshot:0	Epoch:6	Loss:2.175	translation_Loss:2.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.95	Hits@10:38.8	Best:21.95
2024-12-29 04:07:49,274: Snapshot:0	Epoch:7	Loss:1.54	translation_Loss:1.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.97	Hits@10:39.46	Best:22.97
2024-12-29 04:07:55,552: Snapshot:0	Epoch:8	Loss:1.095	translation_Loss:1.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.87	Best:23.71
2024-12-29 04:08:01,524: Snapshot:0	Epoch:9	Loss:0.815	translation_Loss:0.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:39.99	Best:24.13
2024-12-29 04:08:07,541: Snapshot:0	Epoch:10	Loss:0.627	translation_Loss:0.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:40.27	Best:24.31
2024-12-29 04:08:14,076: Snapshot:0	Epoch:11	Loss:0.498	translation_Loss:0.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:40.14	Best:24.41
2024-12-29 04:08:19,944: Snapshot:0	Epoch:12	Loss:0.413	translation_Loss:0.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.52	Hits@10:40.35	Best:24.52
2024-12-29 04:08:26,207: Snapshot:0	Epoch:13	Loss:0.349	translation_Loss:0.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:40.4	Best:24.77
2024-12-29 04:08:32,123: Snapshot:0	Epoch:14	Loss:0.309	translation_Loss:0.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.68	Hits@10:40.52	Best:24.77
2024-12-29 04:08:37,958: Snapshot:0	Epoch:15	Loss:0.274	translation_Loss:0.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.68	Hits@10:40.43	Best:24.77
2024-12-29 04:08:44,126: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 24.77
2024-12-29 04:08:44,126: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.243 MRR:24.69 Best Results: 24.77
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:08:44,127: Snapshot:0	Epoch:16	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.69	Hits@10:40.32	Best:24.77
2024-12-29 04:08:50,453: Snapshot:0	Epoch:17	Loss:27.64	translation_Loss:12.31	multi_layer_Loss:15.33	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.69	Hits@10:40.32	Best:24.77
2024-12-29 04:08:56,569: End of token training: 0 Epoch: 18 Loss:12.615 MRR:24.69 Best Results: 24.77
2024-12-29 04:08:56,570: Snapshot:0	Epoch:18	Loss:12.615	translation_Loss:12.307	multi_layer_Loss:0.309	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.69	Hits@10:40.32	Best:24.77
2024-12-29 04:08:56,824: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 04:08:59,304: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1535 | 0.2825 | 0.332  |  0.3945 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:09:21,956: Snapshot:1	Epoch:0	Loss:7.74	translation_Loss:7.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.295                                                   	MRR:20.69	Hits@10:34.78	Best:20.69
2024-12-29 04:09:28,370: Snapshot:1	Epoch:1	Loss:3.936	translation_Loss:3.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.672                                                   	MRR:22.42	Hits@10:37.18	Best:22.42
2024-12-29 04:09:34,853: Snapshot:1	Epoch:2	Loss:2.526	translation_Loss:1.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.91                                                   	MRR:23.24	Hits@10:38.2	Best:23.24
2024-12-29 04:09:41,306: Snapshot:1	Epoch:3	Loss:2.052	translation_Loss:1.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.005                                                   	MRR:23.3	Hits@10:38.44	Best:23.3
2024-12-29 04:09:47,616: Snapshot:1	Epoch:4	Loss:1.875	translation_Loss:0.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.037                                                   	MRR:23.38	Hits@10:38.52	Best:23.38
2024-12-29 04:09:53,900: Snapshot:1	Epoch:5	Loss:1.798	translation_Loss:0.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.052                                                   	MRR:23.45	Hits@10:38.75	Best:23.45
2024-12-29 04:10:00,233: Snapshot:1	Epoch:6	Loss:1.769	translation_Loss:0.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.066                                                   	MRR:23.35	Hits@10:38.59	Best:23.45
2024-12-29 04:10:07,128: Snapshot:1	Epoch:7	Loss:1.741	translation_Loss:0.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.081                                                   	MRR:23.39	Hits@10:38.76	Best:23.45
2024-12-29 04:10:13,312: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 23.45
2024-12-29 04:10:13,312: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:1.734 MRR:23.27 Best Results: 23.45
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:10:13,312: Snapshot:1	Epoch:8	Loss:1.734	translation_Loss:0.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.086                                                   	MRR:23.27	Hits@10:38.7	Best:23.45
2024-12-29 04:10:19,927: Snapshot:1	Epoch:9	Loss:27.802	translation_Loss:13.003	multi_layer_Loss:14.799	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.27	Hits@10:38.7	Best:23.45
2024-12-29 04:10:26,155: End of token training: 1 Epoch: 10 Loss:13.314 MRR:23.27 Best Results: 23.45
2024-12-29 04:10:26,155: Snapshot:1	Epoch:10	Loss:13.314	translation_Loss:13.006	multi_layer_Loss:0.308	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.27	Hits@10:38.7	Best:23.45
2024-12-29 04:10:26,415: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 04:10:31,675: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1568 | 0.2845 | 0.343  |  0.4134 |
|     1      | 0.2374 | 0.1554 | 0.2756 | 0.3239 |  0.3877 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:10:54,691: Snapshot:2	Epoch:0	Loss:4.209	translation_Loss:3.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.274                                                   	MRR:21.55	Hits@10:37.84	Best:21.55
2024-12-29 04:11:01,201: Snapshot:2	Epoch:1	Loss:1.798	translation_Loss:1.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.521                                                   	MRR:22.02	Hits@10:38.63	Best:22.02
2024-12-29 04:11:07,759: Snapshot:2	Epoch:2	Loss:1.204	translation_Loss:0.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.576                                                   	MRR:22.22	Hits@10:38.65	Best:22.22
2024-12-29 04:11:14,351: Snapshot:2	Epoch:3	Loss:1.062	translation_Loss:0.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.573                                                   	MRR:22.01	Hits@10:38.55	Best:22.22
2024-12-29 04:11:20,811: Snapshot:2	Epoch:4	Loss:1.02	translation_Loss:0.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.579                                                   	MRR:22.07	Hits@10:38.24	Best:22.22
2024-12-29 04:11:27,296: Early Stopping! Snapshot: 2 Epoch: 5 Best Results: 22.22
2024-12-29 04:11:27,296: Start to training tokens! Snapshot: 2 Epoch: 5 Loss:1.005 MRR:21.99 Best Results: 22.22
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:11:27,297: Snapshot:2	Epoch:5	Loss:1.005	translation_Loss:0.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.592                                                   	MRR:21.99	Hits@10:38.21	Best:22.22
2024-12-29 04:11:34,201: Snapshot:2	Epoch:6	Loss:28.295	translation_Loss:12.809	multi_layer_Loss:15.486	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.99	Hits@10:38.21	Best:22.22
2024-12-29 04:11:40,770: End of token training: 2 Epoch: 7 Loss:13.148 MRR:21.99 Best Results: 22.22
2024-12-29 04:11:40,771: Snapshot:2	Epoch:7	Loss:13.148	translation_Loss:12.823	multi_layer_Loss:0.326	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.99	Hits@10:38.21	Best:22.22
2024-12-29 04:11:41,027: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 04:11:49,514: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2249 | 0.1407 | 0.2573 | 0.3131 |  0.3872 |
|     1      | 0.2226 | 0.1402 | 0.2554 | 0.3095 |  0.3831 |
|     2      | 0.2239 | 0.1391 | 0.2578 | 0.3133 |  0.3876 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:12:12,361: Snapshot:3	Epoch:0	Loss:2.016	translation_Loss:1.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:19.72	Hits@10:37.84	Best:19.72
2024-12-29 04:12:18,953: Snapshot:3	Epoch:1	Loss:0.848	translation_Loss:0.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.368                                                   	MRR:20.02	Hits@10:38.32	Best:20.02
2024-12-29 04:12:25,417: Snapshot:3	Epoch:2	Loss:0.611	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:19.9	Hits@10:38.01	Best:20.02
2024-12-29 04:12:31,870: Snapshot:3	Epoch:3	Loss:0.553	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.321                                                   	MRR:20.02	Hits@10:38.17	Best:20.02
2024-12-29 04:12:38,309: Early Stopping! Snapshot: 3 Epoch: 4 Best Results: 20.02
2024-12-29 04:12:38,309: Start to training tokens! Snapshot: 3 Epoch: 4 Loss:0.538 MRR:19.96 Best Results: 20.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:12:38,309: Snapshot:3	Epoch:4	Loss:0.538	translation_Loss:0.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.321                                                   	MRR:19.96	Hits@10:38.32	Best:20.02
2024-12-29 04:12:44,720: Snapshot:3	Epoch:5	Loss:26.902	translation_Loss:12.218	multi_layer_Loss:14.684	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.96	Hits@10:38.32	Best:20.02
2024-12-29 04:12:51,624: End of token training: 3 Epoch: 6 Loss:12.534 MRR:19.96 Best Results: 20.02
2024-12-29 04:12:51,625: Snapshot:3	Epoch:6	Loss:12.534	translation_Loss:12.224	multi_layer_Loss:0.31	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.96	Hits@10:38.32	Best:20.02
2024-12-29 04:12:51,874: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 04:13:03,058: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1993 | 0.1212 | 0.2258 | 0.2764 |  0.3497 |
|     1      | 0.1971 | 0.1214 | 0.2212 | 0.2721 |  0.3447 |
|     2      | 0.1975 | 0.1157 | 0.2231 | 0.2796 |  0.3624 |
|     3      | 0.2005 | 0.1094 | 0.2308 | 0.2953 |  0.3853 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:13:25,755: Snapshot:4	Epoch:0	Loss:1.134	translation_Loss:0.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.166                                                   	MRR:21.4	Hits@10:44.97	Best:21.4
2024-12-29 04:13:32,664: Snapshot:4	Epoch:1	Loss:0.429	translation_Loss:0.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.204                                                   	MRR:21.72	Hits@10:45.55	Best:21.72
2024-12-29 04:13:39,156: Snapshot:4	Epoch:2	Loss:0.258	translation_Loss:0.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:21.76	Hits@10:45.04	Best:21.76
2024-12-29 04:13:45,664: Snapshot:4	Epoch:3	Loss:0.208	translation_Loss:0.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.132                                                   	MRR:21.5	Hits@10:44.94	Best:21.76
2024-12-29 04:13:52,728: Snapshot:4	Epoch:4	Loss:0.198	translation_Loss:0.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.127                                                   	MRR:21.88	Hits@10:45.32	Best:21.88
2024-12-29 04:13:59,436: Snapshot:4	Epoch:5	Loss:0.203	translation_Loss:0.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.133                                                   	MRR:21.35	Hits@10:44.51	Best:21.88
2024-12-29 04:14:06,322: Snapshot:4	Epoch:6	Loss:0.209	translation_Loss:0.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.14                                                   	MRR:21.55	Hits@10:44.97	Best:21.88
2024-12-29 04:14:12,841: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 21.88
2024-12-29 04:14:12,841: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:0.207 MRR:21.86 Best Results: 21.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:14:12,841: Snapshot:4	Epoch:7	Loss:0.207	translation_Loss:0.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:21.86	Hits@10:45.21	Best:21.88
2024-12-29 04:14:19,422: Snapshot:4	Epoch:8	Loss:25.027	translation_Loss:10.257	multi_layer_Loss:14.77	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.86	Hits@10:45.21	Best:21.88
2024-12-29 04:14:26,324: End of token training: 4 Epoch: 9 Loss:10.601 MRR:21.86 Best Results: 21.88
2024-12-29 04:14:26,324: Snapshot:4	Epoch:9	Loss:10.601	translation_Loss:10.284	multi_layer_Loss:0.317	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.86	Hits@10:45.21	Best:21.88
2024-12-29 04:14:26,576: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 04:14:40,783: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1799 | 0.1061 | 0.2011 | 0.252  |  0.3251 |
|     1      | 0.1747 | 0.1027 | 0.195  | 0.2456 |  0.3173 |
|     2      | 0.1744 | 0.0981 | 0.1946 | 0.2465 |  0.3281 |
|     3      | 0.1771 | 0.0923 | 0.1977 | 0.259  |  0.3534 |
|     4      | 0.2195 | 0.1064 | 0.2545 | 0.3376 |  0.4499 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:14:40,785: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1535 | 0.2825 | 0.332  |  0.3945 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1568 | 0.2845 | 0.343  |  0.4134 |
|     1      | 0.2374 | 0.1554 | 0.2756 | 0.3239 |  0.3877 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2249 | 0.1407 | 0.2573 | 0.3131 |  0.3872 |
|     1      | 0.2226 | 0.1402 | 0.2554 | 0.3095 |  0.3831 |
|     2      | 0.2239 | 0.1391 | 0.2578 | 0.3133 |  0.3876 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1993 | 0.1212 | 0.2258 | 0.2764 |  0.3497 |
|     1      | 0.1971 | 0.1214 | 0.2212 | 0.2721 |  0.3447 |
|     2      | 0.1975 | 0.1157 | 0.2231 | 0.2796 |  0.3624 |
|     3      | 0.2005 | 0.1094 | 0.2308 | 0.2953 |  0.3853 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1799 | 0.1061 | 0.2011 | 0.252  |  0.3251 |
|     1      | 0.1747 | 0.1027 | 0.195  | 0.2456 |  0.3173 |
|     2      | 0.1744 | 0.0981 | 0.1946 | 0.2465 |  0.3281 |
|     3      | 0.1771 | 0.0923 | 0.1977 | 0.259  |  0.3534 |
|     4      | 0.2195 | 0.1064 | 0.2545 | 0.3376 |  0.4499 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:14:40,786: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 119.39308881759644 |    0.24   |    0.153     |    0.282     |     0.395     |
|    1     | 84.01523160934448  |   0.242   |    0.156     |     0.28     |     0.401     |
|    2     | 66.17033648490906  |   0.224   |     0.14     |    0.257     |     0.386     |
|    3     | 59.40477466583252  |   0.199   |    0.117     |    0.225     |     0.361     |
|    4     |  80.2007064819336  |   0.185   |    0.101     |    0.209     |     0.355     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:14:40,786: Sum_Training_Time:409.1841380596161
2024-12-29 04:14:40,786: Every_Training_Time:[119.39308881759644, 84.01523160934448, 66.17033648490906, 59.40477466583252, 80.2007064819336]
2024-12-29 04:14:40,786: Forward transfer: 0.178025 Backward transfer: -0.048825
2024-12-29 04:15:17,202: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229041445/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=7000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:15:26,304: Snapshot:0	Epoch:0	Loss:17.024	translation_Loss:17.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.26	Best:6.23
2024-12-29 04:15:32,650: Snapshot:0	Epoch:1	Loss:11.963	translation_Loss:11.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.69	Hits@10:22.61	Best:9.69
2024-12-29 04:15:38,608: Snapshot:0	Epoch:2	Loss:8.603	translation_Loss:8.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.9	Hits@10:28.89	Best:12.9
2024-12-29 04:15:44,967: Snapshot:0	Epoch:3	Loss:6.158	translation_Loss:6.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.77	Hits@10:33.13	Best:15.77
2024-12-29 04:15:50,955: Snapshot:0	Epoch:4	Loss:4.364	translation_Loss:4.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.35	Hits@10:35.93	Best:18.35
2024-12-29 04:15:56,980: Snapshot:0	Epoch:5	Loss:3.085	translation_Loss:3.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.44	Hits@10:37.7	Best:20.44
2024-12-29 04:16:03,397: Snapshot:0	Epoch:6	Loss:2.175	translation_Loss:2.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.96	Hits@10:38.75	Best:21.96
2024-12-29 04:16:09,393: Snapshot:0	Epoch:7	Loss:1.542	translation_Loss:1.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.01	Hits@10:39.53	Best:23.01
2024-12-29 04:16:15,881: Snapshot:0	Epoch:8	Loss:1.094	translation_Loss:1.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:39.85	Best:23.75
2024-12-29 04:16:21,919: Snapshot:0	Epoch:9	Loss:0.815	translation_Loss:0.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.98	Best:24.08
2024-12-29 04:16:27,914: Snapshot:0	Epoch:10	Loss:0.628	translation_Loss:0.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.27	Hits@10:40.12	Best:24.27
2024-12-29 04:16:34,217: Snapshot:0	Epoch:11	Loss:0.496	translation_Loss:0.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:40.29	Best:24.37
2024-12-29 04:16:40,162: Snapshot:0	Epoch:12	Loss:0.411	translation_Loss:0.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:40.36	Best:24.51
2024-12-29 04:16:46,469: Snapshot:0	Epoch:13	Loss:0.351	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.65	Hits@10:40.45	Best:24.65
2024-12-29 04:16:52,420: Snapshot:0	Epoch:14	Loss:0.31	translation_Loss:0.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.69	Hits@10:40.43	Best:24.69
2024-12-29 04:16:58,335: Snapshot:0	Epoch:15	Loss:0.274	translation_Loss:0.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:40.33	Best:24.69
2024-12-29 04:17:04,732: Snapshot:0	Epoch:16	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:40.35	Best:24.69
2024-12-29 04:17:10,739: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 24.69
2024-12-29 04:17:10,739: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.224 MRR:24.68 Best Results: 24.69
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:17:10,739: Snapshot:0	Epoch:17	Loss:0.224	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.68	Hits@10:40.48	Best:24.69
2024-12-29 04:17:17,580: Snapshot:0	Epoch:18	Loss:27.654	translation_Loss:12.324	multi_layer_Loss:15.33	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.68	Hits@10:40.48	Best:24.69
2024-12-29 04:17:23,498: End of token training: 0 Epoch: 19 Loss:12.645 MRR:24.68 Best Results: 24.69
2024-12-29 04:17:23,498: Snapshot:0	Epoch:19	Loss:12.645	translation_Loss:12.337	multi_layer_Loss:0.309	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.68	Hits@10:40.48	Best:24.69
2024-12-29 04:17:23,749: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 04:17:26,420: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2393 | 0.1534 | 0.2823 | 0.333  |  0.3939 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:17:48,794: Snapshot:1	Epoch:0	Loss:8.587	translation_Loss:7.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.112                                                   	MRR:20.04	Hits@10:33.94	Best:20.04
2024-12-29 04:17:55,617: Snapshot:1	Epoch:1	Loss:6.501	translation_Loss:5.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.404                                                   	MRR:20.78	Hits@10:34.85	Best:20.78
2024-12-29 04:18:01,999: Snapshot:1	Epoch:2	Loss:5.826	translation_Loss:4.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.291                                                   	MRR:21.29	Hits@10:35.61	Best:21.29
2024-12-29 04:18:08,844: Snapshot:1	Epoch:3	Loss:5.487	translation_Loss:4.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.292                                                   	MRR:21.45	Hits@10:35.78	Best:21.45
2024-12-29 04:18:15,302: Snapshot:1	Epoch:4	Loss:5.369	translation_Loss:4.104	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.265                                                   	MRR:21.46	Hits@10:35.86	Best:21.46
2024-12-29 04:18:22,011: Snapshot:1	Epoch:5	Loss:5.303	translation_Loss:4.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.265                                                   	MRR:21.51	Hits@10:35.95	Best:21.51
2024-12-29 04:18:28,413: Snapshot:1	Epoch:6	Loss:5.272	translation_Loss:4.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.251                                                   	MRR:21.48	Hits@10:35.85	Best:21.51
2024-12-29 04:18:34,758: Snapshot:1	Epoch:7	Loss:5.251	translation_Loss:3.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.263                                                   	MRR:21.43	Hits@10:35.95	Best:21.51
2024-12-29 04:18:41,534: Snapshot:1	Epoch:8	Loss:5.232	translation_Loss:3.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.25                                                   	MRR:21.52	Hits@10:35.82	Best:21.52
2024-12-29 04:18:47,890: Snapshot:1	Epoch:9	Loss:5.225	translation_Loss:3.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.256                                                   	MRR:21.46	Hits@10:35.92	Best:21.52
2024-12-29 04:18:54,649: Snapshot:1	Epoch:10	Loss:5.215	translation_Loss:3.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.257                                                   	MRR:21.54	Hits@10:35.9	Best:21.54
2024-12-29 04:19:01,011: Snapshot:1	Epoch:11	Loss:5.199	translation_Loss:3.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.256                                                   	MRR:21.47	Hits@10:35.82	Best:21.54
2024-12-29 04:19:07,486: Snapshot:1	Epoch:12	Loss:5.217	translation_Loss:3.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.256                                                   	MRR:21.55	Hits@10:35.86	Best:21.55
2024-12-29 04:19:14,302: Snapshot:1	Epoch:13	Loss:5.208	translation_Loss:3.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.261                                                   	MRR:21.51	Hits@10:35.79	Best:21.55
2024-12-29 04:19:20,650: Snapshot:1	Epoch:14	Loss:5.2	translation_Loss:3.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.256                                                   	MRR:21.47	Hits@10:35.83	Best:21.55
2024-12-29 04:19:26,877: Early Stopping! Snapshot: 1 Epoch: 15 Best Results: 21.55
2024-12-29 04:19:26,877: Start to training tokens! Snapshot: 1 Epoch: 15 Loss:5.193 MRR:21.49 Best Results: 21.55
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:19:26,877: Snapshot:1	Epoch:15	Loss:5.193	translation_Loss:3.935	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.257                                                   	MRR:21.49	Hits@10:35.81	Best:21.55
2024-12-29 04:19:33,532: Snapshot:1	Epoch:16	Loss:29.44	translation_Loss:14.641	multi_layer_Loss:14.799	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.49	Hits@10:35.81	Best:21.55
2024-12-29 04:19:39,759: End of token training: 1 Epoch: 17 Loss:14.941 MRR:21.49 Best Results: 21.55
2024-12-29 04:19:39,760: Snapshot:1	Epoch:17	Loss:14.941	translation_Loss:14.633	multi_layer_Loss:0.308	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.49	Hits@10:35.81	Best:21.55
2024-12-29 04:19:40,007: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 04:19:45,782: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2497 | 0.1626 | 0.2911 | 0.3418 |  0.4086 |
|     1      | 0.2129 | 0.133  | 0.2519 | 0.299  |  0.357  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:20:08,859: Snapshot:2	Epoch:0	Loss:6.608	translation_Loss:5.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.109                                                   	MRR:20.21	Hits@10:35.34	Best:20.21
2024-12-29 04:20:15,419: Snapshot:2	Epoch:1	Loss:5.042	translation_Loss:3.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.507                                                   	MRR:20.69	Hits@10:35.69	Best:20.69
2024-12-29 04:20:21,946: Snapshot:2	Epoch:2	Loss:4.825	translation_Loss:3.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.409                                                   	MRR:20.81	Hits@10:36.14	Best:20.81
2024-12-29 04:20:28,412: Snapshot:2	Epoch:3	Loss:4.692	translation_Loss:3.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.427                                                   	MRR:20.8	Hits@10:35.93	Best:20.81
2024-12-29 04:20:34,877: Snapshot:2	Epoch:4	Loss:4.679	translation_Loss:3.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.416                                                   	MRR:20.83	Hits@10:35.98	Best:20.83
2024-12-29 04:20:41,435: Snapshot:2	Epoch:5	Loss:4.673	translation_Loss:3.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.426                                                   	MRR:20.84	Hits@10:35.94	Best:20.84
2024-12-29 04:20:48,280: Snapshot:2	Epoch:6	Loss:4.66	translation_Loss:3.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.429                                                   	MRR:20.75	Hits@10:36.18	Best:20.84
2024-12-29 04:20:54,726: Snapshot:2	Epoch:7	Loss:4.663	translation_Loss:3.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.428                                                   	MRR:20.84	Hits@10:35.86	Best:20.84
2024-12-29 04:21:01,085: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 20.84
2024-12-29 04:21:01,085: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:4.661 MRR:20.78 Best Results: 20.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:21:01,085: Snapshot:2	Epoch:8	Loss:4.661	translation_Loss:3.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.429                                                   	MRR:20.78	Hits@10:35.86	Best:20.84
2024-12-29 04:21:07,909: Snapshot:2	Epoch:9	Loss:30.011	translation_Loss:14.525	multi_layer_Loss:15.486	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.78	Hits@10:35.86	Best:20.84
2024-12-29 04:21:14,357: End of token training: 2 Epoch: 10 Loss:14.853 MRR:20.78 Best Results: 20.84
2024-12-29 04:21:14,358: Snapshot:2	Epoch:10	Loss:14.853	translation_Loss:14.528	multi_layer_Loss:0.326	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.78	Hits@10:35.86	Best:20.84
2024-12-29 04:21:14,612: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 04:21:23,121: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1593 | 0.2901 | 0.3418 |  0.4129 |
|     1      | 0.2189 | 0.1362 | 0.2562 | 0.3088 |  0.3717 |
|     2      | 0.2095 | 0.1271 | 0.2458 | 0.2971 |  0.3645 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:21:46,069: Snapshot:3	Epoch:0	Loss:4.468	translation_Loss:3.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.0                                                   	MRR:19.51	Hits@10:37.23	Best:19.51
2024-12-29 04:21:52,679: Snapshot:3	Epoch:1	Loss:3.279	translation_Loss:1.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.315                                                   	MRR:19.72	Hits@10:37.02	Best:19.72
2024-12-29 04:21:59,414: Snapshot:3	Epoch:2	Loss:3.228	translation_Loss:2.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.228                                                   	MRR:19.78	Hits@10:37.37	Best:19.78
2024-12-29 04:22:06,176: Snapshot:3	Epoch:3	Loss:3.155	translation_Loss:1.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.248                                                   	MRR:19.74	Hits@10:37.13	Best:19.78
2024-12-29 04:22:12,871: Snapshot:3	Epoch:4	Loss:3.177	translation_Loss:1.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.246                                                   	MRR:19.8	Hits@10:37.3	Best:19.8
2024-12-29 04:22:19,613: Snapshot:3	Epoch:5	Loss:3.172	translation_Loss:1.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.26                                                   	MRR:19.87	Hits@10:37.26	Best:19.87
2024-12-29 04:22:26,542: Snapshot:3	Epoch:6	Loss:3.193	translation_Loss:1.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.268                                                   	MRR:19.69	Hits@10:37.32	Best:19.87
2024-12-29 04:22:33,065: Snapshot:3	Epoch:7	Loss:3.181	translation_Loss:1.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.281                                                   	MRR:19.76	Hits@10:37.19	Best:19.87
2024-12-29 04:22:40,099: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 19.87
2024-12-29 04:22:40,100: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:3.186 MRR:19.7 Best Results: 19.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:22:40,100: Snapshot:3	Epoch:8	Loss:3.186	translation_Loss:1.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.271                                                   	MRR:19.7	Hits@10:37.3	Best:19.87
2024-12-29 04:22:46,604: Snapshot:3	Epoch:9	Loss:28.504	translation_Loss:13.82	multi_layer_Loss:14.684	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.7	Hits@10:37.3	Best:19.87
2024-12-29 04:22:53,006: End of token training: 3 Epoch: 10 Loss:14.132 MRR:19.7 Best Results: 19.87
2024-12-29 04:22:53,007: Snapshot:3	Epoch:10	Loss:14.132	translation_Loss:13.821	multi_layer_Loss:0.31	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.7	Hits@10:37.3	Best:19.87
2024-12-29 04:22:53,262: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 04:23:04,788: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2387 | 0.1515 | 0.2774 | 0.3304 |  0.4024 |
|     1      | 0.2151 | 0.1334 | 0.2482 | 0.3022 |  0.3728 |
|     2      | 0.2082 | 0.1242 | 0.2413 | 0.2975 |  0.3728 |
|     3      | 0.1974 | 0.1088 | 0.2288 | 0.2912 |  0.3744 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:23:27,940: Snapshot:4	Epoch:0	Loss:2.299	translation_Loss:1.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.645                                                   	MRR:21.05	Hits@10:46.52	Best:21.05
2024-12-29 04:23:34,568: Snapshot:4	Epoch:1	Loss:1.293	translation_Loss:0.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.651                                                   	MRR:21.13	Hits@10:44.86	Best:21.13
2024-12-29 04:23:41,291: Snapshot:4	Epoch:2	Loss:1.177	translation_Loss:0.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.583                                                   	MRR:21.66	Hits@10:45.95	Best:21.66
2024-12-29 04:23:47,938: Snapshot:4	Epoch:3	Loss:1.121	translation_Loss:0.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.579                                                   	MRR:21.52	Hits@10:45.84	Best:21.66
2024-12-29 04:23:54,501: Snapshot:4	Epoch:4	Loss:1.131	translation_Loss:0.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.584                                                   	MRR:21.62	Hits@10:45.43	Best:21.66
2024-12-29 04:24:01,064: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 21.66
2024-12-29 04:24:01,064: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:1.122 MRR:21.52 Best Results: 21.66
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:24:01,064: Snapshot:4	Epoch:5	Loss:1.122	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.589                                                   	MRR:21.52	Hits@10:45.99	Best:21.66
2024-12-29 04:24:07,663: Snapshot:4	Epoch:6	Loss:26.177	translation_Loss:11.407	multi_layer_Loss:14.77	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.52	Hits@10:45.99	Best:21.66
2024-12-29 04:24:14,696: End of token training: 4 Epoch: 7 Loss:11.724 MRR:21.52 Best Results: 21.66
2024-12-29 04:24:14,696: Snapshot:4	Epoch:7	Loss:11.724	translation_Loss:11.407	multi_layer_Loss:0.317	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.52	Hits@10:45.99	Best:21.66
2024-12-29 04:24:14,947: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 04:24:29,075: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.228  | 0.1437 | 0.262  | 0.3158 |  0.3886 |
|     1      | 0.2054 | 0.1268 | 0.2349 | 0.2883 |  0.3584 |
|     2      | 0.1989 | 0.1173 | 0.2272 | 0.2835 |   0.36  |
|     3      | 0.1879 | 0.0994 | 0.215  | 0.2781 |  0.371  |
|     4      | 0.2173 | 0.1008 | 0.2588 | 0.3441 |  0.456  |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:24:29,077: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2393 | 0.1534 | 0.2823 | 0.333  |  0.3939 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2497 | 0.1626 | 0.2911 | 0.3418 |  0.4086 |
|     1      | 0.2129 | 0.133  | 0.2519 | 0.299  |  0.357  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1593 | 0.2901 | 0.3418 |  0.4129 |
|     1      | 0.2189 | 0.1362 | 0.2562 | 0.3088 |  0.3717 |
|     2      | 0.2095 | 0.1271 | 0.2458 | 0.2971 |  0.3645 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2387 | 0.1515 | 0.2774 | 0.3304 |  0.4024 |
|     1      | 0.2151 | 0.1334 | 0.2482 | 0.3022 |  0.3728 |
|     2      | 0.2082 | 0.1242 | 0.2413 | 0.2975 |  0.3728 |
|     3      | 0.1974 | 0.1088 | 0.2288 | 0.2912 |  0.3744 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.228  | 0.1437 | 0.262  | 0.3158 |  0.3886 |
|     1      | 0.2054 | 0.1268 | 0.2349 | 0.2883 |  0.3584 |
|     2      | 0.1989 | 0.1173 | 0.2272 | 0.2835 |   0.36  |
|     3      | 0.1879 | 0.0994 | 0.215  | 0.2781 |  0.371  |
|     4      | 0.2173 | 0.1008 | 0.2588 | 0.3441 |  0.456  |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:24:29,077: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 126.29485607147217 |   0.239   |    0.153     |    0.282     |     0.394     |
|    1     | 130.56567645072937 |   0.231   |    0.148     |    0.272     |     0.383     |
|    2     | 85.60006427764893  |   0.225   |    0.141     |    0.264     |     0.383     |
|    3     | 87.16064190864563  |   0.215   |    0.129     |    0.249     |     0.381     |
|    4     | 66.83454418182373  |   0.207   |    0.118     |     0.24     |     0.387     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:24:29,077: Sum_Training_Time:496.4557828903198
2024-12-29 04:24:29,078: Every_Training_Time:[126.29485607147217, 130.56567645072937, 85.60006427764893, 87.16064190864563, 66.83454418182373]
2024-12-29 04:24:29,078: Forward transfer: 0.17005 Backward transfer: -0.009724999999999998
2024-12-29 04:25:05,904: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241229042433/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:25:14,987: Snapshot:0	Epoch:0	Loss:17.871	translation_Loss:17.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-29 04:25:21,294: Snapshot:0	Epoch:1	Loss:17.114	translation_Loss:17.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.51	Hits@10:1.66	Best:1.51
2024-12-29 04:25:27,274: Snapshot:0	Epoch:2	Loss:16.452	translation_Loss:16.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.96	Hits@10:3.03	Best:1.96
2024-12-29 04:25:33,615: Snapshot:0	Epoch:3	Loss:15.815	translation_Loss:15.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.85	Hits@10:5.09	Best:2.85
2024-12-29 04:25:39,534: Snapshot:0	Epoch:4	Loss:15.191	translation_Loss:15.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.68	Hits@10:6.85	Best:3.68
2024-12-29 04:25:45,479: Snapshot:0	Epoch:5	Loss:14.574	translation_Loss:14.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.35	Hits@10:8.36	Best:4.35
2024-12-29 04:25:51,831: Snapshot:0	Epoch:6	Loss:13.981	translation_Loss:13.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.94	Hits@10:9.89	Best:4.94
2024-12-29 04:25:57,715: Snapshot:0	Epoch:7	Loss:13.402	translation_Loss:13.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:11.27	Best:5.49
2024-12-29 04:26:03,917: Snapshot:0	Epoch:8	Loss:12.837	translation_Loss:12.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.74	Best:6.03
2024-12-29 04:26:09,877: Snapshot:0	Epoch:9	Loss:12.295	translation_Loss:12.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.55	Hits@10:14.16	Best:6.55
2024-12-29 04:26:15,686: Snapshot:0	Epoch:10	Loss:11.76	translation_Loss:11.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.38	Best:7.06
2024-12-29 04:26:21,990: Snapshot:0	Epoch:11	Loss:11.237	translation_Loss:11.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.51	Hits@10:16.59	Best:7.51
2024-12-29 04:26:27,933: Snapshot:0	Epoch:12	Loss:10.735	translation_Loss:10.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.88	Best:7.94
2024-12-29 04:26:34,227: Snapshot:0	Epoch:13	Loss:10.246	translation_Loss:10.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.38	Hits@10:19.02	Best:8.38
2024-12-29 04:26:40,130: Snapshot:0	Epoch:14	Loss:9.775	translation_Loss:9.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.81	Hits@10:20.32	Best:8.81
2024-12-29 04:26:46,043: Snapshot:0	Epoch:15	Loss:9.328	translation_Loss:9.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.21	Hits@10:21.48	Best:9.21
2024-12-29 04:26:52,355: Snapshot:0	Epoch:16	Loss:8.889	translation_Loss:8.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.62	Hits@10:22.6	Best:9.62
2024-12-29 04:26:58,275: Snapshot:0	Epoch:17	Loss:8.466	translation_Loss:8.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.03	Hits@10:23.64	Best:10.03
2024-12-29 04:27:04,617: Snapshot:0	Epoch:18	Loss:8.065	translation_Loss:8.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.41	Hits@10:24.69	Best:10.41
2024-12-29 04:27:10,594: Snapshot:0	Epoch:19	Loss:7.676	translation_Loss:7.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:25.58	Best:10.83
2024-12-29 04:27:17,066: Snapshot:0	Epoch:20	Loss:7.299	translation_Loss:7.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.24	Hits@10:26.46	Best:11.24
2024-12-29 04:27:23,023: Snapshot:0	Epoch:21	Loss:6.949	translation_Loss:6.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.64	Hits@10:27.32	Best:11.64
2024-12-29 04:27:28,966: Snapshot:0	Epoch:22	Loss:6.616	translation_Loss:6.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.05	Hits@10:28.06	Best:12.05
2024-12-29 04:27:35,266: Snapshot:0	Epoch:23	Loss:6.284	translation_Loss:6.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.44	Hits@10:28.73	Best:12.44
2024-12-29 04:27:41,120: Snapshot:0	Epoch:24	Loss:5.975	translation_Loss:5.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.8	Hits@10:29.37	Best:12.8
2024-12-29 04:27:47,556: Snapshot:0	Epoch:25	Loss:5.688	translation_Loss:5.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.15	Hits@10:30.03	Best:13.15
2024-12-29 04:27:53,437: Snapshot:0	Epoch:26	Loss:5.403	translation_Loss:5.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.51	Hits@10:30.64	Best:13.51
2024-12-29 04:27:59,461: Snapshot:0	Epoch:27	Loss:5.131	translation_Loss:5.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.9	Hits@10:31.15	Best:13.9
2024-12-29 04:28:05,738: Snapshot:0	Epoch:28	Loss:4.875	translation_Loss:4.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.26	Hits@10:31.66	Best:14.26
2024-12-29 04:28:11,755: Snapshot:0	Epoch:29	Loss:4.623	translation_Loss:4.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.62	Hits@10:32.1	Best:14.62
2024-12-29 04:28:17,745: Snapshot:0	Epoch:30	Loss:4.397	translation_Loss:4.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.96	Hits@10:32.58	Best:14.96
2024-12-29 04:28:24,108: Snapshot:0	Epoch:31	Loss:4.177	translation_Loss:4.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.29	Hits@10:33.11	Best:15.29
2024-12-29 04:28:29,953: Snapshot:0	Epoch:32	Loss:3.969	translation_Loss:3.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.6	Hits@10:33.52	Best:15.6
2024-12-29 04:28:36,301: Snapshot:0	Epoch:33	Loss:3.753	translation_Loss:3.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.93	Hits@10:33.92	Best:15.93
2024-12-29 04:28:42,170: Snapshot:0	Epoch:34	Loss:3.561	translation_Loss:3.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.27	Hits@10:34.3	Best:16.27
2024-12-29 04:28:48,244: Snapshot:0	Epoch:35	Loss:3.375	translation_Loss:3.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.56	Hits@10:34.64	Best:16.56
2024-12-29 04:28:54,454: Snapshot:0	Epoch:36	Loss:3.197	translation_Loss:3.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.89	Hits@10:34.89	Best:16.89
2024-12-29 04:29:00,352: Snapshot:0	Epoch:37	Loss:3.037	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.19	Hits@10:35.28	Best:17.19
2024-12-29 04:29:06,701: Snapshot:0	Epoch:38	Loss:2.873	translation_Loss:2.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.49	Hits@10:35.57	Best:17.49
2024-12-29 04:29:12,686: Snapshot:0	Epoch:39	Loss:2.718	translation_Loss:2.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.82	Hits@10:35.89	Best:17.82
2024-12-29 04:29:18,561: Snapshot:0	Epoch:40	Loss:2.579	translation_Loss:2.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.13	Hits@10:36.13	Best:18.13
2024-12-29 04:29:24,809: Snapshot:0	Epoch:41	Loss:2.44	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.4	Hits@10:36.42	Best:18.4
2024-12-29 04:29:30,649: Snapshot:0	Epoch:42	Loss:2.309	translation_Loss:2.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.61	Hits@10:36.71	Best:18.61
2024-12-29 04:29:36,926: Snapshot:0	Epoch:43	Loss:2.179	translation_Loss:2.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.87	Hits@10:36.89	Best:18.87
2024-12-29 04:29:42,770: Snapshot:0	Epoch:44	Loss:2.061	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.12	Hits@10:37.1	Best:19.12
2024-12-29 04:29:49,090: Snapshot:0	Epoch:45	Loss:1.948	translation_Loss:1.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.32	Hits@10:37.3	Best:19.32
2024-12-29 04:29:54,959: Snapshot:0	Epoch:46	Loss:1.834	translation_Loss:1.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.57	Hits@10:37.46	Best:19.57
2024-12-29 04:30:00,935: Snapshot:0	Epoch:47	Loss:1.738	translation_Loss:1.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:37.63	Best:19.77
2024-12-29 04:30:07,315: Snapshot:0	Epoch:48	Loss:1.647	translation_Loss:1.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.97	Hits@10:37.8	Best:19.97
2024-12-29 04:30:13,269: Snapshot:0	Epoch:49	Loss:1.553	translation_Loss:1.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.13	Hits@10:37.92	Best:20.13
2024-12-29 04:30:19,494: Snapshot:0	Epoch:50	Loss:1.463	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.33	Hits@10:38.0	Best:20.33
2024-12-29 04:30:25,423: Snapshot:0	Epoch:51	Loss:1.383	translation_Loss:1.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.48	Hits@10:38.14	Best:20.48
2024-12-29 04:30:31,279: Snapshot:0	Epoch:52	Loss:1.312	translation_Loss:1.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.67	Hits@10:38.29	Best:20.67
2024-12-29 04:30:37,609: Snapshot:0	Epoch:53	Loss:1.24	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.84	Hits@10:38.32	Best:20.84
2024-12-29 04:30:43,469: Snapshot:0	Epoch:54	Loss:1.17	translation_Loss:1.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.99	Hits@10:38.44	Best:20.99
2024-12-29 04:30:49,366: Snapshot:0	Epoch:55	Loss:1.107	translation_Loss:1.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.16	Hits@10:38.54	Best:21.16
2024-12-29 04:30:55,612: Snapshot:0	Epoch:56	Loss:1.05	translation_Loss:1.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.28	Hits@10:38.61	Best:21.28
2024-12-29 04:31:01,466: Snapshot:0	Epoch:57	Loss:0.995	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.39	Hits@10:38.72	Best:21.39
2024-12-29 04:31:07,697: Snapshot:0	Epoch:58	Loss:0.944	translation_Loss:0.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.52	Hits@10:38.81	Best:21.52
2024-12-29 04:31:13,604: Snapshot:0	Epoch:59	Loss:0.892	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.66	Hits@10:38.87	Best:21.66
2024-12-29 04:31:19,639: Snapshot:0	Epoch:60	Loss:0.841	translation_Loss:0.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.8	Hits@10:38.87	Best:21.8
2024-12-29 04:31:25,894: Snapshot:0	Epoch:61	Loss:0.806	translation_Loss:0.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.9	Hits@10:38.98	Best:21.9
2024-12-29 04:31:31,787: Snapshot:0	Epoch:62	Loss:0.762	translation_Loss:0.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.01	Hits@10:39.04	Best:22.01
2024-12-29 04:31:38,117: Snapshot:0	Epoch:63	Loss:0.723	translation_Loss:0.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.12	Hits@10:39.06	Best:22.12
2024-12-29 04:31:44,019: Snapshot:0	Epoch:64	Loss:0.684	translation_Loss:0.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.18	Hits@10:39.1	Best:22.18
2024-12-29 04:31:49,895: Snapshot:0	Epoch:65	Loss:0.651	translation_Loss:0.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.3	Hits@10:39.15	Best:22.3
2024-12-29 04:31:56,137: Snapshot:0	Epoch:66	Loss:0.626	translation_Loss:0.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.35	Hits@10:39.17	Best:22.35
2024-12-29 04:32:02,003: Snapshot:0	Epoch:67	Loss:0.603	translation_Loss:0.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.45	Hits@10:39.22	Best:22.45
2024-12-29 04:32:08,237: Snapshot:0	Epoch:68	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.5	Hits@10:39.25	Best:22.5
2024-12-29 04:32:14,286: Snapshot:0	Epoch:69	Loss:0.538	translation_Loss:0.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.6	Hits@10:39.29	Best:22.6
2024-12-29 04:32:20,491: Snapshot:0	Epoch:70	Loss:0.521	translation_Loss:0.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.62	Hits@10:39.32	Best:22.62
2024-12-29 04:32:26,457: Snapshot:0	Epoch:71	Loss:0.493	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:39.34	Best:22.69
2024-12-29 04:32:32,321: Snapshot:0	Epoch:72	Loss:0.47	translation_Loss:0.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.77	Hits@10:39.29	Best:22.77
2024-12-29 04:32:38,687: Snapshot:0	Epoch:73	Loss:0.45	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.86	Hits@10:39.36	Best:22.86
2024-12-29 04:32:44,632: Snapshot:0	Epoch:74	Loss:0.431	translation_Loss:0.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.92	Hits@10:39.45	Best:22.92
2024-12-29 04:32:51,005: Snapshot:0	Epoch:75	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.97	Hits@10:39.5	Best:22.97
2024-12-29 04:32:57,057: Snapshot:0	Epoch:76	Loss:0.397	translation_Loss:0.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.01	Hits@10:39.45	Best:23.01
2024-12-29 04:33:03,024: Snapshot:0	Epoch:77	Loss:0.384	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.08	Hits@10:39.52	Best:23.08
2024-12-29 04:33:09,260: Snapshot:0	Epoch:78	Loss:0.365	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.07	Hits@10:39.56	Best:23.08
2024-12-29 04:33:15,142: Snapshot:0	Epoch:79	Loss:0.351	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.11	Hits@10:39.62	Best:23.11
2024-12-29 04:33:21,094: Snapshot:0	Epoch:80	Loss:0.342	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.15	Hits@10:39.59	Best:23.15
2024-12-29 04:33:27,317: Snapshot:0	Epoch:81	Loss:0.329	translation_Loss:0.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:39.6	Best:23.22
2024-12-29 04:33:33,268: Snapshot:0	Epoch:82	Loss:0.317	translation_Loss:0.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.2	Hits@10:39.67	Best:23.22
2024-12-29 04:33:39,479: Snapshot:0	Epoch:83	Loss:0.302	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.21	Hits@10:39.65	Best:23.22
2024-12-29 04:33:45,316: Snapshot:0	Epoch:84	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.25	Hits@10:39.7	Best:23.25
2024-12-29 04:33:51,197: Snapshot:0	Epoch:85	Loss:0.285	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.28	Hits@10:39.69	Best:23.28
2024-12-29 04:33:57,565: Snapshot:0	Epoch:86	Loss:0.273	translation_Loss:0.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.3	Hits@10:39.75	Best:23.3
2024-12-29 04:34:03,644: Snapshot:0	Epoch:87	Loss:0.269	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.33	Hits@10:39.76	Best:23.33
2024-12-29 04:34:10,013: Snapshot:0	Epoch:88	Loss:0.256	translation_Loss:0.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.77	Best:23.36
2024-12-29 04:34:15,904: Snapshot:0	Epoch:89	Loss:0.25	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.34	Hits@10:39.81	Best:23.36
2024-12-29 04:34:21,756: Snapshot:0	Epoch:90	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.39	Hits@10:39.77	Best:23.39
2024-12-29 04:34:28,117: Snapshot:0	Epoch:91	Loss:0.233	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.4	Hits@10:39.76	Best:23.4
2024-12-29 04:34:34,028: Snapshot:0	Epoch:92	Loss:0.227	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.42	Hits@10:39.72	Best:23.42
2024-12-29 04:34:40,309: Snapshot:0	Epoch:93	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:39.73	Best:23.45
2024-12-29 04:34:46,233: Snapshot:0	Epoch:94	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.49	Hits@10:39.76	Best:23.49
2024-12-29 04:34:52,534: Snapshot:0	Epoch:95	Loss:0.209	translation_Loss:0.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.5	Hits@10:39.77	Best:23.5
2024-12-29 04:34:58,370: Snapshot:0	Epoch:96	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.5	Hits@10:39.83	Best:23.5
2024-12-29 04:35:04,254: Snapshot:0	Epoch:97	Loss:0.194	translation_Loss:0.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.55	Hits@10:39.86	Best:23.55
2024-12-29 04:35:10,571: Snapshot:0	Epoch:98	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.56	Hits@10:39.83	Best:23.56
2024-12-29 04:35:16,501: Snapshot:0	Epoch:99	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.85	Best:23.63
2024-12-29 04:35:22,705: Snapshot:0	Epoch:100	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.62	Hits@10:39.81	Best:23.63
2024-12-29 04:35:28,672: Snapshot:0	Epoch:101	Loss:0.177	translation_Loss:0.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.84	Best:23.64
2024-12-29 04:35:34,489: Snapshot:0	Epoch:102	Loss:0.172	translation_Loss:0.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.88	Best:23.64
2024-12-29 04:35:40,713: Snapshot:0	Epoch:103	Loss:0.173	translation_Loss:0.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.83	Best:23.65
2024-12-29 04:35:46,578: Snapshot:0	Epoch:104	Loss:0.164	translation_Loss:0.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.9	Best:23.65
2024-12-29 04:35:52,443: Snapshot:0	Epoch:105	Loss:0.161	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:39.91	Best:23.66
2024-12-29 04:35:58,655: Snapshot:0	Epoch:106	Loss:0.157	translation_Loss:0.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.89	Best:23.66
2024-12-29 04:36:04,520: Snapshot:0	Epoch:107	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.91	Best:23.71
2024-12-29 04:36:10,805: Snapshot:0	Epoch:108	Loss:0.152	translation_Loss:0.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.73	Hits@10:39.9	Best:23.73
2024-12-29 04:36:16,858: Snapshot:0	Epoch:109	Loss:0.146	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.91	Best:23.76
2024-12-29 04:36:22,713: Snapshot:0	Epoch:110	Loss:0.144	translation_Loss:0.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.9	Best:23.76
2024-12-29 04:36:28,930: Snapshot:0	Epoch:111	Loss:0.147	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.78	Hits@10:39.98	Best:23.78
2024-12-29 04:36:34,994: Snapshot:0	Epoch:112	Loss:0.141	translation_Loss:0.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.79	Hits@10:40.03	Best:23.79
2024-12-29 04:36:41,284: Snapshot:0	Epoch:113	Loss:0.137	translation_Loss:0.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.81	Hits@10:40.04	Best:23.81
2024-12-29 04:36:47,172: Snapshot:0	Epoch:114	Loss:0.136	translation_Loss:0.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.84	Hits@10:40.0	Best:23.84
2024-12-29 04:36:53,032: Snapshot:0	Epoch:115	Loss:0.129	translation_Loss:0.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:40.02	Best:23.88
2024-12-29 04:36:59,192: Snapshot:0	Epoch:116	Loss:0.131	translation_Loss:0.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.86	Hits@10:40.01	Best:23.88
2024-12-29 04:37:05,054: Snapshot:0	Epoch:117	Loss:0.126	translation_Loss:0.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.86	Hits@10:40.04	Best:23.88
2024-12-29 04:37:11,430: Early Stopping! Snapshot: 0 Epoch: 118 Best Results: 23.88
2024-12-29 04:37:11,430: Start to training tokens! Snapshot: 0 Epoch: 118 Loss:0.127 MRR:23.88 Best Results: 23.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:37:11,430: Snapshot:0	Epoch:118	Loss:0.127	translation_Loss:0.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:40.05	Best:23.88
2024-12-29 04:37:17,818: Snapshot:0	Epoch:119	Loss:57.295	translation_Loss:12.191	multi_layer_Loss:45.105	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:40.05	Best:23.88
2024-12-29 04:37:24,054: End of token training: 0 Epoch: 120 Loss:45.652 MRR:23.88 Best Results: 23.88
2024-12-29 04:37:24,055: Snapshot:0	Epoch:120	Loss:45.652	translation_Loss:12.191	multi_layer_Loss:33.462	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.88	Hits@10:40.05	Best:23.88
2024-12-29 04:37:24,281: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 04:37:26,415: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2319 | 0.1429 | 0.2793 | 0.3273 |  0.3893 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:37:49,151: Snapshot:1	Epoch:0	Loss:8.601	translation_Loss:8.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.038                                                   	MRR:17.37	Hits@10:30.25	Best:17.37
2024-12-29 04:37:55,486: Snapshot:1	Epoch:1	Loss:7.629	translation_Loss:7.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.118                                                   	MRR:18.03	Hits@10:31.24	Best:18.03
2024-12-29 04:38:01,880: Snapshot:1	Epoch:2	Loss:6.976	translation_Loss:6.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.224                                                   	MRR:18.46	Hits@10:31.85	Best:18.46
2024-12-29 04:38:08,268: Snapshot:1	Epoch:3	Loss:6.457	translation_Loss:6.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:18.85	Hits@10:32.47	Best:18.85
2024-12-29 04:38:14,800: Snapshot:1	Epoch:4	Loss:6.05	translation_Loss:5.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.47                                                   	MRR:19.15	Hits@10:32.96	Best:19.15
2024-12-29 04:38:21,525: Snapshot:1	Epoch:5	Loss:5.701	translation_Loss:5.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.59                                                   	MRR:19.4	Hits@10:33.47	Best:19.4
2024-12-29 04:38:27,880: Snapshot:1	Epoch:6	Loss:5.404	translation_Loss:4.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.701                                                   	MRR:19.61	Hits@10:33.86	Best:19.61
2024-12-29 04:38:34,568: Snapshot:1	Epoch:7	Loss:5.15	translation_Loss:4.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.803                                                   	MRR:19.91	Hits@10:34.29	Best:19.91
2024-12-29 04:38:41,023: Snapshot:1	Epoch:8	Loss:4.907	translation_Loss:4.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.895                                                   	MRR:20.15	Hits@10:34.54	Best:20.15
2024-12-29 04:38:47,873: Snapshot:1	Epoch:9	Loss:4.705	translation_Loss:3.728	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.978                                                   	MRR:20.35	Hits@10:34.81	Best:20.35
2024-12-29 04:38:54,317: Snapshot:1	Epoch:10	Loss:4.504	translation_Loss:3.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.05                                                   	MRR:20.47	Hits@10:35.1	Best:20.47
2024-12-29 04:39:00,747: Snapshot:1	Epoch:11	Loss:4.34	translation_Loss:3.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.115                                                   	MRR:20.63	Hits@10:35.34	Best:20.63
2024-12-29 04:39:07,478: Snapshot:1	Epoch:12	Loss:4.19	translation_Loss:3.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.171                                                   	MRR:20.79	Hits@10:35.53	Best:20.79
2024-12-29 04:39:13,735: Snapshot:1	Epoch:13	Loss:4.07	translation_Loss:2.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.219                                                   	MRR:20.9	Hits@10:35.64	Best:20.9
2024-12-29 04:39:20,464: Snapshot:1	Epoch:14	Loss:3.948	translation_Loss:2.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.262                                                   	MRR:21.02	Hits@10:35.86	Best:21.02
2024-12-29 04:39:26,819: Snapshot:1	Epoch:15	Loss:3.857	translation_Loss:2.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:21.12	Hits@10:36.07	Best:21.12
2024-12-29 04:39:33,130: Snapshot:1	Epoch:16	Loss:3.773	translation_Loss:2.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.33                                                   	MRR:21.24	Hits@10:36.21	Best:21.24
2024-12-29 04:39:39,838: Snapshot:1	Epoch:17	Loss:3.699	translation_Loss:2.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.357                                                   	MRR:21.37	Hits@10:36.32	Best:21.37
2024-12-29 04:39:46,162: Snapshot:1	Epoch:18	Loss:3.637	translation_Loss:2.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.381                                                   	MRR:21.44	Hits@10:36.32	Best:21.44
2024-12-29 04:39:52,539: Snapshot:1	Epoch:19	Loss:3.588	translation_Loss:2.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.401                                                   	MRR:21.53	Hits@10:36.52	Best:21.53
2024-12-29 04:39:59,195: Snapshot:1	Epoch:20	Loss:3.54	translation_Loss:2.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.419                                                   	MRR:21.54	Hits@10:36.6	Best:21.54
2024-12-29 04:40:05,566: Snapshot:1	Epoch:21	Loss:3.507	translation_Loss:2.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.434                                                   	MRR:21.57	Hits@10:36.72	Best:21.57
2024-12-29 04:40:12,385: Snapshot:1	Epoch:22	Loss:3.477	translation_Loss:2.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.446                                                   	MRR:21.64	Hits@10:36.78	Best:21.64
2024-12-29 04:40:18,710: Snapshot:1	Epoch:23	Loss:3.445	translation_Loss:1.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.457                                                   	MRR:21.7	Hits@10:36.75	Best:21.7
2024-12-29 04:40:25,140: Snapshot:1	Epoch:24	Loss:3.429	translation_Loss:1.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:21.7	Hits@10:36.86	Best:21.7
2024-12-29 04:40:31,813: Snapshot:1	Epoch:25	Loss:3.4	translation_Loss:1.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.477                                                   	MRR:21.76	Hits@10:36.91	Best:21.76
2024-12-29 04:40:38,243: Snapshot:1	Epoch:26	Loss:3.382	translation_Loss:1.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.485                                                   	MRR:21.78	Hits@10:36.96	Best:21.78
2024-12-29 04:40:44,975: Snapshot:1	Epoch:27	Loss:3.378	translation_Loss:1.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.492                                                   	MRR:21.81	Hits@10:37.06	Best:21.81
2024-12-29 04:40:51,359: Snapshot:1	Epoch:28	Loss:3.356	translation_Loss:1.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.498                                                   	MRR:21.85	Hits@10:37.13	Best:21.85
2024-12-29 04:40:57,660: Snapshot:1	Epoch:29	Loss:3.343	translation_Loss:1.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.503                                                   	MRR:21.84	Hits@10:37.15	Best:21.85
2024-12-29 04:41:04,352: Snapshot:1	Epoch:30	Loss:3.332	translation_Loss:1.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.51                                                   	MRR:21.88	Hits@10:37.15	Best:21.88
2024-12-29 04:41:10,679: Snapshot:1	Epoch:31	Loss:3.312	translation_Loss:1.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.515                                                   	MRR:21.86	Hits@10:37.15	Best:21.88
2024-12-29 04:41:17,558: Snapshot:1	Epoch:32	Loss:3.31	translation_Loss:1.791	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.519                                                   	MRR:21.89	Hits@10:37.17	Best:21.89
2024-12-29 04:41:23,882: Snapshot:1	Epoch:33	Loss:3.289	translation_Loss:1.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.522                                                   	MRR:21.94	Hits@10:37.31	Best:21.94
2024-12-29 04:41:30,720: Snapshot:1	Epoch:34	Loss:3.283	translation_Loss:1.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.525                                                   	MRR:21.92	Hits@10:37.22	Best:21.94
2024-12-29 04:41:37,182: Snapshot:1	Epoch:35	Loss:3.279	translation_Loss:1.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.527                                                   	MRR:21.92	Hits@10:37.28	Best:21.94
2024-12-29 04:41:43,501: Early Stopping! Snapshot: 1 Epoch: 36 Best Results: 21.94
2024-12-29 04:41:43,501: Start to training tokens! Snapshot: 1 Epoch: 36 Loss:3.276 MRR:21.93 Best Results: 21.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:41:43,501: Snapshot:1	Epoch:36	Loss:3.276	translation_Loss:1.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.53                                                   	MRR:21.93	Hits@10:37.28	Best:21.94
2024-12-29 04:41:50,092: Snapshot:1	Epoch:37	Loss:58.381	translation_Loss:13.856	multi_layer_Loss:44.526	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.93	Hits@10:37.28	Best:21.94
2024-12-29 04:41:56,354: End of token training: 1 Epoch: 38 Loss:46.578 MRR:21.93 Best Results: 21.94
2024-12-29 04:41:56,354: Snapshot:1	Epoch:38	Loss:46.578	translation_Loss:13.862	multi_layer_Loss:32.716	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.93	Hits@10:37.28	Best:21.94
2024-12-29 04:41:56,595: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 04:42:01,840: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2473 | 0.156  | 0.2908 | 0.3464 |  0.4172 |
|     1      | 0.2189 | 0.1328 | 0.2622 | 0.3125 |  0.3751 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:42:24,762: Snapshot:2	Epoch:0	Loss:5.816	translation_Loss:5.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.039                                                   	MRR:19.49	Hits@10:34.79	Best:19.49
2024-12-29 04:42:31,177: Snapshot:2	Epoch:1	Loss:4.859	translation_Loss:4.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.115                                                   	MRR:19.97	Hits@10:35.61	Best:19.97
2024-12-29 04:42:37,592: Snapshot:2	Epoch:2	Loss:4.257	translation_Loss:4.051	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.206                                                   	MRR:20.35	Hits@10:36.07	Best:20.35
2024-12-29 04:42:44,026: Snapshot:2	Epoch:3	Loss:3.842	translation_Loss:3.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.307                                                   	MRR:20.6	Hits@10:36.47	Best:20.6
2024-12-29 04:42:50,653: Snapshot:2	Epoch:4	Loss:3.5	translation_Loss:3.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.406                                                   	MRR:20.79	Hits@10:36.75	Best:20.79
2024-12-29 04:42:57,550: Snapshot:2	Epoch:5	Loss:3.251	translation_Loss:2.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.5                                                   	MRR:20.96	Hits@10:37.04	Best:20.96
2024-12-29 04:43:03,958: Snapshot:2	Epoch:6	Loss:3.049	translation_Loss:2.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.584                                                   	MRR:21.13	Hits@10:37.38	Best:21.13
2024-12-29 04:43:10,434: Snapshot:2	Epoch:7	Loss:2.882	translation_Loss:2.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.659                                                   	MRR:21.25	Hits@10:37.53	Best:21.25
2024-12-29 04:43:17,444: Snapshot:2	Epoch:8	Loss:2.744	translation_Loss:2.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.724                                                   	MRR:21.36	Hits@10:37.73	Best:21.36
2024-12-29 04:43:24,025: Snapshot:2	Epoch:9	Loss:2.62	translation_Loss:1.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.781                                                   	MRR:21.43	Hits@10:37.77	Best:21.43
2024-12-29 04:43:30,944: Snapshot:2	Epoch:10	Loss:2.532	translation_Loss:1.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.83                                                   	MRR:21.48	Hits@10:37.94	Best:21.48
2024-12-29 04:43:37,473: Snapshot:2	Epoch:11	Loss:2.439	translation_Loss:1.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.872                                                   	MRR:21.53	Hits@10:38.02	Best:21.53
2024-12-29 04:43:43,976: Snapshot:2	Epoch:12	Loss:2.364	translation_Loss:1.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.908                                                   	MRR:21.57	Hits@10:38.11	Best:21.57
2024-12-29 04:43:50,742: Snapshot:2	Epoch:13	Loss:2.311	translation_Loss:1.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.937                                                   	MRR:21.66	Hits@10:38.18	Best:21.66
2024-12-29 04:43:57,245: Snapshot:2	Epoch:14	Loss:2.275	translation_Loss:1.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.961                                                   	MRR:21.73	Hits@10:38.34	Best:21.73
2024-12-29 04:44:04,137: Snapshot:2	Epoch:15	Loss:2.236	translation_Loss:1.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.983                                                   	MRR:21.79	Hits@10:38.42	Best:21.79
2024-12-29 04:44:10,599: Snapshot:2	Epoch:16	Loss:2.204	translation_Loss:1.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.001                                                   	MRR:21.74	Hits@10:38.51	Best:21.79
2024-12-29 04:44:17,421: Snapshot:2	Epoch:17	Loss:2.181	translation_Loss:1.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.016                                                   	MRR:21.75	Hits@10:38.55	Best:21.79
2024-12-29 04:44:23,864: Snapshot:2	Epoch:18	Loss:2.166	translation_Loss:1.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.031                                                   	MRR:21.83	Hits@10:38.56	Best:21.83
2024-12-29 04:44:30,283: Snapshot:2	Epoch:19	Loss:2.149	translation_Loss:1.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.042                                                   	MRR:21.83	Hits@10:38.58	Best:21.83
2024-12-29 04:44:37,118: Snapshot:2	Epoch:20	Loss:2.134	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.052                                                   	MRR:21.84	Hits@10:38.61	Best:21.84
2024-12-29 04:44:43,554: Snapshot:2	Epoch:21	Loss:2.122	translation_Loss:1.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.06                                                   	MRR:21.86	Hits@10:38.58	Best:21.86
2024-12-29 04:44:50,356: Snapshot:2	Epoch:22	Loss:2.116	translation_Loss:1.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.068                                                   	MRR:21.86	Hits@10:38.58	Best:21.86
2024-12-29 04:44:56,788: Snapshot:2	Epoch:23	Loss:2.107	translation_Loss:1.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.076                                                   	MRR:21.89	Hits@10:38.63	Best:21.89
2024-12-29 04:45:03,334: Snapshot:2	Epoch:24	Loss:2.096	translation_Loss:1.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.082                                                   	MRR:21.9	Hits@10:38.51	Best:21.9
2024-12-29 04:45:10,197: Snapshot:2	Epoch:25	Loss:2.094	translation_Loss:1.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.088                                                   	MRR:21.92	Hits@10:38.61	Best:21.92
2024-12-29 04:45:16,626: Snapshot:2	Epoch:26	Loss:2.087	translation_Loss:0.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.093                                                   	MRR:21.88	Hits@10:38.61	Best:21.92
2024-12-29 04:45:23,451: Snapshot:2	Epoch:27	Loss:2.08	translation_Loss:0.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:21.89	Hits@10:38.57	Best:21.92
2024-12-29 04:45:29,891: Early Stopping! Snapshot: 2 Epoch: 28 Best Results: 21.92
2024-12-29 04:45:29,891: Start to training tokens! Snapshot: 2 Epoch: 28 Loss:2.081 MRR:21.9 Best Results: 21.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:45:29,891: Snapshot:2	Epoch:28	Loss:2.081	translation_Loss:0.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.101                                                   	MRR:21.9	Hits@10:38.68	Best:21.92
2024-12-29 04:45:36,258: Snapshot:2	Epoch:29	Loss:59.154	translation_Loss:13.551	multi_layer_Loss:45.603	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.9	Hits@10:38.68	Best:21.92
2024-12-29 04:45:43,027: End of token training: 2 Epoch: 30 Loss:47.489 MRR:21.9 Best Results: 21.92
2024-12-29 04:45:43,027: Snapshot:2	Epoch:30	Loss:47.489	translation_Loss:13.558	multi_layer_Loss:33.931	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.9	Hits@10:38.68	Best:21.92
2024-12-29 04:45:43,240: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 04:45:51,396: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.1511 | 0.283  | 0.341  |  0.4126 |
|     1      | 0.225  | 0.1383 | 0.2623 | 0.3195 |  0.3904 |
|     2      | 0.2205 | 0.1325 | 0.2585 | 0.314  |  0.3876 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:46:14,155: Snapshot:3	Epoch:0	Loss:3.175	translation_Loss:3.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.038                                                   	MRR:19.13	Hits@10:36.0	Best:19.13
2024-12-29 04:46:20,717: Snapshot:3	Epoch:1	Loss:2.385	translation_Loss:2.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.106                                                   	MRR:19.54	Hits@10:37.3	Best:19.54
2024-12-29 04:46:27,331: Snapshot:3	Epoch:2	Loss:1.99	translation_Loss:1.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:19.86	Hits@10:37.93	Best:19.86
2024-12-29 04:46:33,822: Snapshot:3	Epoch:3	Loss:1.739	translation_Loss:1.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:20.03	Hits@10:38.27	Best:20.03
2024-12-29 04:46:40,334: Snapshot:3	Epoch:4	Loss:1.564	translation_Loss:1.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.275                                                   	MRR:20.14	Hits@10:38.47	Best:20.14
2024-12-29 04:46:46,781: Snapshot:3	Epoch:5	Loss:1.434	translation_Loss:1.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.318                                                   	MRR:20.25	Hits@10:38.71	Best:20.25
2024-12-29 04:46:53,695: Snapshot:3	Epoch:6	Loss:1.331	translation_Loss:0.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:20.32	Hits@10:38.96	Best:20.32
2024-12-29 04:47:00,188: Snapshot:3	Epoch:7	Loss:1.26	translation_Loss:0.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.385                                                   	MRR:20.41	Hits@10:38.98	Best:20.41
2024-12-29 04:47:06,725: Snapshot:3	Epoch:8	Loss:1.193	translation_Loss:0.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.411                                                   	MRR:20.39	Hits@10:39.05	Best:20.41
2024-12-29 04:47:13,549: Snapshot:3	Epoch:9	Loss:1.146	translation_Loss:0.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.432                                                   	MRR:20.46	Hits@10:39.15	Best:20.46
2024-12-29 04:47:20,003: Snapshot:3	Epoch:10	Loss:1.116	translation_Loss:0.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.45                                                   	MRR:20.48	Hits@10:39.28	Best:20.48
2024-12-29 04:47:26,876: Snapshot:3	Epoch:11	Loss:1.079	translation_Loss:0.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.465                                                   	MRR:20.47	Hits@10:39.15	Best:20.48
2024-12-29 04:47:33,332: Snapshot:3	Epoch:12	Loss:1.067	translation_Loss:0.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.476                                                   	MRR:20.52	Hits@10:39.22	Best:20.52
2024-12-29 04:47:40,218: Snapshot:3	Epoch:13	Loss:1.046	translation_Loss:0.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.487                                                   	MRR:20.55	Hits@10:39.39	Best:20.55
2024-12-29 04:47:46,714: Snapshot:3	Epoch:14	Loss:1.04	translation_Loss:0.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.496                                                   	MRR:20.55	Hits@10:39.47	Best:20.55
2024-12-29 04:47:53,203: Snapshot:3	Epoch:15	Loss:1.021	translation_Loss:0.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.502                                                   	MRR:20.55	Hits@10:39.48	Best:20.55
2024-12-29 04:48:00,083: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 20.55
2024-12-29 04:48:00,084: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:1.014 MRR:20.53 Best Results: 20.55
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:48:00,084: Snapshot:3	Epoch:16	Loss:1.014	translation_Loss:0.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.507                                                   	MRR:20.53	Hits@10:39.49	Best:20.55
2024-12-29 04:48:06,525: Snapshot:3	Epoch:17	Loss:57.849	translation_Loss:13.0	multi_layer_Loss:44.849	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.53	Hits@10:39.49	Best:20.55
2024-12-29 04:48:13,307: End of token training: 3 Epoch: 18 Loss:45.8 MRR:20.53 Best Results: 20.55
2024-12-29 04:48:13,307: Snapshot:3	Epoch:18	Loss:45.8	translation_Loss:13.01	multi_layer_Loss:32.79	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.53	Hits@10:39.49	Best:20.55
2024-12-29 04:48:13,527: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 04:48:24,610: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2292 | 0.1428 | 0.2639 |  0.32  |  0.3946 |
|     1      | 0.215  | 0.1301 | 0.2489 | 0.3048 |  0.379  |
|     2      | 0.2135 | 0.1232 | 0.2497 | 0.3119 |  0.3894 |
|     3      | 0.2075 | 0.1117 | 0.2448 | 0.3108 |  0.3958 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:48:47,397: Snapshot:4	Epoch:0	Loss:1.894	translation_Loss:1.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.033                                                   	MRR:19.19	Hits@10:42.32	Best:19.19
2024-12-29 04:48:53,856: Snapshot:4	Epoch:1	Loss:1.357	translation_Loss:1.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.076                                                   	MRR:20.7	Hits@10:45.43	Best:20.7
2024-12-29 04:49:00,450: Snapshot:4	Epoch:2	Loss:1.121	translation_Loss:1.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.092                                                   	MRR:21.27	Hits@10:46.38	Best:21.27
2024-12-29 04:49:07,049: Snapshot:4	Epoch:3	Loss:0.956	translation_Loss:0.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.103                                                   	MRR:21.54	Hits@10:46.83	Best:21.54
2024-12-29 04:49:13,656: Snapshot:4	Epoch:4	Loss:0.819	translation_Loss:0.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.115                                                   	MRR:21.72	Hits@10:47.38	Best:21.72
2024-12-29 04:49:20,609: Snapshot:4	Epoch:5	Loss:0.716	translation_Loss:0.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.125                                                   	MRR:22.02	Hits@10:47.52	Best:22.02
2024-12-29 04:49:27,078: Snapshot:4	Epoch:6	Loss:0.616	translation_Loss:0.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.133                                                   	MRR:21.98	Hits@10:47.69	Best:22.02
2024-12-29 04:49:33,917: Snapshot:4	Epoch:7	Loss:0.546	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.14                                                   	MRR:22.17	Hits@10:47.79	Best:22.17
2024-12-29 04:49:40,417: Snapshot:4	Epoch:8	Loss:0.48	translation_Loss:0.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.145                                                   	MRR:22.33	Hits@10:47.96	Best:22.33
2024-12-29 04:49:46,984: Snapshot:4	Epoch:9	Loss:0.425	translation_Loss:0.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.147                                                   	MRR:22.48	Hits@10:48.11	Best:22.48
2024-12-29 04:49:53,883: Snapshot:4	Epoch:10	Loss:0.389	translation_Loss:0.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:22.49	Hits@10:48.12	Best:22.49
2024-12-29 04:50:00,358: Snapshot:4	Epoch:11	Loss:0.364	translation_Loss:0.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.15                                                   	MRR:22.48	Hits@10:48.09	Best:22.49
2024-12-29 04:50:07,337: Snapshot:4	Epoch:12	Loss:0.343	translation_Loss:0.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:22.66	Hits@10:48.35	Best:22.66
2024-12-29 04:50:13,939: Snapshot:4	Epoch:13	Loss:0.325	translation_Loss:0.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:22.88	Hits@10:48.65	Best:22.88
2024-12-29 04:50:20,523: Snapshot:4	Epoch:14	Loss:0.318	translation_Loss:0.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:22.98	Hits@10:48.63	Best:22.98
2024-12-29 04:50:27,446: Snapshot:4	Epoch:15	Loss:0.309	translation_Loss:0.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.01	Hits@10:48.46	Best:23.01
2024-12-29 04:50:33,840: Snapshot:4	Epoch:16	Loss:0.301	translation_Loss:0.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.03	Hits@10:48.61	Best:23.03
2024-12-29 04:50:40,828: Snapshot:4	Epoch:17	Loss:0.291	translation_Loss:0.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.07	Hits@10:48.82	Best:23.07
2024-12-29 04:50:47,240: Snapshot:4	Epoch:18	Loss:0.287	translation_Loss:0.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.06	Hits@10:48.71	Best:23.07
2024-12-29 04:50:53,681: Snapshot:4	Epoch:19	Loss:0.288	translation_Loss:0.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.05	Hits@10:48.55	Best:23.07
2024-12-29 04:51:00,568: Snapshot:4	Epoch:20	Loss:0.287	translation_Loss:0.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.11	Hits@10:48.57	Best:23.11
2024-12-29 04:51:07,155: Snapshot:4	Epoch:21	Loss:0.281	translation_Loss:0.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.21	Hits@10:48.67	Best:23.21
2024-12-29 04:51:13,689: Snapshot:4	Epoch:22	Loss:0.279	translation_Loss:0.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.21	Hits@10:48.81	Best:23.21
2024-12-29 04:51:20,642: Snapshot:4	Epoch:23	Loss:0.279	translation_Loss:0.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.1	Hits@10:48.61	Best:23.21
2024-12-29 04:51:27,066: Early Stopping! Snapshot: 4 Epoch: 24 Best Results: 23.21
2024-12-29 04:51:27,066: Start to training tokens! Snapshot: 4 Epoch: 24 Loss:0.276 MRR:23.11 Best Results: 23.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:51:27,066: Snapshot:4	Epoch:24	Loss:0.276	translation_Loss:0.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:23.11	Hits@10:48.59	Best:23.21
2024-12-29 04:51:33,760: Snapshot:4	Epoch:25	Loss:56.11	translation_Loss:11.018	multi_layer_Loss:45.091	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.11	Hits@10:48.59	Best:23.21
2024-12-29 04:51:40,138: End of token training: 4 Epoch: 26 Loss:43.995 MRR:23.11 Best Results: 23.21
2024-12-29 04:51:40,138: Snapshot:4	Epoch:26	Loss:43.995	translation_Loss:11.013	multi_layer_Loss:32.982	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.11	Hits@10:48.59	Best:23.21
2024-12-29 04:51:40,352: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 04:51:54,561: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2139 | 0.1297 | 0.2457 | 0.3022 |  0.3752 |
|     1      | 0.1981 | 0.117  | 0.2271 | 0.2817 |  0.358  |
|     2      | 0.1953 | 0.1095 | 0.2247 | 0.2842 |  0.3692 |
|     3      | 0.1911 | 0.0969 | 0.2205 | 0.2901 |  0.3847 |
|     4      | 0.2326 | 0.1106 | 0.2749 | 0.3641 |  0.4825 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:51:54,564: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2319 | 0.1429 | 0.2793 | 0.3273 |  0.3893 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2473 | 0.156  | 0.2908 | 0.3464 |  0.4172 |
|     1      | 0.2189 | 0.1328 | 0.2622 | 0.3125 |  0.3751 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.1511 | 0.283  | 0.341  |  0.4126 |
|     1      | 0.225  | 0.1383 | 0.2623 | 0.3195 |  0.3904 |
|     2      | 0.2205 | 0.1325 | 0.2585 | 0.314  |  0.3876 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2292 | 0.1428 | 0.2639 |  0.32  |  0.3946 |
|     1      | 0.215  | 0.1301 | 0.2489 | 0.3048 |  0.379  |
|     2      | 0.2135 | 0.1232 | 0.2497 | 0.3119 |  0.3894 |
|     3      | 0.2075 | 0.1117 | 0.2448 | 0.3108 |  0.3958 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2139 | 0.1297 | 0.2457 | 0.3022 |  0.3752 |
|     1      | 0.1981 | 0.117  | 0.2271 | 0.2817 |  0.358  |
|     2      | 0.1953 | 0.1095 | 0.2247 | 0.2842 |  0.3692 |
|     3      | 0.1911 | 0.0969 | 0.2205 | 0.2901 |  0.3847 |
|     4      | 0.2326 | 0.1106 | 0.2749 | 0.3641 |  0.4825 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:51:54,564: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  738.150149345398  |   0.232   |    0.143     |    0.279     |     0.389     |
|    1     | 266.8914451599121  |   0.233   |    0.144     |    0.277     |     0.396     |
|    2     | 218.24756979942322 |   0.229   |    0.141     |    0.268     |     0.397     |
|    3     | 138.93073892593384 |   0.216   |    0.127     |    0.252     |      0.39     |
|    4     | 192.49363374710083 |   0.206   |    0.113     |    0.239     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:51:54,564: Sum_Training_Time:1554.713536977768
2024-12-29 04:51:54,564: Every_Training_Time:[738.150149345398, 266.8914451599121, 218.24756979942322, 138.93073892593384, 192.49363374710083]
2024-12-29 04:51:54,564: Forward transfer: 0.171825 Backward transfer: -0.0201
2024-12-29 04:52:31,719: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241229045159/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=3000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:52:40,823: Snapshot:0	Epoch:0	Loss:17.871	translation_Loss:17.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-29 04:52:47,142: Snapshot:0	Epoch:1	Loss:17.114	translation_Loss:17.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.51	Hits@10:1.66	Best:1.51
2024-12-29 04:52:53,067: Snapshot:0	Epoch:2	Loss:16.452	translation_Loss:16.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.96	Hits@10:3.03	Best:1.96
2024-12-29 04:52:59,497: Snapshot:0	Epoch:3	Loss:15.815	translation_Loss:15.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.85	Hits@10:5.09	Best:2.85
2024-12-29 04:53:05,431: Snapshot:0	Epoch:4	Loss:15.191	translation_Loss:15.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.68	Hits@10:6.85	Best:3.68
2024-12-29 04:53:11,378: Snapshot:0	Epoch:5	Loss:14.574	translation_Loss:14.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.35	Hits@10:8.36	Best:4.35
2024-12-29 04:53:17,719: Snapshot:0	Epoch:6	Loss:13.981	translation_Loss:13.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.94	Hits@10:9.89	Best:4.94
2024-12-29 04:53:23,632: Snapshot:0	Epoch:7	Loss:13.402	translation_Loss:13.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:11.27	Best:5.49
2024-12-29 04:53:30,048: Snapshot:0	Epoch:8	Loss:12.837	translation_Loss:12.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.74	Best:6.03
2024-12-29 04:53:36,005: Snapshot:0	Epoch:9	Loss:12.295	translation_Loss:12.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.55	Hits@10:14.16	Best:6.55
2024-12-29 04:53:41,976: Snapshot:0	Epoch:10	Loss:11.76	translation_Loss:11.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.38	Best:7.06
2024-12-29 04:53:48,510: Snapshot:0	Epoch:11	Loss:11.237	translation_Loss:11.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.51	Hits@10:16.59	Best:7.51
2024-12-29 04:53:54,519: Snapshot:0	Epoch:12	Loss:10.735	translation_Loss:10.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.89	Best:7.94
2024-12-29 04:54:00,827: Snapshot:0	Epoch:13	Loss:10.246	translation_Loss:10.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.38	Hits@10:19.02	Best:8.38
2024-12-29 04:54:06,793: Snapshot:0	Epoch:14	Loss:9.775	translation_Loss:9.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.81	Hits@10:20.32	Best:8.81
2024-12-29 04:54:12,860: Snapshot:0	Epoch:15	Loss:9.328	translation_Loss:9.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.21	Hits@10:21.48	Best:9.21
2024-12-29 04:54:19,044: Snapshot:0	Epoch:16	Loss:8.889	translation_Loss:8.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.62	Hits@10:22.58	Best:9.62
2024-12-29 04:54:25,215: Snapshot:0	Epoch:17	Loss:8.466	translation_Loss:8.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.03	Hits@10:23.64	Best:10.03
2024-12-29 04:54:31,584: Snapshot:0	Epoch:18	Loss:8.065	translation_Loss:8.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.42	Hits@10:24.66	Best:10.42
2024-12-29 04:54:37,508: Snapshot:0	Epoch:19	Loss:7.676	translation_Loss:7.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:25.59	Best:10.83
2024-12-29 04:54:43,828: Snapshot:0	Epoch:20	Loss:7.299	translation_Loss:7.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.23	Hits@10:26.46	Best:11.23
2024-12-29 04:54:49,822: Snapshot:0	Epoch:21	Loss:6.949	translation_Loss:6.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.64	Hits@10:27.33	Best:11.64
2024-12-29 04:54:55,817: Snapshot:0	Epoch:22	Loss:6.616	translation_Loss:6.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.05	Hits@10:28.09	Best:12.05
2024-12-29 04:55:02,156: Snapshot:0	Epoch:23	Loss:6.284	translation_Loss:6.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.44	Hits@10:28.73	Best:12.44
2024-12-29 04:55:08,161: Snapshot:0	Epoch:24	Loss:5.975	translation_Loss:5.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.81	Hits@10:29.35	Best:12.81
2024-12-29 04:55:14,494: Snapshot:0	Epoch:25	Loss:5.688	translation_Loss:5.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.15	Hits@10:30.03	Best:13.15
2024-12-29 04:55:20,434: Snapshot:0	Epoch:26	Loss:5.402	translation_Loss:5.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.52	Hits@10:30.61	Best:13.52
2024-12-29 04:55:26,392: Snapshot:0	Epoch:27	Loss:5.131	translation_Loss:5.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.9	Hits@10:31.16	Best:13.9
2024-12-29 04:55:32,902: Snapshot:0	Epoch:28	Loss:4.875	translation_Loss:4.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.25	Hits@10:31.66	Best:14.25
2024-12-29 04:55:38,906: Snapshot:0	Epoch:29	Loss:4.623	translation_Loss:4.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.62	Hits@10:32.1	Best:14.62
2024-12-29 04:55:44,916: Snapshot:0	Epoch:30	Loss:4.397	translation_Loss:4.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.96	Hits@10:32.58	Best:14.96
2024-12-29 04:55:51,436: Snapshot:0	Epoch:31	Loss:4.177	translation_Loss:4.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.29	Hits@10:33.09	Best:15.29
2024-12-29 04:55:57,440: Snapshot:0	Epoch:32	Loss:3.968	translation_Loss:3.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.6	Hits@10:33.51	Best:15.6
2024-12-29 04:56:03,791: Snapshot:0	Epoch:33	Loss:3.752	translation_Loss:3.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.94	Hits@10:33.91	Best:15.94
2024-12-29 04:56:09,730: Snapshot:0	Epoch:34	Loss:3.561	translation_Loss:3.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.26	Hits@10:34.26	Best:16.26
2024-12-29 04:56:15,713: Snapshot:0	Epoch:35	Loss:3.375	translation_Loss:3.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.57	Hits@10:34.6	Best:16.57
2024-12-29 04:56:21,994: Snapshot:0	Epoch:36	Loss:3.197	translation_Loss:3.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.9	Hits@10:34.92	Best:16.9
2024-12-29 04:56:28,025: Snapshot:0	Epoch:37	Loss:3.037	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.2	Hits@10:35.27	Best:17.2
2024-12-29 04:56:34,330: Snapshot:0	Epoch:38	Loss:2.874	translation_Loss:2.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.51	Hits@10:35.53	Best:17.51
2024-12-29 04:56:40,288: Snapshot:0	Epoch:39	Loss:2.719	translation_Loss:2.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.84	Hits@10:35.84	Best:17.84
2024-12-29 04:56:46,289: Snapshot:0	Epoch:40	Loss:2.58	translation_Loss:2.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.13	Hits@10:36.12	Best:18.13
2024-12-29 04:56:52,664: Snapshot:0	Epoch:41	Loss:2.44	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.38	Hits@10:36.42	Best:18.38
2024-12-29 04:56:58,648: Snapshot:0	Epoch:42	Loss:2.309	translation_Loss:2.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.63	Hits@10:36.72	Best:18.63
2024-12-29 04:57:05,089: Snapshot:0	Epoch:43	Loss:2.179	translation_Loss:2.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.88	Hits@10:36.91	Best:18.88
2024-12-29 04:57:11,084: Snapshot:0	Epoch:44	Loss:2.061	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.12	Hits@10:37.13	Best:19.12
2024-12-29 04:57:17,493: Snapshot:0	Epoch:45	Loss:1.948	translation_Loss:1.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.32	Hits@10:37.32	Best:19.32
2024-12-29 04:57:23,407: Snapshot:0	Epoch:46	Loss:1.834	translation_Loss:1.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.58	Hits@10:37.42	Best:19.58
2024-12-29 04:57:29,359: Snapshot:0	Epoch:47	Loss:1.738	translation_Loss:1.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.78	Hits@10:37.59	Best:19.78
2024-12-29 04:57:35,678: Snapshot:0	Epoch:48	Loss:1.647	translation_Loss:1.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.93	Hits@10:37.82	Best:19.93
2024-12-29 04:57:41,719: Snapshot:0	Epoch:49	Loss:1.553	translation_Loss:1.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.12	Hits@10:37.9	Best:20.12
2024-12-29 04:57:48,285: Snapshot:0	Epoch:50	Loss:1.463	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.32	Hits@10:37.98	Best:20.32
2024-12-29 04:57:54,292: Snapshot:0	Epoch:51	Loss:1.383	translation_Loss:1.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.47	Hits@10:38.1	Best:20.47
2024-12-29 04:58:00,274: Snapshot:0	Epoch:52	Loss:1.312	translation_Loss:1.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.67	Hits@10:38.27	Best:20.67
2024-12-29 04:58:06,746: Snapshot:0	Epoch:53	Loss:1.24	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.82	Hits@10:38.32	Best:20.82
2024-12-29 04:58:12,735: Snapshot:0	Epoch:54	Loss:1.17	translation_Loss:1.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.97	Hits@10:38.42	Best:20.97
2024-12-29 04:58:18,774: Snapshot:0	Epoch:55	Loss:1.108	translation_Loss:1.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.16	Hits@10:38.5	Best:21.16
2024-12-29 04:58:25,245: Snapshot:0	Epoch:56	Loss:1.051	translation_Loss:1.051	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.28	Hits@10:38.61	Best:21.28
2024-12-29 04:58:31,197: Snapshot:0	Epoch:57	Loss:0.995	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.39	Hits@10:38.67	Best:21.39
2024-12-29 04:58:37,562: Snapshot:0	Epoch:58	Loss:0.945	translation_Loss:0.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.51	Hits@10:38.77	Best:21.51
2024-12-29 04:58:43,569: Snapshot:0	Epoch:59	Loss:0.893	translation_Loss:0.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.65	Hits@10:38.88	Best:21.65
2024-12-29 04:58:49,750: Snapshot:0	Epoch:60	Loss:0.841	translation_Loss:0.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.77	Hits@10:38.88	Best:21.77
2024-12-29 04:58:56,151: Snapshot:0	Epoch:61	Loss:0.806	translation_Loss:0.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.87	Hits@10:38.92	Best:21.87
2024-12-29 04:59:02,059: Snapshot:0	Epoch:62	Loss:0.763	translation_Loss:0.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.0	Hits@10:39.02	Best:22.0
2024-12-29 04:59:08,402: Snapshot:0	Epoch:63	Loss:0.723	translation_Loss:0.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.13	Hits@10:39.09	Best:22.13
2024-12-29 04:59:14,405: Snapshot:0	Epoch:64	Loss:0.684	translation_Loss:0.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.19	Hits@10:39.07	Best:22.19
2024-12-29 04:59:20,434: Snapshot:0	Epoch:65	Loss:0.651	translation_Loss:0.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.27	Hits@10:39.12	Best:22.27
2024-12-29 04:59:26,758: Snapshot:0	Epoch:66	Loss:0.626	translation_Loss:0.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.36	Hits@10:39.11	Best:22.36
2024-12-29 04:59:32,765: Snapshot:0	Epoch:67	Loss:0.603	translation_Loss:0.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.47	Hits@10:39.19	Best:22.47
2024-12-29 04:59:39,154: Snapshot:0	Epoch:68	Loss:0.564	translation_Loss:0.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.49	Hits@10:39.21	Best:22.49
2024-12-29 04:59:45,313: Snapshot:0	Epoch:69	Loss:0.538	translation_Loss:0.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.56	Hits@10:39.28	Best:22.56
2024-12-29 04:59:51,719: Snapshot:0	Epoch:70	Loss:0.522	translation_Loss:0.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.59	Hits@10:39.27	Best:22.59
2024-12-29 04:59:57,870: Snapshot:0	Epoch:71	Loss:0.493	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:39.26	Best:22.69
2024-12-29 05:00:03,898: Snapshot:0	Epoch:72	Loss:0.471	translation_Loss:0.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.78	Hits@10:39.29	Best:22.78
2024-12-29 05:00:10,595: Snapshot:0	Epoch:73	Loss:0.45	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.85	Hits@10:39.33	Best:22.85
2024-12-29 05:00:16,748: Snapshot:0	Epoch:74	Loss:0.431	translation_Loss:0.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.92	Hits@10:39.39	Best:22.92
2024-12-29 05:00:23,103: Snapshot:0	Epoch:75	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.94	Hits@10:39.48	Best:22.94
2024-12-29 05:00:29,115: Snapshot:0	Epoch:76	Loss:0.397	translation_Loss:0.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.97	Hits@10:39.43	Best:22.97
2024-12-29 05:00:35,146: Snapshot:0	Epoch:77	Loss:0.384	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.0	Hits@10:39.41	Best:23.0
2024-12-29 05:00:41,605: Snapshot:0	Epoch:78	Loss:0.365	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.04	Hits@10:39.52	Best:23.04
2024-12-29 05:00:47,680: Snapshot:0	Epoch:79	Loss:0.351	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:39.52	Best:23.1
2024-12-29 05:00:53,757: Snapshot:0	Epoch:80	Loss:0.342	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.12	Hits@10:39.6	Best:23.12
2024-12-29 05:01:00,231: Snapshot:0	Epoch:81	Loss:0.329	translation_Loss:0.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.21	Hits@10:39.62	Best:23.21
2024-12-29 05:01:06,282: Snapshot:0	Epoch:82	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.19	Hits@10:39.67	Best:23.21
2024-12-29 05:01:12,706: Snapshot:0	Epoch:83	Loss:0.302	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:39.68	Best:23.22
2024-12-29 05:01:18,746: Snapshot:0	Epoch:84	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:39.76	Best:23.24
2024-12-29 05:01:24,702: Snapshot:0	Epoch:85	Loss:0.285	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.29	Hits@10:39.67	Best:23.29
2024-12-29 05:01:31,155: Snapshot:0	Epoch:86	Loss:0.273	translation_Loss:0.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.73	Best:23.36
2024-12-29 05:01:37,078: Snapshot:0	Epoch:87	Loss:0.269	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.35	Hits@10:39.69	Best:23.36
2024-12-29 05:01:43,371: Snapshot:0	Epoch:88	Loss:0.257	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.37	Hits@10:39.71	Best:23.37
2024-12-29 05:01:49,301: Snapshot:0	Epoch:89	Loss:0.25	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.35	Hits@10:39.78	Best:23.37
2024-12-29 05:01:55,196: Snapshot:0	Epoch:90	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.37	Hits@10:39.78	Best:23.37
2024-12-29 05:02:01,447: Snapshot:0	Epoch:91	Loss:0.233	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.4	Hits@10:39.79	Best:23.4
2024-12-29 05:02:07,410: Snapshot:0	Epoch:92	Loss:0.227	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.41	Hits@10:39.8	Best:23.41
2024-12-29 05:02:13,824: Snapshot:0	Epoch:93	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:39.69	Best:23.43
2024-12-29 05:02:19,780: Snapshot:0	Epoch:94	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.47	Hits@10:39.74	Best:23.47
2024-12-29 05:02:26,358: Snapshot:0	Epoch:95	Loss:0.209	translation_Loss:0.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.5	Hits@10:39.77	Best:23.5
2024-12-29 05:02:32,276: Snapshot:0	Epoch:96	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:39.85	Best:23.51
2024-12-29 05:02:38,246: Snapshot:0	Epoch:97	Loss:0.194	translation_Loss:0.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.54	Hits@10:39.89	Best:23.54
2024-12-29 05:02:44,547: Snapshot:0	Epoch:98	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.55	Hits@10:39.82	Best:23.55
2024-12-29 05:02:50,548: Snapshot:0	Epoch:99	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.82	Best:23.58
2024-12-29 05:02:56,852: Snapshot:0	Epoch:100	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.61	Hits@10:39.81	Best:23.61
2024-12-29 05:03:02,802: Snapshot:0	Epoch:101	Loss:0.178	translation_Loss:0.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.62	Hits@10:39.84	Best:23.62
2024-12-29 05:03:08,826: Snapshot:0	Epoch:102	Loss:0.173	translation_Loss:0.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.81	Best:23.62
2024-12-29 05:03:15,331: Snapshot:0	Epoch:103	Loss:0.174	translation_Loss:0.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.62	Hits@10:39.82	Best:23.62
2024-12-29 05:03:21,491: Snapshot:0	Epoch:104	Loss:0.164	translation_Loss:0.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.88	Best:23.63
2024-12-29 05:03:27,566: Snapshot:0	Epoch:105	Loss:0.161	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.92	Best:23.64
2024-12-29 05:03:33,918: Snapshot:0	Epoch:106	Loss:0.158	translation_Loss:0.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.98	Best:23.67
2024-12-29 05:03:39,859: Snapshot:0	Epoch:107	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.96	Best:23.69
2024-12-29 05:03:46,230: Snapshot:0	Epoch:108	Loss:0.152	translation_Loss:0.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.72	Hits@10:39.98	Best:23.72
2024-12-29 05:03:52,169: Snapshot:0	Epoch:109	Loss:0.146	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.77	Hits@10:39.93	Best:23.77
2024-12-29 05:03:58,097: Snapshot:0	Epoch:110	Loss:0.144	translation_Loss:0.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.94	Best:23.77
2024-12-29 05:04:04,343: Snapshot:0	Epoch:111	Loss:0.147	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:40.02	Best:23.77
2024-12-29 05:04:10,279: Early Stopping! Snapshot: 0 Epoch: 112 Best Results: 23.77
2024-12-29 05:04:10,279: Start to training tokens! Snapshot: 0 Epoch: 112 Loss:0.141 MRR:23.75 Best Results: 23.77
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:04:10,280: Snapshot:0	Epoch:112	Loss:0.141	translation_Loss:0.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:40.04	Best:23.77
2024-12-29 05:04:17,077: Snapshot:0	Epoch:113	Loss:57.284	translation_Loss:12.179	multi_layer_Loss:45.105	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:40.04	Best:23.77
2024-12-29 05:04:22,904: End of token training: 0 Epoch: 114 Loss:45.66 MRR:23.75 Best Results: 23.77
2024-12-29 05:04:22,904: Snapshot:0	Epoch:114	Loss:45.66	translation_Loss:12.198	multi_layer_Loss:33.462	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.75	Hits@10:40.04	Best:23.77
2024-12-29 05:04:23,182: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 05:04:25,636: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2313 | 0.1422 | 0.2789 | 0.3264 |  0.3896 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:04:48,473: Snapshot:1	Epoch:0	Loss:8.681	translation_Loss:8.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.027                                                   	MRR:17.35	Hits@10:30.33	Best:17.35
2024-12-29 05:04:54,836: Snapshot:1	Epoch:1	Loss:7.648	translation_Loss:7.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.087                                                   	MRR:18.07	Hits@10:31.27	Best:18.07
2024-12-29 05:05:01,308: Snapshot:1	Epoch:2	Loss:6.92	translation_Loss:6.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:18.57	Hits@10:32.05	Best:18.57
2024-12-29 05:05:07,907: Snapshot:1	Epoch:3	Loss:6.335	translation_Loss:6.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.254                                                   	MRR:18.9	Hits@10:32.62	Best:18.9
2024-12-29 05:05:14,509: Snapshot:1	Epoch:4	Loss:5.842	translation_Loss:5.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.351                                                   	MRR:19.23	Hits@10:33.2	Best:19.23
2024-12-29 05:05:20,967: Snapshot:1	Epoch:5	Loss:5.416	translation_Loss:4.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.451                                                   	MRR:19.62	Hits@10:33.68	Best:19.62
2024-12-29 05:05:27,728: Snapshot:1	Epoch:6	Loss:5.044	translation_Loss:4.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.548                                                   	MRR:19.87	Hits@10:34.23	Best:19.87
2024-12-29 05:05:34,141: Snapshot:1	Epoch:7	Loss:4.715	translation_Loss:4.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.642                                                   	MRR:20.2	Hits@10:34.64	Best:20.2
2024-12-29 05:05:40,571: Snapshot:1	Epoch:8	Loss:4.42	translation_Loss:3.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.729                                                   	MRR:20.45	Hits@10:34.95	Best:20.45
2024-12-29 05:05:47,284: Snapshot:1	Epoch:9	Loss:4.178	translation_Loss:3.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.81                                                   	MRR:20.62	Hits@10:35.26	Best:20.62
2024-12-29 05:05:53,778: Snapshot:1	Epoch:10	Loss:3.958	translation_Loss:3.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.884                                                   	MRR:20.83	Hits@10:35.58	Best:20.83
2024-12-29 05:06:00,718: Snapshot:1	Epoch:11	Loss:3.748	translation_Loss:2.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.95                                                   	MRR:20.99	Hits@10:35.85	Best:20.99
2024-12-29 05:06:07,164: Snapshot:1	Epoch:12	Loss:3.57	translation_Loss:2.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.01                                                   	MRR:21.16	Hits@10:36.05	Best:21.16
2024-12-29 05:06:13,563: Snapshot:1	Epoch:13	Loss:3.43	translation_Loss:2.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.063                                                   	MRR:21.35	Hits@10:36.22	Best:21.35
2024-12-29 05:06:20,343: Snapshot:1	Epoch:14	Loss:3.306	translation_Loss:2.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.11                                                   	MRR:21.46	Hits@10:36.52	Best:21.46
2024-12-29 05:06:26,792: Snapshot:1	Epoch:15	Loss:3.199	translation_Loss:2.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.152                                                   	MRR:21.57	Hits@10:36.67	Best:21.57
2024-12-29 05:06:33,562: Snapshot:1	Epoch:16	Loss:3.102	translation_Loss:1.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.189                                                   	MRR:21.67	Hits@10:36.92	Best:21.67
2024-12-29 05:06:39,963: Snapshot:1	Epoch:17	Loss:3.016	translation_Loss:1.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.221                                                   	MRR:21.75	Hits@10:37.11	Best:21.75
2024-12-29 05:06:46,358: Snapshot:1	Epoch:18	Loss:2.943	translation_Loss:1.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.249                                                   	MRR:21.89	Hits@10:37.24	Best:21.89
2024-12-29 05:06:53,117: Snapshot:1	Epoch:19	Loss:2.898	translation_Loss:1.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.272                                                   	MRR:21.94	Hits@10:37.38	Best:21.94
2024-12-29 05:06:59,627: Snapshot:1	Epoch:20	Loss:2.847	translation_Loss:1.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.293                                                   	MRR:21.98	Hits@10:37.48	Best:21.98
2024-12-29 05:07:06,026: Snapshot:1	Epoch:21	Loss:2.81	translation_Loss:1.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.312                                                   	MRR:22.04	Hits@10:37.57	Best:22.04
2024-12-29 05:07:12,865: Snapshot:1	Epoch:22	Loss:2.773	translation_Loss:1.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.329                                                   	MRR:22.08	Hits@10:37.69	Best:22.08
2024-12-29 05:07:19,272: Snapshot:1	Epoch:23	Loss:2.744	translation_Loss:1.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:22.12	Hits@10:37.68	Best:22.12
2024-12-29 05:07:26,084: Snapshot:1	Epoch:24	Loss:2.724	translation_Loss:1.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.355                                                   	MRR:22.17	Hits@10:37.82	Best:22.17
2024-12-29 05:07:32,456: Snapshot:1	Epoch:25	Loss:2.703	translation_Loss:1.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.367                                                   	MRR:22.25	Hits@10:37.83	Best:22.25
2024-12-29 05:07:38,780: Snapshot:1	Epoch:26	Loss:2.683	translation_Loss:1.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.377                                                   	MRR:22.24	Hits@10:37.84	Best:22.25
2024-12-29 05:07:45,555: Snapshot:1	Epoch:27	Loss:2.65	translation_Loss:1.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.386                                                   	MRR:22.27	Hits@10:37.96	Best:22.27
2024-12-29 05:07:51,913: Snapshot:1	Epoch:28	Loss:2.643	translation_Loss:1.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.394                                                   	MRR:22.3	Hits@10:37.94	Best:22.3
2024-12-29 05:07:58,662: Snapshot:1	Epoch:29	Loss:2.629	translation_Loss:1.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.402                                                   	MRR:22.33	Hits@10:37.97	Best:22.33
2024-12-29 05:08:05,034: Snapshot:1	Epoch:30	Loss:2.615	translation_Loss:1.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.409                                                   	MRR:22.35	Hits@10:38.02	Best:22.35
2024-12-29 05:08:11,381: Snapshot:1	Epoch:31	Loss:2.595	translation_Loss:1.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.415                                                   	MRR:22.37	Hits@10:38.04	Best:22.37
2024-12-29 05:08:18,158: Snapshot:1	Epoch:32	Loss:2.583	translation_Loss:1.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.421                                                   	MRR:22.38	Hits@10:38.0	Best:22.38
2024-12-29 05:08:24,484: Snapshot:1	Epoch:33	Loss:2.574	translation_Loss:1.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.426                                                   	MRR:22.37	Hits@10:38.1	Best:22.38
2024-12-29 05:08:31,266: Snapshot:1	Epoch:34	Loss:2.564	translation_Loss:1.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.43                                                   	MRR:22.39	Hits@10:38.16	Best:22.39
2024-12-29 05:08:37,606: Snapshot:1	Epoch:35	Loss:2.556	translation_Loss:1.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.435                                                   	MRR:22.42	Hits@10:38.12	Best:22.42
2024-12-29 05:08:44,380: Snapshot:1	Epoch:36	Loss:2.552	translation_Loss:1.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.44                                                   	MRR:22.46	Hits@10:38.13	Best:22.46
2024-12-29 05:08:50,774: Snapshot:1	Epoch:37	Loss:2.542	translation_Loss:1.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.443                                                   	MRR:22.44	Hits@10:38.11	Best:22.46
2024-12-29 05:08:57,213: Snapshot:1	Epoch:38	Loss:2.534	translation_Loss:1.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.445                                                   	MRR:22.47	Hits@10:38.15	Best:22.47
2024-12-29 05:09:03,871: Snapshot:1	Epoch:39	Loss:2.529	translation_Loss:1.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.448                                                   	MRR:22.43	Hits@10:38.19	Best:22.47
2024-12-29 05:09:10,232: Snapshot:1	Epoch:40	Loss:2.529	translation_Loss:1.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.451                                                   	MRR:22.4	Hits@10:38.18	Best:22.47
2024-12-29 05:09:16,989: Early Stopping! Snapshot: 1 Epoch: 41 Best Results: 22.47
2024-12-29 05:09:16,989: Start to training tokens! Snapshot: 1 Epoch: 41 Loss:2.524 MRR:22.43 Best Results: 22.47
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:09:16,990: Snapshot:1	Epoch:41	Loss:2.524	translation_Loss:1.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.454                                                   	MRR:22.43	Hits@10:38.14	Best:22.47
2024-12-29 05:09:23,340: Snapshot:1	Epoch:42	Loss:58.035	translation_Loss:13.51	multi_layer_Loss:44.526	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.43	Hits@10:38.14	Best:22.47
2024-12-29 05:09:29,634: End of token training: 1 Epoch: 43 Loss:46.22 MRR:22.43 Best Results: 22.47
2024-12-29 05:09:29,634: Snapshot:1	Epoch:43	Loss:46.22	translation_Loss:13.504	multi_layer_Loss:32.716	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.43	Hits@10:38.14	Best:22.47
2024-12-29 05:09:29,935: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 05:09:35,632: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2459 | 0.153  |  0.29  | 0.3477 |  0.4188 |
|     1      | 0.2253 | 0.1377 | 0.2699 |  0.32  |  0.382  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:09:58,542: Snapshot:2	Epoch:0	Loss:5.485	translation_Loss:5.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.028                                                   	MRR:20.01	Hits@10:35.35	Best:20.01
2024-12-29 05:10:05,301: Snapshot:2	Epoch:1	Loss:4.481	translation_Loss:4.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.085                                                   	MRR:20.54	Hits@10:36.23	Best:20.54
2024-12-29 05:10:11,990: Snapshot:2	Epoch:2	Loss:3.846	translation_Loss:3.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:20.91	Hits@10:36.81	Best:20.91
2024-12-29 05:10:18,584: Snapshot:2	Epoch:3	Loss:3.374	translation_Loss:3.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.218                                                   	MRR:21.11	Hits@10:37.29	Best:21.11
2024-12-29 05:10:25,100: Snapshot:2	Epoch:4	Loss:3.013	translation_Loss:2.725	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.288                                                   	MRR:21.32	Hits@10:37.55	Best:21.32
2024-12-29 05:10:31,674: Snapshot:2	Epoch:5	Loss:2.727	translation_Loss:2.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.356                                                   	MRR:21.53	Hits@10:37.84	Best:21.53
2024-12-29 05:10:38,701: Snapshot:2	Epoch:6	Loss:2.482	translation_Loss:2.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.418                                                   	MRR:21.65	Hits@10:38.16	Best:21.65
2024-12-29 05:10:45,232: Snapshot:2	Epoch:7	Loss:2.288	translation_Loss:1.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:21.74	Hits@10:38.35	Best:21.74
2024-12-29 05:10:52,217: Snapshot:2	Epoch:8	Loss:2.13	translation_Loss:1.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.525                                                   	MRR:21.85	Hits@10:38.47	Best:21.85
2024-12-29 05:10:58,904: Snapshot:2	Epoch:9	Loss:2.001	translation_Loss:1.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.568                                                   	MRR:21.93	Hits@10:38.59	Best:21.93
2024-12-29 05:11:05,396: Snapshot:2	Epoch:10	Loss:1.886	translation_Loss:1.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:21.9	Hits@10:38.65	Best:21.93
2024-12-29 05:11:12,206: Snapshot:2	Epoch:11	Loss:1.799	translation_Loss:1.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.639                                                   	MRR:21.99	Hits@10:38.75	Best:21.99
2024-12-29 05:11:18,750: Snapshot:2	Epoch:12	Loss:1.73	translation_Loss:1.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.667                                                   	MRR:22.05	Hits@10:38.82	Best:22.05
2024-12-29 05:11:25,462: Snapshot:2	Epoch:13	Loss:1.67	translation_Loss:0.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.691                                                   	MRR:22.06	Hits@10:38.96	Best:22.06
2024-12-29 05:11:32,395: Snapshot:2	Epoch:14	Loss:1.621	translation_Loss:0.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.711                                                   	MRR:22.13	Hits@10:39.02	Best:22.13
2024-12-29 05:11:39,034: Snapshot:2	Epoch:15	Loss:1.586	translation_Loss:0.857	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.729                                                   	MRR:22.13	Hits@10:38.98	Best:22.13
2024-12-29 05:11:45,926: Snapshot:2	Epoch:16	Loss:1.554	translation_Loss:0.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.743                                                   	MRR:22.16	Hits@10:38.99	Best:22.16
2024-12-29 05:11:52,460: Snapshot:2	Epoch:17	Loss:1.533	translation_Loss:0.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.756                                                   	MRR:22.13	Hits@10:38.97	Best:22.16
2024-12-29 05:11:58,913: Snapshot:2	Epoch:18	Loss:1.511	translation_Loss:0.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.766                                                   	MRR:22.16	Hits@10:39.06	Best:22.16
2024-12-29 05:12:05,856: Snapshot:2	Epoch:19	Loss:1.487	translation_Loss:0.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.775                                                   	MRR:22.23	Hits@10:39.09	Best:22.23
2024-12-29 05:12:12,436: Snapshot:2	Epoch:20	Loss:1.481	translation_Loss:0.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.783                                                   	MRR:22.21	Hits@10:39.11	Best:22.23
2024-12-29 05:12:19,348: Snapshot:2	Epoch:21	Loss:1.467	translation_Loss:0.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.79                                                   	MRR:22.22	Hits@10:39.21	Best:22.23
2024-12-29 05:12:25,878: Snapshot:2	Epoch:22	Loss:1.461	translation_Loss:0.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.796                                                   	MRR:22.26	Hits@10:39.09	Best:22.26
2024-12-29 05:12:32,262: Snapshot:2	Epoch:23	Loss:1.447	translation_Loss:0.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.801                                                   	MRR:22.25	Hits@10:39.11	Best:22.26
2024-12-29 05:12:39,139: Snapshot:2	Epoch:24	Loss:1.439	translation_Loss:0.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.805                                                   	MRR:22.24	Hits@10:39.15	Best:22.26
2024-12-29 05:12:45,638: Early Stopping! Snapshot: 2 Epoch: 25 Best Results: 22.26
2024-12-29 05:12:45,638: Start to training tokens! Snapshot: 2 Epoch: 25 Loss:1.431 MRR:22.26 Best Results: 22.26
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:12:45,639: Snapshot:2	Epoch:25	Loss:1.431	translation_Loss:0.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:22.26	Hits@10:39.25	Best:22.26
2024-12-29 05:12:52,498: Snapshot:2	Epoch:26	Loss:58.911	translation_Loss:13.308	multi_layer_Loss:45.603	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.26	Hits@10:39.25	Best:22.26
2024-12-29 05:12:58,855: End of token training: 2 Epoch: 27 Loss:47.236 MRR:22.26 Best Results: 22.26
2024-12-29 05:12:58,856: Snapshot:2	Epoch:27	Loss:47.236	translation_Loss:13.305	multi_layer_Loss:33.931	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.26	Hits@10:39.25	Best:22.26
2024-12-29 05:12:59,095: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 05:13:07,233: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2385 | 0.1478 | 0.2796 | 0.3366 |  0.409  |
|     1      | 0.2267 | 0.1395 | 0.2651 |  0.32  |  0.3934 |
|     2      | 0.2242 | 0.1353 | 0.2634 | 0.3197 |  0.3929 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:13:30,405: Snapshot:3	Epoch:0	Loss:2.846	translation_Loss:2.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.027                                                   	MRR:19.3	Hits@10:36.63	Best:19.3
2024-12-29 05:13:37,184: Snapshot:3	Epoch:1	Loss:2.079	translation_Loss:2.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.077                                                   	MRR:19.76	Hits@10:37.59	Best:19.76
2024-12-29 05:13:43,781: Snapshot:3	Epoch:2	Loss:1.686	translation_Loss:1.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.119                                                   	MRR:20.05	Hits@10:38.31	Best:20.05
2024-12-29 05:13:50,432: Snapshot:3	Epoch:3	Loss:1.434	translation_Loss:1.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:20.24	Hits@10:38.71	Best:20.24
2024-12-29 05:13:57,202: Snapshot:3	Epoch:4	Loss:1.254	translation_Loss:1.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:20.33	Hits@10:38.86	Best:20.33
2024-12-29 05:14:03,850: Snapshot:3	Epoch:5	Loss:1.117	translation_Loss:0.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.219                                                   	MRR:20.48	Hits@10:39.01	Best:20.48
2024-12-29 05:14:10,939: Snapshot:3	Epoch:6	Loss:1.017	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.243                                                   	MRR:20.59	Hits@10:39.18	Best:20.59
2024-12-29 05:14:17,623: Snapshot:3	Epoch:7	Loss:0.937	translation_Loss:0.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.264                                                   	MRR:20.66	Hits@10:39.44	Best:20.66
2024-12-29 05:14:24,208: Snapshot:3	Epoch:8	Loss:0.867	translation_Loss:0.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.282                                                   	MRR:20.64	Hits@10:39.58	Best:20.66
2024-12-29 05:14:31,070: Snapshot:3	Epoch:9	Loss:0.826	translation_Loss:0.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.296                                                   	MRR:20.66	Hits@10:39.64	Best:20.66
2024-12-29 05:14:37,679: Snapshot:3	Epoch:10	Loss:0.784	translation_Loss:0.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.309                                                   	MRR:20.73	Hits@10:39.7	Best:20.73
2024-12-29 05:14:44,720: Snapshot:3	Epoch:11	Loss:0.758	translation_Loss:0.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.319                                                   	MRR:20.66	Hits@10:39.68	Best:20.73
2024-12-29 05:14:51,267: Snapshot:3	Epoch:12	Loss:0.735	translation_Loss:0.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:20.68	Hits@10:39.76	Best:20.73
2024-12-29 05:14:57,750: Snapshot:3	Epoch:13	Loss:0.723	translation_Loss:0.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.336                                                   	MRR:20.74	Hits@10:39.77	Best:20.74
2024-12-29 05:15:04,810: Snapshot:3	Epoch:14	Loss:0.71	translation_Loss:0.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:20.81	Hits@10:39.64	Best:20.81
2024-12-29 05:15:11,427: Snapshot:3	Epoch:15	Loss:0.698	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.347                                                   	MRR:20.7	Hits@10:39.53	Best:20.81
2024-12-29 05:15:18,419: Snapshot:3	Epoch:16	Loss:0.686	translation_Loss:0.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:20.69	Hits@10:39.65	Best:20.81
2024-12-29 05:15:24,953: Early Stopping! Snapshot: 3 Epoch: 17 Best Results: 20.81
2024-12-29 05:15:24,953: Start to training tokens! Snapshot: 3 Epoch: 17 Loss:0.68 MRR:20.74 Best Results: 20.81
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:15:24,953: Snapshot:3	Epoch:17	Loss:0.68	translation_Loss:0.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:20.74	Hits@10:39.73	Best:20.81
2024-12-29 05:15:31,355: Snapshot:3	Epoch:18	Loss:57.642	translation_Loss:12.793	multi_layer_Loss:44.849	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.74	Hits@10:39.73	Best:20.81
2024-12-29 05:15:38,215: End of token training: 3 Epoch: 19 Loss:45.556 MRR:20.74 Best Results: 20.81
2024-12-29 05:15:38,216: Snapshot:3	Epoch:19	Loss:45.556	translation_Loss:12.765	multi_layer_Loss:32.79	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.74	Hits@10:39.73	Best:20.81
2024-12-29 05:15:38,459: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 05:15:49,741: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2228 | 0.1368 | 0.2557 | 0.3131 |  0.3885 |
|     1      | 0.2134 | 0.1283 | 0.2469 | 0.3012 |  0.3763 |
|     2      | 0.2134 | 0.1239 | 0.2471 | 0.309  |  0.3898 |
|     3      | 0.2087 | 0.1131 | 0.2437 | 0.3091 |  0.3978 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:16:12,801: Snapshot:4	Epoch:0	Loss:1.718	translation_Loss:1.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.023                                                   	MRR:19.22	Hits@10:42.57	Best:19.22
2024-12-29 05:16:19,414: Snapshot:4	Epoch:1	Loss:1.229	translation_Loss:1.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.057                                                   	MRR:20.61	Hits@10:45.49	Best:20.61
2024-12-29 05:16:26,146: Snapshot:4	Epoch:2	Loss:1.013	translation_Loss:0.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:21.24	Hits@10:46.55	Best:21.24
2024-12-29 05:16:32,869: Snapshot:4	Epoch:3	Loss:0.856	translation_Loss:0.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.077                                                   	MRR:21.57	Hits@10:47.05	Best:21.57
2024-12-29 05:16:39,485: Snapshot:4	Epoch:4	Loss:0.724	translation_Loss:0.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.083                                                   	MRR:21.75	Hits@10:47.44	Best:21.75
2024-12-29 05:16:46,456: Snapshot:4	Epoch:5	Loss:0.618	translation_Loss:0.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.089                                                   	MRR:21.92	Hits@10:47.67	Best:21.92
2024-12-29 05:16:53,104: Snapshot:4	Epoch:6	Loss:0.521	translation_Loss:0.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.094                                                   	MRR:22.08	Hits@10:47.8	Best:22.08
2024-12-29 05:17:00,087: Snapshot:4	Epoch:7	Loss:0.445	translation_Loss:0.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.098                                                   	MRR:22.13	Hits@10:47.98	Best:22.13
2024-12-29 05:17:06,746: Snapshot:4	Epoch:8	Loss:0.378	translation_Loss:0.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.101                                                   	MRR:22.23	Hits@10:48.15	Best:22.23
2024-12-29 05:17:13,812: Snapshot:4	Epoch:9	Loss:0.332	translation_Loss:0.229	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.103                                                   	MRR:22.43	Hits@10:48.47	Best:22.43
2024-12-29 05:17:20,456: Snapshot:4	Epoch:10	Loss:0.299	translation_Loss:0.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:22.59	Hits@10:48.56	Best:22.59
2024-12-29 05:17:27,255: Snapshot:4	Epoch:11	Loss:0.271	translation_Loss:0.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:22.8	Hits@10:48.63	Best:22.8
2024-12-29 05:17:34,280: Snapshot:4	Epoch:12	Loss:0.25	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:22.79	Hits@10:48.71	Best:22.8
2024-12-29 05:17:40,930: Snapshot:4	Epoch:13	Loss:0.236	translation_Loss:0.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:22.87	Hits@10:48.86	Best:22.87
2024-12-29 05:17:48,038: Snapshot:4	Epoch:14	Loss:0.224	translation_Loss:0.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.105                                                   	MRR:22.97	Hits@10:48.79	Best:22.97
2024-12-29 05:17:54,707: Snapshot:4	Epoch:15	Loss:0.218	translation_Loss:0.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:23.11	Hits@10:48.72	Best:23.11
2024-12-29 05:18:01,240: Snapshot:4	Epoch:16	Loss:0.211	translation_Loss:0.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:23.1	Hits@10:48.99	Best:23.11
2024-12-29 05:18:08,213: Snapshot:4	Epoch:17	Loss:0.205	translation_Loss:0.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:23.17	Hits@10:48.94	Best:23.17
2024-12-29 05:18:14,762: Snapshot:4	Epoch:18	Loss:0.2	translation_Loss:0.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:23.11	Hits@10:48.92	Best:23.17
2024-12-29 05:18:21,725: Snapshot:4	Epoch:19	Loss:0.196	translation_Loss:0.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:23.17	Hits@10:48.77	Best:23.17
2024-12-29 05:18:28,283: Early Stopping! Snapshot: 4 Epoch: 20 Best Results: 23.17
2024-12-29 05:18:28,283: Start to training tokens! Snapshot: 4 Epoch: 20 Loss:0.195 MRR:23.1 Best Results: 23.17
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:18:28,283: Snapshot:4	Epoch:20	Loss:0.195	translation_Loss:0.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.104                                                   	MRR:23.1	Hits@10:48.74	Best:23.17
2024-12-29 05:18:34,726: Snapshot:4	Epoch:21	Loss:55.972	translation_Loss:10.881	multi_layer_Loss:45.091	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:48.74	Best:23.17
2024-12-29 05:18:41,714: End of token training: 4 Epoch: 22 Loss:43.856 MRR:23.1 Best Results: 23.17
2024-12-29 05:18:41,714: Snapshot:4	Epoch:22	Loss:43.856	translation_Loss:10.874	multi_layer_Loss:32.982	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.1	Hits@10:48.74	Best:23.17
2024-12-29 05:18:41,973: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 05:18:56,348: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2072 | 0.1249 | 0.236  | 0.2924 |  0.3656 |
|     1      | 0.1957 | 0.1147 | 0.2236 | 0.2787 |  0.3549 |
|     2      | 0.1933 | 0.1078 | 0.222  | 0.2807 |  0.3648 |
|     3      | 0.1897 | 0.0961 | 0.2188 | 0.2863 |  0.3805 |
|     4      | 0.2317 | 0.1068 | 0.2753 | 0.3704 |  0.4886 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 05:18:56,351: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2313 | 0.1422 | 0.2789 | 0.3264 |  0.3896 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2459 | 0.153  |  0.29  | 0.3477 |  0.4188 |
|     1      | 0.2253 | 0.1377 | 0.2699 |  0.32  |  0.382  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2385 | 0.1478 | 0.2796 | 0.3366 |  0.409  |
|     1      | 0.2267 | 0.1395 | 0.2651 |  0.32  |  0.3934 |
|     2      | 0.2242 | 0.1353 | 0.2634 | 0.3197 |  0.3929 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2228 | 0.1368 | 0.2557 | 0.3131 |  0.3885 |
|     1      | 0.2134 | 0.1283 | 0.2469 | 0.3012 |  0.3763 |
|     2      | 0.2134 | 0.1239 | 0.2471 | 0.309  |  0.3898 |
|     3      | 0.2087 | 0.1131 | 0.2437 | 0.3091 |  0.3978 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2072 | 0.1249 | 0.236  | 0.2924 |  0.3656 |
|     1      | 0.1957 | 0.1147 | 0.2236 | 0.2787 |  0.3549 |
|     2      | 0.1933 | 0.1078 | 0.222  | 0.2807 |  0.3648 |
|     3      | 0.1897 | 0.0961 | 0.2188 | 0.2863 |  0.3805 |
|     4      | 0.2317 | 0.1068 | 0.2753 | 0.3704 |  0.4886 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 05:18:56,351: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 711.1845684051514  |   0.231   |    0.142     |    0.279     |      0.39     |
|    1     | 301.2289674282074  |   0.236   |    0.145     |     0.28     |      0.4      |
|    2     | 200.26262187957764 |    0.23   |    0.141     |    0.269     |     0.398     |
|    3     | 147.70884013175964 |   0.215   |    0.126     |    0.248     |     0.388     |
|    4     | 169.2321515083313  |   0.204   |     0.11     |    0.235     |     0.391     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 05:18:56,351: Sum_Training_Time:1529.6171493530273
2024-12-29 05:18:56,351: Every_Training_Time:[711.1845684051514, 301.2289674282074, 200.26262187957764, 147.70884013175964, 169.2321515083313]
2024-12-29 05:18:56,351: Forward transfer: 0.174 Backward transfer: -0.0259
2024-12-29 05:19:33,295: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241229051901/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=500.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 05:19:42,404: Snapshot:0	Epoch:0	Loss:17.871	translation_Loss:17.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-29 05:19:48,647: Snapshot:0	Epoch:1	Loss:17.114	translation_Loss:17.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.51	Hits@10:1.66	Best:1.51
2024-12-29 05:19:54,537: Snapshot:0	Epoch:2	Loss:16.452	translation_Loss:16.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.96	Hits@10:3.03	Best:1.96
2024-12-29 05:20:00,847: Snapshot:0	Epoch:3	Loss:15.815	translation_Loss:15.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.85	Hits@10:5.09	Best:2.85
2024-12-29 05:20:06,922: Snapshot:0	Epoch:4	Loss:15.191	translation_Loss:15.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.68	Hits@10:6.85	Best:3.68
2024-12-29 05:20:12,936: Snapshot:0	Epoch:5	Loss:14.574	translation_Loss:14.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.35	Hits@10:8.36	Best:4.35
2024-12-29 05:20:19,313: Snapshot:0	Epoch:6	Loss:13.981	translation_Loss:13.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.94	Hits@10:9.89	Best:4.94
2024-12-29 05:20:25,207: Snapshot:0	Epoch:7	Loss:13.402	translation_Loss:13.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:11.27	Best:5.49
2024-12-29 05:20:31,383: Snapshot:0	Epoch:8	Loss:12.837	translation_Loss:12.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.74	Best:6.03
2024-12-29 05:20:37,316: Snapshot:0	Epoch:9	Loss:12.295	translation_Loss:12.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.55	Hits@10:14.16	Best:6.55
2024-12-29 05:20:43,197: Snapshot:0	Epoch:10	Loss:11.76	translation_Loss:11.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.38	Best:7.06
2024-12-29 05:20:49,522: Snapshot:0	Epoch:11	Loss:11.237	translation_Loss:11.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.51	Hits@10:16.59	Best:7.51
2024-12-29 05:20:55,447: Snapshot:0	Epoch:12	Loss:10.735	translation_Loss:10.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.88	Best:7.94
2024-12-29 05:21:01,643: Snapshot:0	Epoch:13	Loss:10.246	translation_Loss:10.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.38	Hits@10:19.03	Best:8.38
2024-12-29 05:21:07,535: Snapshot:0	Epoch:14	Loss:9.775	translation_Loss:9.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.81	Hits@10:20.31	Best:8.81
2024-12-29 05:21:13,451: Snapshot:0	Epoch:15	Loss:9.328	translation_Loss:9.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.2	Hits@10:21.48	Best:9.2
2024-12-29 05:21:19,792: Snapshot:0	Epoch:16	Loss:8.889	translation_Loss:8.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.62	Hits@10:22.6	Best:9.62
2024-12-29 05:21:25,701: Snapshot:0	Epoch:17	Loss:8.466	translation_Loss:8.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.03	Hits@10:23.65	Best:10.03
2024-12-29 05:21:31,942: Snapshot:0	Epoch:18	Loss:8.065	translation_Loss:8.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.41	Hits@10:24.67	Best:10.41
2024-12-29 05:21:37,875: Snapshot:0	Epoch:19	Loss:7.676	translation_Loss:7.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:25.61	Best:10.83
2024-12-29 05:21:44,180: Snapshot:0	Epoch:20	Loss:7.299	translation_Loss:7.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.24	Hits@10:26.44	Best:11.24
2024-12-29 05:21:50,108: Snapshot:0	Epoch:21	Loss:6.949	translation_Loss:6.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.64	Hits@10:27.32	Best:11.64
2024-12-29 05:21:56,060: Snapshot:0	Epoch:22	Loss:6.616	translation_Loss:6.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.05	Hits@10:28.08	Best:12.05
2024-12-29 05:22:02,387: Snapshot:0	Epoch:23	Loss:6.284	translation_Loss:6.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.45	Hits@10:28.71	Best:12.45
2024-12-29 05:22:08,306: Snapshot:0	Epoch:24	Loss:5.975	translation_Loss:5.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.81	Hits@10:29.39	Best:12.81
2024-12-29 05:22:14,694: Snapshot:0	Epoch:25	Loss:5.688	translation_Loss:5.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.15	Hits@10:30.0	Best:13.15
2024-12-29 05:22:20,646: Snapshot:0	Epoch:26	Loss:5.402	translation_Loss:5.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.52	Hits@10:30.58	Best:13.52
2024-12-29 05:22:26,602: Snapshot:0	Epoch:27	Loss:5.131	translation_Loss:5.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.9	Hits@10:31.14	Best:13.9
2024-12-29 05:22:32,868: Snapshot:0	Epoch:28	Loss:4.875	translation_Loss:4.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.26	Hits@10:31.67	Best:14.26
2024-12-29 05:22:38,778: Snapshot:0	Epoch:29	Loss:4.623	translation_Loss:4.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.61	Hits@10:32.14	Best:14.61
2024-12-29 05:22:44,714: Snapshot:0	Epoch:30	Loss:4.397	translation_Loss:4.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.97	Hits@10:32.53	Best:14.97
2024-12-29 05:22:51,032: Snapshot:0	Epoch:31	Loss:4.177	translation_Loss:4.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.3	Hits@10:33.06	Best:15.3
2024-12-29 05:22:56,940: Snapshot:0	Epoch:32	Loss:3.969	translation_Loss:3.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.61	Hits@10:33.49	Best:15.61
2024-12-29 05:23:03,219: Snapshot:0	Epoch:33	Loss:3.752	translation_Loss:3.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.94	Hits@10:33.91	Best:15.94
2024-12-29 05:23:09,081: Snapshot:0	Epoch:34	Loss:3.561	translation_Loss:3.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.25	Hits@10:34.28	Best:16.25
2024-12-29 05:23:15,105: Snapshot:0	Epoch:35	Loss:3.375	translation_Loss:3.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.57	Hits@10:34.57	Best:16.57
2024-12-29 05:23:21,394: Snapshot:0	Epoch:36	Loss:3.197	translation_Loss:3.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.89	Hits@10:34.9	Best:16.89
2024-12-29 05:23:27,267: Snapshot:0	Epoch:37	Loss:3.037	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.2	Hits@10:35.22	Best:17.2
2024-12-29 05:23:33,567: Snapshot:0	Epoch:38	Loss:2.874	translation_Loss:2.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.5	Hits@10:35.56	Best:17.5
2024-12-29 05:23:39,482: Snapshot:0	Epoch:39	Loss:2.719	translation_Loss:2.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.83	Hits@10:35.86	Best:17.83
2024-12-29 05:23:45,407: Snapshot:0	Epoch:40	Loss:2.579	translation_Loss:2.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.1	Hits@10:36.14	Best:18.1
2024-12-29 05:23:51,707: Snapshot:0	Epoch:41	Loss:2.44	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.38	Hits@10:36.46	Best:18.38
2024-12-29 05:23:57,621: Snapshot:0	Epoch:42	Loss:2.309	translation_Loss:2.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.61	Hits@10:36.7	Best:18.61
2024-12-29 05:24:03,959: Snapshot:0	Epoch:43	Loss:2.179	translation_Loss:2.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.86	Hits@10:36.9	Best:18.86
2024-12-29 05:24:09,884: Snapshot:0	Epoch:44	Loss:2.061	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.1	Hits@10:37.06	Best:19.1
2024-12-29 05:24:16,209: Snapshot:0	Epoch:45	Loss:1.948	translation_Loss:1.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.31	Hits@10:37.29	Best:19.31
2024-12-29 05:24:22,105: Snapshot:0	Epoch:46	Loss:1.834	translation_Loss:1.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.55	Hits@10:37.39	Best:19.55
2024-12-29 05:24:27,992: Snapshot:0	Epoch:47	Loss:1.738	translation_Loss:1.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.76	Hits@10:37.58	Best:19.76
2024-12-29 05:24:34,233: Snapshot:0	Epoch:48	Loss:1.648	translation_Loss:1.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.95	Hits@10:37.8	Best:19.95
2024-12-29 05:24:40,189: Snapshot:0	Epoch:49	Loss:1.553	translation_Loss:1.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.14	Hits@10:37.88	Best:20.14
2024-12-29 05:24:46,487: Snapshot:0	Epoch:50	Loss:1.463	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.34	Hits@10:37.96	Best:20.34
2024-12-29 05:24:52,424: Snapshot:0	Epoch:51	Loss:1.383	translation_Loss:1.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.52	Hits@10:38.09	Best:20.52
2024-12-29 05:24:58,327: Snapshot:0	Epoch:52	Loss:1.312	translation_Loss:1.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.68	Hits@10:38.21	Best:20.68
2024-12-29 05:25:04,657: Snapshot:0	Epoch:53	Loss:1.24	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.82	Hits@10:38.29	Best:20.82
2024-12-29 05:25:10,639: Snapshot:0	Epoch:54	Loss:1.17	translation_Loss:1.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.0	Hits@10:38.46	Best:21.0
2024-12-29 05:25:16,562: Snapshot:0	Epoch:55	Loss:1.107	translation_Loss:1.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.15	Hits@10:38.51	Best:21.15
2024-12-29 05:25:22,871: Snapshot:0	Epoch:56	Loss:1.05	translation_Loss:1.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.28	Hits@10:38.61	Best:21.28
2024-12-29 05:25:28,802: Snapshot:0	Epoch:57	Loss:0.995	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.39	Hits@10:38.69	Best:21.39
2024-12-29 05:25:35,072: Snapshot:0	Epoch:58	Loss:0.944	translation_Loss:0.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.52	Hits@10:38.76	Best:21.52
2024-12-29 05:25:40,976: Snapshot:0	Epoch:59	Loss:0.892	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.65	Hits@10:38.81	Best:21.65
2024-12-29 05:25:46,966: Snapshot:0	Epoch:60	Loss:0.841	translation_Loss:0.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.77	Hits@10:38.84	Best:21.77
2024-12-29 05:25:53,171: Snapshot:0	Epoch:61	Loss:0.806	translation_Loss:0.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.84	Hits@10:38.97	Best:21.84
2024-12-29 05:25:59,143: Snapshot:0	Epoch:62	Loss:0.763	translation_Loss:0.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.98	Hits@10:39.0	Best:21.98
2024-12-29 05:26:05,414: Snapshot:0	Epoch:63	Loss:0.723	translation_Loss:0.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.11	Hits@10:39.08	Best:22.11
2024-12-29 05:26:11,331: Snapshot:0	Epoch:64	Loss:0.684	translation_Loss:0.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.15	Hits@10:39.08	Best:22.15
2024-12-29 05:26:17,251: Snapshot:0	Epoch:65	Loss:0.652	translation_Loss:0.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.28	Hits@10:39.12	Best:22.28
2024-12-29 05:26:23,692: Snapshot:0	Epoch:66	Loss:0.626	translation_Loss:0.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.33	Hits@10:39.21	Best:22.33
2024-12-29 05:26:29,672: Snapshot:0	Epoch:67	Loss:0.603	translation_Loss:0.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.44	Hits@10:39.17	Best:22.44
2024-12-29 05:26:35,995: Snapshot:0	Epoch:68	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.52	Hits@10:39.23	Best:22.52
2024-12-29 05:26:41,899: Snapshot:0	Epoch:69	Loss:0.538	translation_Loss:0.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.56	Hits@10:39.27	Best:22.56
2024-12-29 05:26:48,161: Snapshot:0	Epoch:70	Loss:0.521	translation_Loss:0.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.62	Hits@10:39.27	Best:22.62
2024-12-29 05:26:54,064: Snapshot:0	Epoch:71	Loss:0.492	translation_Loss:0.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.7	Hits@10:39.27	Best:22.7
2024-12-29 05:26:59,961: Snapshot:0	Epoch:72	Loss:0.471	translation_Loss:0.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.78	Hits@10:39.34	Best:22.78
2024-12-29 05:27:06,208: Snapshot:0	Epoch:73	Loss:0.45	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.85	Hits@10:39.33	Best:22.85
2024-12-29 05:27:12,226: Snapshot:0	Epoch:74	Loss:0.431	translation_Loss:0.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.89	Hits@10:39.38	Best:22.89
2024-12-29 05:27:18,659: Snapshot:0	Epoch:75	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.92	Hits@10:39.41	Best:22.92
2024-12-29 05:27:24,572: Snapshot:0	Epoch:76	Loss:0.397	translation_Loss:0.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.98	Hits@10:39.42	Best:22.98
2024-12-29 05:27:30,470: Snapshot:0	Epoch:77	Loss:0.384	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.02	Hits@10:39.45	Best:23.02
2024-12-29 05:27:36,783: Snapshot:0	Epoch:78	Loss:0.365	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.04	Hits@10:39.5	Best:23.04
2024-12-29 05:27:42,712: Snapshot:0	Epoch:79	Loss:0.351	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.09	Hits@10:39.52	Best:23.09
2024-12-29 05:27:48,671: Snapshot:0	Epoch:80	Loss:0.342	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.15	Hits@10:39.6	Best:23.15
2024-12-29 05:27:54,897: Snapshot:0	Epoch:81	Loss:0.329	translation_Loss:0.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:39.55	Best:23.24
2024-12-29 05:28:00,798: Snapshot:0	Epoch:82	Loss:0.317	translation_Loss:0.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.2	Hits@10:39.69	Best:23.24
2024-12-29 05:28:07,026: Snapshot:0	Epoch:83	Loss:0.302	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:39.66	Best:23.24
2024-12-29 05:28:12,975: Snapshot:0	Epoch:84	Loss:0.291	translation_Loss:0.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.25	Hits@10:39.69	Best:23.25
2024-12-29 05:28:18,928: Snapshot:0	Epoch:85	Loss:0.285	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:39.63	Best:23.25
2024-12-29 05:28:25,241: Snapshot:0	Epoch:86	Loss:0.273	translation_Loss:0.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.32	Hits@10:39.7	Best:23.32
2024-12-29 05:28:31,194: Snapshot:0	Epoch:87	Loss:0.268	translation_Loss:0.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.33	Hits@10:39.66	Best:23.33
2024-12-29 05:28:37,487: Snapshot:0	Epoch:88	Loss:0.257	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.33	Hits@10:39.67	Best:23.33
2024-12-29 05:28:43,366: Snapshot:0	Epoch:89	Loss:0.25	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.35	Hits@10:39.73	Best:23.35
2024-12-29 05:28:49,329: Snapshot:0	Epoch:90	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.37	Hits@10:39.71	Best:23.37
2024-12-29 05:28:55,581: Snapshot:0	Epoch:91	Loss:0.232	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.4	Hits@10:39.75	Best:23.4
2024-12-29 05:29:01,478: Snapshot:0	Epoch:92	Loss:0.227	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.4	Hits@10:39.74	Best:23.4
2024-12-29 05:29:07,713: Snapshot:0	Epoch:93	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:39.73	Best:23.43
2024-12-29 05:29:13,627: Snapshot:0	Epoch:94	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.46	Hits@10:39.75	Best:23.46
2024-12-29 05:29:19,995: Snapshot:0	Epoch:95	Loss:0.209	translation_Loss:0.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.49	Hits@10:39.76	Best:23.49
2024-12-29 05:29:25,918: Snapshot:0	Epoch:96	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:39.77	Best:23.51
2024-12-29 05:29:31,897: Snapshot:0	Epoch:97	Loss:0.194	translation_Loss:0.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.52	Hits@10:39.81	Best:23.52
2024-12-29 05:29:38,234: Snapshot:0	Epoch:98	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.56	Hits@10:39.79	Best:23.56
2024-12-29 05:29:44,232: Snapshot:0	Epoch:99	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.59	Hits@10:39.83	Best:23.59
2024-12-29 05:29:50,574: Snapshot:0	Epoch:100	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.6	Hits@10:39.83	Best:23.6
2024-12-29 05:29:56,444: Snapshot:0	Epoch:101	Loss:0.177	translation_Loss:0.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.6	Hits@10:39.8	Best:23.6
2024-12-29 05:30:02,384: Snapshot:0	Epoch:102	Loss:0.173	translation_Loss:0.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.8	Best:23.64
2024-12-29 05:30:08,854: Snapshot:0	Epoch:103	Loss:0.174	translation_Loss:0.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.88	Best:23.64
2024-12-29 05:30:14,783: Snapshot:0	Epoch:104	Loss:0.164	translation_Loss:0.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.93	Best:23.65
2024-12-29 05:30:20,712: Snapshot:0	Epoch:105	Loss:0.161	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:39.9	Best:23.66
2024-12-29 05:30:27,010: Snapshot:0	Epoch:106	Loss:0.157	translation_Loss:0.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:39.92	Best:23.66
2024-12-29 05:30:32,910: Snapshot:0	Epoch:107	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.72	Hits@10:39.9	Best:23.72
2024-12-29 05:30:39,349: Snapshot:0	Epoch:108	Loss:0.152	translation_Loss:0.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.73	Hits@10:39.92	Best:23.73
2024-12-29 05:30:45,355: Snapshot:0	Epoch:109	Loss:0.146	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:39.94	Best:23.75
2024-12-29 05:30:51,310: Snapshot:0	Epoch:110	Loss:0.143	translation_Loss:0.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.99	Best:23.76
2024-12-29 05:30:57,663: Snapshot:0	Epoch:111	Loss:0.147	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:40.03	Best:23.76
2024-12-29 05:31:03,736: Snapshot:0	Epoch:112	Loss:0.141	translation_Loss:0.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.79	Hits@10:39.95	Best:23.79
2024-12-29 05:31:10,263: Snapshot:0	Epoch:113	Loss:0.137	translation_Loss:0.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.81	Hits@10:40.0	Best:23.81
2024-12-29 05:31:16,266: Snapshot:0	Epoch:114	Loss:0.136	translation_Loss:0.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.82	Hits@10:39.96	Best:23.82
2024-12-29 05:31:22,240: Snapshot:0	Epoch:115	Loss:0.129	translation_Loss:0.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.84	Hits@10:40.0	Best:23.84
2024-12-29 05:31:28,463: Snapshot:0	Epoch:116	Loss:0.131	translation_Loss:0.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.86	Hits@10:40.0	Best:23.86
2024-12-29 05:31:34,396: Snapshot:0	Epoch:117	Loss:0.126	translation_Loss:0.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.81	Hits@10:40.02	Best:23.86
2024-12-29 05:31:40,654: Snapshot:0	Epoch:118	Loss:0.127	translation_Loss:0.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.85	Hits@10:40.04	Best:23.86
2024-12-29 05:31:46,569: Snapshot:0	Epoch:119	Loss:0.125	translation_Loss:0.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:40.12	Best:23.88
2024-12-29 05:31:53,005: Snapshot:0	Epoch:120	Loss:0.119	translation_Loss:0.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:40.09	Best:23.89
2024-12-29 05:31:58,914: Snapshot:0	Epoch:121	Loss:0.118	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:40.1	Best:23.89
2024-12-29 05:32:04,759: Snapshot:0	Epoch:122	Loss:0.113	translation_Loss:0.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:40.11	Best:23.89
2024-12-29 05:32:11,033: Early Stopping! Snapshot: 0 Epoch: 123 Best Results: 23.89
2024-12-29 05:32:11,033: Start to training tokens! Snapshot: 0 Epoch: 123 Loss:0.118 MRR:23.89 Best Results: 23.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:32:11,034: Snapshot:0	Epoch:123	Loss:0.118	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:40.04	Best:23.89
2024-12-29 05:32:17,446: Snapshot:0	Epoch:124	Loss:57.298	translation_Loss:12.193	multi_layer_Loss:45.105	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:40.04	Best:23.89
2024-12-29 05:32:23,552: End of token training: 0 Epoch: 125 Loss:45.661 MRR:23.89 Best Results: 23.89
2024-12-29 05:32:23,552: Snapshot:0	Epoch:125	Loss:45.661	translation_Loss:12.2	multi_layer_Loss:33.462	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.89	Hits@10:40.04	Best:23.89
2024-12-29 05:32:23,798: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 05:32:25,988: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2327 | 0.1441 | 0.2789 | 0.3274 |  0.3893 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:32:48,706: Snapshot:1	Epoch:0	Loss:8.506	translation_Loss:8.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.007                                                   	MRR:17.4	Hits@10:30.33	Best:17.4
2024-12-29 05:32:55,053: Snapshot:1	Epoch:1	Loss:7.382	translation_Loss:7.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.028                                                   	MRR:18.2	Hits@10:31.46	Best:18.2
2024-12-29 05:33:01,385: Snapshot:1	Epoch:2	Loss:6.541	translation_Loss:6.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.054                                                   	MRR:18.77	Hits@10:32.19	Best:18.77
2024-12-29 05:33:07,784: Snapshot:1	Epoch:3	Loss:5.809	translation_Loss:5.727	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.082                                                   	MRR:19.27	Hits@10:32.94	Best:19.27
2024-12-29 05:33:14,501: Snapshot:1	Epoch:4	Loss:5.177	translation_Loss:5.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.111                                                   	MRR:19.68	Hits@10:33.63	Best:19.68
2024-12-29 05:33:20,936: Snapshot:1	Epoch:5	Loss:4.589	translation_Loss:4.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.141                                                   	MRR:20.06	Hits@10:34.3	Best:20.06
2024-12-29 05:33:27,258: Snapshot:1	Epoch:6	Loss:4.076	translation_Loss:3.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:20.48	Hits@10:34.83	Best:20.48
2024-12-29 05:33:34,050: Snapshot:1	Epoch:7	Loss:3.597	translation_Loss:3.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.2                                                   	MRR:20.8	Hits@10:35.33	Best:20.8
2024-12-29 05:33:40,393: Snapshot:1	Epoch:8	Loss:3.204	translation_Loss:2.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:21.16	Hits@10:35.8	Best:21.16
2024-12-29 05:33:46,719: Snapshot:1	Epoch:9	Loss:2.829	translation_Loss:2.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:21.49	Hits@10:36.13	Best:21.49
2024-12-29 05:33:53,435: Snapshot:1	Epoch:10	Loss:2.531	translation_Loss:2.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.282                                                   	MRR:21.67	Hits@10:36.6	Best:21.67
2024-12-29 05:33:59,928: Snapshot:1	Epoch:11	Loss:2.245	translation_Loss:1.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.305                                                   	MRR:21.88	Hits@10:36.72	Best:21.88
2024-12-29 05:34:06,596: Snapshot:1	Epoch:12	Loss:2.024	translation_Loss:1.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.327                                                   	MRR:22.04	Hits@10:36.99	Best:22.04
2024-12-29 05:34:13,066: Snapshot:1	Epoch:13	Loss:1.827	translation_Loss:1.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:22.21	Hits@10:37.22	Best:22.21
2024-12-29 05:34:19,444: Snapshot:1	Epoch:14	Loss:1.661	translation_Loss:1.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.363                                                   	MRR:22.37	Hits@10:37.35	Best:22.37
2024-12-29 05:34:26,223: Snapshot:1	Epoch:15	Loss:1.519	translation_Loss:1.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.378                                                   	MRR:22.54	Hits@10:37.67	Best:22.54
2024-12-29 05:34:32,649: Snapshot:1	Epoch:16	Loss:1.413	translation_Loss:1.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:22.63	Hits@10:37.69	Best:22.63
2024-12-29 05:34:39,410: Snapshot:1	Epoch:17	Loss:1.318	translation_Loss:0.915	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.403                                                   	MRR:22.68	Hits@10:37.8	Best:22.68
2024-12-29 05:34:45,707: Snapshot:1	Epoch:18	Loss:1.245	translation_Loss:0.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.413                                                   	MRR:22.75	Hits@10:37.9	Best:22.75
2024-12-29 05:34:52,072: Snapshot:1	Epoch:19	Loss:1.177	translation_Loss:0.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.422                                                   	MRR:22.78	Hits@10:38.06	Best:22.78
2024-12-29 05:34:58,699: Snapshot:1	Epoch:20	Loss:1.128	translation_Loss:0.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:22.84	Hits@10:38.16	Best:22.84
2024-12-29 05:35:04,994: Snapshot:1	Epoch:21	Loss:1.083	translation_Loss:0.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.436                                                   	MRR:22.88	Hits@10:38.29	Best:22.88
2024-12-29 05:35:11,734: Snapshot:1	Epoch:22	Loss:1.049	translation_Loss:0.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.442                                                   	MRR:22.9	Hits@10:38.24	Best:22.9
2024-12-29 05:35:18,070: Snapshot:1	Epoch:23	Loss:1.015	translation_Loss:0.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.448                                                   	MRR:22.91	Hits@10:38.2	Best:22.91
2024-12-29 05:35:24,339: Snapshot:1	Epoch:24	Loss:0.989	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.453                                                   	MRR:22.88	Hits@10:38.22	Best:22.91
2024-12-29 05:35:30,964: Snapshot:1	Epoch:25	Loss:0.965	translation_Loss:0.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.457                                                   	MRR:22.92	Hits@10:38.3	Best:22.92
2024-12-29 05:35:37,380: Snapshot:1	Epoch:26	Loss:0.944	translation_Loss:0.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.461                                                   	MRR:22.96	Hits@10:38.25	Best:22.96
2024-12-29 05:35:44,221: Snapshot:1	Epoch:27	Loss:0.926	translation_Loss:0.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.465                                                   	MRR:22.96	Hits@10:38.22	Best:22.96
2024-12-29 05:35:50,538: Snapshot:1	Epoch:28	Loss:0.903	translation_Loss:0.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.468                                                   	MRR:22.98	Hits@10:38.32	Best:22.98
2024-12-29 05:35:56,864: Snapshot:1	Epoch:29	Loss:0.892	translation_Loss:0.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.471                                                   	MRR:23.03	Hits@10:38.28	Best:23.03
2024-12-29 05:36:03,580: Snapshot:1	Epoch:30	Loss:0.88	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.473                                                   	MRR:22.98	Hits@10:38.33	Best:23.03
2024-12-29 05:36:09,912: Snapshot:1	Epoch:31	Loss:0.866	translation_Loss:0.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:23.0	Hits@10:38.44	Best:23.03
2024-12-29 05:36:16,513: Early Stopping! Snapshot: 1 Epoch: 32 Best Results: 23.03
2024-12-29 05:36:16,513: Start to training tokens! Snapshot: 1 Epoch: 32 Loss:0.858 MRR:22.95 Best Results: 23.03
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:36:16,514: Snapshot:1	Epoch:32	Loss:0.858	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.477                                                   	MRR:22.95	Hits@10:38.41	Best:23.03
2024-12-29 05:36:22,794: Snapshot:1	Epoch:33	Loss:57.786	translation_Loss:13.26	multi_layer_Loss:44.526	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.95	Hits@10:38.41	Best:23.03
2024-12-29 05:36:29,044: End of token training: 1 Epoch: 34 Loss:45.963 MRR:22.95 Best Results: 23.03
2024-12-29 05:36:29,044: Snapshot:1	Epoch:34	Loss:45.963	translation_Loss:13.247	multi_layer_Loss:32.716	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.95	Hits@10:38.41	Best:23.03
2024-12-29 05:36:29,284: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 05:36:34,875: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1495 | 0.2832 | 0.3391 |  0.4106 |
|     1      |  0.23  | 0.1449 | 0.2712 | 0.321  |  0.3836 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:36:57,736: Snapshot:2	Epoch:0	Loss:4.909	translation_Loss:4.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.007                                                   	MRR:20.33	Hits@10:35.65	Best:20.33
2024-12-29 05:37:04,341: Snapshot:2	Epoch:1	Loss:3.876	translation_Loss:3.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.026                                                   	MRR:20.96	Hits@10:36.77	Best:20.96
2024-12-29 05:37:10,847: Snapshot:2	Epoch:2	Loss:3.2	translation_Loss:3.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.048                                                   	MRR:21.33	Hits@10:37.32	Best:21.33
2024-12-29 05:37:17,309: Snapshot:2	Epoch:3	Loss:2.671	translation_Loss:2.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.068                                                   	MRR:21.62	Hits@10:37.63	Best:21.62
2024-12-29 05:37:23,778: Snapshot:2	Epoch:4	Loss:2.247	translation_Loss:2.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.086                                                   	MRR:21.85	Hits@10:38.14	Best:21.85
2024-12-29 05:37:30,236: Snapshot:2	Epoch:5	Loss:1.9	translation_Loss:1.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.103                                                   	MRR:22.0	Hits@10:38.34	Best:22.0
2024-12-29 05:37:37,203: Snapshot:2	Epoch:6	Loss:1.628	translation_Loss:1.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.119                                                   	MRR:22.15	Hits@10:38.65	Best:22.15
2024-12-29 05:37:43,832: Snapshot:2	Epoch:7	Loss:1.387	translation_Loss:1.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.133                                                   	MRR:22.28	Hits@10:38.71	Best:22.28
2024-12-29 05:37:50,836: Snapshot:2	Epoch:8	Loss:1.205	translation_Loss:1.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.146                                                   	MRR:22.35	Hits@10:38.82	Best:22.35
2024-12-29 05:37:57,261: Snapshot:2	Epoch:9	Loss:1.038	translation_Loss:0.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:22.36	Hits@10:38.84	Best:22.36
2024-12-29 05:38:03,714: Snapshot:2	Epoch:10	Loss:0.927	translation_Loss:0.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:22.39	Hits@10:38.9	Best:22.39
2024-12-29 05:38:10,740: Snapshot:2	Epoch:11	Loss:0.822	translation_Loss:0.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:22.44	Hits@10:38.93	Best:22.44
2024-12-29 05:38:17,116: Snapshot:2	Epoch:12	Loss:0.747	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.182                                                   	MRR:22.45	Hits@10:38.94	Best:22.45
2024-12-29 05:38:23,800: Snapshot:2	Epoch:13	Loss:0.685	translation_Loss:0.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:22.42	Hits@10:38.95	Best:22.45
2024-12-29 05:38:30,080: Snapshot:2	Epoch:14	Loss:0.638	translation_Loss:0.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:22.45	Hits@10:39.06	Best:22.45
2024-12-29 05:38:36,497: Early Stopping! Snapshot: 2 Epoch: 15 Best Results: 22.45
2024-12-29 05:38:36,497: Start to training tokens! Snapshot: 2 Epoch: 15 Loss:0.599 MRR:22.4 Best Results: 22.45
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:38:36,498: Snapshot:2	Epoch:15	Loss:0.599	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.198                                                   	MRR:22.4	Hits@10:39.06	Best:22.45
2024-12-29 05:38:43,168: Snapshot:2	Epoch:16	Loss:58.958	translation_Loss:13.355	multi_layer_Loss:45.603	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.4	Hits@10:39.06	Best:22.45
2024-12-29 05:38:49,570: End of token training: 2 Epoch: 17 Loss:47.288 MRR:22.4 Best Results: 22.45
2024-12-29 05:38:49,571: Snapshot:2	Epoch:17	Loss:47.288	translation_Loss:13.357	multi_layer_Loss:33.931	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.4	Hits@10:39.06	Best:22.45
2024-12-29 05:38:49,817: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 05:38:58,293: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2276 | 0.1395 | 0.2663 | 0.3201 |  0.3944 |
|     1      | 0.2262 |  0.14  | 0.2631 | 0.3183 |  0.3914 |
|     2      | 0.2269 | 0.1401 | 0.2635 | 0.3186 |  0.3931 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:39:21,182: Snapshot:3	Epoch:0	Loss:2.525	translation_Loss:2.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.006                                                   	MRR:19.19	Hits@10:36.51	Best:19.19
2024-12-29 05:39:27,654: Snapshot:3	Epoch:1	Loss:1.785	translation_Loss:1.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.022                                                   	MRR:19.79	Hits@10:37.48	Best:19.79
2024-12-29 05:39:34,143: Snapshot:3	Epoch:2	Loss:1.366	translation_Loss:1.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.036                                                   	MRR:20.11	Hits@10:38.15	Best:20.11
2024-12-29 05:39:40,801: Snapshot:3	Epoch:3	Loss:1.086	translation_Loss:1.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.048                                                   	MRR:20.33	Hits@10:38.63	Best:20.33
2024-12-29 05:39:47,384: Snapshot:3	Epoch:4	Loss:0.871	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.057                                                   	MRR:20.5	Hits@10:38.91	Best:20.5
2024-12-29 05:39:54,254: Snapshot:3	Epoch:5	Loss:0.722	translation_Loss:0.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.066                                                   	MRR:20.65	Hits@10:39.18	Best:20.65
2024-12-29 05:40:00,824: Snapshot:3	Epoch:6	Loss:0.603	translation_Loss:0.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.073                                                   	MRR:20.76	Hits@10:39.32	Best:20.76
2024-12-29 05:40:07,895: Snapshot:3	Epoch:7	Loss:0.516	translation_Loss:0.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.078                                                   	MRR:20.81	Hits@10:39.36	Best:20.81
2024-12-29 05:40:14,447: Snapshot:3	Epoch:8	Loss:0.439	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.083                                                   	MRR:20.79	Hits@10:39.4	Best:20.81
2024-12-29 05:40:20,953: Snapshot:3	Epoch:9	Loss:0.399	translation_Loss:0.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.087                                                   	MRR:20.83	Hits@10:39.4	Best:20.83
2024-12-29 05:40:27,934: Snapshot:3	Epoch:10	Loss:0.354	translation_Loss:0.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.091                                                   	MRR:20.8	Hits@10:39.49	Best:20.83
2024-12-29 05:40:34,438: Snapshot:3	Epoch:11	Loss:0.321	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.093                                                   	MRR:20.8	Hits@10:39.54	Best:20.83
2024-12-29 05:40:41,245: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 20.83
2024-12-29 05:40:41,246: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:0.297 MRR:20.8 Best Results: 20.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:40:41,246: Snapshot:3	Epoch:12	Loss:0.297	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.095                                                   	MRR:20.8	Hits@10:39.55	Best:20.83
2024-12-29 05:40:47,751: Snapshot:3	Epoch:13	Loss:57.701	translation_Loss:12.853	multi_layer_Loss:44.849	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.8	Hits@10:39.55	Best:20.83
2024-12-29 05:40:54,527: End of token training: 3 Epoch: 14 Loss:45.65 MRR:20.8 Best Results: 20.83
2024-12-29 05:40:54,527: Snapshot:3	Epoch:14	Loss:45.65	translation_Loss:12.86	multi_layer_Loss:32.79	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.8	Hits@10:39.55	Best:20.83
2024-12-29 05:40:54,771: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 05:41:05,973: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2057 | 0.1238 | 0.2376 | 0.2898 |  0.3623 |
|     1      | 0.207  | 0.125  | 0.2371 | 0.2909 |  0.3657 |
|     2      | 0.2106 | 0.1235 | 0.2416 | 0.3018 |  0.3854 |
|     3      | 0.2088 | 0.1138 | 0.245  | 0.309  |  0.3974 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:41:29,017: Snapshot:4	Epoch:0	Loss:1.543	translation_Loss:1.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.005                                                   	MRR:19.12	Hits@10:42.07	Best:19.12
2024-12-29 05:41:35,743: Snapshot:4	Epoch:1	Loss:1.088	translation_Loss:1.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.017                                                   	MRR:20.68	Hits@10:44.73	Best:20.68
2024-12-29 05:41:42,300: Snapshot:4	Epoch:2	Loss:0.866	translation_Loss:0.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.026                                                   	MRR:21.4	Hits@10:46.01	Best:21.4
2024-12-29 05:41:48,738: Snapshot:4	Epoch:3	Loss:0.696	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.032                                                   	MRR:21.74	Hits@10:46.7	Best:21.74
2024-12-29 05:41:55,251: Snapshot:4	Epoch:4	Loss:0.561	translation_Loss:0.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.037                                                   	MRR:21.99	Hits@10:47.16	Best:21.99
2024-12-29 05:42:01,761: Snapshot:4	Epoch:5	Loss:0.446	translation_Loss:0.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.042                                                   	MRR:22.33	Hits@10:47.45	Best:22.33
2024-12-29 05:42:08,705: Snapshot:4	Epoch:6	Loss:0.355	translation_Loss:0.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.045                                                   	MRR:22.57	Hits@10:47.83	Best:22.57
2024-12-29 05:42:15,379: Snapshot:4	Epoch:7	Loss:0.282	translation_Loss:0.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.048                                                   	MRR:22.75	Hits@10:47.97	Best:22.75
2024-12-29 05:42:21,952: Snapshot:4	Epoch:8	Loss:0.232	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.049                                                   	MRR:22.94	Hits@10:48.0	Best:22.94
2024-12-29 05:42:29,028: Snapshot:4	Epoch:9	Loss:0.19	translation_Loss:0.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.049                                                   	MRR:22.98	Hits@10:48.11	Best:22.98
2024-12-29 05:42:35,567: Snapshot:4	Epoch:10	Loss:0.164	translation_Loss:0.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.049                                                   	MRR:23.08	Hits@10:48.16	Best:23.08
2024-12-29 05:42:42,418: Snapshot:4	Epoch:11	Loss:0.146	translation_Loss:0.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.047                                                   	MRR:23.14	Hits@10:48.18	Best:23.14
2024-12-29 05:42:48,979: Snapshot:4	Epoch:12	Loss:0.13	translation_Loss:0.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.046                                                   	MRR:23.27	Hits@10:48.13	Best:23.27
2024-12-29 05:42:55,836: Snapshot:4	Epoch:13	Loss:0.119	translation_Loss:0.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.045                                                   	MRR:23.24	Hits@10:48.24	Best:23.27
2024-12-29 05:43:02,194: Snapshot:4	Epoch:14	Loss:0.108	translation_Loss:0.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.043                                                   	MRR:23.17	Hits@10:48.42	Best:23.27
2024-12-29 05:43:08,657: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 23.27
2024-12-29 05:43:08,657: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:0.1 MRR:23.14 Best Results: 23.27
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:43:08,657: Snapshot:4	Epoch:15	Loss:0.1	translation_Loss:0.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.042                                                   	MRR:23.14	Hits@10:48.32	Best:23.27
2024-12-29 05:43:15,452: Snapshot:4	Epoch:16	Loss:55.995	translation_Loss:10.904	multi_layer_Loss:45.091	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.14	Hits@10:48.32	Best:23.27
2024-12-29 05:43:21,906: End of token training: 4 Epoch: 17 Loss:43.88 MRR:23.14 Best Results: 23.27
2024-12-29 05:43:21,906: Snapshot:4	Epoch:17	Loss:43.88	translation_Loss:10.898	multi_layer_Loss:32.982	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.14	Hits@10:48.32	Best:23.27
2024-12-29 05:43:22,202: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 05:43:36,614: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1846 | 0.1056 | 0.213  | 0.2634 |  0.3368 |
|     1      | 0.184  | 0.1046 | 0.2118 | 0.2644 |  0.3389 |
|     2      | 0.1849 | 0.1012 | 0.2129 | 0.2704 |  0.3537 |
|     3      | 0.1841 | 0.0904 | 0.215  | 0.2811 |  0.373  |
|     4      | 0.2317 | 0.1121 | 0.2714 | 0.3609 |  0.4759 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 05:43:36,616: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2327 | 0.1441 | 0.2789 | 0.3274 |  0.3893 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1495 | 0.2832 | 0.3391 |  0.4106 |
|     1      |  0.23  | 0.1449 | 0.2712 | 0.321  |  0.3836 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2276 | 0.1395 | 0.2663 | 0.3201 |  0.3944 |
|     1      | 0.2262 |  0.14  | 0.2631 | 0.3183 |  0.3914 |
|     2      | 0.2269 | 0.1401 | 0.2635 | 0.3186 |  0.3931 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2057 | 0.1238 | 0.2376 | 0.2898 |  0.3623 |
|     1      | 0.207  | 0.125  | 0.2371 | 0.2909 |  0.3657 |
|     2      | 0.2106 | 0.1235 | 0.2416 | 0.3018 |  0.3854 |
|     3      | 0.2088 | 0.1138 | 0.245  | 0.309  |  0.3974 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1846 | 0.1056 | 0.213  | 0.2634 |  0.3368 |
|     1      | 0.184  | 0.1046 | 0.2118 | 0.2644 |  0.3389 |
|     2      | 0.1849 | 0.1012 | 0.2129 | 0.2704 |  0.3537 |
|     3      | 0.1841 | 0.0904 | 0.215  | 0.2811 |  0.373  |
|     4      | 0.2317 | 0.1121 | 0.2714 | 0.3609 |  0.4759 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 05:43:36,617: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 770.2561082839966  |   0.233   |    0.144     |    0.279     |     0.389     |
|    1     | 239.99938941001892 |   0.235   |    0.147     |    0.277     |     0.397     |
|    2     | 132.02475690841675 |   0.227   |     0.14     |    0.264     |     0.393     |
|    3     | 113.24516105651855 |   0.208   |    0.122     |     0.24     |     0.378     |
|    4     | 132.8841941356659  |   0.194   |    0.103     |    0.225     |     0.376     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 05:43:36,617: Sum_Training_Time:1388.4096097946167
2024-12-29 05:43:36,617: Every_Training_Time:[770.2561082839966, 239.99938941001892, 132.02475690841675, 113.24516105651855, 132.8841941356659]
2024-12-29 05:43:36,617: Forward transfer: 0.1754 Backward transfer: -0.0402
2024-12-29 05:44:13,850: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241229054341/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=7000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 05:44:22,951: Snapshot:0	Epoch:0	Loss:17.871	translation_Loss:17.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-29 05:44:29,212: Snapshot:0	Epoch:1	Loss:17.114	translation_Loss:17.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.51	Hits@10:1.66	Best:1.51
2024-12-29 05:44:35,147: Snapshot:0	Epoch:2	Loss:16.452	translation_Loss:16.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.96	Hits@10:3.03	Best:1.96
2024-12-29 05:44:41,596: Snapshot:0	Epoch:3	Loss:15.815	translation_Loss:15.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.85	Hits@10:5.09	Best:2.85
2024-12-29 05:44:47,568: Snapshot:0	Epoch:4	Loss:15.191	translation_Loss:15.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.68	Hits@10:6.85	Best:3.68
2024-12-29 05:44:53,479: Snapshot:0	Epoch:5	Loss:14.574	translation_Loss:14.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.35	Hits@10:8.36	Best:4.35
2024-12-29 05:44:59,870: Snapshot:0	Epoch:6	Loss:13.981	translation_Loss:13.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.94	Hits@10:9.89	Best:4.94
2024-12-29 05:45:05,828: Snapshot:0	Epoch:7	Loss:13.402	translation_Loss:13.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:11.27	Best:5.49
2024-12-29 05:45:12,156: Snapshot:0	Epoch:8	Loss:12.837	translation_Loss:12.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.74	Best:6.03
2024-12-29 05:45:18,089: Snapshot:0	Epoch:9	Loss:12.295	translation_Loss:12.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.55	Hits@10:14.16	Best:6.55
2024-12-29 05:45:24,027: Snapshot:0	Epoch:10	Loss:11.76	translation_Loss:11.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.38	Best:7.06
2024-12-29 05:45:30,385: Snapshot:0	Epoch:11	Loss:11.237	translation_Loss:11.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.51	Hits@10:16.59	Best:7.51
2024-12-29 05:45:36,411: Snapshot:0	Epoch:12	Loss:10.735	translation_Loss:10.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.88	Best:7.94
2024-12-29 05:45:42,722: Snapshot:0	Epoch:13	Loss:10.246	translation_Loss:10.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.38	Hits@10:19.03	Best:8.38
2024-12-29 05:45:48,534: Snapshot:0	Epoch:14	Loss:9.775	translation_Loss:9.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.81	Hits@10:20.31	Best:8.81
2024-12-29 05:45:54,482: Snapshot:0	Epoch:15	Loss:9.328	translation_Loss:9.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.2	Hits@10:21.48	Best:9.2
2024-12-29 05:46:00,749: Snapshot:0	Epoch:16	Loss:8.889	translation_Loss:8.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.62	Hits@10:22.59	Best:9.62
2024-12-29 05:46:06,690: Snapshot:0	Epoch:17	Loss:8.466	translation_Loss:8.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.03	Hits@10:23.65	Best:10.03
2024-12-29 05:46:13,066: Snapshot:0	Epoch:18	Loss:8.065	translation_Loss:8.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.41	Hits@10:24.67	Best:10.41
2024-12-29 05:46:19,016: Snapshot:0	Epoch:19	Loss:7.676	translation_Loss:7.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.82	Hits@10:25.58	Best:10.82
2024-12-29 05:46:25,355: Snapshot:0	Epoch:20	Loss:7.299	translation_Loss:7.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.23	Hits@10:26.48	Best:11.23
2024-12-29 05:46:31,281: Snapshot:0	Epoch:21	Loss:6.949	translation_Loss:6.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.64	Hits@10:27.3	Best:11.64
2024-12-29 05:46:37,189: Snapshot:0	Epoch:22	Loss:6.616	translation_Loss:6.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.05	Hits@10:28.08	Best:12.05
2024-12-29 05:46:43,523: Snapshot:0	Epoch:23	Loss:6.284	translation_Loss:6.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.44	Hits@10:28.73	Best:12.44
2024-12-29 05:46:49,503: Snapshot:0	Epoch:24	Loss:5.975	translation_Loss:5.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.81	Hits@10:29.33	Best:12.81
2024-12-29 05:46:55,775: Snapshot:0	Epoch:25	Loss:5.688	translation_Loss:5.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.15	Hits@10:30.01	Best:13.15
2024-12-29 05:47:01,704: Snapshot:0	Epoch:26	Loss:5.402	translation_Loss:5.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.51	Hits@10:30.63	Best:13.51
2024-12-29 05:47:07,652: Snapshot:0	Epoch:27	Loss:5.131	translation_Loss:5.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.91	Hits@10:31.11	Best:13.91
2024-12-29 05:47:14,049: Snapshot:0	Epoch:28	Loss:4.875	translation_Loss:4.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.26	Hits@10:31.64	Best:14.26
2024-12-29 05:47:19,976: Snapshot:0	Epoch:29	Loss:4.624	translation_Loss:4.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.61	Hits@10:32.15	Best:14.61
2024-12-29 05:47:25,906: Snapshot:0	Epoch:30	Loss:4.397	translation_Loss:4.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.96	Hits@10:32.56	Best:14.96
2024-12-29 05:47:32,258: Snapshot:0	Epoch:31	Loss:4.177	translation_Loss:4.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.3	Hits@10:33.09	Best:15.3
2024-12-29 05:47:38,203: Snapshot:0	Epoch:32	Loss:3.969	translation_Loss:3.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.62	Hits@10:33.51	Best:15.62
2024-12-29 05:47:44,549: Snapshot:0	Epoch:33	Loss:3.752	translation_Loss:3.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.94	Hits@10:33.92	Best:15.94
2024-12-29 05:47:50,550: Snapshot:0	Epoch:34	Loss:3.561	translation_Loss:3.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.25	Hits@10:34.27	Best:16.25
2024-12-29 05:47:56,499: Snapshot:0	Epoch:35	Loss:3.375	translation_Loss:3.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.55	Hits@10:34.62	Best:16.55
2024-12-29 05:48:02,770: Snapshot:0	Epoch:36	Loss:3.197	translation_Loss:3.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.89	Hits@10:34.97	Best:16.89
2024-12-29 05:48:08,770: Snapshot:0	Epoch:37	Loss:3.037	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.2	Hits@10:35.29	Best:17.2
2024-12-29 05:48:15,245: Snapshot:0	Epoch:38	Loss:2.874	translation_Loss:2.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.5	Hits@10:35.52	Best:17.5
2024-12-29 05:48:21,198: Snapshot:0	Epoch:39	Loss:2.719	translation_Loss:2.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.81	Hits@10:35.9	Best:17.81
2024-12-29 05:48:27,168: Snapshot:0	Epoch:40	Loss:2.579	translation_Loss:2.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.11	Hits@10:36.11	Best:18.11
2024-12-29 05:48:33,517: Snapshot:0	Epoch:41	Loss:2.44	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.38	Hits@10:36.42	Best:18.38
2024-12-29 05:48:39,478: Snapshot:0	Epoch:42	Loss:2.309	translation_Loss:2.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.63	Hits@10:36.76	Best:18.63
2024-12-29 05:48:45,805: Snapshot:0	Epoch:43	Loss:2.179	translation_Loss:2.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.87	Hits@10:36.92	Best:18.87
2024-12-29 05:48:51,808: Snapshot:0	Epoch:44	Loss:2.061	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.12	Hits@10:37.13	Best:19.12
2024-12-29 05:48:58,133: Snapshot:0	Epoch:45	Loss:1.948	translation_Loss:1.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.31	Hits@10:37.32	Best:19.31
2024-12-29 05:49:04,159: Snapshot:0	Epoch:46	Loss:1.834	translation_Loss:1.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.56	Hits@10:37.48	Best:19.56
2024-12-29 05:49:10,102: Snapshot:0	Epoch:47	Loss:1.738	translation_Loss:1.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:37.6	Best:19.77
2024-12-29 05:49:16,563: Snapshot:0	Epoch:48	Loss:1.647	translation_Loss:1.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.95	Hits@10:37.79	Best:19.95
2024-12-29 05:49:22,470: Snapshot:0	Epoch:49	Loss:1.553	translation_Loss:1.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.12	Hits@10:37.95	Best:20.12
2024-12-29 05:49:28,819: Snapshot:0	Epoch:50	Loss:1.463	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.33	Hits@10:38.01	Best:20.33
2024-12-29 05:49:34,754: Snapshot:0	Epoch:51	Loss:1.383	translation_Loss:1.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.5	Hits@10:38.08	Best:20.5
2024-12-29 05:49:40,783: Snapshot:0	Epoch:52	Loss:1.312	translation_Loss:1.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.67	Hits@10:38.24	Best:20.67
2024-12-29 05:49:47,224: Snapshot:0	Epoch:53	Loss:1.24	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.84	Hits@10:38.32	Best:20.84
2024-12-29 05:49:53,169: Snapshot:0	Epoch:54	Loss:1.17	translation_Loss:1.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.99	Hits@10:38.45	Best:20.99
2024-12-29 05:49:59,082: Snapshot:0	Epoch:55	Loss:1.107	translation_Loss:1.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.15	Hits@10:38.5	Best:21.15
2024-12-29 05:50:05,472: Snapshot:0	Epoch:56	Loss:1.05	translation_Loss:1.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.3	Hits@10:38.59	Best:21.3
2024-12-29 05:50:11,556: Snapshot:0	Epoch:57	Loss:0.995	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.39	Hits@10:38.65	Best:21.39
2024-12-29 05:50:17,903: Snapshot:0	Epoch:58	Loss:0.944	translation_Loss:0.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.53	Hits@10:38.83	Best:21.53
2024-12-29 05:50:23,889: Snapshot:0	Epoch:59	Loss:0.892	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.63	Hits@10:38.87	Best:21.63
2024-12-29 05:50:29,864: Snapshot:0	Epoch:60	Loss:0.84	translation_Loss:0.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.74	Hits@10:38.92	Best:21.74
2024-12-29 05:50:36,176: Snapshot:0	Epoch:61	Loss:0.806	translation_Loss:0.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.88	Hits@10:38.98	Best:21.88
2024-12-29 05:50:42,034: Snapshot:0	Epoch:62	Loss:0.763	translation_Loss:0.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.99	Hits@10:39.0	Best:21.99
2024-12-29 05:50:48,425: Snapshot:0	Epoch:63	Loss:0.723	translation_Loss:0.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.1	Hits@10:39.04	Best:22.1
2024-12-29 05:50:54,382: Snapshot:0	Epoch:64	Loss:0.684	translation_Loss:0.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.18	Hits@10:38.99	Best:22.18
2024-12-29 05:51:00,305: Snapshot:0	Epoch:65	Loss:0.651	translation_Loss:0.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.25	Hits@10:39.04	Best:22.25
2024-12-29 05:51:06,582: Snapshot:0	Epoch:66	Loss:0.626	translation_Loss:0.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.36	Hits@10:39.09	Best:22.36
2024-12-29 05:51:12,522: Snapshot:0	Epoch:67	Loss:0.603	translation_Loss:0.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.45	Hits@10:39.09	Best:22.45
2024-12-29 05:51:18,862: Snapshot:0	Epoch:68	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.5	Hits@10:39.23	Best:22.5
2024-12-29 05:51:24,876: Snapshot:0	Epoch:69	Loss:0.538	translation_Loss:0.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.58	Hits@10:39.24	Best:22.58
2024-12-29 05:51:31,199: Snapshot:0	Epoch:70	Loss:0.521	translation_Loss:0.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.64	Hits@10:39.26	Best:22.64
2024-12-29 05:51:37,193: Snapshot:0	Epoch:71	Loss:0.493	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.7	Hits@10:39.21	Best:22.7
2024-12-29 05:51:43,150: Snapshot:0	Epoch:72	Loss:0.471	translation_Loss:0.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.78	Hits@10:39.28	Best:22.78
2024-12-29 05:51:49,586: Snapshot:0	Epoch:73	Loss:0.45	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.85	Hits@10:39.29	Best:22.85
2024-12-29 05:51:55,559: Snapshot:0	Epoch:74	Loss:0.431	translation_Loss:0.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.91	Hits@10:39.37	Best:22.91
2024-12-29 05:52:01,912: Snapshot:0	Epoch:75	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.92	Hits@10:39.46	Best:22.92
2024-12-29 05:52:07,865: Snapshot:0	Epoch:76	Loss:0.397	translation_Loss:0.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.98	Hits@10:39.38	Best:22.98
2024-12-29 05:52:13,904: Snapshot:0	Epoch:77	Loss:0.384	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.02	Hits@10:39.42	Best:23.02
2024-12-29 05:52:20,322: Snapshot:0	Epoch:78	Loss:0.365	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.05	Hits@10:39.52	Best:23.05
2024-12-29 05:52:26,251: Snapshot:0	Epoch:79	Loss:0.351	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.08	Hits@10:39.54	Best:23.08
2024-12-29 05:52:32,163: Snapshot:0	Epoch:80	Loss:0.342	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.13	Hits@10:39.58	Best:23.13
2024-12-29 05:52:38,434: Snapshot:0	Epoch:81	Loss:0.329	translation_Loss:0.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.19	Hits@10:39.61	Best:23.19
2024-12-29 05:52:44,366: Snapshot:0	Epoch:82	Loss:0.317	translation_Loss:0.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.19	Hits@10:39.69	Best:23.19
2024-12-29 05:52:50,689: Snapshot:0	Epoch:83	Loss:0.302	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.19	Hits@10:39.67	Best:23.19
2024-12-29 05:52:56,593: Snapshot:0	Epoch:84	Loss:0.291	translation_Loss:0.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:39.67	Best:23.24
2024-12-29 05:53:02,513: Snapshot:0	Epoch:85	Loss:0.285	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.29	Hits@10:39.73	Best:23.29
2024-12-29 05:53:08,888: Snapshot:0	Epoch:86	Loss:0.273	translation_Loss:0.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.35	Hits@10:39.73	Best:23.35
2024-12-29 05:53:14,868: Snapshot:0	Epoch:87	Loss:0.269	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.73	Best:23.36
2024-12-29 05:53:21,215: Snapshot:0	Epoch:88	Loss:0.257	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.38	Hits@10:39.7	Best:23.38
2024-12-29 05:53:27,176: Snapshot:0	Epoch:89	Loss:0.25	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.38	Hits@10:39.77	Best:23.38
2024-12-29 05:53:33,095: Snapshot:0	Epoch:90	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.39	Hits@10:39.72	Best:23.39
2024-12-29 05:53:39,365: Snapshot:0	Epoch:91	Loss:0.232	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.41	Hits@10:39.73	Best:23.41
2024-12-29 05:53:45,337: Snapshot:0	Epoch:92	Loss:0.227	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:39.74	Best:23.45
2024-12-29 05:53:51,617: Snapshot:0	Epoch:93	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:39.75	Best:23.45
2024-12-29 05:53:57,544: Snapshot:0	Epoch:94	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.46	Hits@10:39.74	Best:23.46
2024-12-29 05:54:03,949: Snapshot:0	Epoch:95	Loss:0.209	translation_Loss:0.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:39.75	Best:23.51
2024-12-29 05:54:09,882: Snapshot:0	Epoch:96	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.52	Hits@10:39.82	Best:23.52
2024-12-29 05:54:15,885: Snapshot:0	Epoch:97	Loss:0.194	translation_Loss:0.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.55	Hits@10:39.83	Best:23.55
2024-12-29 05:54:22,221: Snapshot:0	Epoch:98	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.56	Hits@10:39.8	Best:23.56
2024-12-29 05:54:28,165: Snapshot:0	Epoch:99	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.59	Hits@10:39.85	Best:23.59
2024-12-29 05:54:34,506: Snapshot:0	Epoch:100	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.62	Hits@10:39.86	Best:23.62
2024-12-29 05:54:40,546: Snapshot:0	Epoch:101	Loss:0.178	translation_Loss:0.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.9	Best:23.65
2024-12-29 05:54:46,461: Snapshot:0	Epoch:102	Loss:0.172	translation_Loss:0.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.8	Best:23.65
2024-12-29 05:54:52,771: Snapshot:0	Epoch:103	Loss:0.174	translation_Loss:0.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.85	Best:23.65
2024-12-29 05:54:58,698: Snapshot:0	Epoch:104	Loss:0.165	translation_Loss:0.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.68	Hits@10:39.88	Best:23.68
2024-12-29 05:55:04,665: Snapshot:0	Epoch:105	Loss:0.161	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.97	Best:23.69
2024-12-29 05:55:11,019: Snapshot:0	Epoch:106	Loss:0.157	translation_Loss:0.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.94	Best:23.69
2024-12-29 05:55:16,983: Snapshot:0	Epoch:107	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.9	Best:23.71
2024-12-29 05:55:23,432: Snapshot:0	Epoch:108	Loss:0.152	translation_Loss:0.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:39.94	Best:23.74
2024-12-29 05:55:29,376: Snapshot:0	Epoch:109	Loss:0.146	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.77	Hits@10:39.95	Best:23.77
2024-12-29 05:55:35,331: Snapshot:0	Epoch:110	Loss:0.143	translation_Loss:0.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:40.0	Best:23.77
2024-12-29 05:55:41,590: Snapshot:0	Epoch:111	Loss:0.147	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.77	Hits@10:40.04	Best:23.77
2024-12-29 05:55:47,500: Snapshot:0	Epoch:112	Loss:0.141	translation_Loss:0.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.8	Hits@10:40.03	Best:23.8
2024-12-29 05:55:53,805: Snapshot:0	Epoch:113	Loss:0.137	translation_Loss:0.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.82	Hits@10:40.02	Best:23.82
2024-12-29 05:55:59,758: Snapshot:0	Epoch:114	Loss:0.136	translation_Loss:0.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.82	Hits@10:40.01	Best:23.82
2024-12-29 05:56:05,684: Snapshot:0	Epoch:115	Loss:0.129	translation_Loss:0.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.83	Hits@10:40.1	Best:23.83
2024-12-29 05:56:12,086: Snapshot:0	Epoch:116	Loss:0.131	translation_Loss:0.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.86	Hits@10:40.06	Best:23.86
2024-12-29 05:56:18,010: Snapshot:0	Epoch:117	Loss:0.126	translation_Loss:0.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:40.07	Best:23.89
2024-12-29 05:56:24,476: Snapshot:0	Epoch:118	Loss:0.127	translation_Loss:0.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.9	Hits@10:40.13	Best:23.9
2024-12-29 05:56:30,447: Snapshot:0	Epoch:119	Loss:0.125	translation_Loss:0.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.9	Hits@10:40.14	Best:23.9
2024-12-29 05:56:36,756: Snapshot:0	Epoch:120	Loss:0.119	translation_Loss:0.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:40.04	Best:23.95
2024-12-29 05:56:42,639: Snapshot:0	Epoch:121	Loss:0.118	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:40.06	Best:23.95
2024-12-29 05:56:48,549: Snapshot:0	Epoch:122	Loss:0.113	translation_Loss:0.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.92	Hits@10:40.05	Best:23.95
2024-12-29 05:56:54,821: Early Stopping! Snapshot: 0 Epoch: 123 Best Results: 23.95
2024-12-29 05:56:54,822: Start to training tokens! Snapshot: 0 Epoch: 123 Loss:0.118 MRR:23.88 Best Results: 23.95
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 05:56:54,822: Snapshot:0	Epoch:123	Loss:0.118	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:40.07	Best:23.95
2024-12-29 05:57:01,185: Snapshot:0	Epoch:124	Loss:57.297	translation_Loss:12.192	multi_layer_Loss:45.105	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:40.07	Best:23.95
2024-12-29 05:57:07,344: End of token training: 0 Epoch: 125 Loss:45.66 MRR:23.88 Best Results: 23.95
2024-12-29 05:57:07,344: Snapshot:0	Epoch:125	Loss:45.66	translation_Loss:12.198	multi_layer_Loss:33.462	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.88	Hits@10:40.07	Best:23.95
2024-12-29 05:57:07,586: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 05:57:09,746: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2324 | 0.1439 | 0.2786 | 0.3276 |  0.3883 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 05:57:32,568: Snapshot:1	Epoch:0	Loss:8.559	translation_Loss:8.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.047                                                   	MRR:17.31	Hits@10:30.27	Best:17.31
2024-12-29 05:57:38,922: Snapshot:1	Epoch:1	Loss:7.612	translation_Loss:7.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.145                                                   	MRR:17.96	Hits@10:31.2	Best:17.96
2024-12-29 05:57:45,373: Snapshot:1	Epoch:2	Loss:7.02	translation_Loss:6.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.274                                                   	MRR:18.44	Hits@10:31.83	Best:18.44
2024-12-29 05:57:51,720: Snapshot:1	Epoch:3	Loss:6.575	translation_Loss:6.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.415                                                   	MRR:18.73	Hits@10:32.45	Best:18.73
2024-12-29 05:57:58,513: Snapshot:1	Epoch:4	Loss:6.236	translation_Loss:5.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.551                                                   	MRR:19.05	Hits@10:32.87	Best:19.05
2024-12-29 05:58:04,855: Snapshot:1	Epoch:5	Loss:5.932	translation_Loss:5.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.674                                                   	MRR:19.24	Hits@10:33.33	Best:19.24
2024-12-29 05:58:11,302: Snapshot:1	Epoch:6	Loss:5.676	translation_Loss:4.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.785                                                   	MRR:19.46	Hits@10:33.78	Best:19.46
2024-12-29 05:58:18,056: Snapshot:1	Epoch:7	Loss:5.434	translation_Loss:4.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.881                                                   	MRR:19.74	Hits@10:34.02	Best:19.74
2024-12-29 05:58:24,420: Snapshot:1	Epoch:8	Loss:5.244	translation_Loss:4.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.965                                                   	MRR:19.91	Hits@10:34.29	Best:19.91
2024-12-29 05:58:30,832: Snapshot:1	Epoch:9	Loss:5.048	translation_Loss:4.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.037                                                   	MRR:20.11	Hits@10:34.52	Best:20.11
2024-12-29 05:58:37,527: Snapshot:1	Epoch:10	Loss:4.898	translation_Loss:3.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.101                                                   	MRR:20.25	Hits@10:34.72	Best:20.25
2024-12-29 05:58:43,887: Snapshot:1	Epoch:11	Loss:4.735	translation_Loss:3.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.156                                                   	MRR:20.36	Hits@10:34.96	Best:20.36
2024-12-29 05:58:50,783: Snapshot:1	Epoch:12	Loss:4.601	translation_Loss:3.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.201                                                   	MRR:20.48	Hits@10:35.09	Best:20.48
2024-12-29 05:58:57,146: Snapshot:1	Epoch:13	Loss:4.488	translation_Loss:3.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.24                                                   	MRR:20.61	Hits@10:35.29	Best:20.61
2024-12-29 05:59:03,515: Snapshot:1	Epoch:14	Loss:4.38	translation_Loss:3.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.273                                                   	MRR:20.71	Hits@10:35.41	Best:20.71
2024-12-29 05:59:10,304: Snapshot:1	Epoch:15	Loss:4.284	translation_Loss:2.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.302                                                   	MRR:20.82	Hits@10:35.64	Best:20.82
2024-12-29 05:59:16,836: Snapshot:1	Epoch:16	Loss:4.206	translation_Loss:2.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.325                                                   	MRR:20.91	Hits@10:35.73	Best:20.91
2024-12-29 05:59:23,675: Snapshot:1	Epoch:17	Loss:4.141	translation_Loss:2.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.346                                                   	MRR:21.0	Hits@10:35.92	Best:21.0
2024-12-29 05:59:30,040: Snapshot:1	Epoch:18	Loss:4.08	translation_Loss:2.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.362                                                   	MRR:21.09	Hits@10:35.94	Best:21.09
2024-12-29 05:59:36,362: Snapshot:1	Epoch:19	Loss:4.03	translation_Loss:2.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.376                                                   	MRR:21.14	Hits@10:36.02	Best:21.14
2024-12-29 05:59:42,999: Snapshot:1	Epoch:20	Loss:3.987	translation_Loss:2.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.388                                                   	MRR:21.18	Hits@10:36.09	Best:21.18
2024-12-29 05:59:49,359: Snapshot:1	Epoch:21	Loss:3.948	translation_Loss:2.551	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.397                                                   	MRR:21.26	Hits@10:36.22	Best:21.26
2024-12-29 05:59:56,107: Snapshot:1	Epoch:22	Loss:3.917	translation_Loss:2.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.405                                                   	MRR:21.25	Hits@10:36.35	Best:21.26
2024-12-29 06:00:02,571: Snapshot:1	Epoch:23	Loss:3.896	translation_Loss:2.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.411                                                   	MRR:21.3	Hits@10:36.32	Best:21.3
2024-12-29 06:00:09,051: Snapshot:1	Epoch:24	Loss:3.876	translation_Loss:2.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.417                                                   	MRR:21.32	Hits@10:36.34	Best:21.32
2024-12-29 06:00:15,824: Snapshot:1	Epoch:25	Loss:3.854	translation_Loss:2.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.422                                                   	MRR:21.37	Hits@10:36.35	Best:21.37
2024-12-29 06:00:22,210: Snapshot:1	Epoch:26	Loss:3.837	translation_Loss:2.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.426                                                   	MRR:21.38	Hits@10:36.42	Best:21.38
2024-12-29 06:00:29,179: Snapshot:1	Epoch:27	Loss:3.819	translation_Loss:2.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.43                                                   	MRR:21.39	Hits@10:36.41	Best:21.39
2024-12-29 06:00:35,625: Snapshot:1	Epoch:28	Loss:3.798	translation_Loss:2.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.433                                                   	MRR:21.48	Hits@10:36.53	Best:21.48
2024-12-29 06:00:41,977: Snapshot:1	Epoch:29	Loss:3.792	translation_Loss:2.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.437                                                   	MRR:21.5	Hits@10:36.56	Best:21.5
2024-12-29 06:00:48,733: Snapshot:1	Epoch:30	Loss:3.78	translation_Loss:2.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.439                                                   	MRR:21.48	Hits@10:36.61	Best:21.5
2024-12-29 06:00:55,089: Snapshot:1	Epoch:31	Loss:3.76	translation_Loss:2.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.44                                                   	MRR:21.52	Hits@10:36.69	Best:21.52
2024-12-29 06:01:01,830: Snapshot:1	Epoch:32	Loss:3.756	translation_Loss:2.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.442                                                   	MRR:21.51	Hits@10:36.56	Best:21.52
2024-12-29 06:01:08,227: Snapshot:1	Epoch:33	Loss:3.756	translation_Loss:2.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.444                                                   	MRR:21.51	Hits@10:36.65	Best:21.52
2024-12-29 06:01:14,573: Snapshot:1	Epoch:34	Loss:3.753	translation_Loss:2.308	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.446                                                   	MRR:21.55	Hits@10:36.62	Best:21.55
2024-12-29 06:01:21,339: Snapshot:1	Epoch:35	Loss:3.734	translation_Loss:2.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.447                                                   	MRR:21.54	Hits@10:36.67	Best:21.55
2024-12-29 06:01:27,595: Snapshot:1	Epoch:36	Loss:3.729	translation_Loss:2.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.449                                                   	MRR:21.56	Hits@10:36.66	Best:21.56
2024-12-29 06:01:34,356: Snapshot:1	Epoch:37	Loss:3.717	translation_Loss:2.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.449                                                   	MRR:21.57	Hits@10:36.62	Best:21.57
2024-12-29 06:01:40,818: Snapshot:1	Epoch:38	Loss:3.697	translation_Loss:2.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.449                                                   	MRR:21.62	Hits@10:36.64	Best:21.62
2024-12-29 06:01:47,181: Snapshot:1	Epoch:39	Loss:3.709	translation_Loss:2.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.449                                                   	MRR:21.59	Hits@10:36.6	Best:21.62
2024-12-29 06:01:53,951: Snapshot:1	Epoch:40	Loss:3.699	translation_Loss:2.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.452                                                   	MRR:21.59	Hits@10:36.67	Best:21.62
2024-12-29 06:02:00,241: Early Stopping! Snapshot: 1 Epoch: 41 Best Results: 21.62
2024-12-29 06:02:00,241: Start to training tokens! Snapshot: 1 Epoch: 41 Loss:3.698 MRR:21.54 Best Results: 21.62
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 06:02:00,241: Snapshot:1	Epoch:41	Loss:3.698	translation_Loss:2.245	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:21.54	Hits@10:36.63	Best:21.62
2024-12-29 06:02:06,868: Snapshot:1	Epoch:42	Loss:58.606	translation_Loss:14.08	multi_layer_Loss:44.526	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.54	Hits@10:36.63	Best:21.62
2024-12-29 06:02:13,226: End of token training: 1 Epoch: 43 Loss:46.787 MRR:21.54 Best Results: 21.62
2024-12-29 06:02:13,226: Snapshot:1	Epoch:43	Loss:46.787	translation_Loss:14.071	multi_layer_Loss:32.716	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.54	Hits@10:36.63	Best:21.62
2024-12-29 06:02:13,466: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 06:02:18,750: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2466 | 0.1553 | 0.2912 | 0.3447 |  0.4144 |
|     1      | 0.2143 | 0.1285 | 0.2583 | 0.3062 |  0.3689 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 06:02:41,191: Snapshot:2	Epoch:0	Loss:6.003	translation_Loss:5.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.048                                                   	MRR:19.2	Hits@10:34.05	Best:19.2
2024-12-29 06:02:48,138: Snapshot:2	Epoch:1	Loss:5.082	translation_Loss:4.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.139                                                   	MRR:19.72	Hits@10:34.92	Best:19.72
2024-12-29 06:02:54,670: Snapshot:2	Epoch:2	Loss:4.533	translation_Loss:4.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.254                                                   	MRR:20.07	Hits@10:35.43	Best:20.07
2024-12-29 06:03:01,537: Snapshot:2	Epoch:3	Loss:4.162	translation_Loss:3.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:20.33	Hits@10:35.94	Best:20.33
2024-12-29 06:03:08,054: Snapshot:2	Epoch:4	Loss:3.872	translation_Loss:3.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.495                                                   	MRR:20.49	Hits@10:36.19	Best:20.49
2024-12-29 06:03:14,544: Snapshot:2	Epoch:5	Loss:3.648	translation_Loss:3.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.601                                                   	MRR:20.63	Hits@10:36.51	Best:20.63
2024-12-29 06:03:21,413: Snapshot:2	Epoch:6	Loss:3.471	translation_Loss:2.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.694                                                   	MRR:20.77	Hits@10:36.7	Best:20.77
2024-12-29 06:03:27,859: Snapshot:2	Epoch:7	Loss:3.324	translation_Loss:2.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.776                                                   	MRR:20.88	Hits@10:36.96	Best:20.88
2024-12-29 06:03:34,768: Snapshot:2	Epoch:8	Loss:3.191	translation_Loss:2.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:21.01	Hits@10:37.18	Best:21.01
2024-12-29 06:03:41,485: Snapshot:2	Epoch:9	Loss:3.096	translation_Loss:2.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.905                                                   	MRR:21.09	Hits@10:37.3	Best:21.09
2024-12-29 06:03:48,073: Snapshot:2	Epoch:10	Loss:3.01	translation_Loss:2.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.956                                                   	MRR:21.15	Hits@10:37.4	Best:21.15
2024-12-29 06:03:54,850: Snapshot:2	Epoch:11	Loss:2.939	translation_Loss:1.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.999                                                   	MRR:21.27	Hits@10:37.53	Best:21.27
2024-12-29 06:04:01,446: Snapshot:2	Epoch:12	Loss:2.87	translation_Loss:1.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.036                                                   	MRR:21.32	Hits@10:37.65	Best:21.32
2024-12-29 06:04:07,919: Snapshot:2	Epoch:13	Loss:2.82	translation_Loss:1.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.066                                                   	MRR:21.33	Hits@10:37.71	Best:21.33
2024-12-29 06:04:14,907: Snapshot:2	Epoch:14	Loss:2.779	translation_Loss:1.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.091                                                   	MRR:21.35	Hits@10:37.86	Best:21.35
2024-12-29 06:04:21,410: Snapshot:2	Epoch:15	Loss:2.748	translation_Loss:1.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.113                                                   	MRR:21.37	Hits@10:37.89	Best:21.37
2024-12-29 06:04:28,384: Snapshot:2	Epoch:16	Loss:2.716	translation_Loss:1.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.131                                                   	MRR:21.44	Hits@10:37.83	Best:21.44
2024-12-29 06:04:34,829: Snapshot:2	Epoch:17	Loss:2.692	translation_Loss:1.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.147                                                   	MRR:21.48	Hits@10:37.9	Best:21.48
2024-12-29 06:04:41,398: Snapshot:2	Epoch:18	Loss:2.684	translation_Loss:1.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.16                                                   	MRR:21.52	Hits@10:37.99	Best:21.52
2024-12-29 06:04:48,229: Snapshot:2	Epoch:19	Loss:2.663	translation_Loss:1.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.171                                                   	MRR:21.5	Hits@10:38.04	Best:21.52
2024-12-29 06:04:54,771: Snapshot:2	Epoch:20	Loss:2.643	translation_Loss:1.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.181                                                   	MRR:21.55	Hits@10:38.11	Best:21.55
2024-12-29 06:05:01,554: Snapshot:2	Epoch:21	Loss:2.637	translation_Loss:1.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.189                                                   	MRR:21.54	Hits@10:38.09	Best:21.55
2024-12-29 06:05:08,095: Snapshot:2	Epoch:22	Loss:2.629	translation_Loss:1.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.197                                                   	MRR:21.54	Hits@10:38.18	Best:21.55
2024-12-29 06:05:14,675: Snapshot:2	Epoch:23	Loss:2.622	translation_Loss:1.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.204                                                   	MRR:21.63	Hits@10:38.15	Best:21.63
2024-12-29 06:05:21,591: Snapshot:2	Epoch:24	Loss:2.617	translation_Loss:1.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.21                                                   	MRR:21.58	Hits@10:38.14	Best:21.63
2024-12-29 06:05:28,004: Snapshot:2	Epoch:25	Loss:2.605	translation_Loss:1.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.215                                                   	MRR:21.59	Hits@10:38.2	Best:21.63
2024-12-29 06:05:34,827: Early Stopping! Snapshot: 2 Epoch: 26 Best Results: 21.63
2024-12-29 06:05:34,828: Start to training tokens! Snapshot: 2 Epoch: 26 Loss:2.605 MRR:21.62 Best Results: 21.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 06:05:34,828: Snapshot:2	Epoch:26	Loss:2.605	translation_Loss:1.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.22                                                   	MRR:21.62	Hits@10:38.09	Best:21.63
2024-12-29 06:05:41,278: Snapshot:2	Epoch:27	Loss:59.402	translation_Loss:13.799	multi_layer_Loss:45.603	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.62	Hits@10:38.09	Best:21.63
2024-12-29 06:05:48,051: End of token training: 2 Epoch: 28 Loss:47.717 MRR:21.62 Best Results: 21.63
2024-12-29 06:05:48,051: Snapshot:2	Epoch:28	Loss:47.717	translation_Loss:13.786	multi_layer_Loss:33.931	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.62	Hits@10:38.09	Best:21.63
2024-12-29 06:05:48,268: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 06:05:56,640: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2442 | 0.1539 | 0.2851 | 0.3406 |  0.4133 |
|     1      | 0.2224 | 0.1355 | 0.2615 | 0.3154 |  0.3855 |
|     2      | 0.2157 | 0.1279 | 0.2547 | 0.3087 |   0.38  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 06:06:20,095: Snapshot:3	Epoch:0	Loss:3.489	translation_Loss:3.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.048                                                   	MRR:18.81	Hits@10:35.5	Best:18.81
2024-12-29 06:06:26,696: Snapshot:3	Epoch:1	Loss:2.701	translation_Loss:2.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:19.32	Hits@10:36.78	Best:19.32
2024-12-29 06:06:33,222: Snapshot:3	Epoch:2	Loss:2.304	translation_Loss:2.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.21                                                   	MRR:19.58	Hits@10:37.32	Best:19.58
2024-12-29 06:06:39,857: Snapshot:3	Epoch:3	Loss:2.045	translation_Loss:1.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.286                                                   	MRR:19.82	Hits@10:37.68	Best:19.82
2024-12-29 06:06:46,452: Snapshot:3	Epoch:4	Loss:1.881	translation_Loss:1.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.353                                                   	MRR:19.89	Hits@10:37.9	Best:19.89
2024-12-29 06:06:53,094: Snapshot:3	Epoch:5	Loss:1.758	translation_Loss:1.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.41                                                   	MRR:20.0	Hits@10:38.29	Best:20.0
2024-12-29 06:06:59,941: Snapshot:3	Epoch:6	Loss:1.668	translation_Loss:1.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.457                                                   	MRR:20.09	Hits@10:38.39	Best:20.09
2024-12-29 06:07:06,575: Snapshot:3	Epoch:7	Loss:1.585	translation_Loss:1.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.496                                                   	MRR:20.24	Hits@10:38.46	Best:20.24
2024-12-29 06:07:13,128: Snapshot:3	Epoch:8	Loss:1.528	translation_Loss:1.001	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.528                                                   	MRR:20.24	Hits@10:38.56	Best:20.24
2024-12-29 06:07:20,029: Snapshot:3	Epoch:9	Loss:1.477	translation_Loss:0.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.555                                                   	MRR:20.37	Hits@10:38.68	Best:20.37
2024-12-29 06:07:26,550: Snapshot:3	Epoch:10	Loss:1.452	translation_Loss:0.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.576                                                   	MRR:20.35	Hits@10:38.71	Best:20.37
2024-12-29 06:07:33,143: Snapshot:3	Epoch:11	Loss:1.412	translation_Loss:0.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.593                                                   	MRR:20.39	Hits@10:38.77	Best:20.39
2024-12-29 06:07:40,029: Snapshot:3	Epoch:12	Loss:1.391	translation_Loss:0.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.606                                                   	MRR:20.36	Hits@10:38.81	Best:20.39
2024-12-29 06:07:46,536: Snapshot:3	Epoch:13	Loss:1.376	translation_Loss:0.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.618                                                   	MRR:20.33	Hits@10:38.87	Best:20.39
2024-12-29 06:07:53,482: Snapshot:3	Epoch:14	Loss:1.367	translation_Loss:0.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.628                                                   	MRR:20.43	Hits@10:38.93	Best:20.43
2024-12-29 06:07:59,950: Snapshot:3	Epoch:15	Loss:1.352	translation_Loss:0.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.635                                                   	MRR:20.42	Hits@10:38.95	Best:20.43
2024-12-29 06:08:06,508: Snapshot:3	Epoch:16	Loss:1.346	translation_Loss:0.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.642                                                   	MRR:20.44	Hits@10:38.94	Best:20.44
2024-12-29 06:08:13,548: Snapshot:3	Epoch:17	Loss:1.342	translation_Loss:0.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.648                                                   	MRR:20.47	Hits@10:38.97	Best:20.47
2024-12-29 06:08:19,975: Snapshot:3	Epoch:18	Loss:1.342	translation_Loss:0.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.653                                                   	MRR:20.42	Hits@10:39.0	Best:20.47
2024-12-29 06:08:26,801: Snapshot:3	Epoch:19	Loss:1.336	translation_Loss:0.678	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.658                                                   	MRR:20.45	Hits@10:38.94	Best:20.47
2024-12-29 06:08:33,339: Early Stopping! Snapshot: 3 Epoch: 20 Best Results: 20.47
2024-12-29 06:08:33,339: Start to training tokens! Snapshot: 3 Epoch: 20 Loss:1.329 MRR:20.47 Best Results: 20.47
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 06:08:33,339: Snapshot:3	Epoch:20	Loss:1.329	translation_Loss:0.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.661                                                   	MRR:20.47	Hits@10:38.92	Best:20.47
2024-12-29 06:08:39,729: Snapshot:3	Epoch:21	Loss:58.004	translation_Loss:13.155	multi_layer_Loss:44.849	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.47	Hits@10:38.92	Best:20.47
2024-12-29 06:08:46,560: End of token training: 3 Epoch: 22 Loss:45.946 MRR:20.47 Best Results: 20.47
2024-12-29 06:08:46,560: Snapshot:3	Epoch:22	Loss:45.946	translation_Loss:13.156	multi_layer_Loss:32.79	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.47	Hits@10:38.92	Best:20.47
2024-12-29 06:08:46,826: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 06:08:57,999: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2321 | 0.1443 | 0.2683 | 0.3225 |  0.3987 |
|     1      | 0.2154 |  0.13  | 0.2507 | 0.3061 |  0.3781 |
|     2      | 0.2119 | 0.1222 | 0.2479 | 0.3081 |  0.3867 |
|     3      | 0.2055 | 0.1105 | 0.2416 | 0.3064 |  0.393  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 06:09:21,044: Snapshot:4	Epoch:0	Loss:2.021	translation_Loss:1.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.042                                                   	MRR:19.11	Hits@10:41.96	Best:19.11
2024-12-29 06:09:27,642: Snapshot:4	Epoch:1	Loss:1.447	translation_Loss:1.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.091                                                   	MRR:20.57	Hits@10:44.74	Best:20.57
2024-12-29 06:09:34,626: Snapshot:4	Epoch:2	Loss:1.206	translation_Loss:1.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.113                                                   	MRR:21.06	Hits@10:46.11	Best:21.06
2024-12-29 06:09:41,200: Snapshot:4	Epoch:3	Loss:1.044	translation_Loss:0.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.13                                                   	MRR:21.37	Hits@10:46.79	Best:21.37
2024-12-29 06:09:48,193: Snapshot:4	Epoch:4	Loss:0.907	translation_Loss:0.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.146                                                   	MRR:21.48	Hits@10:46.97	Best:21.48
2024-12-29 06:09:54,851: Snapshot:4	Epoch:5	Loss:0.8	translation_Loss:0.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.159                                                   	MRR:21.61	Hits@10:47.22	Best:21.61
2024-12-29 06:10:01,466: Snapshot:4	Epoch:6	Loss:0.707	translation_Loss:0.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:21.75	Hits@10:47.6	Best:21.75
2024-12-29 06:10:08,586: Snapshot:4	Epoch:7	Loss:0.628	translation_Loss:0.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:21.85	Hits@10:47.56	Best:21.85
2024-12-29 06:10:15,187: Snapshot:4	Epoch:8	Loss:0.561	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.181                                                   	MRR:22.11	Hits@10:47.66	Best:22.11
2024-12-29 06:10:22,214: Snapshot:4	Epoch:9	Loss:0.516	translation_Loss:0.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.184                                                   	MRR:22.34	Hits@10:47.94	Best:22.34
2024-12-29 06:10:28,814: Snapshot:4	Epoch:10	Loss:0.472	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:22.49	Hits@10:47.98	Best:22.49
2024-12-29 06:10:35,434: Snapshot:4	Epoch:11	Loss:0.448	translation_Loss:0.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:22.58	Hits@10:47.98	Best:22.58
2024-12-29 06:10:42,226: Snapshot:4	Epoch:12	Loss:0.426	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.189                                                   	MRR:22.71	Hits@10:47.93	Best:22.71
2024-12-29 06:10:48,765: Snapshot:4	Epoch:13	Loss:0.408	translation_Loss:0.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:22.64	Hits@10:47.87	Best:22.71
2024-12-29 06:10:55,509: Snapshot:4	Epoch:14	Loss:0.395	translation_Loss:0.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:22.66	Hits@10:48.07	Best:22.71
2024-12-29 06:11:02,027: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 22.71
2024-12-29 06:11:02,029: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:0.388 MRR:22.71 Best Results: 22.71
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 06:11:02,029: Snapshot:4	Epoch:15	Loss:0.388	translation_Loss:0.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:22.71	Hits@10:48.14	Best:22.71
2024-12-29 06:11:08,598: Snapshot:4	Epoch:16	Loss:56.293	translation_Loss:11.202	multi_layer_Loss:45.091	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.71	Hits@10:48.14	Best:22.71
2024-12-29 06:11:15,568: End of token training: 4 Epoch: 17 Loss:44.182 MRR:22.71 Best Results: 22.71
2024-12-29 06:11:15,568: Snapshot:4	Epoch:17	Loss:44.182	translation_Loss:11.2	multi_layer_Loss:32.982	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.71	Hits@10:48.14	Best:22.71
2024-12-29 06:11:15,793: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 06:11:30,259: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2206 | 0.1363 | 0.2529 | 0.3077 |  0.3819 |
|     1      | 0.2033 | 0.1222 | 0.2332 | 0.288  |  0.3598 |
|     2      | 0.1981 | 0.1131 | 0.2278 | 0.2873 |  0.3688 |
|     3      | 0.1919 | 0.0968 | 0.2243 | 0.2904 |  0.3836 |
|     4      | 0.2262 | 0.1035 | 0.2698 | 0.3575 |  0.4768 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 06:11:30,261: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2324 | 0.1439 | 0.2786 | 0.3276 |  0.3883 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2466 | 0.1553 | 0.2912 | 0.3447 |  0.4144 |
|     1      | 0.2143 | 0.1285 | 0.2583 | 0.3062 |  0.3689 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2442 | 0.1539 | 0.2851 | 0.3406 |  0.4133 |
|     1      | 0.2224 | 0.1355 | 0.2615 | 0.3154 |  0.3855 |
|     2      | 0.2157 | 0.1279 | 0.2547 | 0.3087 |   0.38  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2321 | 0.1443 | 0.2683 | 0.3225 |  0.3987 |
|     1      | 0.2154 |  0.13  | 0.2507 | 0.3061 |  0.3781 |
|     2      | 0.2119 | 0.1222 | 0.2479 | 0.3081 |  0.3867 |
|     3      | 0.2055 | 0.1105 | 0.2416 | 0.3064 |  0.393  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2206 | 0.1363 | 0.2529 | 0.3077 |  0.3819 |
|     1      | 0.2033 | 0.1222 | 0.2332 | 0.288  |  0.3598 |
|     2      | 0.1981 | 0.1131 | 0.2278 | 0.2873 |  0.3688 |
|     3      | 0.1919 | 0.0968 | 0.2243 | 0.2904 |  0.3836 |
|     4      | 0.2262 | 0.1035 | 0.2698 | 0.3575 |  0.4768 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 06:11:30,261: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 773.4937660694122  |   0.232   |    0.144     |    0.279     |     0.388     |
|    1     | 300.38328433036804 |    0.23   |    0.142     |    0.275     |     0.392     |
|    2     | 206.37374591827393 |   0.227   |    0.139     |    0.267     |     0.393     |
|    3     | 166.95684123039246 |   0.216   |    0.127     |    0.252     |     0.389     |
|    4     | 134.57958126068115 |   0.208   |    0.114     |    0.242     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 06:11:30,261: Sum_Training_Time:1581.7872188091278
2024-12-29 06:11:30,261: Every_Training_Time:[773.4937660694122, 300.38328433036804, 206.37374591827393, 166.95684123039246, 134.57958126068115]
2024-12-29 06:11:30,261: Forward transfer: 0.16970000000000002 Backward transfer: -0.013499999999999998
