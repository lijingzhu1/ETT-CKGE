2024-12-29 04:14:58,387: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229041427/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=8000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 04:15:13,202: Snapshot:0	Epoch:0	Loss:24.263	translation_Loss:24.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.68	Hits@10:26.04	Best:10.68
2024-12-29 04:15:23,828: Snapshot:0	Epoch:1	Loss:15.05	translation_Loss:15.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.45	Hits@10:40.39	Best:18.45
2024-12-29 04:15:34,788: Snapshot:0	Epoch:2	Loss:8.578	translation_Loss:8.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:45.19	Best:23.51
2024-12-29 04:15:46,011: Snapshot:0	Epoch:3	Loss:4.719	translation_Loss:4.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:47.19	Best:25.73
2024-12-29 04:15:56,670: Snapshot:0	Epoch:4	Loss:2.731	translation_Loss:2.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.76	Hits@10:48.03	Best:26.76
2024-12-29 04:16:07,544: Snapshot:0	Epoch:5	Loss:1.812	translation_Loss:1.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.2	Hits@10:48.29	Best:27.2
2024-12-29 04:16:18,389: Snapshot:0	Epoch:6	Loss:1.354	translation_Loss:1.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.36	Hits@10:48.33	Best:27.36
2024-12-29 04:16:28,813: Snapshot:0	Epoch:7	Loss:1.103	translation_Loss:1.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.19	Best:27.37
2024-12-29 04:16:39,821: Snapshot:0	Epoch:8	Loss:0.953	translation_Loss:0.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.52	Hits@10:48.12	Best:27.52
2024-12-29 04:16:50,647: Snapshot:0	Epoch:9	Loss:0.846	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.4	Hits@10:48.06	Best:27.52
2024-12-29 04:17:01,125: Snapshot:0	Epoch:10	Loss:0.775	translation_Loss:0.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:47.97	Best:27.52
2024-12-29 04:17:12,046: Snapshot:0	Epoch:11	Loss:0.722	translation_Loss:0.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.26	Hits@10:47.64	Best:27.52
2024-12-29 04:17:22,816: Snapshot:0	Epoch:12	Loss:0.679	translation_Loss:0.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.23	Hits@10:47.59	Best:27.52
2024-12-29 04:17:33,179: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 27.52
2024-12-29 04:17:33,179: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.638 MRR:27.01 Best Results: 27.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:17:33,179: Snapshot:0	Epoch:13	Loss:0.638	translation_Loss:0.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.01	Hits@10:47.45	Best:27.52
2024-12-29 04:17:44,428: Snapshot:0	Epoch:14	Loss:32.095	translation_Loss:16.517	multi_layer_Loss:15.578	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.01	Hits@10:47.45	Best:27.52
2024-12-29 04:17:55,086: End of token training: 0 Epoch: 15 Loss:16.624 MRR:27.01 Best Results: 27.52
2024-12-29 04:17:55,086: Snapshot:0	Epoch:15	Loss:16.624	translation_Loss:16.534	multi_layer_Loss:0.09	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.01	Hits@10:47.45	Best:27.52
2024-12-29 04:17:55,358: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2024-12-29 04:18:00,091: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2777 | 0.1669 | 0.3426 | 0.4117 |  0.4834 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:18:32,701: Snapshot:1	Epoch:0	Loss:20.688	translation_Loss:19.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.536                                                   	MRR:9.85	Hits@10:23.88	Best:9.85
2024-12-29 04:18:42,142: Snapshot:1	Epoch:1	Loss:11.926	translation_Loss:9.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.018                                                   	MRR:14.59	Hits@10:29.54	Best:14.59
2024-12-29 04:18:51,369: Snapshot:1	Epoch:2	Loss:8.234	translation_Loss:6.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.898                                                   	MRR:14.82	Hits@10:29.96	Best:14.82
2024-12-29 04:19:00,899: Snapshot:1	Epoch:3	Loss:6.833	translation_Loss:5.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.826                                                   	MRR:14.9	Hits@10:30.08	Best:14.9
2024-12-29 04:19:10,294: Snapshot:1	Epoch:4	Loss:6.307	translation_Loss:4.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.767                                                   	MRR:15.11	Hits@10:29.99	Best:15.11
2024-12-29 04:19:20,163: Snapshot:1	Epoch:5	Loss:6.055	translation_Loss:4.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.726                                                   	MRR:15.33	Hits@10:29.89	Best:15.33
2024-12-29 04:19:29,731: Snapshot:1	Epoch:6	Loss:5.922	translation_Loss:4.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.698                                                   	MRR:15.38	Hits@10:29.99	Best:15.38
2024-12-29 04:19:39,454: Snapshot:1	Epoch:7	Loss:5.834	translation_Loss:4.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.676                                                   	MRR:15.29	Hits@10:30.02	Best:15.38
2024-12-29 04:19:48,770: Snapshot:1	Epoch:8	Loss:5.79	translation_Loss:4.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.662                                                   	MRR:15.27	Hits@10:29.65	Best:15.38
2024-12-29 04:19:58,480: Snapshot:1	Epoch:9	Loss:5.748	translation_Loss:4.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.651                                                   	MRR:15.17	Hits@10:29.87	Best:15.38
2024-12-29 04:20:08,580: Snapshot:1	Epoch:10	Loss:5.729	translation_Loss:4.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.65                                                   	MRR:15.09	Hits@10:29.82	Best:15.38
2024-12-29 04:20:17,908: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 15.38
2024-12-29 04:20:17,909: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:5.687 MRR:15.09 Best Results: 15.38
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:20:17,909: Snapshot:1	Epoch:11	Loss:5.687	translation_Loss:4.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.642                                                   	MRR:15.09	Hits@10:29.92	Best:15.38
2024-12-29 04:20:27,509: Snapshot:1	Epoch:12	Loss:35.956	translation_Loss:20.267	multi_layer_Loss:15.689	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.09	Hits@10:29.92	Best:15.38
2024-12-29 04:20:36,796: End of token training: 1 Epoch: 13 Loss:20.375 MRR:15.09 Best Results: 15.38
2024-12-29 04:20:36,796: Snapshot:1	Epoch:13	Loss:20.375	translation_Loss:20.264	multi_layer_Loss:0.111	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.09	Hits@10:29.92	Best:15.38
2024-12-29 04:20:37,072: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2024-12-29 04:20:46,452: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2599 | 0.1435 | 0.328  | 0.4008 |  0.4746 |
|     1      | 0.1549 | 0.0798 | 0.183  | 0.233  |  0.2998 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:21:11,649: Snapshot:2	Epoch:0	Loss:13.516	translation_Loss:12.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.431                                                   	MRR:10.82	Hits@10:25.39	Best:10.82
2024-12-29 04:21:18,746: Snapshot:2	Epoch:1	Loss:6.915	translation_Loss:4.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.423                                                   	MRR:19.68	Hits@10:36.99	Best:19.68
2024-12-29 04:21:25,988: Snapshot:2	Epoch:2	Loss:4.834	translation_Loss:2.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.935                                                   	MRR:20.32	Hits@10:37.38	Best:20.32
2024-12-29 04:21:33,079: Snapshot:2	Epoch:3	Loss:3.871	translation_Loss:2.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.624                                                   	MRR:20.89	Hits@10:37.25	Best:20.89
2024-12-29 04:21:40,011: Snapshot:2	Epoch:4	Loss:3.354	translation_Loss:1.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.41                                                   	MRR:20.97	Hits@10:37.36	Best:20.97
2024-12-29 04:21:46,982: Snapshot:2	Epoch:5	Loss:3.069	translation_Loss:1.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.284                                                   	MRR:20.74	Hits@10:36.91	Best:20.97
2024-12-29 04:21:54,423: Snapshot:2	Epoch:6	Loss:2.906	translation_Loss:1.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.204                                                   	MRR:20.8	Hits@10:36.91	Best:20.97
2024-12-29 04:22:01,454: Snapshot:2	Epoch:7	Loss:2.825	translation_Loss:1.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.16                                                   	MRR:20.7	Hits@10:36.92	Best:20.97
2024-12-29 04:22:08,547: Snapshot:2	Epoch:8	Loss:2.768	translation_Loss:1.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.132                                                   	MRR:20.81	Hits@10:36.71	Best:20.97
2024-12-29 04:22:15,994: Snapshot:2	Epoch:9	Loss:2.741	translation_Loss:1.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.116                                                   	MRR:21.06	Hits@10:36.86	Best:21.06
2024-12-29 04:22:23,104: Snapshot:2	Epoch:10	Loss:2.723	translation_Loss:1.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.111                                                   	MRR:20.76	Hits@10:36.83	Best:21.06
2024-12-29 04:22:30,550: Snapshot:2	Epoch:11	Loss:2.701	translation_Loss:1.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.1                                                   	MRR:20.92	Hits@10:36.74	Best:21.06
2024-12-29 04:22:37,506: Snapshot:2	Epoch:12	Loss:2.689	translation_Loss:1.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:20.83	Hits@10:36.67	Best:21.06
2024-12-29 04:22:44,828: Snapshot:2	Epoch:13	Loss:2.676	translation_Loss:1.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.091                                                   	MRR:20.79	Hits@10:36.79	Best:21.06
2024-12-29 04:22:51,794: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 21.06
2024-12-29 04:22:51,795: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:2.689 MRR:20.6 Best Results: 21.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:22:51,795: Snapshot:2	Epoch:14	Loss:2.689	translation_Loss:1.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:20.6	Hits@10:36.25	Best:21.06
2024-12-29 04:22:58,766: Snapshot:2	Epoch:15	Loss:31.334	translation_Loss:15.135	multi_layer_Loss:16.199	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.6	Hits@10:36.25	Best:21.06
2024-12-29 04:23:05,987: End of token training: 2 Epoch: 16 Loss:15.441 MRR:20.6 Best Results: 21.06
2024-12-29 04:23:05,987: Snapshot:2	Epoch:16	Loss:15.441	translation_Loss:15.135	multi_layer_Loss:0.306	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.6	Hits@10:36.25	Best:21.06
2024-12-29 04:23:06,263: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2024-12-29 04:23:18,026: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2231 | 0.1179 | 0.2799 | 0.3471 |  0.421  |
|     1      | 0.1491 | 0.0732 | 0.1765 | 0.2273 |  0.2952 |
|     2      | 0.2073 |  0.13  | 0.2303 | 0.2826 |  0.3643 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:23:32,042: Snapshot:3	Epoch:0	Loss:6.503	translation_Loss:6.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:7.11	Hits@10:16.56	Best:7.11
2024-12-29 04:23:35,370: Snapshot:3	Epoch:1	Loss:3.891	translation_Loss:3.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.692                                                   	MRR:18.8	Hits@10:37.7	Best:18.8
2024-12-29 04:23:38,671: Snapshot:3	Epoch:2	Loss:2.584	translation_Loss:1.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.886                                                   	MRR:24.25	Hits@10:42.34	Best:24.25
2024-12-29 04:23:42,332: Snapshot:3	Epoch:3	Loss:1.925	translation_Loss:1.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.88                                                   	MRR:26.31	Hits@10:44.22	Best:26.31
2024-12-29 04:23:45,675: Snapshot:3	Epoch:4	Loss:1.587	translation_Loss:0.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.802                                                   	MRR:27.3	Hits@10:44.98	Best:27.3
2024-12-29 04:23:49,035: Snapshot:3	Epoch:5	Loss:1.38	translation_Loss:0.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.725                                                   	MRR:27.69	Hits@10:45.33	Best:27.69
2024-12-29 04:23:52,436: Snapshot:3	Epoch:6	Loss:1.238	translation_Loss:0.568	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.67                                                   	MRR:28.01	Hits@10:45.37	Best:28.01
2024-12-29 04:23:55,766: Snapshot:3	Epoch:7	Loss:1.138	translation_Loss:0.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.629                                                   	MRR:28.04	Hits@10:44.83	Best:28.04
2024-12-29 04:23:59,516: Snapshot:3	Epoch:8	Loss:1.06	translation_Loss:0.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.592                                                   	MRR:28.43	Hits@10:44.89	Best:28.43
2024-12-29 04:24:02,879: Snapshot:3	Epoch:9	Loss:0.991	translation_Loss:0.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.564                                                   	MRR:28.64	Hits@10:44.9	Best:28.64
2024-12-29 04:24:06,292: Snapshot:3	Epoch:10	Loss:0.954	translation_Loss:0.415	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.54                                                   	MRR:28.74	Hits@10:44.86	Best:28.74
2024-12-29 04:24:09,577: Snapshot:3	Epoch:11	Loss:0.921	translation_Loss:0.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.521                                                   	MRR:28.55	Hits@10:44.47	Best:28.74
2024-12-29 04:24:12,788: Snapshot:3	Epoch:12	Loss:0.895	translation_Loss:0.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.509                                                   	MRR:28.32	Hits@10:44.69	Best:28.74
2024-12-29 04:24:16,458: Snapshot:3	Epoch:13	Loss:0.869	translation_Loss:0.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.495                                                   	MRR:28.2	Hits@10:44.74	Best:28.74
2024-12-29 04:24:19,612: Snapshot:3	Epoch:14	Loss:0.853	translation_Loss:0.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.484                                                   	MRR:28.53	Hits@10:44.56	Best:28.74
2024-12-29 04:24:22,784: Early Stopping! Snapshot: 3 Epoch: 15 Best Results: 28.74
2024-12-29 04:24:22,784: Start to training tokens! Snapshot: 3 Epoch: 15 Loss:0.84 MRR:28.15 Best Results: 28.74
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:24:22,784: Snapshot:3	Epoch:15	Loss:0.84	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:28.15	Hits@10:43.93	Best:28.74
2024-12-29 04:24:25,955: Snapshot:3	Epoch:16	Loss:17.673	translation_Loss:5.212	multi_layer_Loss:12.462	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.15	Hits@10:43.93	Best:28.74
2024-12-29 04:24:29,187: End of token training: 3 Epoch: 17 Loss:6.607 MRR:28.15 Best Results: 28.74
2024-12-29 04:24:29,187: Snapshot:3	Epoch:17	Loss:6.607	translation_Loss:5.202	multi_layer_Loss:1.405	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.15	Hits@10:43.93	Best:28.74
2024-12-29 04:24:29,464: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2024-12-29 04:24:43,069: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2202 | 0.1173 | 0.2767 | 0.3429 |  0.4154 |
|     1      | 0.147  | 0.0715 | 0.172  | 0.2236 |  0.293  |
|     2      | 0.1826 | 0.1099 | 0.2011 | 0.2489 |  0.3253 |
|     3      | 0.2862 | 0.1998 | 0.3188 | 0.3756 |  0.4495 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:24:54,330: Snapshot:4	Epoch:0	Loss:3.71	translation_Loss:3.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.203                                                   	MRR:8.61	Hits@10:22.2	Best:8.61
2024-12-29 04:24:56,732: Snapshot:4	Epoch:1	Loss:2.189	translation_Loss:1.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.427                                                   	MRR:15.29	Hits@10:34.93	Best:15.29
2024-12-29 04:24:59,152: Snapshot:4	Epoch:2	Loss:1.544	translation_Loss:0.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.574                                                   	MRR:19.75	Hits@10:42.14	Best:19.75
2024-12-29 04:25:01,546: Snapshot:4	Epoch:3	Loss:1.162	translation_Loss:0.555	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:23.26	Hits@10:44.98	Best:23.26
2024-12-29 04:25:03,961: Snapshot:4	Epoch:4	Loss:0.928	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.566                                                   	MRR:24.8	Hits@10:46.67	Best:24.8
2024-12-29 04:25:06,781: Snapshot:4	Epoch:5	Loss:0.794	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.511                                                   	MRR:25.97	Hits@10:47.42	Best:25.97
2024-12-29 04:25:09,225: Snapshot:4	Epoch:6	Loss:0.7	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.463                                                   	MRR:26.48	Hits@10:48.34	Best:26.48
2024-12-29 04:25:11,673: Snapshot:4	Epoch:7	Loss:0.625	translation_Loss:0.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.425                                                   	MRR:26.55	Hits@10:48.83	Best:26.55
2024-12-29 04:25:14,069: Snapshot:4	Epoch:8	Loss:0.57	translation_Loss:0.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:26.99	Hits@10:49.36	Best:26.99
2024-12-29 04:25:16,373: Snapshot:4	Epoch:9	Loss:0.526	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.364                                                   	MRR:27.23	Hits@10:49.97	Best:27.23
2024-12-29 04:25:18,808: Snapshot:4	Epoch:10	Loss:0.491	translation_Loss:0.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:27.34	Hits@10:49.91	Best:27.34
2024-12-29 04:25:21,210: Snapshot:4	Epoch:11	Loss:0.462	translation_Loss:0.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.321                                                   	MRR:27.24	Hits@10:50.1	Best:27.34
2024-12-29 04:25:23,645: Snapshot:4	Epoch:12	Loss:0.44	translation_Loss:0.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.304                                                   	MRR:27.49	Hits@10:50.17	Best:27.49
2024-12-29 04:25:26,435: Snapshot:4	Epoch:13	Loss:0.422	translation_Loss:0.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.292                                                   	MRR:27.4	Hits@10:50.02	Best:27.49
2024-12-29 04:25:28,757: Snapshot:4	Epoch:14	Loss:0.412	translation_Loss:0.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.282                                                   	MRR:27.22	Hits@10:50.02	Best:27.49
2024-12-29 04:25:31,088: Snapshot:4	Epoch:15	Loss:0.4	translation_Loss:0.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.274                                                   	MRR:27.05	Hits@10:50.29	Best:27.49
2024-12-29 04:25:33,429: Snapshot:4	Epoch:16	Loss:0.387	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.266                                                   	MRR:27.48	Hits@10:49.98	Best:27.49
2024-12-29 04:25:35,739: Early Stopping! Snapshot: 4 Epoch: 17 Best Results: 27.49
2024-12-29 04:25:35,739: Start to training tokens! Snapshot: 4 Epoch: 17 Loss:0.376 MRR:27.29 Best Results: 27.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:25:35,739: Snapshot:4	Epoch:17	Loss:0.376	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:27.29	Hits@10:50.03	Best:27.49
2024-12-29 04:25:38,128: Snapshot:4	Epoch:18	Loss:15.228	translation_Loss:3.218	multi_layer_Loss:12.01	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.29	Hits@10:50.03	Best:27.49
2024-12-29 04:25:40,388: End of token training: 4 Epoch: 19 Loss:6.038 MRR:27.29 Best Results: 27.49
2024-12-29 04:25:40,389: Snapshot:4	Epoch:19	Loss:6.038	translation_Loss:3.232	multi_layer_Loss:2.806	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.29	Hits@10:50.03	Best:27.49
2024-12-29 04:25:40,611: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2024-12-29 04:25:55,730: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2085 | 0.1086 | 0.2621 | 0.3252 |  0.3955 |
|     1      | 0.1462 | 0.0713 | 0.1717 | 0.2229 |  0.2928 |
|     2      | 0.1716 | 0.1036 | 0.1873 | 0.2341 |  0.3076 |
|     3      | 0.2676 | 0.1907 | 0.2944 | 0.3383 |  0.4072 |
|     4      | 0.2701 | 0.1571 | 0.3062 | 0.3846 |  0.502  |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:25:55,732: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2777 | 0.1669 | 0.3426 | 0.4117 |  0.4834 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2599 | 0.1435 | 0.328  | 0.4008 |  0.4746 |
|     1      | 0.1549 | 0.0798 | 0.183  | 0.233  |  0.2998 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2231 | 0.1179 | 0.2799 | 0.3471 |  0.421  |
|     1      | 0.1491 | 0.0732 | 0.1765 | 0.2273 |  0.2952 |
|     2      | 0.2073 |  0.13  | 0.2303 | 0.2826 |  0.3643 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2202 | 0.1173 | 0.2767 | 0.3429 |  0.4154 |
|     1      | 0.147  | 0.0715 | 0.172  | 0.2236 |  0.293  |
|     2      | 0.1826 | 0.1099 | 0.2011 | 0.2489 |  0.3253 |
|     3      | 0.2862 | 0.1998 | 0.3188 | 0.3756 |  0.4495 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2085 | 0.1086 | 0.2621 | 0.3252 |  0.3955 |
|     1      | 0.1462 | 0.0713 | 0.1717 | 0.2229 |  0.2928 |
|     2      | 0.1716 | 0.1036 | 0.1873 | 0.2341 |  0.3076 |
|     3      | 0.2676 | 0.1907 | 0.2944 | 0.3383 |  0.4072 |
|     4      | 0.2701 | 0.1571 | 0.3062 | 0.3846 |  0.502  |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:25:55,732: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 176.69837856292725 |   0.278   |    0.167     |    0.343     |     0.483     |
|    1     | 152.94279599189758 |   0.209   |    0.113     |    0.257     |      0.39     |
|    2     | 136.35947394371033 |   0.192   |    0.105     |     0.23     |     0.361     |
|    3     | 69.42521929740906  |   0.195   |    0.109     |     0.23     |     0.359     |
|    4     | 55.980815410614014 |   0.192   |    0.108     |    0.225     |     0.354     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:25:55,732: Sum_Training_Time:591.4066832065582
2024-12-29 04:25:55,732: Every_Training_Time:[176.69837856292725, 152.94279599189758, 136.35947394371033, 69.42521929740906, 55.980815410614014]
2024-12-29 04:25:55,733: Forward transfer: 0.0177 Backward transfer: -0.03305000000000001
[lijing@p0315 IncDE]$ python main.py -dataset RELATION -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 3 -multi_layer_weight 1 -token_distillation_weight 3000 3000 50000 50000
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2025-01-03 22:04:34,770: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103220403/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[3000.0, 3000.0, 50000.0, 50000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 22:04:51,259: Snapshot:0	Epoch:0	Loss:24.263	translation_Loss:24.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.68	Hits@10:26.04	Best:10.68
2025-01-03 22:05:03,263: Snapshot:0	Epoch:1	Loss:15.05	translation_Loss:15.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.44	Hits@10:40.4	Best:18.44
2025-01-03 22:05:15,715: Snapshot:0	Epoch:2	Loss:8.578	translation_Loss:8.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.52	Hits@10:45.18	Best:23.52
2025-01-03 22:05:28,038: Snapshot:0	Epoch:3	Loss:4.718	translation_Loss:4.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:47.24	Best:25.74
2025-01-03 22:05:40,261: Snapshot:0	Epoch:4	Loss:2.731	translation_Loss:2.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.76	Hits@10:47.97	Best:26.76
2025-01-03 22:05:52,714: Snapshot:0	Epoch:5	Loss:1.812	translation_Loss:1.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.23	Hits@10:48.15	Best:27.23
2025-01-03 22:06:05,254: Snapshot:0	Epoch:6	Loss:1.353	translation_Loss:1.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.41	Hits@10:48.18	Best:27.41
2025-01-03 22:06:17,532: Snapshot:0	Epoch:7	Loss:1.101	translation_Loss:1.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.27	Best:27.41
2025-01-03 22:06:29,928: Snapshot:0	Epoch:8	Loss:0.955	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.51	Hits@10:48.28	Best:27.51
2025-01-03 22:06:42,436: Snapshot:0	Epoch:9	Loss:0.847	translation_Loss:0.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.38	Hits@10:48.17	Best:27.51
2025-01-03 22:06:54,356: Snapshot:0	Epoch:10	Loss:0.776	translation_Loss:0.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.3	Hits@10:47.92	Best:27.51
2025-01-03 22:07:06,770: Early Stopping! Snapshot: 0 Epoch: 11 Best Results: 27.51
2025-01-03 22:07:06,771: Start to training tokens! Snapshot: 0 Epoch: 11 Loss:0.723 MRR:27.36 Best Results: 27.51
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:07:06,771: Snapshot:0	Epoch:11	Loss:0.723	translation_Loss:0.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.36	Hits@10:47.61	Best:27.51
2025-01-03 22:07:19,621: Snapshot:0	Epoch:12	Loss:32.129	translation_Loss:16.551	multi_layer_Loss:15.578	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.36	Hits@10:47.61	Best:27.51
2025-01-03 22:07:31,652: End of token training: 0 Epoch: 13 Loss:16.652 MRR:27.36 Best Results: 27.51
2025-01-03 22:07:31,652: Snapshot:0	Epoch:13	Loss:16.652	translation_Loss:16.562	multi_layer_Loss:0.09	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.36	Hits@10:47.61	Best:27.51
2025-01-03 22:07:31,922: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2025-01-03 22:07:37,512: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.277 | 0.1658 | 0.3422 | 0.4111 |  0.4841 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:08:13,067: Snapshot:1	Epoch:0	Loss:20.105	translation_Loss:18.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.223                                                   	MRR:10.46	Hits@10:25.18	Best:10.46
2025-01-03 22:08:24,486: Snapshot:1	Epoch:1	Loss:10.283	translation_Loss:8.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.205                                                   	MRR:15.65	Hits@10:32.03	Best:15.65
2025-01-03 22:08:36,171: Snapshot:1	Epoch:2	Loss:6.589	translation_Loss:4.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.345                                                   	MRR:16.89	Hits@10:33.42	Best:16.89
2025-01-03 22:08:47,249: Snapshot:1	Epoch:3	Loss:5.234	translation_Loss:3.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.226                                                   	MRR:16.89	Hits@10:33.15	Best:16.89
2025-01-03 22:08:58,618: Snapshot:1	Epoch:4	Loss:4.721	translation_Loss:2.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.11                                                   	MRR:16.96	Hits@10:32.82	Best:16.96
2025-01-03 22:09:09,948: Snapshot:1	Epoch:5	Loss:4.506	translation_Loss:2.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.041                                                   	MRR:17.02	Hits@10:32.81	Best:17.02
2025-01-03 22:09:21,118: Snapshot:1	Epoch:6	Loss:4.408	translation_Loss:2.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.008                                                   	MRR:16.93	Hits@10:32.75	Best:17.02
2025-01-03 22:09:32,400: Snapshot:1	Epoch:7	Loss:4.325	translation_Loss:2.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.979                                                   	MRR:16.93	Hits@10:32.76	Best:17.02
2025-01-03 22:09:43,218: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 17.02
2025-01-03 22:09:43,218: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:4.281 MRR:16.88 Best Results: 17.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:09:43,219: Snapshot:1	Epoch:8	Loss:4.281	translation_Loss:2.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.961                                                   	MRR:16.88	Hits@10:32.65	Best:17.02
2025-01-03 22:09:54,510: Snapshot:1	Epoch:9	Loss:34.626	translation_Loss:18.938	multi_layer_Loss:15.689	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.88	Hits@10:32.65	Best:17.02
2025-01-03 22:10:05,921: End of token training: 1 Epoch: 10 Loss:19.021 MRR:16.88 Best Results: 17.02
2025-01-03 22:10:05,921: Snapshot:1	Epoch:10	Loss:19.021	translation_Loss:18.91	multi_layer_Loss:0.111	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:16.88	Hits@10:32.65	Best:17.02
2025-01-03 22:10:06,199: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2025-01-03 22:10:16,133: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2476 | 0.1386 | 0.3073 | 0.3782 |  0.4527 |
|     1      | 0.1703 | 0.0887 | 0.2015 | 0.2556 |  0.3267 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:10:43,042: Snapshot:2	Epoch:0	Loss:12.357	translation_Loss:11.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.944                                                   	MRR:11.46	Hits@10:26.88	Best:11.46
2025-01-03 22:10:51,154: Snapshot:2	Epoch:1	Loss:5.123	translation_Loss:3.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.817                                                   	MRR:19.38	Hits@10:37.67	Best:19.38
2025-01-03 22:10:59,929: Snapshot:2	Epoch:2	Loss:3.339	translation_Loss:1.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.64                                                   	MRR:22.0	Hits@10:39.94	Best:22.0
2025-01-03 22:11:08,112: Snapshot:2	Epoch:3	Loss:2.636	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.401                                                   	MRR:22.08	Hits@10:39.92	Best:22.08
2025-01-03 22:11:16,602: Snapshot:2	Epoch:4	Loss:2.259	translation_Loss:1.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.249                                                   	MRR:22.29	Hits@10:39.88	Best:22.29
2025-01-03 22:11:24,703: Snapshot:2	Epoch:5	Loss:2.043	translation_Loss:0.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.137                                                   	MRR:22.17	Hits@10:39.95	Best:22.29
2025-01-03 22:11:33,354: Snapshot:2	Epoch:6	Loss:1.913	translation_Loss:0.849	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.064                                                   	MRR:22.03	Hits@10:39.4	Best:22.29
2025-01-03 22:11:41,611: Snapshot:2	Epoch:7	Loss:1.818	translation_Loss:0.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.009                                                   	MRR:22.35	Hits@10:39.95	Best:22.35
2025-01-03 22:11:50,105: Snapshot:2	Epoch:8	Loss:1.767	translation_Loss:0.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.977                                                   	MRR:22.42	Hits@10:39.44	Best:22.42
2025-01-03 22:11:58,218: Snapshot:2	Epoch:9	Loss:1.719	translation_Loss:0.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.953                                                   	MRR:22.27	Hits@10:39.18	Best:22.42
2025-01-03 22:12:06,233: Snapshot:2	Epoch:10	Loss:1.698	translation_Loss:0.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.934                                                   	MRR:21.76	Hits@10:39.04	Best:22.42
2025-01-03 22:12:14,812: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 22.42
2025-01-03 22:12:14,813: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:1.678 MRR:22.26 Best Results: 22.42
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:12:14,813: Snapshot:2	Epoch:11	Loss:1.678	translation_Loss:0.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.925                                                   	MRR:22.26	Hits@10:39.12	Best:22.42
2025-01-03 22:12:22,873: Snapshot:2	Epoch:12	Loss:29.926	translation_Loss:13.728	multi_layer_Loss:16.199	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.26	Hits@10:39.12	Best:22.42
2025-01-03 22:12:31,269: End of token training: 2 Epoch: 13 Loss:14.032 MRR:22.26 Best Results: 22.42
2025-01-03 22:12:31,270: Snapshot:2	Epoch:13	Loss:14.032	translation_Loss:13.726	multi_layer_Loss:0.306	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.26	Hits@10:39.12	Best:22.42
2025-01-03 22:12:31,549: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2025-01-03 22:12:44,314: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2082 | 0.1115 | 0.2609 | 0.3219 |  0.3885 |
|     1      | 0.1562 | 0.0773 | 0.1841 | 0.2369 |  0.3079 |
|     2      | 0.2185 | 0.1344 | 0.2422 | 0.3008 |  0.3885 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:12:59,685: Snapshot:3	Epoch:0	Loss:6.519	translation_Loss:5.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.586                                                   	MRR:7.43	Hits@10:16.66	Best:7.43
2025-01-03 22:13:03,518: Snapshot:3	Epoch:1	Loss:4.447	translation_Loss:3.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.733                                                   	MRR:15.33	Hits@10:32.21	Best:15.33
2025-01-03 22:13:07,360: Snapshot:3	Epoch:2	Loss:3.227	translation_Loss:2.582	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.645                                                   	MRR:19.3	Hits@10:36.52	Best:19.3
2025-01-03 22:13:11,175: Snapshot:3	Epoch:3	Loss:2.524	translation_Loss:1.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.563                                                   	MRR:22.42	Hits@10:38.17	Best:22.42
2025-01-03 22:13:14,948: Snapshot:3	Epoch:4	Loss:2.122	translation_Loss:1.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.509                                                   	MRR:24.04	Hits@10:39.54	Best:24.04
2025-01-03 22:13:18,764: Snapshot:3	Epoch:5	Loss:1.856	translation_Loss:1.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.464                                                   	MRR:24.94	Hits@10:40.34	Best:24.94
2025-01-03 22:13:22,525: Snapshot:3	Epoch:6	Loss:1.659	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.424                                                   	MRR:25.49	Hits@10:40.56	Best:25.49
2025-01-03 22:13:26,280: Snapshot:3	Epoch:7	Loss:1.515	translation_Loss:1.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.394                                                   	MRR:25.86	Hits@10:40.83	Best:25.86
2025-01-03 22:13:30,013: Snapshot:3	Epoch:8	Loss:1.412	translation_Loss:1.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.365                                                   	MRR:26.14	Hits@10:40.74	Best:26.14
2025-01-03 22:13:33,703: Snapshot:3	Epoch:9	Loss:1.334	translation_Loss:0.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.348                                                   	MRR:26.06	Hits@10:40.8	Best:26.14
2025-01-03 22:13:37,932: Snapshot:3	Epoch:10	Loss:1.28	translation_Loss:0.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:26.24	Hits@10:40.52	Best:26.24
2025-01-03 22:13:41,766: Snapshot:3	Epoch:11	Loss:1.248	translation_Loss:0.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:26.34	Hits@10:40.59	Best:26.34
2025-01-03 22:13:45,463: Snapshot:3	Epoch:12	Loss:1.215	translation_Loss:0.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:26.33	Hits@10:40.65	Best:26.34
2025-01-03 22:13:49,315: Snapshot:3	Epoch:13	Loss:1.184	translation_Loss:0.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.308                                                   	MRR:26.36	Hits@10:40.42	Best:26.36
2025-01-03 22:13:53,135: Snapshot:3	Epoch:14	Loss:1.169	translation_Loss:0.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.302                                                   	MRR:26.32	Hits@10:40.52	Best:26.36
2025-01-03 22:13:57,339: Snapshot:3	Epoch:15	Loss:1.152	translation_Loss:0.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.298                                                   	MRR:26.42	Hits@10:40.71	Best:26.42
2025-01-03 22:14:01,124: Snapshot:3	Epoch:16	Loss:1.142	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.296                                                   	MRR:26.4	Hits@10:40.8	Best:26.42
2025-01-03 22:14:04,866: Snapshot:3	Epoch:17	Loss:1.137	translation_Loss:0.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.293                                                   	MRR:26.49	Hits@10:40.67	Best:26.49
2025-01-03 22:14:08,597: Snapshot:3	Epoch:18	Loss:1.129	translation_Loss:0.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.292                                                   	MRR:26.33	Hits@10:40.57	Best:26.49
2025-01-03 22:14:12,304: Snapshot:3	Epoch:19	Loss:1.118	translation_Loss:0.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.29                                                   	MRR:26.32	Hits@10:40.72	Best:26.49
2025-01-03 22:14:16,436: Early Stopping! Snapshot: 3 Epoch: 20 Best Results: 26.49
2025-01-03 22:14:16,436: Start to training tokens! Snapshot: 3 Epoch: 20 Loss:1.113 MRR:26.43 Best Results: 26.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:14:16,437: Snapshot:3	Epoch:20	Loss:1.113	translation_Loss:0.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.286                                                   	MRR:26.43	Hits@10:40.44	Best:26.49
2025-01-03 22:14:20,130: Snapshot:3	Epoch:21	Loss:18.203	translation_Loss:5.741	multi_layer_Loss:12.462	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.43	Hits@10:40.44	Best:26.49
2025-01-03 22:14:23,800: End of token training: 3 Epoch: 22 Loss:7.143 MRR:26.43 Best Results: 26.49
2025-01-03 22:14:23,800: Snapshot:3	Epoch:22	Loss:7.143	translation_Loss:5.739	multi_layer_Loss:1.405	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:26.43	Hits@10:40.44	Best:26.49
2025-01-03 22:14:24,140: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2025-01-03 22:14:38,836: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2086 | 0.1125 | 0.2608 | 0.3224 |  0.3889 |
|     1      | 0.1567 | 0.077  | 0.185  | 0.2383 |  0.3097 |
|     2      | 0.2123 | 0.1262 | 0.236  | 0.2971 |  0.3865 |
|     3      | 0.2596 | 0.1819 | 0.2926 | 0.3334 |  0.3965 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:14:51,152: Snapshot:4	Epoch:0	Loss:4.33	translation_Loss:3.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.411                                                   	MRR:7.03	Hits@10:18.6	Best:7.03
2025-01-03 22:14:53,883: Snapshot:4	Epoch:1	Loss:3.109	translation_Loss:2.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.709                                                   	MRR:12.12	Hits@10:29.8	Best:12.12
2025-01-03 22:14:56,648: Snapshot:4	Epoch:2	Loss:2.496	translation_Loss:1.75	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.746                                                   	MRR:15.46	Hits@10:36.24	Best:15.46
2025-01-03 22:14:59,376: Snapshot:4	Epoch:3	Loss:2.045	translation_Loss:1.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.692                                                   	MRR:19.24	Hits@10:38.94	Best:19.24
2025-01-03 22:15:02,033: Snapshot:4	Epoch:4	Loss:1.731	translation_Loss:1.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.625                                                   	MRR:21.03	Hits@10:40.94	Best:21.03
2025-01-03 22:15:04,788: Snapshot:4	Epoch:5	Loss:1.514	translation_Loss:0.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.568                                                   	MRR:22.27	Hits@10:42.95	Best:22.27
2025-01-03 22:15:07,495: Snapshot:4	Epoch:6	Loss:1.345	translation_Loss:0.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.523                                                   	MRR:22.97	Hits@10:44.13	Best:22.97
2025-01-03 22:15:10,207: Snapshot:4	Epoch:7	Loss:1.215	translation_Loss:0.73	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.485                                                   	MRR:23.47	Hits@10:43.88	Best:23.47
2025-01-03 22:15:13,288: Snapshot:4	Epoch:8	Loss:1.117	translation_Loss:0.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.457                                                   	MRR:23.37	Hits@10:44.36	Best:23.47
2025-01-03 22:15:16,011: Snapshot:4	Epoch:9	Loss:1.034	translation_Loss:0.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:23.49	Hits@10:44.2	Best:23.49
2025-01-03 22:15:18,742: Snapshot:4	Epoch:10	Loss:0.978	translation_Loss:0.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.411                                                   	MRR:23.63	Hits@10:44.3	Best:23.63
2025-01-03 22:15:21,388: Snapshot:4	Epoch:11	Loss:0.935	translation_Loss:0.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.395                                                   	MRR:23.44	Hits@10:44.37	Best:23.63
2025-01-03 22:15:24,037: Snapshot:4	Epoch:12	Loss:0.9	translation_Loss:0.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.382                                                   	MRR:23.21	Hits@10:43.9	Best:23.63
2025-01-03 22:15:26,698: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 23.63
2025-01-03 22:15:26,698: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:0.872 MRR:23.24 Best Results: 23.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:15:26,698: Snapshot:4	Epoch:13	Loss:0.872	translation_Loss:0.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.371                                                   	MRR:23.24	Hits@10:44.0	Best:23.63
2025-01-03 22:15:29,351: Snapshot:4	Epoch:14	Loss:15.813	translation_Loss:3.803	multi_layer_Loss:12.01	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:44.0	Best:23.63
2025-01-03 22:15:32,425: End of token training: 4 Epoch: 15 Loss:6.611 MRR:23.24 Best Results: 23.63
2025-01-03 22:15:32,426: Snapshot:4	Epoch:15	Loss:6.611	translation_Loss:3.805	multi_layer_Loss:2.806	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.24	Hits@10:44.0	Best:23.63
2025-01-03 22:15:32,712: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2025-01-03 22:15:48,645: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2036 | 0.1078 | 0.2542 | 0.3158 |  0.3841 |
|     1      | 0.1561 | 0.077  | 0.1844 | 0.2375 |  0.309  |
|     2      | 0.2032 | 0.1213 | 0.2245 | 0.2838 |  0.3675 |
|     3      | 0.2514 | 0.1733 | 0.2787 | 0.3247 |  0.3969 |
|     4      | 0.2335 | 0.1356 | 0.2585 | 0.3284 |  0.4355 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 22:15:48,647: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.277 | 0.1658 | 0.3422 | 0.4111 |  0.4841 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2476 | 0.1386 | 0.3073 | 0.3782 |  0.4527 |
|     1      | 0.1703 | 0.0887 | 0.2015 | 0.2556 |  0.3267 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2082 | 0.1115 | 0.2609 | 0.3219 |  0.3885 |
|     1      | 0.1562 | 0.0773 | 0.1841 | 0.2369 |  0.3079 |
|     2      | 0.2185 | 0.1344 | 0.2422 | 0.3008 |  0.3885 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2086 | 0.1125 | 0.2608 | 0.3224 |  0.3889 |
|     1      | 0.1567 | 0.077  | 0.185  | 0.2383 |  0.3097 |
|     2      | 0.2123 | 0.1262 | 0.236  | 0.2971 |  0.3865 |
|     3      | 0.2596 | 0.1819 | 0.2926 | 0.3334 |  0.3965 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2036 | 0.1078 | 0.2542 | 0.3158 |  0.3841 |
|     1      | 0.1561 | 0.077  | 0.1844 | 0.2375 |  0.309  |
|     2      | 0.2032 | 0.1213 | 0.2245 | 0.2838 |  0.3675 |
|     3      | 0.2514 | 0.1733 | 0.2787 | 0.3247 |  0.3969 |
|     4      | 0.2335 | 0.1356 | 0.2585 | 0.3284 |  0.4355 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 22:15:48,649: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 176.8812198638916  |   0.277   |    0.166     |    0.342     |     0.484     |
|    1     | 144.2104687690735  |    0.21   |    0.114     |    0.256     |     0.391     |
|    2     | 131.68839526176453 |   0.192   |    0.105     |    0.228     |     0.359     |
|    3     | 97.62408590316772  |   0.198   |    0.111     |    0.234     |     0.363     |
|    4     | 52.18702006340027  |   0.196   |     0.11     |    0.229     |     0.363     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 22:15:48,649: Sum_Training_Time:602.5911898612976
2025-01-03 22:15:48,649: Every_Training_Time:[176.8812198638916, 144.2104687690735, 131.68839526176453, 97.62408590316772, 52.18702006340027]
2025-01-03 22:15:48,649: Forward transfer: 0.0159 Backward transfer: -0.027775000000000008
[lijing@p0314 IncDE]$ python main.py -dataset RELATION -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 3 -multi_layer_weight 1 -token_distillation_weight 3000 6000 80000 80000
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2025-01-03 22:47:18,629: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103224642/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[3000.0, 6000.0, 80000.0, 80000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 22:47:33,845: Snapshot:0	Epoch:0	Loss:24.263	translation_Loss:24.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.68	Hits@10:26.04	Best:10.68
2025-01-03 22:47:44,265: Snapshot:0	Epoch:1	Loss:15.05	translation_Loss:15.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.44	Hits@10:40.4	Best:18.44
2025-01-03 22:47:55,078: Snapshot:0	Epoch:2	Loss:8.578	translation_Loss:8.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:45.17	Best:23.51
2025-01-03 22:48:06,082: Snapshot:0	Epoch:3	Loss:4.718	translation_Loss:4.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:47.22	Best:25.71
2025-01-03 22:48:16,637: Snapshot:0	Epoch:4	Loss:2.731	translation_Loss:2.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.72	Hits@10:47.86	Best:26.72
2025-01-03 22:48:27,613: Snapshot:0	Epoch:5	Loss:1.812	translation_Loss:1.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.2	Hits@10:48.26	Best:27.2
2025-01-03 22:48:38,521: Snapshot:0	Epoch:6	Loss:1.355	translation_Loss:1.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.43	Hits@10:48.26	Best:27.43
2025-01-03 22:48:48,974: Snapshot:0	Epoch:7	Loss:1.101	translation_Loss:1.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.33	Hits@10:48.37	Best:27.43
2025-01-03 22:48:59,713: Snapshot:0	Epoch:8	Loss:0.953	translation_Loss:0.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.37	Hits@10:48.16	Best:27.43
2025-01-03 22:49:10,506: Snapshot:0	Epoch:9	Loss:0.845	translation_Loss:0.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.47	Hits@10:48.05	Best:27.47
2025-01-03 22:49:20,928: Snapshot:0	Epoch:10	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.39	Hits@10:47.97	Best:27.47
2025-01-03 22:49:31,682: Snapshot:0	Epoch:11	Loss:0.722	translation_Loss:0.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.22	Hits@10:47.64	Best:27.47
2025-01-03 22:49:42,471: Early Stopping! Snapshot: 0 Epoch: 12 Best Results: 27.47
2025-01-03 22:49:42,471: Start to training tokens! Snapshot: 0 Epoch: 12 Loss:0.679 MRR:27.22 Best Results: 27.47
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:49:42,472: Snapshot:0	Epoch:12	Loss:0.679	translation_Loss:0.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.22	Hits@10:47.58	Best:27.47
2025-01-03 22:49:53,388: Snapshot:0	Epoch:13	Loss:32.117	translation_Loss:16.539	multi_layer_Loss:15.578	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.22	Hits@10:47.58	Best:27.47
2025-01-03 22:50:04,197: End of token training: 0 Epoch: 14 Loss:16.612 MRR:27.22 Best Results: 27.47
2025-01-03 22:50:04,198: Snapshot:0	Epoch:14	Loss:16.612	translation_Loss:16.523	multi_layer_Loss:0.09	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.22	Hits@10:47.58	Best:27.47
2025-01-03 22:50:04,487: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2025-01-03 22:50:09,362: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2762 | 0.1652 | 0.3415 | 0.4091 |  0.4827 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:50:42,050: Snapshot:1	Epoch:0	Loss:20.018	translation_Loss:18.801	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.217                                                   	MRR:10.38	Hits@10:25.13	Best:10.38
2025-01-03 22:50:52,196: Snapshot:1	Epoch:1	Loss:10.187	translation_Loss:7.998	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.189                                                   	MRR:15.73	Hits@10:31.98	Best:15.73
2025-01-03 22:51:01,999: Snapshot:1	Epoch:2	Loss:6.496	translation_Loss:4.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.32                                                   	MRR:16.65	Hits@10:32.79	Best:16.65
2025-01-03 22:51:11,397: Snapshot:1	Epoch:3	Loss:5.17	translation_Loss:2.98	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.191                                                   	MRR:16.47	Hits@10:32.81	Best:16.65
2025-01-03 22:51:21,310: Snapshot:1	Epoch:4	Loss:4.659	translation_Loss:2.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.07                                                   	MRR:16.85	Hits@10:32.82	Best:16.85
2025-01-03 22:51:30,639: Snapshot:1	Epoch:5	Loss:4.441	translation_Loss:2.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.011                                                   	MRR:16.55	Hits@10:32.62	Best:16.85
2025-01-03 22:51:40,384: Snapshot:1	Epoch:6	Loss:4.318	translation_Loss:2.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.97                                                   	MRR:16.81	Hits@10:32.73	Best:16.85
2025-01-03 22:51:50,186: Early Stopping! Snapshot: 1 Epoch: 7 Best Results: 16.85
2025-01-03 22:51:50,186: Start to training tokens! Snapshot: 1 Epoch: 7 Loss:4.263 MRR:16.81 Best Results: 16.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:51:50,186: Snapshot:1	Epoch:7	Loss:4.263	translation_Loss:2.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.943                                                   	MRR:16.81	Hits@10:32.71	Best:16.85
2025-01-03 22:51:59,488: Snapshot:1	Epoch:8	Loss:34.524	translation_Loss:18.835	multi_layer_Loss:15.689	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.81	Hits@10:32.71	Best:16.85
2025-01-03 22:52:09,171: End of token training: 1 Epoch: 9 Loss:18.952 MRR:16.81 Best Results: 16.85
2025-01-03 22:52:09,173: Snapshot:1	Epoch:9	Loss:18.952	translation_Loss:18.841	multi_layer_Loss:0.111	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:16.81	Hits@10:32.71	Best:16.85
2025-01-03 22:52:09,478: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2025-01-03 22:52:18,258: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.249  | 0.1414 | 0.3084 | 0.3763 |  0.4506 |
|     1      | 0.1692 | 0.0865 | 0.2006 | 0.2565 |  0.3298 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:52:43,452: Snapshot:2	Epoch:0	Loss:12.799	translation_Loss:11.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.289                                                   	MRR:11.31	Hits@10:26.47	Best:11.31
2025-01-03 22:52:50,462: Snapshot:2	Epoch:1	Loss:6.026	translation_Loss:3.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.241                                                   	MRR:19.92	Hits@10:37.85	Best:19.92
2025-01-03 22:52:57,501: Snapshot:2	Epoch:2	Loss:4.098	translation_Loss:2.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.796                                                   	MRR:21.3	Hits@10:39.11	Best:21.3
2025-01-03 22:53:04,891: Snapshot:2	Epoch:3	Loss:3.223	translation_Loss:1.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.501                                                   	MRR:21.91	Hits@10:39.43	Best:21.91
2025-01-03 22:53:11,902: Snapshot:2	Epoch:4	Loss:2.774	translation_Loss:1.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.303                                                   	MRR:21.89	Hits@10:39.0	Best:21.91
2025-01-03 22:53:19,309: Snapshot:2	Epoch:5	Loss:2.503	translation_Loss:1.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.176                                                   	MRR:21.76	Hits@10:38.67	Best:21.91
2025-01-03 22:53:26,259: Snapshot:2	Epoch:6	Loss:2.357	translation_Loss:1.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.099                                                   	MRR:21.93	Hits@10:38.46	Best:21.93
2025-01-03 22:53:33,212: Snapshot:2	Epoch:7	Loss:2.287	translation_Loss:1.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.051                                                   	MRR:21.91	Hits@10:38.21	Best:21.93
2025-01-03 22:53:40,546: Snapshot:2	Epoch:8	Loss:2.223	translation_Loss:1.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.024                                                   	MRR:21.66	Hits@10:38.09	Best:21.93
2025-01-03 22:53:47,576: Snapshot:2	Epoch:9	Loss:2.193	translation_Loss:1.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.009                                                   	MRR:21.96	Hits@10:38.03	Best:21.96
2025-01-03 22:53:54,996: Snapshot:2	Epoch:10	Loss:2.17	translation_Loss:1.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.996                                                   	MRR:21.99	Hits@10:38.07	Best:21.99
2025-01-03 22:54:01,921: Snapshot:2	Epoch:11	Loss:2.156	translation_Loss:1.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.989                                                   	MRR:21.65	Hits@10:38.07	Best:21.99
2025-01-03 22:54:09,238: Snapshot:2	Epoch:12	Loss:2.147	translation_Loss:1.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.987                                                   	MRR:21.48	Hits@10:37.97	Best:21.99
2025-01-03 22:54:16,216: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.99
2025-01-03 22:54:16,216: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:2.144 MRR:21.69 Best Results: 21.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:54:16,217: Snapshot:2	Epoch:13	Loss:2.144	translation_Loss:1.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.982                                                   	MRR:21.69	Hits@10:37.98	Best:21.99
2025-01-03 22:54:23,260: Snapshot:2	Epoch:14	Loss:30.619	translation_Loss:14.42	multi_layer_Loss:16.199	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.69	Hits@10:37.98	Best:21.99
2025-01-03 22:54:30,633: End of token training: 2 Epoch: 15 Loss:14.741 MRR:21.69 Best Results: 21.99
2025-01-03 22:54:30,633: Snapshot:2	Epoch:15	Loss:14.741	translation_Loss:14.435	multi_layer_Loss:0.306	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.69	Hits@10:37.98	Best:21.99
2025-01-03 22:54:30,954: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2025-01-03 22:54:42,697: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2145 | 0.1157 | 0.268  | 0.3308 |  0.3973 |
|     1      | 0.1582 | 0.0757 | 0.1883 | 0.2427 |  0.3178 |
|     2      | 0.2178 | 0.1371 | 0.2415 | 0.296  |  0.3777 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:54:56,936: Snapshot:3	Epoch:0	Loss:6.768	translation_Loss:6.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.682                                                   	MRR:6.23	Hits@10:14.24	Best:6.23
2025-01-03 22:55:00,323: Snapshot:3	Epoch:1	Loss:4.771	translation_Loss:4.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.698                                                   	MRR:14.18	Hits@10:29.92	Best:14.18
2025-01-03 22:55:03,638: Snapshot:3	Epoch:2	Loss:3.474	translation_Loss:2.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.587                                                   	MRR:17.84	Hits@10:34.3	Best:17.84
2025-01-03 22:55:06,972: Snapshot:3	Epoch:3	Loss:2.75	translation_Loss:2.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.491                                                   	MRR:21.35	Hits@10:36.51	Best:21.35
2025-01-03 22:55:10,276: Snapshot:3	Epoch:4	Loss:2.326	translation_Loss:1.884	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.442                                                   	MRR:23.05	Hits@10:38.2	Best:23.05
2025-01-03 22:55:13,997: Snapshot:3	Epoch:5	Loss:2.025	translation_Loss:1.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:24.28	Hits@10:38.62	Best:24.28
2025-01-03 22:55:17,307: Snapshot:3	Epoch:6	Loss:1.819	translation_Loss:1.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.363                                                   	MRR:24.64	Hits@10:39.45	Best:24.64
2025-01-03 22:55:20,670: Snapshot:3	Epoch:7	Loss:1.66	translation_Loss:1.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:25.14	Hits@10:39.31	Best:25.14
2025-01-03 22:55:23,946: Snapshot:3	Epoch:8	Loss:1.546	translation_Loss:1.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.31                                                   	MRR:25.4	Hits@10:39.55	Best:25.4
2025-01-03 22:55:27,320: Snapshot:3	Epoch:9	Loss:1.461	translation_Loss:1.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.292                                                   	MRR:25.49	Hits@10:39.45	Best:25.49
2025-01-03 22:55:31,189: Snapshot:3	Epoch:10	Loss:1.416	translation_Loss:1.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.281                                                   	MRR:25.82	Hits@10:39.5	Best:25.82
2025-01-03 22:55:34,541: Snapshot:3	Epoch:11	Loss:1.366	translation_Loss:1.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.275                                                   	MRR:25.95	Hits@10:39.51	Best:25.95
2025-01-03 22:55:37,892: Snapshot:3	Epoch:12	Loss:1.339	translation_Loss:1.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.266                                                   	MRR:25.99	Hits@10:39.6	Best:25.99
2025-01-03 22:55:41,286: Snapshot:3	Epoch:13	Loss:1.314	translation_Loss:1.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:26.05	Hits@10:39.55	Best:26.05
2025-01-03 22:55:44,675: Snapshot:3	Epoch:14	Loss:1.299	translation_Loss:1.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:26.06	Hits@10:39.54	Best:26.06
2025-01-03 22:55:48,401: Snapshot:3	Epoch:15	Loss:1.284	translation_Loss:1.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:26.15	Hits@10:39.56	Best:26.15
2025-01-03 22:55:51,665: Snapshot:3	Epoch:16	Loss:1.273	translation_Loss:1.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:26.08	Hits@10:39.55	Best:26.15
2025-01-03 22:55:54,936: Snapshot:3	Epoch:17	Loss:1.259	translation_Loss:1.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:26.3	Hits@10:39.86	Best:26.3
2025-01-03 22:55:58,172: Snapshot:3	Epoch:18	Loss:1.253	translation_Loss:1.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.249                                                   	MRR:26.11	Hits@10:39.71	Best:26.3
2025-01-03 22:56:01,391: Snapshot:3	Epoch:19	Loss:1.243	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.249                                                   	MRR:26.08	Hits@10:39.63	Best:26.3
2025-01-03 22:56:04,610: Early Stopping! Snapshot: 3 Epoch: 20 Best Results: 26.3
2025-01-03 22:56:04,610: Start to training tokens! Snapshot: 3 Epoch: 20 Loss:1.236 MRR:26.1 Best Results: 26.3
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:56:04,610: Snapshot:3	Epoch:20	Loss:1.236	translation_Loss:0.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:26.1	Hits@10:39.67	Best:26.3
2025-01-03 22:56:08,205: Snapshot:3	Epoch:21	Loss:18.382	translation_Loss:5.92	multi_layer_Loss:12.462	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.1	Hits@10:39.67	Best:26.3
2025-01-03 22:56:11,397: End of token training: 3 Epoch: 22 Loss:7.319 MRR:26.1 Best Results: 26.3
2025-01-03 22:56:11,397: Snapshot:3	Epoch:22	Loss:7.319	translation_Loss:5.914	multi_layer_Loss:1.405	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:26.1	Hits@10:39.67	Best:26.3
2025-01-03 22:56:11,699: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2025-01-03 22:56:25,265: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2142 | 0.115  | 0.2682 | 0.3303 |  0.3968 |
|     1      | 0.159  | 0.0765 | 0.1888 | 0.244  |  0.3188 |
|     2      | 0.2109 | 0.1288 | 0.2334 | 0.2919 |  0.3753 |
|     3      | 0.2592 | 0.1833 | 0.2897 | 0.3352 |  0.3946 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:56:37,105: Snapshot:4	Epoch:0	Loss:4.596	translation_Loss:4.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.514                                                   	MRR:6.21	Hits@10:17.12	Best:6.21
2025-01-03 22:56:39,466: Snapshot:4	Epoch:1	Loss:3.474	translation_Loss:2.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.74                                                   	MRR:10.58	Hits@10:27.07	Best:10.58
2025-01-03 22:56:41,809: Snapshot:4	Epoch:2	Loss:2.831	translation_Loss:2.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.739                                                   	MRR:13.99	Hits@10:33.11	Best:13.99
2025-01-03 22:56:44,192: Snapshot:4	Epoch:3	Loss:2.358	translation_Loss:1.67	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.688                                                   	MRR:17.76	Hits@10:36.07	Best:17.76
2025-01-03 22:56:46,515: Snapshot:4	Epoch:4	Loss:2.026	translation_Loss:1.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.633                                                   	MRR:19.75	Hits@10:38.25	Best:19.75
2025-01-03 22:56:48,845: Snapshot:4	Epoch:5	Loss:1.781	translation_Loss:1.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.582                                                   	MRR:20.99	Hits@10:40.34	Best:20.99
2025-01-03 22:56:51,194: Snapshot:4	Epoch:6	Loss:1.607	translation_Loss:1.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.551                                                   	MRR:21.88	Hits@10:41.58	Best:21.88
2025-01-03 22:56:53,508: Snapshot:4	Epoch:7	Loss:1.465	translation_Loss:0.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.518                                                   	MRR:22.12	Hits@10:41.63	Best:22.12
2025-01-03 22:56:55,824: Snapshot:4	Epoch:8	Loss:1.343	translation_Loss:0.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.489                                                   	MRR:22.37	Hits@10:42.02	Best:22.37
2025-01-03 22:56:58,195: Snapshot:4	Epoch:9	Loss:1.271	translation_Loss:0.805	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.466                                                   	MRR:22.64	Hits@10:42.21	Best:22.64
2025-01-03 22:57:00,569: Snapshot:4	Epoch:10	Loss:1.209	translation_Loss:0.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.454                                                   	MRR:22.65	Hits@10:42.27	Best:22.65
2025-01-03 22:57:02,840: Snapshot:4	Epoch:11	Loss:1.157	translation_Loss:0.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.439                                                   	MRR:22.56	Hits@10:42.3	Best:22.65
2025-01-03 22:57:05,539: Snapshot:4	Epoch:12	Loss:1.121	translation_Loss:0.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.427                                                   	MRR:22.49	Hits@10:42.26	Best:22.65
2025-01-03 22:57:07,793: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 22.65
2025-01-03 22:57:07,793: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:1.097 MRR:22.37 Best Results: 22.65
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-03 22:57:07,794: Snapshot:4	Epoch:13	Loss:1.097	translation_Loss:0.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.421                                                   	MRR:22.37	Hits@10:42.15	Best:22.65
2025-01-03 22:57:10,089: Snapshot:4	Epoch:14	Loss:16.005	translation_Loss:3.995	multi_layer_Loss:12.01	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.37	Hits@10:42.15	Best:22.65
2025-01-03 22:57:12,381: End of token training: 4 Epoch: 15 Loss:6.802 MRR:22.37 Best Results: 22.65
2025-01-03 22:57:12,381: Snapshot:4	Epoch:15	Loss:6.802	translation_Loss:3.996	multi_layer_Loss:2.806	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.37	Hits@10:42.15	Best:22.65
2025-01-03 22:57:12,683: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2025-01-03 22:57:27,847: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2133 | 0.1147 | 0.2669 | 0.3283 |  0.3951 |
|     1      | 0.1581 | 0.0757 | 0.1888 | 0.2431 |  0.3181 |
|     2      | 0.2018 | 0.1238 | 0.2209 | 0.2766 |  0.3567 |
|     3      | 0.2538 | 0.1764 | 0.2823 | 0.3302 |  0.396  |
|     4      | 0.2264 | 0.1312 | 0.2483 | 0.3169 |  0.4184 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 22:57:27,849: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2762 | 0.1652 | 0.3415 | 0.4091 |  0.4827 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.249  | 0.1414 | 0.3084 | 0.3763 |  0.4506 |
|     1      | 0.1692 | 0.0865 | 0.2006 | 0.2565 |  0.3298 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2145 | 0.1157 | 0.268  | 0.3308 |  0.3973 |
|     1      | 0.1582 | 0.0757 | 0.1883 | 0.2427 |  0.3178 |
|     2      | 0.2178 | 0.1371 | 0.2415 | 0.296  |  0.3777 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2142 | 0.115  | 0.2682 | 0.3303 |  0.3968 |
|     1      | 0.159  | 0.0765 | 0.1888 | 0.244  |  0.3188 |
|     2      | 0.2109 | 0.1288 | 0.2334 | 0.2919 |  0.3753 |
|     3      | 0.2592 | 0.1833 | 0.2897 | 0.3352 |  0.3946 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2133 | 0.1147 | 0.2669 | 0.3283 |  0.3951 |
|     1      | 0.1581 | 0.0757 | 0.1888 | 0.2431 |  0.3181 |
|     2      | 0.2018 | 0.1238 | 0.2209 | 0.2766 |  0.3567 |
|     3      | 0.2538 | 0.1764 | 0.2823 | 0.3302 |  0.396  |
|     4      | 0.2264 | 0.1312 | 0.2483 | 0.3169 |  0.4184 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 22:57:27,850: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 165.56808733940125 |   0.276   |    0.165     |    0.342     |     0.483     |
|    1     | 115.94906306266785 |    0.21   |    0.115     |    0.256     |     0.392     |
|    2     | 129.1907079219818  |   0.195   |    0.107     |    0.232     |     0.364     |
|    3     | 86.94190573692322  |    0.2    |    0.113     |    0.237     |     0.366     |
|    4     | 45.766125202178955 |   0.199   |    0.112     |    0.234     |     0.365     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 22:57:27,851: Sum_Training_Time:543.4158892631531
2025-01-03 22:57:27,851: Every_Training_Time:[165.56808733940125, 115.94906306266785, 129.1907079219818, 86.94190573692322, 45.766125202178955]
2025-01-03 22:57:27,851: Forward transfer: 0.0157 Backward transfer: -0.02384999999999999
[lijing@p0315 IncDE]$ python main.py -dataset RELATION -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 3 -multi_layer_weight 1 -token_distillation_weight 3000 15000 80000 80000
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2025-01-05 01:41:50,446: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250105014120/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[3000.0, 15000.0, 80000.0, 80000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-05 01:42:05,262: Snapshot:0	Epoch:0	Loss:24.263	translation_Loss:24.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.68	Hits@10:26.04	Best:10.68
2025-01-05 01:42:15,532: Snapshot:0	Epoch:1	Loss:15.05	translation_Loss:15.05	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.45	Hits@10:40.39	Best:18.45
2025-01-05 01:42:26,272: Snapshot:0	Epoch:2	Loss:8.578	translation_Loss:8.578	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.52	Hits@10:45.17	Best:23.52
2025-01-05 01:42:36,882: Snapshot:0	Epoch:3	Loss:4.718	translation_Loss:4.718	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.73	Hits@10:47.19	Best:25.73
2025-01-05 01:42:47,121: Snapshot:0	Epoch:4	Loss:2.73	translation_Loss:2.73	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:26.69	Hits@10:47.9	Best:26.69
2025-01-05 01:42:57,826: Snapshot:0	Epoch:5	Loss:1.812	translation_Loss:1.812	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.22	Hits@10:48.17	Best:27.22
2025-01-05 01:43:08,524: Snapshot:0	Epoch:6	Loss:1.354	translation_Loss:1.354	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.33	Hits@10:48.26	Best:27.33
2025-01-05 01:43:18,766: Snapshot:0	Epoch:7	Loss:1.102	translation_Loss:1.102	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.42	Hits@10:48.21	Best:27.42
2025-01-05 01:43:29,488: Snapshot:0	Epoch:8	Loss:0.954	translation_Loss:0.954	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.43	Hits@10:48.24	Best:27.43
2025-01-05 01:43:40,162: Snapshot:0	Epoch:9	Loss:0.847	translation_Loss:0.847	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.4	Hits@10:48.0	Best:27.43
2025-01-05 01:43:50,370: Snapshot:0	Epoch:10	Loss:0.773	translation_Loss:0.773	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.29	Hits@10:47.84	Best:27.43
2025-01-05 01:44:01,056: Early Stopping! Snapshot: 0 Epoch: 11 Best Results: 27.43
2025-01-05 01:44:01,057: Start to training tokens! Snapshot: 0 Epoch: 11 Loss:0.723 MRR:27.21 Best Results: 27.43
Token added to optimizer, embeddings excluded successfully.
2025-01-05 01:44:01,057: Snapshot:0	Epoch:11	Loss:0.723	translation_Loss:0.723	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.21	Hits@10:47.53	Best:27.43
2025-01-05 01:44:12,182: Snapshot:0	Epoch:12	Loss:32.067	translation_Loss:16.489	token_training_loss:15.578	distillation_Loss:0.0                                                   	MRR:27.21	Hits@10:47.53	Best:27.43
2025-01-05 01:44:22,430: End of token training: 0 Epoch: 13 Loss:16.59 MRR:27.21 Best Results: 27.43
2025-01-05 01:44:22,430: Snapshot:0	Epoch:13	Loss:16.59	translation_Loss:16.5	token_training_loss:0.09	distillation_Loss:0.0                                                           	MRR:27.21	Hits@10:47.53	Best:27.43
2025-01-05 01:44:22,754: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2025-01-05 01:44:27,710: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2767 | 0.1653 | 0.3426 | 0.4112 |  0.4826 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 01:44:44,885: Snapshot:1	Epoch:0	Loss:20.116	translation_Loss:18.894	token_training_loss:0.0	distillation_Loss:1.222                                                   	MRR:10.53	Hits@10:24.98	Best:10.53
2025-01-05 01:44:54,503: Snapshot:1	Epoch:1	Loss:10.289	translation_Loss:8.084	token_training_loss:0.0	distillation_Loss:2.206                                                   	MRR:15.98	Hits@10:32.33	Best:15.98
2025-01-05 01:45:03,883: Snapshot:1	Epoch:2	Loss:6.586	translation_Loss:4.235	token_training_loss:0.0	distillation_Loss:2.351                                                   	MRR:16.38	Hits@10:32.99	Best:16.38
2025-01-05 01:45:13,576: Snapshot:1	Epoch:3	Loss:5.229	translation_Loss:3.004	token_training_loss:0.0	distillation_Loss:2.225                                                   	MRR:16.98	Hits@10:33.18	Best:16.98
2025-01-05 01:45:23,136: Snapshot:1	Epoch:4	Loss:4.728	translation_Loss:2.617	token_training_loss:0.0	distillation_Loss:2.112                                                   	MRR:16.72	Hits@10:32.92	Best:16.98
2025-01-05 01:45:32,354: Snapshot:1	Epoch:5	Loss:4.505	translation_Loss:2.459	token_training_loss:0.0	distillation_Loss:2.046                                                   	MRR:16.84	Hits@10:32.8	Best:16.98
2025-01-05 01:45:42,011: Early Stopping! Snapshot: 1 Epoch: 6 Best Results: 16.98
2025-01-05 01:45:42,012: Start to training tokens! Snapshot: 1 Epoch: 6 Loss:4.394 MRR:16.91 Best Results: 16.98
Token added to optimizer, embeddings excluded successfully.
2025-01-05 01:45:42,012: Snapshot:1	Epoch:6	Loss:4.394	translation_Loss:2.392	token_training_loss:0.0	distillation_Loss:2.002                                                   	MRR:16.91	Hits@10:32.81	Best:16.98
2025-01-05 01:45:51,248: Snapshot:1	Epoch:7	Loss:34.378	translation_Loss:18.689	token_training_loss:15.689	distillation_Loss:0.0                                                   	MRR:16.91	Hits@10:32.81	Best:16.98
2025-01-05 01:46:00,851: End of token training: 1 Epoch: 8 Loss:18.792 MRR:16.91 Best Results: 16.98
2025-01-05 01:46:00,851: Snapshot:1	Epoch:8	Loss:18.792	translation_Loss:18.681	token_training_loss:0.111	distillation_Loss:0.0                                                           	MRR:16.91	Hits@10:32.81	Best:16.98
2025-01-05 01:46:01,133: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2025-01-05 01:46:09,570: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.244  | 0.1365 | 0.3043 | 0.3712 |  0.4442 |
|     1      | 0.1711 | 0.0884 | 0.2033 | 0.2583 |  0.3308 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 01:46:23,917: Snapshot:2	Epoch:0	Loss:13.642	translation_Loss:11.832	token_training_loss:0.0	distillation_Loss:1.81                                                   	MRR:11.58	Hits@10:26.33	Best:11.58
2025-01-05 01:46:30,850: Snapshot:2	Epoch:1	Loss:7.528	translation_Loss:4.906	token_training_loss:0.0	distillation_Loss:2.621                                                   	MRR:19.13	Hits@10:36.26	Best:19.13
2025-01-05 01:46:38,103: Snapshot:2	Epoch:2	Loss:5.282	translation_Loss:3.288	token_training_loss:0.0	distillation_Loss:1.994                                                   	MRR:20.32	Hits@10:36.57	Best:20.32
2025-01-05 01:46:45,018: Snapshot:2	Epoch:3	Loss:4.197	translation_Loss:2.573	token_training_loss:0.0	distillation_Loss:1.624                                                   	MRR:21.09	Hits@10:37.35	Best:21.09
2025-01-05 01:46:52,376: Snapshot:2	Epoch:4	Loss:3.604	translation_Loss:2.228	token_training_loss:0.0	distillation_Loss:1.376                                                   	MRR:21.13	Hits@10:37.36	Best:21.13
2025-01-05 01:46:59,348: Snapshot:2	Epoch:5	Loss:3.301	translation_Loss:2.063	token_training_loss:0.0	distillation_Loss:1.237                                                   	MRR:20.99	Hits@10:37.28	Best:21.13
2025-01-05 01:47:06,602: Snapshot:2	Epoch:6	Loss:3.149	translation_Loss:1.988	token_training_loss:0.0	distillation_Loss:1.161                                                   	MRR:21.05	Hits@10:37.0	Best:21.13
2025-01-05 01:47:13,524: Snapshot:2	Epoch:7	Loss:3.068	translation_Loss:1.947	token_training_loss:0.0	distillation_Loss:1.122                                                   	MRR:21.31	Hits@10:37.04	Best:21.31
2025-01-05 01:47:20,721: Snapshot:2	Epoch:8	Loss:3.02	translation_Loss:1.915	token_training_loss:0.0	distillation_Loss:1.105                                                   	MRR:21.19	Hits@10:37.0	Best:21.31
2025-01-05 01:47:27,687: Snapshot:2	Epoch:9	Loss:2.999	translation_Loss:1.916	token_training_loss:0.0	distillation_Loss:1.083                                                   	MRR:20.92	Hits@10:37.02	Best:21.31
2025-01-05 01:47:34,567: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 21.31
2025-01-05 01:47:34,567: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:2.978 MRR:20.97 Best Results: 21.31
Token added to optimizer, embeddings excluded successfully.
2025-01-05 01:47:34,568: Snapshot:2	Epoch:10	Loss:2.978	translation_Loss:1.894	token_training_loss:0.0	distillation_Loss:1.085                                                   	MRR:20.97	Hits@10:36.61	Best:21.31
2025-01-05 01:47:41,768: Snapshot:2	Epoch:11	Loss:31.35	translation_Loss:15.151	token_training_loss:16.199	distillation_Loss:0.0                                                   	MRR:20.97	Hits@10:36.61	Best:21.31
2025-01-05 01:47:48,596: End of token training: 2 Epoch: 12 Loss:15.463 MRR:20.97 Best Results: 21.31
2025-01-05 01:47:48,596: Snapshot:2	Epoch:12	Loss:15.463	translation_Loss:15.157	token_training_loss:0.306	distillation_Loss:0.0                                                           	MRR:20.97	Hits@10:36.61	Best:21.31
2025-01-05 01:47:48,880: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2025-01-05 01:48:00,918: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2144 | 0.114  | 0.2694 | 0.3331 |  0.4008 |
|     1      | 0.1632 | 0.0802 | 0.1945 | 0.2493 |  0.3244 |
|     2      | 0.2117 | 0.1355 | 0.2317 | 0.2852 |  0.3644 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 01:48:09,940: Snapshot:3	Epoch:0	Loss:6.858	translation_Loss:6.17	token_training_loss:0.0	distillation_Loss:0.688                                                   	MRR:6.4	Hits@10:14.24	Best:6.4
2025-01-05 01:48:13,155: Snapshot:3	Epoch:1	Loss:4.898	translation_Loss:4.192	token_training_loss:0.0	distillation_Loss:0.706                                                   	MRR:13.74	Hits@10:30.07	Best:13.74
2025-01-05 01:48:16,411: Snapshot:3	Epoch:2	Loss:3.619	translation_Loss:3.019	token_training_loss:0.0	distillation_Loss:0.6                                                   	MRR:17.51	Hits@10:34.12	Best:17.51
2025-01-05 01:48:19,630: Snapshot:3	Epoch:3	Loss:2.893	translation_Loss:2.383	token_training_loss:0.0	distillation_Loss:0.51                                                   	MRR:20.78	Hits@10:36.42	Best:20.78
2025-01-05 01:48:22,887: Snapshot:3	Epoch:4	Loss:2.456	translation_Loss:1.996	token_training_loss:0.0	distillation_Loss:0.46                                                   	MRR:22.95	Hits@10:37.69	Best:22.95
2025-01-05 01:48:26,481: Snapshot:3	Epoch:5	Loss:2.167	translation_Loss:1.747	token_training_loss:0.0	distillation_Loss:0.42                                                   	MRR:24.0	Hits@10:38.71	Best:24.0
2025-01-05 01:48:29,697: Snapshot:3	Epoch:6	Loss:1.95	translation_Loss:1.565	token_training_loss:0.0	distillation_Loss:0.385                                                   	MRR:24.45	Hits@10:39.17	Best:24.45
2025-01-05 01:48:32,872: Snapshot:3	Epoch:7	Loss:1.787	translation_Loss:1.433	token_training_loss:0.0	distillation_Loss:0.354                                                   	MRR:24.86	Hits@10:39.15	Best:24.86
2025-01-05 01:48:36,101: Snapshot:3	Epoch:8	Loss:1.674	translation_Loss:1.341	token_training_loss:0.0	distillation_Loss:0.333                                                   	MRR:25.25	Hits@10:39.18	Best:25.25
2025-01-05 01:48:39,325: Snapshot:3	Epoch:9	Loss:1.593	translation_Loss:1.278	token_training_loss:0.0	distillation_Loss:0.316                                                   	MRR:25.48	Hits@10:39.34	Best:25.48
2025-01-05 01:48:42,947: Snapshot:3	Epoch:10	Loss:1.53	translation_Loss:1.226	token_training_loss:0.0	distillation_Loss:0.304                                                   	MRR:25.77	Hits@10:39.3	Best:25.77
2025-01-05 01:48:46,198: Snapshot:3	Epoch:11	Loss:1.493	translation_Loss:1.198	token_training_loss:0.0	distillation_Loss:0.295                                                   	MRR:25.92	Hits@10:39.16	Best:25.92
2025-01-05 01:48:49,388: Snapshot:3	Epoch:12	Loss:1.462	translation_Loss:1.172	token_training_loss:0.0	distillation_Loss:0.29                                                   	MRR:26.05	Hits@10:39.5	Best:26.05
2025-01-05 01:48:52,534: Snapshot:3	Epoch:13	Loss:1.438	translation_Loss:1.152	token_training_loss:0.0	distillation_Loss:0.285                                                   	MRR:25.87	Hits@10:39.15	Best:26.05
2025-01-05 01:48:55,682: Snapshot:3	Epoch:14	Loss:1.418	translation_Loss:1.138	token_training_loss:0.0	distillation_Loss:0.281                                                   	MRR:25.9	Hits@10:39.16	Best:26.05
2025-01-05 01:48:59,331: Early Stopping! Snapshot: 3 Epoch: 15 Best Results: 26.05
2025-01-05 01:48:59,332: Start to training tokens! Snapshot: 3 Epoch: 15 Loss:1.403 MRR:25.94 Best Results: 26.05
Token added to optimizer, embeddings excluded successfully.
2025-01-05 01:48:59,332: Snapshot:3	Epoch:15	Loss:1.403	translation_Loss:1.125	token_training_loss:0.0	distillation_Loss:0.279                                                   	MRR:25.94	Hits@10:39.02	Best:26.05
2025-01-05 01:49:02,468: Snapshot:3	Epoch:16	Loss:18.329	translation_Loss:5.867	token_training_loss:12.462	distillation_Loss:0.0                                                   	MRR:25.94	Hits@10:39.02	Best:26.05
2025-01-05 01:49:05,584: End of token training: 3 Epoch: 17 Loss:7.262 MRR:25.94 Best Results: 26.05
2025-01-05 01:49:05,584: Snapshot:3	Epoch:17	Loss:7.262	translation_Loss:5.858	token_training_loss:1.405	distillation_Loss:0.0                                                           	MRR:25.94	Hits@10:39.02	Best:26.05
2025-01-05 01:49:05,904: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2025-01-05 01:49:19,211: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2139 | 0.1134 | 0.2687 | 0.3328 |  0.4011 |
|     1      | 0.1644 | 0.0806 | 0.1964 | 0.2528 |  0.3265 |
|     2      | 0.2036 | 0.1255 | 0.2226 | 0.2802 |  0.3638 |
|     3      | 0.2563 | 0.1801 | 0.2889 | 0.3335 |  0.3934 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 01:49:27,103: Snapshot:4	Epoch:0	Loss:4.646	translation_Loss:4.131	token_training_loss:0.0	distillation_Loss:0.515                                                   	MRR:6.14	Hits@10:16.99	Best:6.14
2025-01-05 01:49:29,392: Snapshot:4	Epoch:1	Loss:3.514	translation_Loss:2.765	token_training_loss:0.0	distillation_Loss:0.748                                                   	MRR:10.79	Hits@10:27.49	Best:10.79
2025-01-05 01:49:31,653: Snapshot:4	Epoch:2	Loss:2.881	translation_Loss:2.123	token_training_loss:0.0	distillation_Loss:0.759                                                   	MRR:14.33	Hits@10:33.29	Best:14.33
2025-01-05 01:49:33,914: Snapshot:4	Epoch:3	Loss:2.411	translation_Loss:1.7	token_training_loss:0.0	distillation_Loss:0.711                                                   	MRR:17.76	Hits@10:36.48	Best:17.76
2025-01-05 01:49:36,598: Snapshot:4	Epoch:4	Loss:2.083	translation_Loss:1.426	token_training_loss:0.0	distillation_Loss:0.657                                                   	MRR:19.54	Hits@10:38.62	Best:19.54
2025-01-05 01:49:38,868: Snapshot:4	Epoch:5	Loss:1.853	translation_Loss:1.243	token_training_loss:0.0	distillation_Loss:0.61                                                   	MRR:20.79	Hits@10:39.88	Best:20.79
2025-01-05 01:49:41,179: Snapshot:4	Epoch:6	Loss:1.67	translation_Loss:1.094	token_training_loss:0.0	distillation_Loss:0.576                                                   	MRR:21.72	Hits@10:41.29	Best:21.72
2025-01-05 01:49:43,502: Snapshot:4	Epoch:7	Loss:1.534	translation_Loss:0.987	token_training_loss:0.0	distillation_Loss:0.547                                                   	MRR:22.21	Hits@10:41.58	Best:22.21
2025-01-05 01:49:45,765: Snapshot:4	Epoch:8	Loss:1.428	translation_Loss:0.909	token_training_loss:0.0	distillation_Loss:0.52                                                   	MRR:22.52	Hits@10:42.08	Best:22.52
2025-01-05 01:49:48,089: Snapshot:4	Epoch:9	Loss:1.34	translation_Loss:0.84	token_training_loss:0.0	distillation_Loss:0.5                                                   	MRR:22.72	Hits@10:41.72	Best:22.72
2025-01-05 01:49:50,399: Snapshot:4	Epoch:10	Loss:1.283	translation_Loss:0.8	token_training_loss:0.0	distillation_Loss:0.484                                                   	MRR:22.81	Hits@10:42.2	Best:22.81
2025-01-05 01:49:52,695: Snapshot:4	Epoch:11	Loss:1.23	translation_Loss:0.757	token_training_loss:0.0	distillation_Loss:0.473                                                   	MRR:22.94	Hits@10:42.26	Best:22.94
2025-01-05 01:49:55,280: Snapshot:4	Epoch:12	Loss:1.194	translation_Loss:0.73	token_training_loss:0.0	distillation_Loss:0.464                                                   	MRR:22.47	Hits@10:42.14	Best:22.94
2025-01-05 01:49:57,518: Snapshot:4	Epoch:13	Loss:1.168	translation_Loss:0.712	token_training_loss:0.0	distillation_Loss:0.456                                                   	MRR:22.25	Hits@10:41.92	Best:22.94
2025-01-05 01:49:59,773: Early Stopping! Snapshot: 4 Epoch: 14 Best Results: 22.94
2025-01-05 01:49:59,774: Start to training tokens! Snapshot: 4 Epoch: 14 Loss:1.148 MRR:22.43 Best Results: 22.94
Token added to optimizer, embeddings excluded successfully.
2025-01-05 01:49:59,774: Snapshot:4	Epoch:14	Loss:1.148	translation_Loss:0.7	token_training_loss:0.0	distillation_Loss:0.448                                                   	MRR:22.43	Hits@10:41.75	Best:22.94
2025-01-05 01:50:02,039: Snapshot:4	Epoch:15	Loss:16.009	translation_Loss:3.999	token_training_loss:12.01	distillation_Loss:0.0                                                   	MRR:22.43	Hits@10:41.75	Best:22.94
2025-01-05 01:50:04,282: End of token training: 4 Epoch: 16 Loss:6.809 MRR:22.43 Best Results: 22.94
2025-01-05 01:50:04,282: Snapshot:4	Epoch:16	Loss:6.809	translation_Loss:4.003	token_training_loss:2.806	distillation_Loss:0.0                                                           	MRR:22.43	Hits@10:41.75	Best:22.94
2025-01-05 01:50:04,521: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2025-01-05 01:50:19,636: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2133 | 0.1133 | 0.267  | 0.3298 |  0.3998 |
|     1      | 0.1636 | 0.0808 | 0.1945 |  0.25  |  0.3229 |
|     2      | 0.1952 | 0.1205 | 0.2137 | 0.2647 |  0.3448 |
|     3      | 0.2507 | 0.1731 | 0.2813 | 0.3291 |  0.3959 |
|     4      | 0.2257 | 0.1305 | 0.2477 | 0.3154 |  0.4197 |
+------------+--------+--------+--------+--------+---------+
2025-01-05 01:50:19,639: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2767 | 0.1653 | 0.3426 | 0.4112 |  0.4826 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.244  | 0.1365 | 0.3043 | 0.3712 |  0.4442 |
|     1      | 0.1711 | 0.0884 | 0.2033 | 0.2583 |  0.3308 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2144 | 0.114  | 0.2694 | 0.3331 |  0.4008 |
|     1      | 0.1632 | 0.0802 | 0.1945 | 0.2493 |  0.3244 |
|     2      | 0.2117 | 0.1355 | 0.2317 | 0.2852 |  0.3644 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2139 | 0.1134 | 0.2687 | 0.3328 |  0.4011 |
|     1      | 0.1644 | 0.0806 | 0.1964 | 0.2528 |  0.3265 |
|     2      | 0.2036 | 0.1255 | 0.2226 | 0.2802 |  0.3638 |
|     3      | 0.2563 | 0.1801 | 0.2889 | 0.3335 |  0.3934 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2133 | 0.1133 | 0.267  | 0.3298 |  0.3998 |
|     1      | 0.1636 | 0.0808 | 0.1945 |  0.25  |  0.3229 |
|     2      | 0.1952 | 0.1205 | 0.2137 | 0.2647 |  0.3448 |
|     3      | 0.2507 | 0.1731 | 0.2813 | 0.3291 |  0.3959 |
|     4      | 0.2257 | 0.1305 | 0.2477 | 0.3154 |  0.4197 |
+------------+--------+--------+--------+--------+---------+]
2025-01-05 01:50:19,640: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 151.98342156410217 |   0.277   |    0.165     |    0.343     |     0.483     |
|    1     | 89.30666494369507  |   0.209   |    0.113     |    0.255     |     0.389     |
|    2     | 95.89467406272888  |   0.195   |    0.107     |    0.233     |     0.364     |
|    3     | 62.92593336105347  |    0.2    |    0.112     |    0.237     |     0.368     |
|    4     | 43.759090185165405 |   0.199   |    0.112     |    0.234     |     0.366     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-05 01:50:19,640: Sum_Training_Time:443.869784116745
2025-01-05 01:50:19,640: Every_Training_Time:[151.98342156410217, 89.30666494369507, 95.89467406272888, 62.92593336105347, 43.759090185165405]
2025-01-05 01:50:19,640: Forward transfer: 0.015525 Backward transfer: -0.02325