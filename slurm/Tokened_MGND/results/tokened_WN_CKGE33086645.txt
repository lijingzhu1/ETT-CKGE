2025-01-06 23:16:15,664: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106231607/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 23:16:25,308: Snapshot:0	Epoch:0	Loss:15.314	translation_Loss:15.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.68	Hits@10:4.8	Best:1.68
2025-01-06 23:16:33,562: Snapshot:0	Epoch:1	Loss:8.263	translation_Loss:8.263	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.17	Hits@10:16.63	Best:6.17
2025-01-06 23:16:42,046: Snapshot:0	Epoch:2	Loss:3.796	translation_Loss:3.796	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.92	Hits@10:27.93	Best:10.92
2025-01-06 23:16:50,662: Snapshot:0	Epoch:3	Loss:1.587	translation_Loss:1.587	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.19	Hits@10:33.2	Best:13.19
2025-01-06 23:16:58,890: Snapshot:0	Epoch:4	Loss:0.854	translation_Loss:0.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.05	Hits@10:35.43	Best:14.05
2025-01-06 23:17:07,346: Snapshot:0	Epoch:5	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.61	Hits@10:36.55	Best:14.61
2025-01-06 23:17:15,585: Snapshot:0	Epoch:6	Loss:0.35	translation_Loss:0.35	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.97	Hits@10:37.42	Best:14.97
2025-01-06 23:17:24,319: Snapshot:0	Epoch:7	Loss:0.244	translation_Loss:0.244	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.2	Hits@10:37.96	Best:15.2
2025-01-06 23:17:32,571: Snapshot:0	Epoch:8	Loss:0.178	translation_Loss:0.178	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.34	Hits@10:38.2	Best:15.34
2025-01-06 23:17:41,089: Snapshot:0	Epoch:9	Loss:0.138	translation_Loss:0.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.49	Hits@10:38.49	Best:15.49
2025-01-06 23:17:49,623: Snapshot:0	Epoch:10	Loss:0.112	translation_Loss:0.112	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.61	Hits@10:38.76	Best:15.61
2025-01-06 23:17:57,826: Snapshot:0	Epoch:11	Loss:0.093	translation_Loss:0.093	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.7	Hits@10:38.93	Best:15.7
2025-01-06 23:18:06,304: Snapshot:0	Epoch:12	Loss:0.079	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.73	Hits@10:39.04	Best:15.73
2025-01-06 23:18:14,485: Snapshot:0	Epoch:13	Loss:0.069	translation_Loss:0.069	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.81	Hits@10:39.32	Best:15.81
2025-01-06 23:18:23,067: Snapshot:0	Epoch:14	Loss:0.059	translation_Loss:0.059	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.81	Hits@10:39.44	Best:15.81
2025-01-06 23:18:31,513: Snapshot:0	Epoch:15	Loss:0.052	translation_Loss:0.052	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.87	Hits@10:39.59	Best:15.87
2025-01-06 23:18:39,754: Snapshot:0	Epoch:16	Loss:0.047	translation_Loss:0.047	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.87	Hits@10:39.58	Best:15.87
2025-01-06 23:18:48,262: Snapshot:0	Epoch:17	Loss:0.043	translation_Loss:0.043	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.67	Best:15.92
2025-01-06 23:18:56,421: Snapshot:0	Epoch:18	Loss:0.04	translation_Loss:0.04	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.76	Best:15.92
2025-01-06 23:19:04,840: Snapshot:0	Epoch:19	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.78	Best:15.98
2025-01-06 23:19:13,042: Snapshot:0	Epoch:20	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.99	Hits@10:39.9	Best:15.99
2025-01-06 23:19:21,575: Snapshot:0	Epoch:21	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.87	Best:15.99
2025-01-06 23:19:29,785: Snapshot:0	Epoch:22	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.02	Hits@10:39.84	Best:16.02
2025-01-06 23:19:38,233: Snapshot:0	Epoch:23	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:40.06	Best:16.04
2025-01-06 23:19:46,711: Snapshot:0	Epoch:24	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.12	Best:16.06
2025-01-06 23:19:54,996: Snapshot:0	Epoch:25	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.09	Hits@10:40.11	Best:16.09
2025-01-06 23:20:03,460: Snapshot:0	Epoch:26	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.11	Hits@10:40.11	Best:16.11
2025-01-06 23:20:11,924: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.24	Best:16.14
2025-01-06 23:20:20,380: Snapshot:0	Epoch:28	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.42	Best:16.14
2025-01-06 23:20:28,793: Snapshot:0	Epoch:29	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.44	Best:16.14
2025-01-06 23:20:37,002: Snapshot:0	Epoch:30	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.45	Best:16.15
2025-01-06 23:20:45,387: Snapshot:0	Epoch:31	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.46	Best:16.15
2025-01-06 23:20:53,619: Snapshot:0	Epoch:32	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.61	Best:16.15
2025-01-06 23:21:02,045: Early Stopping! Snapshot: 0 Epoch: 33 Best Results: 16.15
2025-01-06 23:21:02,045: Start to training tokens! Snapshot: 0 Epoch: 33 Loss:0.026 MRR:16.12 Best Results: 16.15
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:21:02,046: Snapshot:0	Epoch:33	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.6	Best:16.15
2025-01-06 23:21:10,845: Snapshot:0	Epoch:34	Loss:17.269	translation_Loss:5.458	token_training_loss:11.811	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.6	Best:16.15
2025-01-06 23:21:19,367: End of token training: 0 Epoch: 35 Loss:5.839 MRR:16.12 Best Results: 16.15
2025-01-06 23:21:19,367: Snapshot:0	Epoch:35	Loss:5.839	translation_Loss:5.46	token_training_loss:0.38	distillation_Loss:0.0                                                           	MRR:16.12	Hits@10:40.6	Best:16.15
2025-01-06 23:21:19,467: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-06 23:21:23,490: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1636 | 0.0062 | 0.2933 | 0.3554 |  0.4088 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,919,000
Trainable params: 1,200
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:21:27,308: Snapshot:1	Epoch:0	Loss:2.693	translation_Loss:2.649	token_training_loss:0.0	distillation_Loss:0.044                                                   	MRR:2.59	Hits@10:6.77	Best:2.59
2025-01-06 23:21:29,009: Snapshot:1	Epoch:1	Loss:1.719	translation_Loss:1.606	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:7.56	Hits@10:19.7	Best:7.56
2025-01-06 23:21:30,717: Snapshot:1	Epoch:2	Loss:0.929	translation_Loss:0.789	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:10.7	Hits@10:26.75	Best:10.7
2025-01-06 23:21:32,389: Snapshot:1	Epoch:3	Loss:0.45	translation_Loss:0.289	token_training_loss:0.0	distillation_Loss:0.161                                                   	MRR:12.48	Hits@10:29.81	Best:12.48
2025-01-06 23:21:34,041: Snapshot:1	Epoch:4	Loss:0.288	translation_Loss:0.107	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:13.38	Hits@10:31.53	Best:13.38
2025-01-06 23:21:35,948: Snapshot:1	Epoch:5	Loss:0.231	translation_Loss:0.041	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:13.77	Hits@10:32.18	Best:13.77
2025-01-06 23:21:37,617: Snapshot:1	Epoch:6	Loss:0.204	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.184                                                   	MRR:13.91	Hits@10:32.8	Best:13.91
2025-01-06 23:21:39,325: Snapshot:1	Epoch:7	Loss:0.181	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.169                                                   	MRR:13.97	Hits@10:33.06	Best:13.97
2025-01-06 23:21:40,910: Snapshot:1	Epoch:8	Loss:0.157	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.149                                                   	MRR:13.96	Hits@10:33.25	Best:13.97
2025-01-06 23:21:42,474: Snapshot:1	Epoch:9	Loss:0.134	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.127                                                   	MRR:13.95	Hits@10:33.25	Best:13.97
2025-01-06 23:21:44,105: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.97
2025-01-06 23:21:44,105: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.112 MRR:13.86 Best Results: 13.97
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:21:44,106: Snapshot:1	Epoch:10	Loss:0.112	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:13.86	Hits@10:33.23	Best:13.97
2025-01-06 23:21:45,669: Snapshot:1	Epoch:11	Loss:8.011	translation_Loss:1.144	token_training_loss:6.867	distillation_Loss:0.0                                                   	MRR:13.86	Hits@10:33.23	Best:13.97
2025-01-06 23:21:47,271: End of token training: 1 Epoch: 12 Loss:4.662 MRR:13.86 Best Results: 13.97
2025-01-06 23:21:47,271: Snapshot:1	Epoch:12	Loss:4.662	translation_Loss:1.145	token_training_loss:3.517	distillation_Loss:0.0                                                           	MRR:13.86	Hits@10:33.23	Best:13.97
2025-01-06 23:21:47,351: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-06 23:21:52,192: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.162  | 0.0073 | 0.2847 | 0.3558 |  0.413  |
|     1      | 0.1417 | 0.0073 | 0.2567 | 0.2911 |  0.3269 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,737,600
Trainable params: 1,200
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:21:56,254: Snapshot:2	Epoch:0	Loss:2.613	translation_Loss:2.589	token_training_loss:0.0	distillation_Loss:0.024                                                   	MRR:2.96	Hits@10:8.15	Best:2.96
2025-01-06 23:21:58,111: Snapshot:2	Epoch:1	Loss:1.591	translation_Loss:1.509	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:7.83	Hits@10:19.52	Best:7.83
2025-01-06 23:21:59,921: Snapshot:2	Epoch:2	Loss:0.799	translation_Loss:0.678	token_training_loss:0.0	distillation_Loss:0.121                                                   	MRR:10.83	Hits@10:25.91	Best:10.83
2025-01-06 23:22:01,733: Snapshot:2	Epoch:3	Loss:0.354	translation_Loss:0.217	token_training_loss:0.0	distillation_Loss:0.137                                                   	MRR:12.27	Hits@10:28.2	Best:12.27
2025-01-06 23:22:03,578: Snapshot:2	Epoch:4	Loss:0.214	translation_Loss:0.073	token_training_loss:0.0	distillation_Loss:0.141                                                   	MRR:13.02	Hits@10:29.35	Best:13.02
2025-01-06 23:22:05,420: Snapshot:2	Epoch:5	Loss:0.169	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.136                                                   	MRR:13.32	Hits@10:30.08	Best:13.32
2025-01-06 23:22:07,250: Snapshot:2	Epoch:6	Loss:0.145	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.126                                                   	MRR:13.49	Hits@10:30.51	Best:13.49
2025-01-06 23:22:09,090: Snapshot:2	Epoch:7	Loss:0.127	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.116                                                   	MRR:13.57	Hits@10:30.78	Best:13.57
2025-01-06 23:22:11,196: Snapshot:2	Epoch:8	Loss:0.114	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.105                                                   	MRR:13.6	Hits@10:30.91	Best:13.6
2025-01-06 23:22:12,927: Snapshot:2	Epoch:9	Loss:0.101	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.095                                                   	MRR:13.6	Hits@10:30.99	Best:13.6
2025-01-06 23:22:14,781: Snapshot:2	Epoch:10	Loss:0.09	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.086                                                   	MRR:13.63	Hits@10:31.18	Best:13.63
2025-01-06 23:22:16,644: Snapshot:2	Epoch:11	Loss:0.08	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.077                                                   	MRR:13.64	Hits@10:31.34	Best:13.64
2025-01-06 23:22:18,574: Snapshot:2	Epoch:12	Loss:0.071	translation_Loss:0.003	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:13.66	Hits@10:31.45	Best:13.66
2025-01-06 23:22:20,281: Snapshot:2	Epoch:13	Loss:0.064	translation_Loss:0.003	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:13.66	Hits@10:31.4	Best:13.66
2025-01-06 23:22:22,038: Snapshot:2	Epoch:14	Loss:0.057	translation_Loss:0.004	token_training_loss:0.0	distillation_Loss:0.054                                                   	MRR:13.61	Hits@10:31.48	Best:13.66
2025-01-06 23:22:23,748: Early Stopping! Snapshot: 2 Epoch: 15 Best Results: 13.66
2025-01-06 23:22:23,749: Start to training tokens! Snapshot: 2 Epoch: 15 Loss:0.05 MRR:13.63 Best Results: 13.66
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:22:23,749: Snapshot:2	Epoch:15	Loss:0.05	translation_Loss:0.002	token_training_loss:0.0	distillation_Loss:0.048                                                   	MRR:13.63	Hits@10:31.45	Best:13.66
2025-01-06 23:22:25,441: Snapshot:2	Epoch:16	Loss:7.736	translation_Loss:1.111	token_training_loss:6.625	distillation_Loss:0.0                                                   	MRR:13.63	Hits@10:31.45	Best:13.66
2025-01-06 23:22:27,105: End of token training: 2 Epoch: 17 Loss:4.435 MRR:13.63 Best Results: 13.66
2025-01-06 23:22:27,105: Snapshot:2	Epoch:17	Loss:4.435	translation_Loss:1.109	token_training_loss:3.325	distillation_Loss:0.0                                                           	MRR:13.63	Hits@10:31.45	Best:13.66
2025-01-06 23:22:27,235: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-06 23:22:33,244: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1584 | 0.0069 | 0.2761 | 0.3488 |  0.4097 |
|     1      | 0.139  | 0.0081 | 0.2476 | 0.2852 |  0.3272 |
|     2      | 0.1403 | 0.0083 | 0.2532 | 0.282  |  0.3188 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,556,400
Trainable params: 1,200
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:22:37,631: Snapshot:3	Epoch:0	Loss:2.596	translation_Loss:2.554	token_training_loss:0.0	distillation_Loss:0.042                                                   	MRR:2.95	Hits@10:7.26	Best:2.95
2025-01-06 23:22:39,798: Snapshot:3	Epoch:1	Loss:1.572	translation_Loss:1.448	token_training_loss:0.0	distillation_Loss:0.125                                                   	MRR:7.52	Hits@10:19.01	Best:7.52
2025-01-06 23:22:41,734: Snapshot:3	Epoch:2	Loss:0.776	translation_Loss:0.614	token_training_loss:0.0	distillation_Loss:0.162                                                   	MRR:10.7	Hits@10:25.75	Best:10.7
2025-01-06 23:22:43,712: Snapshot:3	Epoch:3	Loss:0.363	translation_Loss:0.19	token_training_loss:0.0	distillation_Loss:0.173                                                   	MRR:12.37	Hits@10:27.63	Best:12.37
2025-01-06 23:22:45,678: Snapshot:3	Epoch:4	Loss:0.244	translation_Loss:0.068	token_training_loss:0.0	distillation_Loss:0.176                                                   	MRR:12.92	Hits@10:29.17	Best:12.92
2025-01-06 23:22:47,643: Snapshot:3	Epoch:5	Loss:0.203	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.172                                                   	MRR:13.25	Hits@10:29.81	Best:13.25
2025-01-06 23:22:49,646: Snapshot:3	Epoch:6	Loss:0.18	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.16                                                   	MRR:13.45	Hits@10:29.97	Best:13.45
2025-01-06 23:22:51,601: Snapshot:3	Epoch:7	Loss:0.159	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:13.56	Hits@10:30.32	Best:13.56
2025-01-06 23:22:53,408: Snapshot:3	Epoch:8	Loss:0.14	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.13                                                   	MRR:13.55	Hits@10:30.65	Best:13.56
2025-01-06 23:22:55,401: Snapshot:3	Epoch:9	Loss:0.121	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.114                                                   	MRR:13.67	Hits@10:30.75	Best:13.67
2025-01-06 23:22:57,231: Snapshot:3	Epoch:10	Loss:0.104	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.097                                                   	MRR:13.64	Hits@10:30.65	Best:13.67
2025-01-06 23:22:59,181: Snapshot:3	Epoch:11	Loss:0.088	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:13.68	Hits@10:30.73	Best:13.68
2025-01-06 23:23:01,008: Snapshot:3	Epoch:12	Loss:0.075	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:13.63	Hits@10:30.67	Best:13.68
2025-01-06 23:23:03,029: Snapshot:3	Epoch:13	Loss:0.064	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:13.62	Hits@10:30.81	Best:13.68
2025-01-06 23:23:04,827: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 13.68
2025-01-06 23:23:04,827: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:0.056 MRR:13.61 Best Results: 13.68
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:23:04,827: Snapshot:3	Epoch:14	Loss:0.056	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.051                                                   	MRR:13.61	Hits@10:30.86	Best:13.68
2025-01-06 23:23:06,598: Snapshot:3	Epoch:15	Loss:7.666	translation_Loss:1.109	token_training_loss:6.557	distillation_Loss:0.0                                                   	MRR:13.61	Hits@10:30.86	Best:13.68
2025-01-06 23:23:08,365: End of token training: 3 Epoch: 16 Loss:4.296 MRR:13.61 Best Results: 13.68
2025-01-06 23:23:08,365: Snapshot:3	Epoch:16	Loss:4.296	translation_Loss:1.11	token_training_loss:3.186	distillation_Loss:0.0                                                           	MRR:13.61	Hits@10:30.86	Best:13.68
2025-01-06 23:23:08,445: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-06 23:23:15,414: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1569 | 0.0075 | 0.2717 | 0.3458 |  0.4065 |
|     1      | 0.137  | 0.007  | 0.2444 | 0.2841 |  0.3242 |
|     2      | 0.1384 | 0.0091 | 0.2454 | 0.2785 |  0.3191 |
|     3      | 0.1373 | 0.0091 | 0.2406 | 0.2804 |  0.3218 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,375,200
Trainable params: 1,200
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:23:19,763: Snapshot:4	Epoch:0	Loss:2.524	translation_Loss:2.483	token_training_loss:0.0	distillation_Loss:0.041                                                   	MRR:9.92	Hits@10:24.17	Best:9.92
2025-01-06 23:23:21,852: Snapshot:4	Epoch:1	Loss:1.492	translation_Loss:1.365	token_training_loss:0.0	distillation_Loss:0.127                                                   	MRR:11.56	Hits@10:28.01	Best:11.56
2025-01-06 23:23:24,175: Snapshot:4	Epoch:2	Loss:0.714	translation_Loss:0.545	token_training_loss:0.0	distillation_Loss:0.169                                                   	MRR:12.65	Hits@10:30.19	Best:12.65
2025-01-06 23:23:26,297: Snapshot:4	Epoch:3	Loss:0.329	translation_Loss:0.152	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:13.32	Hits@10:31.02	Best:13.32
2025-01-06 23:23:28,375: Snapshot:4	Epoch:4	Loss:0.228	translation_Loss:0.057	token_training_loss:0.0	distillation_Loss:0.172                                                   	MRR:13.66	Hits@10:31.53	Best:13.66
2025-01-06 23:23:30,475: Snapshot:4	Epoch:5	Loss:0.191	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.16                                                   	MRR:13.89	Hits@10:31.83	Best:13.89
2025-01-06 23:23:32,571: Snapshot:4	Epoch:6	Loss:0.167	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.145                                                   	MRR:13.99	Hits@10:32.26	Best:13.99
2025-01-06 23:23:34,630: Snapshot:4	Epoch:7	Loss:0.145	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.131                                                   	MRR:14.05	Hits@10:32.45	Best:14.05
2025-01-06 23:23:36,677: Snapshot:4	Epoch:8	Loss:0.129	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.117                                                   	MRR:14.08	Hits@10:32.66	Best:14.08
2025-01-06 23:23:38,708: Snapshot:4	Epoch:9	Loss:0.112	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.103                                                   	MRR:14.17	Hits@10:32.96	Best:14.17
2025-01-06 23:23:40,838: Snapshot:4	Epoch:10	Loss:0.096	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:14.2	Hits@10:32.88	Best:14.2
2025-01-06 23:23:42,766: Snapshot:4	Epoch:11	Loss:0.083	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.077                                                   	MRR:14.15	Hits@10:32.88	Best:14.2
2025-01-06 23:23:44,701: Snapshot:4	Epoch:12	Loss:0.072	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.066                                                   	MRR:14.16	Hits@10:32.9	Best:14.2
2025-01-06 23:23:46,868: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 14.2
2025-01-06 23:23:46,868: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:0.062 MRR:14.17 Best Results: 14.2
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:23:46,868: Snapshot:4	Epoch:13	Loss:0.062	translation_Loss:0.005	token_training_loss:0.0	distillation_Loss:0.057                                                   	MRR:14.17	Hits@10:32.8	Best:14.2
2025-01-06 23:23:48,792: Snapshot:4	Epoch:14	Loss:7.672	translation_Loss:1.093	token_training_loss:6.58	distillation_Loss:0.0                                                   	MRR:14.17	Hits@10:32.8	Best:14.2
2025-01-06 23:23:50,696: End of token training: 4 Epoch: 15 Loss:4.372 MRR:14.17 Best Results: 14.2
2025-01-06 23:23:50,697: Snapshot:4	Epoch:15	Loss:4.372	translation_Loss:1.088	token_training_loss:3.284	distillation_Loss:0.0                                                           	MRR:14.17	Hits@10:32.8	Best:14.2
2025-01-06 23:23:50,798: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-06 23:23:58,863: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1542 | 0.0068 | 0.2673 | 0.3389 |  0.4023 |
|     1      | 0.1333 | 0.0054 | 0.2358 | 0.2815 |  0.3226 |
|     2      | 0.1356 | 0.0075 | 0.2411 | 0.278  |  0.3167 |
|     3      | 0.1365 | 0.0083 | 0.2406 | 0.2852 |  0.321  |
|     4      | 0.1343 | 0.0081 | 0.2372 | 0.2899 |  0.3238 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   1,200
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,194,200
Trainable params: 1,200
Non-trainable params: 8,193,000
=================================================================
2025-01-06 23:23:58,866: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1636 | 0.0062 | 0.2933 | 0.3554 |  0.4088 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.162  | 0.0073 | 0.2847 | 0.3558 |  0.413  |
|     1      | 0.1417 | 0.0073 | 0.2567 | 0.2911 |  0.3269 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1584 | 0.0069 | 0.2761 | 0.3488 |  0.4097 |
|     1      | 0.139  | 0.0081 | 0.2476 | 0.2852 |  0.3272 |
|     2      | 0.1403 | 0.0083 | 0.2532 | 0.282  |  0.3188 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1569 | 0.0075 | 0.2717 | 0.3458 |  0.4065 |
|     1      | 0.137  | 0.007  | 0.2444 | 0.2841 |  0.3242 |
|     2      | 0.1384 | 0.0091 | 0.2454 | 0.2785 |  0.3191 |
|     3      | 0.1373 | 0.0091 | 0.2406 | 0.2804 |  0.3218 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1542 | 0.0068 | 0.2673 | 0.3389 |  0.4023 |
|     1      | 0.1333 | 0.0054 | 0.2358 | 0.2815 |  0.3226 |
|     2      | 0.1356 | 0.0075 | 0.2411 | 0.278  |  0.3167 |
|     3      | 0.1365 | 0.0083 | 0.2406 | 0.2852 |  0.321  |
|     4      | 0.1343 | 0.0081 | 0.2372 | 0.2899 |  0.3238 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 23:23:58,866: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 303.70210552215576 |   0.164   |    0.006     |    0.293     |     0.409     |
|    1     | 22.744298934936523 |   0.159   |    0.007     |    0.281     |     0.401     |
|    2     | 33.97049593925476  |   0.154   |    0.007     |     0.27     |     0.388     |
|    3     |  33.7980170249939  |    0.15   |    0.008     |    0.262     |     0.378     |
|    4     | 34.145877838134766 |   0.146   |    0.007     |    0.256     |      0.37     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 23:23:58,867: Sum_Training_Time:428.3607952594757
2025-01-06 23:23:58,867: Every_Training_Time:[303.70210552215576, 22.744298934936523, 33.97049593925476, 33.7980170249939, 34.145877838134766]
2025-01-06 23:23:58,867: Forward transfer: 0.059625 Backward transfer: -0.005824999999999997
