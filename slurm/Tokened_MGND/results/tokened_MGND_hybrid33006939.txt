2024-12-27 16:44:43,790: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227164408/HYBRIDHYBRID_0.01_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 16:44:53,659: Snapshot:0	Epoch:0	Loss:65.947	translation_Loss:65.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.57	Hits@10:38.22	Best:19.57
2024-12-27 16:44:59,713: Snapshot:0	Epoch:1	Loss:27.285	translation_Loss:27.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.78	Hits@10:43.47	Best:23.78
2024-12-27 16:45:05,855: Snapshot:0	Epoch:2	Loss:12.486	translation_Loss:12.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:44.37	Best:24.41
2024-12-27 16:45:11,940: Snapshot:0	Epoch:3	Loss:6.481	translation_Loss:6.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:44.26	Best:24.5
2024-12-27 16:45:18,018: Snapshot:0	Epoch:4	Loss:4.219	translation_Loss:4.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.56	Hits@10:43.93	Best:24.56
2024-12-27 16:45:24,104: Snapshot:0	Epoch:5	Loss:3.264	translation_Loss:3.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:43.78	Best:24.56
2024-12-27 16:45:30,197: Snapshot:0	Epoch:6	Loss:2.75	translation_Loss:2.75	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:43.91	Best:24.56
2024-12-27 16:45:36,300: Snapshot:0	Epoch:7	Loss:2.378	translation_Loss:2.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:43.45	Best:24.56
2024-12-27 16:45:42,768: Snapshot:0	Epoch:8	Loss:2.186	translation_Loss:2.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:43.2	Best:24.56
2024-12-27 16:45:48,799: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 24.56
2024-12-27 16:45:48,799: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:2.045 MRR:23.89 Best Results: 24.56
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 16:45:48,799: Snapshot:0	Epoch:9	Loss:2.045	translation_Loss:2.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:43.23	Best:24.56
2024-12-27 16:45:56,553: Snapshot:0	Epoch:10	Loss:67.457	translation_Loss:67.42	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:43.23	Best:24.56
2024-12-27 16:46:02,822: End of token training: 0 Epoch: 11 Loss:67.316 MRR:23.89 Best Results: 24.56
2024-12-27 16:46:02,822: Snapshot:0	Epoch:11	Loss:67.316	translation_Loss:67.316	multi_layer_Loss:-0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.89	Hits@10:43.23	Best:24.56
2024-12-27 16:46:03,079: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_1000/0model_best.tar'
2024-12-27 16:46:05,704: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1423 | 0.2928 | 0.357  |  0.436  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 16:46:17,190: Snapshot:1	Epoch:0	Loss:15.469	translation_Loss:12.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.358                                                   	MRR:25.51	Hits@10:43.77	Best:25.51
2024-12-27 16:46:19,635: Snapshot:1	Epoch:1	Loss:6.753	translation_Loss:4.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.166                                                   	MRR:26.72	Hits@10:45.83	Best:26.72
2024-12-27 16:46:22,120: Snapshot:1	Epoch:2	Loss:5.409	translation_Loss:3.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.961                                                   	MRR:27.3	Hits@10:46.54	Best:27.3
2024-12-27 16:46:24,571: Snapshot:1	Epoch:3	Loss:5.024	translation_Loss:3.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.91                                                   	MRR:26.87	Hits@10:46.35	Best:27.3
2024-12-27 16:46:27,042: Snapshot:1	Epoch:4	Loss:4.869	translation_Loss:2.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.884                                                   	MRR:27.58	Hits@10:46.59	Best:27.58
2024-12-27 16:46:29,544: Snapshot:1	Epoch:5	Loss:4.766	translation_Loss:2.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.875                                                   	MRR:27.22	Hits@10:46.68	Best:27.58
2024-12-27 16:46:31,973: Snapshot:1	Epoch:6	Loss:4.664	translation_Loss:2.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.869                                                   	MRR:27.6	Hits@10:46.96	Best:27.6
2024-12-27 16:46:34,412: Snapshot:1	Epoch:7	Loss:4.609	translation_Loss:2.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.846                                                   	MRR:27.52	Hits@10:46.89	Best:27.6
2024-12-27 16:46:36,837: Snapshot:1	Epoch:8	Loss:4.56	translation_Loss:2.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.837                                                   	MRR:27.46	Hits@10:47.01	Best:27.6
2024-12-27 16:46:39,269: Snapshot:1	Epoch:9	Loss:4.515	translation_Loss:2.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.839                                                   	MRR:27.81	Hits@10:46.87	Best:27.81
2024-12-27 16:46:41,705: Snapshot:1	Epoch:10	Loss:4.465	translation_Loss:2.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.807                                                   	MRR:27.73	Hits@10:47.38	Best:27.81
2024-12-27 16:46:44,115: Snapshot:1	Epoch:11	Loss:4.387	translation_Loss:2.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.795                                                   	MRR:27.77	Hits@10:47.19	Best:27.81
2024-12-27 16:46:46,577: Snapshot:1	Epoch:12	Loss:4.387	translation_Loss:2.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.793                                                   	MRR:28.11	Hits@10:47.16	Best:28.11
2024-12-27 16:46:49,104: Snapshot:1	Epoch:13	Loss:4.333	translation_Loss:2.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.775                                                   	MRR:28.23	Hits@10:47.27	Best:28.23
2024-12-27 16:46:51,541: Snapshot:1	Epoch:14	Loss:4.321	translation_Loss:2.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.777                                                   	MRR:27.78	Hits@10:47.15	Best:28.23
2024-12-27 16:46:54,004: Snapshot:1	Epoch:15	Loss:4.272	translation_Loss:2.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.75                                                   	MRR:27.94	Hits@10:47.11	Best:28.23
2024-12-27 16:46:56,448: Snapshot:1	Epoch:16	Loss:4.253	translation_Loss:2.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:28.04	Hits@10:47.2	Best:28.23
2024-12-27 16:46:58,914: Snapshot:1	Epoch:17	Loss:4.255	translation_Loss:2.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.761                                                   	MRR:28.28	Hits@10:47.41	Best:28.28
2024-12-27 16:47:01,332: Snapshot:1	Epoch:18	Loss:4.246	translation_Loss:2.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.761                                                   	MRR:28.07	Hits@10:47.49	Best:28.28
2024-12-27 16:47:03,773: Snapshot:1	Epoch:19	Loss:4.203	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.741                                                   	MRR:28.26	Hits@10:47.39	Best:28.28
2024-12-27 16:47:06,172: Snapshot:1	Epoch:20	Loss:4.139	translation_Loss:2.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.707                                                   	MRR:28.16	Hits@10:47.25	Best:28.28
2024-12-27 16:47:08,607: Snapshot:1	Epoch:21	Loss:4.166	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.726                                                   	MRR:28.07	Hits@10:47.21	Best:28.28
2024-12-27 16:47:11,074: Snapshot:1	Epoch:22	Loss:4.18	translation_Loss:2.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.728                                                   	MRR:28.37	Hits@10:47.22	Best:28.37
2024-12-27 16:47:13,553: Snapshot:1	Epoch:23	Loss:4.174	translation_Loss:2.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.737                                                   	MRR:28.24	Hits@10:47.19	Best:28.37
2024-12-27 16:47:15,946: Snapshot:1	Epoch:24	Loss:4.161	translation_Loss:2.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.752                                                   	MRR:28.28	Hits@10:47.58	Best:28.37
2024-12-27 16:47:18,416: Snapshot:1	Epoch:25	Loss:4.128	translation_Loss:2.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.714                                                   	MRR:28.21	Hits@10:47.4	Best:28.37
2024-12-27 16:47:20,802: Snapshot:1	Epoch:26	Loss:4.164	translation_Loss:2.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.744                                                   	MRR:28.23	Hits@10:47.37	Best:28.37
2024-12-27 16:47:23,253: Early Stopping! Snapshot: 1 Epoch: 27 Best Results: 28.37
2024-12-27 16:47:23,253: Start to training tokens! Snapshot: 1 Epoch: 27 Loss:4.094 MRR:28.19 Best Results: 28.37
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 16:47:23,254: Snapshot:1	Epoch:27	Loss:4.094	translation_Loss:2.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.72                                                   	MRR:28.19	Hits@10:47.62	Best:28.37
2024-12-27 16:47:25,612: Snapshot:1	Epoch:28	Loss:25.188	translation_Loss:25.152	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.19	Hits@10:47.62	Best:28.37
2024-12-27 16:47:27,980: End of token training: 1 Epoch: 29 Loss:25.179 MRR:28.19 Best Results: 28.37
2024-12-27 16:47:27,980: Snapshot:1	Epoch:29	Loss:25.179	translation_Loss:25.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.19	Hits@10:47.62	Best:28.37
2024-12-27 16:47:28,313: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_1000/1model_best.tar'
2024-12-27 16:47:32,460: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1424 | 0.2928 | 0.3597 |  0.4379 |
|     1      | 0.2782 | 0.1804 | 0.3208 | 0.3884 |  0.4636 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 16:48:07,241: Snapshot:2	Epoch:0	Loss:54.785	translation_Loss:40.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.927                                                   	MRR:18.15	Hits@10:33.06	Best:18.15
2024-12-27 16:48:17,878: Snapshot:2	Epoch:1	Loss:35.885	translation_Loss:23.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.787                                                   	MRR:18.83	Hits@10:33.92	Best:18.83
2024-12-27 16:48:28,574: Snapshot:2	Epoch:2	Loss:33.261	translation_Loss:20.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.551                                                   	MRR:18.98	Hits@10:34.0	Best:18.98
2024-12-27 16:48:39,368: Snapshot:2	Epoch:3	Loss:32.138	translation_Loss:19.727	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.411                                                   	MRR:19.03	Hits@10:33.9	Best:19.03
2024-12-27 16:48:50,004: Snapshot:2	Epoch:4	Loss:31.609	translation_Loss:19.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.376                                                   	MRR:19.15	Hits@10:34.07	Best:19.15
2024-12-27 16:49:00,693: Snapshot:2	Epoch:5	Loss:31.166	translation_Loss:18.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.323                                                   	MRR:19.29	Hits@10:34.45	Best:19.29
2024-12-27 16:49:11,416: Snapshot:2	Epoch:6	Loss:30.896	translation_Loss:18.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.311                                                   	MRR:19.43	Hits@10:34.71	Best:19.43
2024-12-27 16:49:22,134: Snapshot:2	Epoch:7	Loss:30.525	translation_Loss:18.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.223                                                   	MRR:19.29	Hits@10:34.63	Best:19.43
2024-12-27 16:49:32,821: Snapshot:2	Epoch:8	Loss:30.4	translation_Loss:18.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.228                                                   	MRR:19.42	Hits@10:34.78	Best:19.43
2024-12-27 16:49:43,520: Snapshot:2	Epoch:9	Loss:30.32	translation_Loss:18.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.256                                                   	MRR:19.42	Hits@10:34.62	Best:19.43
2024-12-27 16:49:54,186: Snapshot:2	Epoch:10	Loss:30.229	translation_Loss:17.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.258                                                   	MRR:19.46	Hits@10:34.59	Best:19.46
2024-12-27 16:50:04,955: Snapshot:2	Epoch:11	Loss:30.05	translation_Loss:17.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.21                                                   	MRR:19.44	Hits@10:34.79	Best:19.46
2024-12-27 16:50:15,837: Snapshot:2	Epoch:12	Loss:29.977	translation_Loss:17.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.204                                                   	MRR:19.61	Hits@10:34.77	Best:19.61
2024-12-27 16:50:26,483: Snapshot:2	Epoch:13	Loss:29.996	translation_Loss:17.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.229                                                   	MRR:19.59	Hits@10:34.91	Best:19.61
2024-12-27 16:50:37,287: Snapshot:2	Epoch:14	Loss:29.846	translation_Loss:17.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.201                                                   	MRR:19.56	Hits@10:34.64	Best:19.61
2024-12-27 16:50:48,523: Snapshot:2	Epoch:15	Loss:29.877	translation_Loss:17.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.232                                                   	MRR:19.59	Hits@10:34.89	Best:19.61
2024-12-27 16:50:59,196: Snapshot:2	Epoch:16	Loss:29.787	translation_Loss:17.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.199                                                   	MRR:19.6	Hits@10:34.9	Best:19.61
2024-12-27 16:51:09,829: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 19.61
2024-12-27 16:51:09,830: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:29.706 MRR:19.5 Best Results: 19.61
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 16:51:09,830: Snapshot:2	Epoch:17	Loss:29.706	translation_Loss:17.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.183                                                   	MRR:19.5	Hits@10:34.9	Best:19.61
2024-12-27 16:51:20,290: Snapshot:2	Epoch:18	Loss:112.847	translation_Loss:112.811	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.5	Hits@10:34.9	Best:19.61
2024-12-27 16:51:30,714: End of token training: 2 Epoch: 19 Loss:112.648 MRR:19.5 Best Results: 19.61
2024-12-27 16:51:30,715: Snapshot:2	Epoch:19	Loss:112.648	translation_Loss:112.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.5	Hits@10:34.9	Best:19.61
2024-12-27 16:51:30,976: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_1000/2model_best.tar'
2024-12-27 16:51:39,234: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2388 | 0.1349 | 0.2884 | 0.3568 |  0.4363 |
|     1      | 0.269  | 0.1679 | 0.3129 | 0.3824 |  0.4647 |
|     2      | 0.1927 | 0.1147 | 0.2164 | 0.2709 |  0.3451 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 16:52:20,514: Snapshot:3	Epoch:0	Loss:60.898	translation_Loss:43.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:17.818                                                   	MRR:17.49	Hits@10:32.34	Best:17.49
2024-12-27 16:52:33,654: Snapshot:3	Epoch:1	Loss:46.618	translation_Loss:29.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:17.108                                                   	MRR:17.98	Hits@10:33.27	Best:17.98
2024-12-27 16:52:46,801: Snapshot:3	Epoch:2	Loss:44.901	translation_Loss:27.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.98                                                   	MRR:18.16	Hits@10:33.55	Best:18.16
2024-12-27 16:53:00,113: Snapshot:3	Epoch:3	Loss:44.164	translation_Loss:27.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.964                                                   	MRR:18.33	Hits@10:33.55	Best:18.33
2024-12-27 16:53:13,236: Snapshot:3	Epoch:4	Loss:43.696	translation_Loss:26.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.95                                                   	MRR:18.39	Hits@10:33.72	Best:18.39
2024-12-27 16:53:26,442: Snapshot:3	Epoch:5	Loss:43.358	translation_Loss:26.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.918                                                   	MRR:18.6	Hits@10:33.83	Best:18.6
2024-12-27 16:53:39,531: Snapshot:3	Epoch:6	Loss:43.097	translation_Loss:26.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.92                                                   	MRR:18.59	Hits@10:33.79	Best:18.6
2024-12-27 16:53:52,696: Snapshot:3	Epoch:7	Loss:42.873	translation_Loss:26.034	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.84                                                   	MRR:18.61	Hits@10:33.76	Best:18.61
2024-12-27 16:54:05,940: Snapshot:3	Epoch:8	Loss:42.868	translation_Loss:25.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.908                                                   	MRR:18.62	Hits@10:33.83	Best:18.62
2024-12-27 16:54:18,991: Snapshot:3	Epoch:9	Loss:42.821	translation_Loss:25.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.951                                                   	MRR:18.7	Hits@10:33.92	Best:18.7
2024-12-27 16:54:32,067: Snapshot:3	Epoch:10	Loss:42.67	translation_Loss:25.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.891                                                   	MRR:18.59	Hits@10:34.1	Best:18.7
2024-12-27 16:54:45,185: Snapshot:3	Epoch:11	Loss:42.575	translation_Loss:25.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.923                                                   	MRR:18.73	Hits@10:34.0	Best:18.73
2024-12-27 16:54:58,165: Snapshot:3	Epoch:12	Loss:42.375	translation_Loss:25.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.831                                                   	MRR:18.72	Hits@10:34.06	Best:18.73
2024-12-27 16:55:11,270: Snapshot:3	Epoch:13	Loss:42.466	translation_Loss:25.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.928                                                   	MRR:18.64	Hits@10:33.99	Best:18.73
2024-12-27 16:55:24,443: Snapshot:3	Epoch:14	Loss:42.288	translation_Loss:25.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.854                                                   	MRR:18.77	Hits@10:34.1	Best:18.77
2024-12-27 16:55:37,459: Snapshot:3	Epoch:15	Loss:42.29	translation_Loss:25.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.878                                                   	MRR:18.6	Hits@10:34.02	Best:18.77
2024-12-27 16:55:50,437: Snapshot:3	Epoch:16	Loss:42.169	translation_Loss:25.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.867                                                   	MRR:18.77	Hits@10:34.1	Best:18.77
2024-12-27 16:56:03,458: Snapshot:3	Epoch:17	Loss:42.079	translation_Loss:25.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.812                                                   	MRR:18.78	Hits@10:34.14	Best:18.78
2024-12-27 16:56:16,545: Snapshot:3	Epoch:18	Loss:42.027	translation_Loss:25.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.825                                                   	MRR:18.83	Hits@10:34.08	Best:18.83
2024-12-27 16:56:29,560: Snapshot:3	Epoch:19	Loss:42.089	translation_Loss:25.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.851                                                   	MRR:18.83	Hits@10:34.06	Best:18.83
2024-12-27 16:56:43,094: Snapshot:3	Epoch:20	Loss:41.875	translation_Loss:25.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.745                                                   	MRR:18.9	Hits@10:34.11	Best:18.9
2024-12-27 16:56:56,136: Snapshot:3	Epoch:21	Loss:42.05	translation_Loss:25.182	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.868                                                   	MRR:18.88	Hits@10:34.06	Best:18.9
2024-12-27 16:57:09,226: Snapshot:3	Epoch:22	Loss:41.86	translation_Loss:25.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.769                                                   	MRR:18.93	Hits@10:34.09	Best:18.93
2024-12-27 16:57:22,601: Snapshot:3	Epoch:23	Loss:41.86	translation_Loss:25.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.801                                                   	MRR:18.85	Hits@10:34.14	Best:18.93
2024-12-27 16:57:35,627: Snapshot:3	Epoch:24	Loss:41.832	translation_Loss:25.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.725                                                   	MRR:18.82	Hits@10:33.98	Best:18.93
2024-12-27 16:57:48,707: Snapshot:3	Epoch:25	Loss:41.857	translation_Loss:25.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.814                                                   	MRR:18.74	Hits@10:34.06	Best:18.93
2024-12-27 16:58:01,769: Snapshot:3	Epoch:26	Loss:41.704	translation_Loss:24.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.731                                                   	MRR:18.86	Hits@10:34.08	Best:18.93
2024-12-27 16:58:14,738: Early Stopping! Snapshot: 3 Epoch: 27 Best Results: 18.93
2024-12-27 16:58:14,739: Start to training tokens! Snapshot: 3 Epoch: 27 Loss:41.819 MRR:18.77 Best Results: 18.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 16:58:14,739: Snapshot:3	Epoch:27	Loss:41.819	translation_Loss:25.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:16.791                                                   	MRR:18.77	Hits@10:34.01	Best:18.93
2024-12-27 16:58:27,360: Snapshot:3	Epoch:28	Loss:124.544	translation_Loss:124.507	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.77	Hits@10:34.01	Best:18.93
2024-12-27 16:58:40,181: End of token training: 3 Epoch: 29 Loss:124.522 MRR:18.77 Best Results: 18.93
2024-12-27 16:58:40,182: Snapshot:3	Epoch:29	Loss:124.522	translation_Loss:124.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.77	Hits@10:34.01	Best:18.93
2024-12-27 16:58:40,509: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_1000/3model_best.tar'
2024-12-27 16:58:54,813: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2368 | 0.1322 | 0.2868 | 0.3575 |  0.4371 |
|     1      | 0.2669 | 0.1672 | 0.3108 | 0.376  |  0.4616 |
|     2      | 0.1916 | 0.113  | 0.2159 | 0.2698 |  0.3461 |
|     3      | 0.1889 | 0.1073 | 0.2223 | 0.2746 |  0.3425 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 16:59:13,933: Snapshot:4	Epoch:0	Loss:28.081	translation_Loss:20.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.772                                                   	MRR:19.22	Hits@10:30.8	Best:19.22
2024-12-27 16:59:19,306: Snapshot:4	Epoch:1	Loss:24.929	translation_Loss:16.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.23                                                   	MRR:19.28	Hits@10:30.75	Best:19.28
2024-12-27 16:59:24,674: Snapshot:4	Epoch:2	Loss:24.81	translation_Loss:16.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.33                                                   	MRR:19.34	Hits@10:30.73	Best:19.34
2024-12-27 16:59:30,071: Snapshot:4	Epoch:3	Loss:24.753	translation_Loss:16.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.339                                                   	MRR:19.26	Hits@10:30.59	Best:19.34
2024-12-27 16:59:35,429: Snapshot:4	Epoch:4	Loss:24.747	translation_Loss:16.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.383                                                   	MRR:19.33	Hits@10:30.79	Best:19.34
2024-12-27 16:59:40,810: Snapshot:4	Epoch:5	Loss:24.835	translation_Loss:16.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.425                                                   	MRR:19.33	Hits@10:30.84	Best:19.34
2024-12-27 16:59:46,240: Snapshot:4	Epoch:6	Loss:24.772	translation_Loss:16.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.417                                                   	MRR:19.35	Hits@10:30.98	Best:19.35
2024-12-27 16:59:51,532: Snapshot:4	Epoch:7	Loss:24.772	translation_Loss:16.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.415                                                   	MRR:19.29	Hits@10:30.84	Best:19.35
2024-12-27 16:59:56,942: Snapshot:4	Epoch:8	Loss:24.716	translation_Loss:16.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.423                                                   	MRR:19.47	Hits@10:31.01	Best:19.47
2024-12-27 17:00:02,276: Snapshot:4	Epoch:9	Loss:24.692	translation_Loss:16.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.424                                                   	MRR:19.22	Hits@10:31.04	Best:19.47
2024-12-27 17:00:07,687: Snapshot:4	Epoch:10	Loss:24.71	translation_Loss:16.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.448                                                   	MRR:19.28	Hits@10:30.67	Best:19.47
2024-12-27 17:00:12,998: Snapshot:4	Epoch:11	Loss:24.717	translation_Loss:16.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.453                                                   	MRR:19.4	Hits@10:30.95	Best:19.47
2024-12-27 17:00:18,305: Snapshot:4	Epoch:12	Loss:24.611	translation_Loss:16.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.406                                                   	MRR:19.36	Hits@10:30.7	Best:19.47
2024-12-27 17:00:23,612: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 19.47
2024-12-27 17:00:23,613: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:24.74 MRR:19.35 Best Results: 19.47
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:00:23,613: Snapshot:4	Epoch:13	Loss:24.74	translation_Loss:16.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.445                                                   	MRR:19.35	Hits@10:30.96	Best:19.47
2024-12-27 17:00:28,718: Snapshot:4	Epoch:14	Loss:62.543	translation_Loss:62.506	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.35	Hits@10:30.96	Best:19.47
2024-12-27 17:00:33,829: End of token training: 4 Epoch: 15 Loss:62.458 MRR:19.35 Best Results: 19.47
2024-12-27 17:00:33,829: Snapshot:4	Epoch:15	Loss:62.458	translation_Loss:62.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.35	Hits@10:30.96	Best:19.47
2024-12-27 17:00:34,163: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_1000/4model_best.tar'
2024-12-27 17:00:51,218: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2219 | 0.1213 | 0.2662 | 0.334  |  0.4147 |
|     1      | 0.2646 | 0.1668 | 0.3059 | 0.3729 |  0.4556 |
|     2      | 0.1882 | 0.1107 | 0.2112 | 0.2646 |  0.3399 |
|     3      | 0.1872 | 0.1058 | 0.2197 | 0.272  |  0.3403 |
|     4      | 0.1933 | 0.1349 | 0.2123 | 0.2529 |  0.3062 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 17:00:51,220: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1423 | 0.2928 | 0.357  |  0.436  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1424 | 0.2928 | 0.3597 |  0.4379 |
|     1      | 0.2782 | 0.1804 | 0.3208 | 0.3884 |  0.4636 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2388 | 0.1349 | 0.2884 | 0.3568 |  0.4363 |
|     1      | 0.269  | 0.1679 | 0.3129 | 0.3824 |  0.4647 |
|     2      | 0.1927 | 0.1147 | 0.2164 | 0.2709 |  0.3451 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2368 | 0.1322 | 0.2868 | 0.3575 |  0.4371 |
|     1      | 0.2669 | 0.1672 | 0.3108 | 0.376  |  0.4616 |
|     2      | 0.1916 | 0.113  | 0.2159 | 0.2698 |  0.3461 |
|     3      | 0.1889 | 0.1073 | 0.2223 | 0.2746 |  0.3425 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2219 | 0.1213 | 0.2662 | 0.334  |  0.4147 |
|     1      | 0.2646 | 0.1668 | 0.3059 | 0.3729 |  0.4556 |
|     2      | 0.1882 | 0.1107 | 0.2112 | 0.2646 |  0.3399 |
|     3      | 0.1872 | 0.1058 | 0.2197 | 0.272  |  0.3403 |
|     4      | 0.1933 | 0.1349 | 0.2123 | 0.2529 |  0.3062 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 17:00:51,221: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 79.03188467025757  |   0.244   |    0.142     |    0.293     |     0.436     |
|    1     | 81.00427389144897  |   0.253   |    0.153     |     0.3      |     0.445     |
|    2     | 234.03461503982544 |   0.218   |    0.128     |    0.253     |     0.392     |
|    3     | 415.6282341480255  |   0.206   |    0.119     |    0.241     |     0.373     |
|    4     |  96.4314477443695  |    0.2    |    0.118     |    0.231     |     0.357     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 17:00:51,221: Sum_Training_Time:906.130455493927
2024-12-27 17:00:51,221: Every_Training_Time:[79.03188467025757, 81.00427389144897, 234.03461503982544, 415.6282341480255, 96.4314477443695]
2024-12-27 17:00:51,221: Forward transfer: 0.034625 Backward transfer: -0.01050000000000001
2024-12-27 17:01:24,830: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227170055/HYBRIDHYBRID_0.01_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 17:01:34,606: Snapshot:0	Epoch:0	Loss:65.947	translation_Loss:65.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.59	Hits@10:38.22	Best:19.59
2024-12-27 17:01:40,718: Snapshot:0	Epoch:1	Loss:27.283	translation_Loss:27.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:43.72	Best:24.0
2024-12-27 17:01:46,855: Snapshot:0	Epoch:2	Loss:12.488	translation_Loss:12.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:44.28	Best:24.37
2024-12-27 17:01:52,987: Snapshot:0	Epoch:3	Loss:6.484	translation_Loss:6.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.61	Hits@10:44.14	Best:24.61
2024-12-27 17:01:59,145: Snapshot:0	Epoch:4	Loss:4.214	translation_Loss:4.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:43.81	Best:24.61
2024-12-27 17:02:05,355: Snapshot:0	Epoch:5	Loss:3.235	translation_Loss:3.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:43.75	Best:24.61
2024-12-27 17:02:11,545: Snapshot:0	Epoch:6	Loss:2.737	translation_Loss:2.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:43.67	Best:24.61
2024-12-27 17:02:17,732: Snapshot:0	Epoch:7	Loss:2.397	translation_Loss:2.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:43.16	Best:24.61
2024-12-27 17:02:24,271: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 24.61
2024-12-27 17:02:24,271: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:2.17 MRR:24.08 Best Results: 24.61
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:02:24,272: Snapshot:0	Epoch:8	Loss:2.17	translation_Loss:2.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:43.32	Best:24.61
2024-12-27 17:02:31,078: Snapshot:0	Epoch:9	Loss:67.38	translation_Loss:67.343	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:43.32	Best:24.61
2024-12-27 17:02:37,354: End of token training: 0 Epoch: 10 Loss:67.366 MRR:24.08 Best Results: 24.61
2024-12-27 17:02:37,355: Snapshot:0	Epoch:10	Loss:67.366	translation_Loss:67.366	multi_layer_Loss:-0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.08	Hits@10:43.32	Best:24.61
2024-12-27 17:02:37,609: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_5000/0model_best.tar'
2024-12-27 17:02:40,184: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.1415 | 0.2949 | 0.3573 |  0.435  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:02:51,724: Snapshot:1	Epoch:0	Loss:19.166	translation_Loss:12.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.238                                                   	MRR:25.4	Hits@10:42.59	Best:25.4
2024-12-27 17:02:54,202: Snapshot:1	Epoch:1	Loss:7.222	translation_Loss:5.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.793                                                   	MRR:26.7	Hits@10:44.55	Best:26.7
2024-12-27 17:02:56,673: Snapshot:1	Epoch:2	Loss:5.833	translation_Loss:4.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.617                                                   	MRR:26.59	Hits@10:45.58	Best:26.7
2024-12-27 17:02:59,190: Snapshot:1	Epoch:3	Loss:5.469	translation_Loss:3.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.566                                                   	MRR:26.86	Hits@10:45.6	Best:26.86
2024-12-27 17:03:01,677: Snapshot:1	Epoch:4	Loss:5.307	translation_Loss:3.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.567                                                   	MRR:26.98	Hits@10:45.76	Best:26.98
2024-12-27 17:03:04,163: Snapshot:1	Epoch:5	Loss:5.193	translation_Loss:3.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.558                                                   	MRR:27.07	Hits@10:45.72	Best:27.07
2024-12-27 17:03:06,663: Snapshot:1	Epoch:6	Loss:5.111	translation_Loss:3.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.547                                                   	MRR:27.21	Hits@10:46.04	Best:27.21
2024-12-27 17:03:09,248: Snapshot:1	Epoch:7	Loss:5.065	translation_Loss:3.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.558                                                   	MRR:27.16	Hits@10:46.12	Best:27.21
2024-12-27 17:03:11,741: Snapshot:1	Epoch:8	Loss:5.026	translation_Loss:3.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.554                                                   	MRR:27.36	Hits@10:45.84	Best:27.36
2024-12-27 17:03:14,173: Snapshot:1	Epoch:9	Loss:4.993	translation_Loss:3.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.559                                                   	MRR:27.04	Hits@10:45.89	Best:27.36
2024-12-27 17:03:16,637: Snapshot:1	Epoch:10	Loss:4.976	translation_Loss:3.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.564                                                   	MRR:27.24	Hits@10:45.76	Best:27.36
2024-12-27 17:03:19,089: Snapshot:1	Epoch:11	Loss:4.92	translation_Loss:3.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.55                                                   	MRR:27.15	Hits@10:46.05	Best:27.36
2024-12-27 17:03:21,528: Snapshot:1	Epoch:12	Loss:4.891	translation_Loss:3.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.543                                                   	MRR:27.04	Hits@10:45.65	Best:27.36
2024-12-27 17:03:23,959: Snapshot:1	Epoch:13	Loss:4.88	translation_Loss:3.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.565                                                   	MRR:27.65	Hits@10:46.17	Best:27.65
2024-12-27 17:03:26,391: Snapshot:1	Epoch:14	Loss:4.799	translation_Loss:3.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.526                                                   	MRR:27.16	Hits@10:46.17	Best:27.65
2024-12-27 17:03:28,813: Snapshot:1	Epoch:15	Loss:4.817	translation_Loss:3.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.536                                                   	MRR:27.12	Hits@10:46.31	Best:27.65
2024-12-27 17:03:31,260: Snapshot:1	Epoch:16	Loss:4.794	translation_Loss:3.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.535                                                   	MRR:27.68	Hits@10:46.16	Best:27.68
2024-12-27 17:03:33,718: Snapshot:1	Epoch:17	Loss:4.805	translation_Loss:3.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.546                                                   	MRR:27.32	Hits@10:46.43	Best:27.68
2024-12-27 17:03:36,154: Snapshot:1	Epoch:18	Loss:4.803	translation_Loss:3.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.554                                                   	MRR:27.15	Hits@10:46.07	Best:27.68
2024-12-27 17:03:38,591: Snapshot:1	Epoch:19	Loss:4.789	translation_Loss:3.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.543                                                   	MRR:27.31	Hits@10:46.34	Best:27.68
2024-12-27 17:03:41,072: Snapshot:1	Epoch:20	Loss:4.754	translation_Loss:3.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.549                                                   	MRR:27.77	Hits@10:46.46	Best:27.77
2024-12-27 17:03:43,527: Snapshot:1	Epoch:21	Loss:4.742	translation_Loss:3.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.537                                                   	MRR:27.5	Hits@10:46.25	Best:27.77
2024-12-27 17:03:45,984: Snapshot:1	Epoch:22	Loss:4.761	translation_Loss:3.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.544                                                   	MRR:27.12	Hits@10:46.44	Best:27.77
2024-12-27 17:03:48,416: Snapshot:1	Epoch:23	Loss:4.715	translation_Loss:3.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.52                                                   	MRR:27.35	Hits@10:46.44	Best:27.77
2024-12-27 17:03:50,865: Snapshot:1	Epoch:24	Loss:4.723	translation_Loss:3.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.543                                                   	MRR:27.29	Hits@10:46.63	Best:27.77
2024-12-27 17:03:53,278: Early Stopping! Snapshot: 1 Epoch: 25 Best Results: 27.77
2024-12-27 17:03:53,278: Start to training tokens! Snapshot: 1 Epoch: 25 Loss:4.759 MRR:27.49 Best Results: 27.77
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:03:53,279: Snapshot:1	Epoch:25	Loss:4.759	translation_Loss:3.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.557                                                   	MRR:27.49	Hits@10:46.78	Best:27.77
2024-12-27 17:03:55,657: Snapshot:1	Epoch:26	Loss:25.799	translation_Loss:25.763	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.49	Hits@10:46.78	Best:27.77
2024-12-27 17:03:58,037: End of token training: 1 Epoch: 27 Loss:25.784 MRR:27.49 Best Results: 27.77
2024-12-27 17:03:58,037: Snapshot:1	Epoch:27	Loss:25.784	translation_Loss:25.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.49	Hits@10:46.78	Best:27.77
2024-12-27 17:03:58,372: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_5000/1model_best.tar'
2024-12-27 17:04:02,478: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2448 | 0.1433 | 0.2943 | 0.3566 |  0.4356 |
|     1      | 0.2748 | 0.1753 | 0.3162 | 0.3796 |  0.466  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:04:37,551: Snapshot:2	Epoch:0	Loss:59.851	translation_Loss:44.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.012                                                   	MRR:17.95	Hits@10:32.57	Best:17.95
2024-12-27 17:04:48,418: Snapshot:2	Epoch:1	Loss:38.147	translation_Loss:26.823	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.324                                                   	MRR:18.39	Hits@10:33.39	Best:18.39
2024-12-27 17:04:59,193: Snapshot:2	Epoch:2	Loss:35.264	translation_Loss:24.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.115                                                   	MRR:18.66	Hits@10:33.92	Best:18.66
2024-12-27 17:05:10,141: Snapshot:2	Epoch:3	Loss:34.034	translation_Loss:23.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.972                                                   	MRR:18.77	Hits@10:33.83	Best:18.77
2024-12-27 17:05:20,983: Snapshot:2	Epoch:4	Loss:33.362	translation_Loss:22.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.958                                                   	MRR:18.82	Hits@10:34.23	Best:18.82
2024-12-27 17:05:31,730: Snapshot:2	Epoch:5	Loss:32.986	translation_Loss:22.021	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.965                                                   	MRR:18.95	Hits@10:34.06	Best:18.95
2024-12-27 17:05:42,455: Snapshot:2	Epoch:6	Loss:32.731	translation_Loss:21.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.971                                                   	MRR:18.97	Hits@10:34.4	Best:18.97
2024-12-27 17:05:53,172: Snapshot:2	Epoch:7	Loss:32.455	translation_Loss:21.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.897                                                   	MRR:19.06	Hits@10:34.28	Best:19.06
2024-12-27 17:06:04,005: Snapshot:2	Epoch:8	Loss:32.232	translation_Loss:21.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.901                                                   	MRR:19.17	Hits@10:34.48	Best:19.17
2024-12-27 17:06:14,933: Snapshot:2	Epoch:9	Loss:32.058	translation_Loss:21.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.885                                                   	MRR:19.32	Hits@10:34.27	Best:19.32
2024-12-27 17:06:25,878: Snapshot:2	Epoch:10	Loss:32.039	translation_Loss:21.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.931                                                   	MRR:19.12	Hits@10:34.4	Best:19.32
2024-12-27 17:06:36,736: Snapshot:2	Epoch:11	Loss:31.795	translation_Loss:20.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.854                                                   	MRR:19.19	Hits@10:34.48	Best:19.32
2024-12-27 17:06:47,444: Snapshot:2	Epoch:12	Loss:31.805	translation_Loss:20.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.878                                                   	MRR:19.25	Hits@10:34.26	Best:19.32
2024-12-27 17:06:58,183: Snapshot:2	Epoch:13	Loss:31.639	translation_Loss:20.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.85                                                   	MRR:19.29	Hits@10:34.24	Best:19.32
2024-12-27 17:07:08,899: Snapshot:2	Epoch:14	Loss:31.609	translation_Loss:20.724	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.885                                                   	MRR:19.37	Hits@10:34.51	Best:19.37
2024-12-27 17:07:20,177: Snapshot:2	Epoch:15	Loss:31.476	translation_Loss:20.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.854                                                   	MRR:19.29	Hits@10:34.46	Best:19.37
2024-12-27 17:07:30,884: Snapshot:2	Epoch:16	Loss:31.4	translation_Loss:20.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.771                                                   	MRR:19.27	Hits@10:34.57	Best:19.37
2024-12-27 17:07:41,728: Snapshot:2	Epoch:17	Loss:31.367	translation_Loss:20.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.847                                                   	MRR:19.58	Hits@10:34.58	Best:19.58
2024-12-27 17:07:52,408: Snapshot:2	Epoch:18	Loss:31.376	translation_Loss:20.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.865                                                   	MRR:19.44	Hits@10:34.63	Best:19.58
2024-12-27 17:08:03,254: Snapshot:2	Epoch:19	Loss:31.196	translation_Loss:20.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.788                                                   	MRR:19.36	Hits@10:34.54	Best:19.58
2024-12-27 17:08:13,956: Snapshot:2	Epoch:20	Loss:31.106	translation_Loss:20.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.793                                                   	MRR:19.41	Hits@10:34.44	Best:19.58
2024-12-27 17:08:24,731: Snapshot:2	Epoch:21	Loss:31.185	translation_Loss:20.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.806                                                   	MRR:19.22	Hits@10:34.53	Best:19.58
2024-12-27 17:08:35,486: Early Stopping! Snapshot: 2 Epoch: 22 Best Results: 19.58
2024-12-27 17:08:35,486: Start to training tokens! Snapshot: 2 Epoch: 22 Loss:31.1 MRR:19.48 Best Results: 19.58
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:08:35,487: Snapshot:2	Epoch:22	Loss:31.1	translation_Loss:20.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.81                                                   	MRR:19.48	Hits@10:34.66	Best:19.58
2024-12-27 17:08:45,946: Snapshot:2	Epoch:23	Loss:114.711	translation_Loss:114.675	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.48	Hits@10:34.66	Best:19.58
2024-12-27 17:08:56,530: End of token training: 2 Epoch: 24 Loss:114.756 MRR:19.48 Best Results: 19.58
2024-12-27 17:08:56,530: Snapshot:2	Epoch:24	Loss:114.756	translation_Loss:114.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.48	Hits@10:34.66	Best:19.58
2024-12-27 17:08:56,857: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_5000/2model_best.tar'
2024-12-27 17:09:05,463: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.139  | 0.2915 | 0.3567 |  0.4351 |
|     1      | 0.2735 | 0.1732 | 0.3159 | 0.3813 |  0.4651 |
|     2      | 0.1942 | 0.117  | 0.2199 | 0.2724 |  0.3436 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:09:47,578: Snapshot:3	Epoch:0	Loss:66.5	translation_Loss:48.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:18.477                                                   	MRR:17.57	Hits@10:32.26	Best:17.57
2024-12-27 17:10:00,724: Snapshot:3	Epoch:1	Loss:49.448	translation_Loss:34.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.221                                                   	MRR:17.98	Hits@10:32.8	Best:17.98
2024-12-27 17:10:14,090: Snapshot:3	Epoch:2	Loss:47.672	translation_Loss:32.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.131                                                   	MRR:18.2	Hits@10:33.17	Best:18.2
2024-12-27 17:10:27,323: Snapshot:3	Epoch:3	Loss:46.851	translation_Loss:31.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.068                                                   	MRR:18.49	Hits@10:33.74	Best:18.49
2024-12-27 17:10:40,564: Snapshot:3	Epoch:4	Loss:46.35	translation_Loss:31.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.061                                                   	MRR:18.42	Hits@10:33.61	Best:18.49
2024-12-27 17:10:53,782: Snapshot:3	Epoch:5	Loss:46.075	translation_Loss:31.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.058                                                   	MRR:18.54	Hits@10:33.62	Best:18.54
2024-12-27 17:11:06,992: Snapshot:3	Epoch:6	Loss:45.907	translation_Loss:30.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.074                                                   	MRR:18.59	Hits@10:33.78	Best:18.59
2024-12-27 17:11:20,263: Snapshot:3	Epoch:7	Loss:45.628	translation_Loss:30.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.055                                                   	MRR:18.7	Hits@10:33.83	Best:18.7
2024-12-27 17:11:33,452: Snapshot:3	Epoch:8	Loss:45.548	translation_Loss:30.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.071                                                   	MRR:18.66	Hits@10:33.73	Best:18.7
2024-12-27 17:11:46,645: Snapshot:3	Epoch:9	Loss:45.437	translation_Loss:30.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.037                                                   	MRR:18.78	Hits@10:33.76	Best:18.78
2024-12-27 17:11:59,724: Snapshot:3	Epoch:10	Loss:45.253	translation_Loss:30.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.026                                                   	MRR:18.77	Hits@10:33.82	Best:18.78
2024-12-27 17:12:12,925: Snapshot:3	Epoch:11	Loss:45.152	translation_Loss:30.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.99                                                   	MRR:18.75	Hits@10:33.63	Best:18.78
2024-12-27 17:12:26,055: Snapshot:3	Epoch:12	Loss:44.972	translation_Loss:29.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.988                                                   	MRR:18.82	Hits@10:33.83	Best:18.82
2024-12-27 17:12:39,208: Snapshot:3	Epoch:13	Loss:44.906	translation_Loss:29.967	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.939                                                   	MRR:18.72	Hits@10:33.8	Best:18.82
2024-12-27 17:12:52,262: Snapshot:3	Epoch:14	Loss:44.847	translation_Loss:29.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.958                                                   	MRR:18.72	Hits@10:33.74	Best:18.82
2024-12-27 17:13:05,421: Snapshot:3	Epoch:15	Loss:44.921	translation_Loss:29.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.054                                                   	MRR:18.71	Hits@10:33.72	Best:18.82
2024-12-27 17:13:18,538: Snapshot:3	Epoch:16	Loss:44.797	translation_Loss:29.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.988                                                   	MRR:18.69	Hits@10:33.5	Best:18.82
2024-12-27 17:13:31,771: Early Stopping! Snapshot: 3 Epoch: 17 Best Results: 18.82
2024-12-27 17:13:31,772: Start to training tokens! Snapshot: 3 Epoch: 17 Loss:44.62 MRR:18.82 Best Results: 18.82
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:13:31,772: Snapshot:3	Epoch:17	Loss:44.62	translation_Loss:29.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.959                                                   	MRR:18.82	Hits@10:33.8	Best:18.82
2024-12-27 17:13:44,724: Snapshot:3	Epoch:18	Loss:126.825	translation_Loss:126.789	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.82	Hits@10:33.8	Best:18.82
2024-12-27 17:13:57,470: End of token training: 3 Epoch: 19 Loss:126.835 MRR:18.82 Best Results: 18.82
2024-12-27 17:13:57,470: Snapshot:3	Epoch:19	Loss:126.835	translation_Loss:126.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.82	Hits@10:33.8	Best:18.82
2024-12-27 17:13:57,798: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_5000/3model_best.tar'
2024-12-27 17:14:11,292: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1413 | 0.2939 | 0.3574 |  0.4353 |
|     1      | 0.2751 | 0.1752 | 0.3166 | 0.3835 |  0.4679 |
|     2      | 0.1945 | 0.1172 | 0.2196 | 0.2725 |  0.3446 |
|     3      | 0.188  | 0.1091 | 0.2181 | 0.2692 |  0.3371 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:14:30,465: Snapshot:4	Epoch:0	Loss:31.66	translation_Loss:22.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.677                                                   	MRR:18.01	Hits@10:28.57	Best:18.01
2024-12-27 17:14:35,798: Snapshot:4	Epoch:1	Loss:27.071	translation_Loss:19.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.317                                                   	MRR:17.95	Hits@10:28.85	Best:18.01
2024-12-27 17:14:41,219: Snapshot:4	Epoch:2	Loss:26.994	translation_Loss:19.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.332                                                   	MRR:18.03	Hits@10:28.86	Best:18.03
2024-12-27 17:14:46,595: Snapshot:4	Epoch:3	Loss:26.985	translation_Loss:19.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.361                                                   	MRR:17.97	Hits@10:28.99	Best:18.03
2024-12-27 17:14:51,914: Snapshot:4	Epoch:4	Loss:26.908	translation_Loss:19.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.345                                                   	MRR:18.0	Hits@10:28.88	Best:18.03
2024-12-27 17:14:57,283: Snapshot:4	Epoch:5	Loss:26.854	translation_Loss:19.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.354                                                   	MRR:18.09	Hits@10:28.79	Best:18.09
2024-12-27 17:15:02,696: Snapshot:4	Epoch:6	Loss:26.894	translation_Loss:19.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.366                                                   	MRR:18.16	Hits@10:28.92	Best:18.16
2024-12-27 17:15:08,221: Snapshot:4	Epoch:7	Loss:26.86	translation_Loss:19.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.364                                                   	MRR:18.1	Hits@10:28.91	Best:18.16
2024-12-27 17:15:13,631: Snapshot:4	Epoch:8	Loss:26.834	translation_Loss:19.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.374                                                   	MRR:18.07	Hits@10:28.83	Best:18.16
2024-12-27 17:15:19,026: Snapshot:4	Epoch:9	Loss:26.844	translation_Loss:19.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.398                                                   	MRR:17.94	Hits@10:28.78	Best:18.16
2024-12-27 17:15:24,405: Snapshot:4	Epoch:10	Loss:26.817	translation_Loss:19.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.423                                                   	MRR:18.0	Hits@10:28.79	Best:18.16
2024-12-27 17:15:30,185: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 18.16
2024-12-27 17:15:30,186: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:26.866 MRR:18.06 Best Results: 18.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:15:30,186: Snapshot:4	Epoch:11	Loss:26.866	translation_Loss:19.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.413                                                   	MRR:18.06	Hits@10:28.87	Best:18.16
2024-12-27 17:15:35,397: Snapshot:4	Epoch:12	Loss:64.22	translation_Loss:64.183	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.06	Hits@10:28.87	Best:18.16
2024-12-27 17:15:40,528: End of token training: 4 Epoch: 13 Loss:64.144 MRR:18.06 Best Results: 18.16
2024-12-27 17:15:40,528: Snapshot:4	Epoch:13	Loss:64.144	translation_Loss:64.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.06	Hits@10:28.87	Best:18.16
2024-12-27 17:15:40,843: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_5000/4model_best.tar'
2024-12-27 17:15:57,796: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2368 | 0.1334 | 0.285  | 0.3516 |  0.4346 |
|     1      | 0.2718 | 0.1704 | 0.3143 | 0.3838 |  0.4675 |
|     2      | 0.192  | 0.114  | 0.2171 | 0.2703 |  0.3449 |
|     3      | 0.1874 | 0.1079 | 0.217  | 0.2698 |  0.3391 |
|     4      | 0.1793 | 0.1234 | 0.1976 | 0.2349 |  0.2893 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 17:15:57,798: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.1415 | 0.2949 | 0.3573 |  0.435  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2448 | 0.1433 | 0.2943 | 0.3566 |  0.4356 |
|     1      | 0.2748 | 0.1753 | 0.3162 | 0.3796 |  0.466  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.139  | 0.2915 | 0.3567 |  0.4351 |
|     1      | 0.2735 | 0.1732 | 0.3159 | 0.3813 |  0.4651 |
|     2      | 0.1942 | 0.117  | 0.2199 | 0.2724 |  0.3436 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1413 | 0.2939 | 0.3574 |  0.4353 |
|     1      | 0.2751 | 0.1752 | 0.3166 | 0.3835 |  0.4679 |
|     2      | 0.1945 | 0.1172 | 0.2196 | 0.2725 |  0.3446 |
|     3      | 0.188  | 0.1091 | 0.2181 | 0.2692 |  0.3371 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2368 | 0.1334 | 0.285  | 0.3516 |  0.4346 |
|     1      | 0.2718 | 0.1704 | 0.3143 | 0.3838 |  0.4675 |
|     2      | 0.192  | 0.114  | 0.2171 | 0.2703 |  0.3449 |
|     3      | 0.1874 | 0.1079 | 0.217  | 0.2698 |  0.3391 |
|     4      | 0.1793 | 0.1234 | 0.1976 | 0.2349 |  0.2893 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 17:15:57,798: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 72.52378296852112  |   0.244   |    0.141     |    0.295     |     0.435     |
|    1     | 76.59221172332764  |   0.253   |    0.152     |     0.3      |     0.444     |
|    2     | 289.74059319496155 |   0.221   |    0.132     |    0.257     |      0.39     |
|    3     | 286.7039453983307  |   0.209   |    0.124     |    0.242     |     0.371     |
|    4     | 86.63495349884033  |   0.203   |    0.121     |    0.234     |     0.361     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 17:15:57,798: Sum_Training_Time:812.1954867839813
2024-12-27 17:15:57,798: Every_Training_Time:[72.52378296852112, 76.59221172332764, 289.74059319496155, 286.7039453983307, 86.63495349884033]
2024-12-27 17:15:57,798: Forward transfer: 0.03615 Backward transfer: -0.0031999999999999945
2024-12-27 17:16:32,533: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227171602/HYBRIDHYBRID_0.01_512_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_512_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_512_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 17:16:42,401: Snapshot:0	Epoch:0	Loss:65.947	translation_Loss:65.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.58	Hits@10:38.26	Best:19.58
2024-12-27 17:16:48,519: Snapshot:0	Epoch:1	Loss:27.279	translation_Loss:27.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:43.57	Best:23.74
2024-12-27 17:16:54,706: Snapshot:0	Epoch:2	Loss:12.483	translation_Loss:12.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:44.25	Best:24.42
2024-12-27 17:17:00,902: Snapshot:0	Epoch:3	Loss:6.487	translation_Loss:6.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.56	Hits@10:44.35	Best:24.56
2024-12-27 17:17:07,078: Snapshot:0	Epoch:4	Loss:4.219	translation_Loss:4.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:43.88	Best:24.56
2024-12-27 17:17:13,190: Snapshot:0	Epoch:5	Loss:3.274	translation_Loss:3.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:43.46	Best:24.56
2024-12-27 17:17:19,327: Snapshot:0	Epoch:6	Loss:2.74	translation_Loss:2.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:43.62	Best:24.56
2024-12-27 17:17:25,403: Snapshot:0	Epoch:7	Loss:2.399	translation_Loss:2.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:43.65	Best:24.56
2024-12-27 17:17:32,015: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 24.56
2024-12-27 17:17:32,015: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:2.186 MRR:23.89 Best Results: 24.56
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:17:32,015: Snapshot:0	Epoch:8	Loss:2.186	translation_Loss:2.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:43.01	Best:24.56
2024-12-27 17:17:38,853: Snapshot:0	Epoch:9	Loss:67.224	translation_Loss:67.187	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:43.01	Best:24.56
2024-12-27 17:17:45,177: End of token training: 0 Epoch: 10 Loss:67.201 MRR:23.89 Best Results: 24.56
2024-12-27 17:17:45,178: Snapshot:0	Epoch:10	Loss:67.201	translation_Loss:67.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.89	Hits@10:43.01	Best:24.56
2024-12-27 17:17:45,432: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_10000/0model_best.tar'
2024-12-27 17:17:48,173: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2435 | 0.1416 | 0.2929 | 0.3566 |  0.4372 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:17:59,636: Snapshot:1	Epoch:0	Loss:23.193	translation_Loss:13.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.18                                                   	MRR:25.21	Hits@10:43.1	Best:25.21
2024-12-27 17:18:02,114: Snapshot:1	Epoch:1	Loss:6.976	translation_Loss:5.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:26.27	Hits@10:45.04	Best:26.27
2024-12-27 17:18:04,652: Snapshot:1	Epoch:2	Loss:5.605	translation_Loss:4.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.324                                                   	MRR:26.46	Hits@10:45.45	Best:26.46
2024-12-27 17:18:07,151: Snapshot:1	Epoch:3	Loss:5.294	translation_Loss:3.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.308                                                   	MRR:26.55	Hits@10:45.35	Best:26.55
2024-12-27 17:18:09,649: Snapshot:1	Epoch:4	Loss:5.129	translation_Loss:3.823	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.307                                                   	MRR:26.46	Hits@10:45.57	Best:26.55
2024-12-27 17:18:12,120: Snapshot:1	Epoch:5	Loss:5.034	translation_Loss:3.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.319                                                   	MRR:26.28	Hits@10:45.07	Best:26.55
2024-12-27 17:18:14,608: Snapshot:1	Epoch:6	Loss:4.95	translation_Loss:3.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.32                                                   	MRR:26.92	Hits@10:45.82	Best:26.92
2024-12-27 17:18:17,097: Snapshot:1	Epoch:7	Loss:4.918	translation_Loss:3.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.334                                                   	MRR:27.05	Hits@10:45.68	Best:27.05
2024-12-27 17:18:19,531: Snapshot:1	Epoch:8	Loss:4.853	translation_Loss:3.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.315                                                   	MRR:26.98	Hits@10:46.1	Best:27.05
2024-12-27 17:18:21,950: Snapshot:1	Epoch:9	Loss:4.834	translation_Loss:3.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.336                                                   	MRR:27.04	Hits@10:45.9	Best:27.05
2024-12-27 17:18:24,395: Snapshot:1	Epoch:10	Loss:4.823	translation_Loss:3.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:27.05	Hits@10:45.83	Best:27.05
2024-12-27 17:18:26,847: Snapshot:1	Epoch:11	Loss:4.792	translation_Loss:3.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.341                                                   	MRR:26.94	Hits@10:45.76	Best:27.05
2024-12-27 17:18:29,268: Early Stopping! Snapshot: 1 Epoch: 12 Best Results: 27.05
2024-12-27 17:18:29,268: Start to training tokens! Snapshot: 1 Epoch: 12 Loss:4.767 MRR:26.58 Best Results: 27.05
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:18:29,268: Snapshot:1	Epoch:12	Loss:4.767	translation_Loss:3.423	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.344                                                   	MRR:26.58	Hits@10:45.6	Best:27.05
2024-12-27 17:18:31,616: Snapshot:1	Epoch:13	Loss:25.771	translation_Loss:25.734	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.58	Hits@10:45.6	Best:27.05
2024-12-27 17:18:33,976: End of token training: 1 Epoch: 14 Loss:25.696 MRR:26.58 Best Results: 27.05
2024-12-27 17:18:33,976: Snapshot:1	Epoch:14	Loss:25.696	translation_Loss:25.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:26.58	Hits@10:45.6	Best:27.05
2024-12-27 17:18:34,238: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_10000/1model_best.tar'
2024-12-27 17:18:38,084: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1416 | 0.2933 | 0.3568 |  0.4381 |
|     1      | 0.2624 | 0.1641 |  0.3   | 0.3702 |  0.4563 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:19:13,468: Snapshot:2	Epoch:0	Loss:62.702	translation_Loss:45.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:17.506                                                   	MRR:18.13	Hits@10:32.99	Best:18.13
2024-12-27 17:19:24,411: Snapshot:2	Epoch:1	Loss:37.6	translation_Loss:27.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.488                                                   	MRR:18.42	Hits@10:33.51	Best:18.42
2024-12-27 17:19:35,233: Snapshot:2	Epoch:2	Loss:34.77	translation_Loss:24.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.32                                                   	MRR:18.87	Hits@10:34.09	Best:18.87
2024-12-27 17:19:46,029: Snapshot:2	Epoch:3	Loss:33.626	translation_Loss:23.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.237                                                   	MRR:18.95	Hits@10:34.13	Best:18.95
2024-12-27 17:19:56,825: Snapshot:2	Epoch:4	Loss:32.941	translation_Loss:22.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.221                                                   	MRR:19.06	Hits@10:34.27	Best:19.06
2024-12-27 17:20:07,707: Snapshot:2	Epoch:5	Loss:32.647	translation_Loss:22.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.244                                                   	MRR:19.1	Hits@10:34.28	Best:19.1
2024-12-27 17:20:18,508: Snapshot:2	Epoch:6	Loss:32.331	translation_Loss:22.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.246                                                   	MRR:19.23	Hits@10:34.37	Best:19.23
2024-12-27 17:20:29,242: Snapshot:2	Epoch:7	Loss:32.089	translation_Loss:21.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.242                                                   	MRR:19.23	Hits@10:34.46	Best:19.23
2024-12-27 17:20:39,978: Snapshot:2	Epoch:8	Loss:31.943	translation_Loss:21.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.279                                                   	MRR:19.16	Hits@10:34.44	Best:19.23
2024-12-27 17:20:50,810: Snapshot:2	Epoch:9	Loss:31.742	translation_Loss:21.485	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.257                                                   	MRR:19.29	Hits@10:34.51	Best:19.29
2024-12-27 17:21:01,619: Snapshot:2	Epoch:10	Loss:31.698	translation_Loss:21.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.325                                                   	MRR:19.21	Hits@10:34.48	Best:19.29
2024-12-27 17:21:12,329: Snapshot:2	Epoch:11	Loss:31.598	translation_Loss:21.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.303                                                   	MRR:19.27	Hits@10:34.51	Best:19.29
2024-12-27 17:21:23,235: Snapshot:2	Epoch:12	Loss:31.428	translation_Loss:21.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.261                                                   	MRR:19.19	Hits@10:34.58	Best:19.29
2024-12-27 17:21:34,027: Snapshot:2	Epoch:13	Loss:31.351	translation_Loss:21.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.244                                                   	MRR:19.37	Hits@10:34.64	Best:19.37
2024-12-27 17:21:44,733: Snapshot:2	Epoch:14	Loss:31.341	translation_Loss:21.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.256                                                   	MRR:19.28	Hits@10:34.6	Best:19.37
2024-12-27 17:21:55,530: Snapshot:2	Epoch:15	Loss:31.198	translation_Loss:20.952	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.247                                                   	MRR:19.39	Hits@10:34.77	Best:19.39
2024-12-27 17:22:06,439: Snapshot:2	Epoch:16	Loss:31.165	translation_Loss:20.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.24                                                   	MRR:19.41	Hits@10:34.58	Best:19.41
2024-12-27 17:22:17,167: Snapshot:2	Epoch:17	Loss:31.171	translation_Loss:20.896	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.275                                                   	MRR:19.36	Hits@10:34.64	Best:19.41
2024-12-27 17:22:27,939: Snapshot:2	Epoch:18	Loss:31.003	translation_Loss:20.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.224                                                   	MRR:19.25	Hits@10:34.64	Best:19.41
2024-12-27 17:22:38,691: Snapshot:2	Epoch:19	Loss:30.954	translation_Loss:20.73	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.225                                                   	MRR:19.51	Hits@10:34.86	Best:19.51
2024-12-27 17:22:49,475: Snapshot:2	Epoch:20	Loss:30.835	translation_Loss:20.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.153                                                   	MRR:19.35	Hits@10:34.66	Best:19.51
2024-12-27 17:23:00,297: Snapshot:2	Epoch:21	Loss:30.928	translation_Loss:20.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.23                                                   	MRR:19.42	Hits@10:34.68	Best:19.51
2024-12-27 17:23:11,025: Snapshot:2	Epoch:22	Loss:30.865	translation_Loss:20.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.21                                                   	MRR:19.34	Hits@10:34.71	Best:19.51
2024-12-27 17:23:21,965: Snapshot:2	Epoch:23	Loss:30.754	translation_Loss:20.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.184                                                   	MRR:19.46	Hits@10:34.74	Best:19.51
2024-12-27 17:23:32,745: Early Stopping! Snapshot: 2 Epoch: 24 Best Results: 19.51
2024-12-27 17:23:32,745: Start to training tokens! Snapshot: 2 Epoch: 24 Loss:30.789 MRR:19.49 Best Results: 19.51
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:23:32,745: Snapshot:2	Epoch:24	Loss:30.789	translation_Loss:20.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.175                                                   	MRR:19.49	Hits@10:34.66	Best:19.51
2024-12-27 17:23:43,357: Snapshot:2	Epoch:25	Loss:114.595	translation_Loss:114.559	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.49	Hits@10:34.66	Best:19.51
2024-12-27 17:23:53,794: End of token training: 2 Epoch: 26 Loss:114.594 MRR:19.49 Best Results: 19.51
2024-12-27 17:23:53,795: Snapshot:2	Epoch:26	Loss:114.594	translation_Loss:114.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.49	Hits@10:34.66	Best:19.51
2024-12-27 17:23:54,121: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_10000/2model_best.tar'
2024-12-27 17:24:02,498: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1406 | 0.2926 | 0.3566 |  0.4375 |
|     1      | 0.2611 | 0.1625 | 0.2994 | 0.3685 |  0.4565 |
|     2      | 0.194  | 0.1154 | 0.2199 | 0.2731 |  0.3469 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:24:43,803: Snapshot:3	Epoch:0	Loss:69.274	translation_Loss:48.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:20.635                                                   	MRR:17.56	Hits@10:32.13	Best:17.56
2024-12-27 17:24:56,964: Snapshot:3	Epoch:1	Loss:48.948	translation_Loss:34.791	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.158                                                   	MRR:18.09	Hits@10:33.2	Best:18.09
2024-12-27 17:25:10,252: Snapshot:3	Epoch:2	Loss:47.079	translation_Loss:33.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.037                                                   	MRR:18.29	Hits@10:33.57	Best:18.29
2024-12-27 17:25:23,651: Snapshot:3	Epoch:3	Loss:46.18	translation_Loss:32.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.983                                                   	MRR:18.41	Hits@10:33.75	Best:18.41
2024-12-27 17:25:36,872: Snapshot:3	Epoch:4	Loss:45.813	translation_Loss:31.749	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.064                                                   	MRR:18.39	Hits@10:33.61	Best:18.41
2024-12-27 17:25:50,479: Snapshot:3	Epoch:5	Loss:45.524	translation_Loss:31.485	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.039                                                   	MRR:18.66	Hits@10:33.85	Best:18.66
2024-12-27 17:26:03,663: Snapshot:3	Epoch:6	Loss:45.426	translation_Loss:31.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.155                                                   	MRR:18.62	Hits@10:33.9	Best:18.66
2024-12-27 17:26:16,834: Snapshot:3	Epoch:7	Loss:45.183	translation_Loss:31.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.152                                                   	MRR:18.58	Hits@10:33.93	Best:18.66
2024-12-27 17:26:30,204: Snapshot:3	Epoch:8	Loss:44.964	translation_Loss:30.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.125                                                   	MRR:18.77	Hits@10:34.1	Best:18.77
2024-12-27 17:26:43,327: Snapshot:3	Epoch:9	Loss:44.968	translation_Loss:30.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.187                                                   	MRR:18.66	Hits@10:33.87	Best:18.77
2024-12-27 17:26:56,429: Snapshot:3	Epoch:10	Loss:44.838	translation_Loss:30.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.153                                                   	MRR:18.66	Hits@10:33.94	Best:18.77
2024-12-27 17:27:09,596: Snapshot:3	Epoch:11	Loss:44.742	translation_Loss:30.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.145                                                   	MRR:18.77	Hits@10:33.94	Best:18.77
2024-12-27 17:27:22,903: Snapshot:3	Epoch:12	Loss:44.646	translation_Loss:30.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.16                                                   	MRR:18.69	Hits@10:33.98	Best:18.77
2024-12-27 17:27:36,139: Early Stopping! Snapshot: 3 Epoch: 13 Best Results: 18.77
2024-12-27 17:27:36,140: Start to training tokens! Snapshot: 3 Epoch: 13 Loss:44.462 MRR:18.75 Best Results: 18.77
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:27:36,140: Snapshot:3	Epoch:13	Loss:44.462	translation_Loss:30.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.067                                                   	MRR:18.75	Hits@10:34.01	Best:18.77
2024-12-27 17:27:48,929: Snapshot:3	Epoch:14	Loss:127.238	translation_Loss:127.202	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.75	Hits@10:34.01	Best:18.77
2024-12-27 17:28:02,136: End of token training: 3 Epoch: 15 Loss:127.051 MRR:18.75 Best Results: 18.77
2024-12-27 17:28:02,137: Snapshot:3	Epoch:15	Loss:127.051	translation_Loss:127.051	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.75	Hits@10:34.01	Best:18.77
2024-12-27 17:28:02,438: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_10000/3model_best.tar'
2024-12-27 17:28:16,670: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1405 | 0.292  | 0.3572 |  0.4372 |
|     1      | 0.2626 | 0.1648 | 0.2986 | 0.3675 |  0.4566 |
|     2      | 0.1938 | 0.115  |  0.22  | 0.2732 |  0.3466 |
|     3      | 0.1879 | 0.1079 | 0.2193 | 0.2725 |  0.3409 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:28:36,044: Snapshot:4	Epoch:0	Loss:33.556	translation_Loss:23.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.033                                                   	MRR:17.74	Hits@10:28.29	Best:17.74
2024-12-27 17:28:41,574: Snapshot:4	Epoch:1	Loss:27.143	translation_Loss:20.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.742                                                   	MRR:17.94	Hits@10:28.58	Best:17.94
2024-12-27 17:28:46,928: Snapshot:4	Epoch:2	Loss:27.1	translation_Loss:20.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.787                                                   	MRR:17.82	Hits@10:28.64	Best:17.94
2024-12-27 17:28:52,255: Snapshot:4	Epoch:3	Loss:27.061	translation_Loss:20.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.824                                                   	MRR:17.81	Hits@10:28.55	Best:17.94
2024-12-27 17:28:57,577: Snapshot:4	Epoch:4	Loss:27.07	translation_Loss:20.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.881                                                   	MRR:17.89	Hits@10:28.55	Best:17.94
2024-12-27 17:29:02,895: Snapshot:4	Epoch:5	Loss:27.081	translation_Loss:20.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.921                                                   	MRR:17.84	Hits@10:28.5	Best:17.94
2024-12-27 17:29:08,280: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 17.94
2024-12-27 17:29:08,280: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:27.093 MRR:17.75 Best Results: 17.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:29:08,280: Snapshot:4	Epoch:6	Loss:27.093	translation_Loss:20.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.956                                                   	MRR:17.75	Hits@10:28.5	Best:17.94
2024-12-27 17:29:13,506: Snapshot:4	Epoch:7	Loss:64.278	translation_Loss:64.24	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.75	Hits@10:28.5	Best:17.94
2024-12-27 17:29:18,723: End of token training: 4 Epoch: 8 Loss:64.291 MRR:17.75 Best Results: 17.94
2024-12-27 17:29:18,723: Snapshot:4	Epoch:8	Loss:64.291	translation_Loss:64.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.75	Hits@10:28.5	Best:17.94
2024-12-27 17:29:19,056: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_512_10000/4model_best.tar'
2024-12-27 17:29:35,516: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2428 | 0.1391 | 0.2938 | 0.3574 |  0.4374 |
|     1      | 0.263  | 0.1647 | 0.2998 | 0.3705 |  0.4565 |
|     2      | 0.1936 | 0.1145 | 0.2201 | 0.2743 |  0.3472 |
|     3      | 0.1888 | 0.1085 | 0.2202 | 0.2737 |  0.3426 |
|     4      | 0.1753 | 0.1217 | 0.1912 | 0.2272 |  0.2765 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 17:29:35,518: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2435 | 0.1416 | 0.2929 | 0.3566 |  0.4372 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1416 | 0.2933 | 0.3568 |  0.4381 |
|     1      | 0.2624 | 0.1641 |  0.3   | 0.3702 |  0.4563 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1406 | 0.2926 | 0.3566 |  0.4375 |
|     1      | 0.2611 | 0.1625 | 0.2994 | 0.3685 |  0.4565 |
|     2      | 0.194  | 0.1154 | 0.2199 | 0.2731 |  0.3469 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1405 | 0.292  | 0.3572 |  0.4372 |
|     1      | 0.2626 | 0.1648 | 0.2986 | 0.3675 |  0.4566 |
|     2      | 0.1938 | 0.115  |  0.22  | 0.2732 |  0.3466 |
|     3      | 0.1879 | 0.1079 | 0.2193 | 0.2725 |  0.3409 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2428 | 0.1391 | 0.2938 | 0.3574 |  0.4374 |
|     1      | 0.263  | 0.1647 | 0.2998 | 0.3705 |  0.4565 |
|     2      | 0.1936 | 0.1145 | 0.2201 | 0.2743 |  0.3472 |
|     3      | 0.1888 | 0.1085 | 0.2202 | 0.2737 |  0.3426 |
|     4      | 0.1753 | 0.1217 | 0.1912 | 0.2272 |  0.2765 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 17:29:35,519: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 72.64389109611511  |   0.243   |    0.142     |    0.293     |     0.437     |
|    1     | 44.544416666030884 |   0.249   |    0.148     |    0.295     |     0.443     |
|    2     | 311.2212829589844  |   0.219   |     0.13     |    0.255     |     0.392     |
|    3     | 234.35043120384216 |   0.207   |    0.122     |    0.241     |     0.372     |
|    4     | 59.48752427101135  |   0.203   |    0.121     |    0.235     |     0.361     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 17:29:35,519: Sum_Training_Time:722.2475461959839
2024-12-27 17:29:35,519: Every_Training_Time:[72.64389109611511, 44.544416666030884, 311.2212829589844, 234.35043120384216, 59.48752427101135]
2024-12-27 17:29:35,519: Forward transfer: 0.036000000000000004 Backward transfer: 9.999999999998899e-05
2024-12-27 17:30:08,986: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227172939/HYBRIDHYBRID_0.01_1024_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_1024_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_1024_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 17:30:18,616: Snapshot:0	Epoch:0	Loss:35.025	translation_Loss:35.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.86	Hits@10:36.83	Best:18.86
2024-12-27 17:30:24,618: Snapshot:0	Epoch:1	Loss:15.116	translation_Loss:15.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:43.69	Best:23.88
2024-12-27 17:30:30,649: Snapshot:0	Epoch:2	Loss:6.999	translation_Loss:6.999	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.26	Hits@10:44.98	Best:25.26
2024-12-27 17:30:36,681: Snapshot:0	Epoch:3	Loss:3.628	translation_Loss:3.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.3	Hits@10:45.16	Best:25.3
2024-12-27 17:30:42,754: Snapshot:0	Epoch:4	Loss:2.286	translation_Loss:2.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:44.88	Best:25.3
2024-12-27 17:30:48,755: Snapshot:0	Epoch:5	Loss:1.742	translation_Loss:1.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.61	Hits@10:44.45	Best:25.3
2024-12-27 17:30:55,202: Snapshot:0	Epoch:6	Loss:1.478	translation_Loss:1.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:44.17	Best:25.3
2024-12-27 17:31:01,260: Snapshot:0	Epoch:7	Loss:1.296	translation_Loss:1.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.45	Hits@10:44.18	Best:25.3
2024-12-27 17:31:07,213: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 25.3
2024-12-27 17:31:07,213: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:1.167 MRR:24.58 Best Results: 25.3
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:31:07,214: Snapshot:0	Epoch:8	Loss:1.167	translation_Loss:1.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.58	Hits@10:44.12	Best:25.3
2024-12-27 17:31:13,946: Snapshot:0	Epoch:9	Loss:34.59	translation_Loss:34.554	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.58	Hits@10:44.12	Best:25.3
2024-12-27 17:31:20,133: End of token training: 0 Epoch: 10 Loss:34.574 MRR:24.58 Best Results: 25.3
2024-12-27 17:31:20,133: Snapshot:0	Epoch:10	Loss:34.574	translation_Loss:34.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.58	Hits@10:44.12	Best:25.3
2024-12-27 17:31:20,441: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_1000/0model_best.tar'
2024-12-27 17:31:23,066: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2528 |  0.15  | 0.3047 | 0.3685 |  0.444  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:31:34,220: Snapshot:1	Epoch:0	Loss:9.219	translation_Loss:7.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.156                                                   	MRR:25.57	Hits@10:44.16	Best:25.57
2024-12-27 17:31:36,661: Snapshot:1	Epoch:1	Loss:4.213	translation_Loss:2.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.309                                                   	MRR:28.31	Hits@10:47.22	Best:28.31
2024-12-27 17:31:39,041: Snapshot:1	Epoch:2	Loss:3.27	translation_Loss:2.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.092                                                   	MRR:29.03	Hits@10:48.42	Best:29.03
2024-12-27 17:31:41,450: Snapshot:1	Epoch:3	Loss:2.988	translation_Loss:1.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.024                                                   	MRR:29.3	Hits@10:48.42	Best:29.3
2024-12-27 17:31:43,841: Snapshot:1	Epoch:4	Loss:2.866	translation_Loss:1.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.997                                                   	MRR:29.28	Hits@10:48.41	Best:29.3
2024-12-27 17:31:46,216: Snapshot:1	Epoch:5	Loss:2.791	translation_Loss:1.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.984                                                   	MRR:29.31	Hits@10:48.71	Best:29.31
2024-12-27 17:31:48,592: Snapshot:1	Epoch:6	Loss:2.749	translation_Loss:1.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.987                                                   	MRR:29.51	Hits@10:49.29	Best:29.51
2024-12-27 17:31:50,976: Snapshot:1	Epoch:7	Loss:2.713	translation_Loss:1.728	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.985                                                   	MRR:29.58	Hits@10:49.08	Best:29.58
2024-12-27 17:31:53,384: Snapshot:1	Epoch:8	Loss:2.697	translation_Loss:1.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.985                                                   	MRR:29.79	Hits@10:49.13	Best:29.79
2024-12-27 17:31:55,733: Snapshot:1	Epoch:9	Loss:2.668	translation_Loss:1.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.973                                                   	MRR:29.77	Hits@10:49.13	Best:29.79
2024-12-27 17:31:58,116: Snapshot:1	Epoch:10	Loss:2.679	translation_Loss:1.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.986                                                   	MRR:29.98	Hits@10:48.98	Best:29.98
2024-12-27 17:32:00,430: Snapshot:1	Epoch:11	Loss:2.644	translation_Loss:1.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.975                                                   	MRR:29.96	Hits@10:49.09	Best:29.98
2024-12-27 17:32:02,780: Snapshot:1	Epoch:12	Loss:2.623	translation_Loss:1.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.972                                                   	MRR:29.53	Hits@10:48.56	Best:29.98
2024-12-27 17:32:05,082: Snapshot:1	Epoch:13	Loss:2.607	translation_Loss:1.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.977                                                   	MRR:29.65	Hits@10:49.16	Best:29.98
2024-12-27 17:32:07,407: Snapshot:1	Epoch:14	Loss:2.572	translation_Loss:1.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.958                                                   	MRR:29.62	Hits@10:49.31	Best:29.98
2024-12-27 17:32:09,723: Early Stopping! Snapshot: 1 Epoch: 15 Best Results: 29.98
2024-12-27 17:32:09,723: Start to training tokens! Snapshot: 1 Epoch: 15 Loss:2.585 MRR:29.49 Best Results: 29.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:32:09,723: Snapshot:1	Epoch:15	Loss:2.585	translation_Loss:1.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.964                                                   	MRR:29.49	Hits@10:49.19	Best:29.98
2024-12-27 17:32:12,019: Snapshot:1	Epoch:16	Loss:13.081	translation_Loss:13.046	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.49	Hits@10:49.19	Best:29.98
2024-12-27 17:32:14,305: End of token training: 1 Epoch: 17 Loss:13.042 MRR:29.49 Best Results: 29.98
2024-12-27 17:32:14,305: Snapshot:1	Epoch:17	Loss:13.042	translation_Loss:13.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.49	Hits@10:49.19	Best:29.98
2024-12-27 17:32:14,576: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_1000/1model_best.tar'
2024-12-27 17:32:18,726: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2526 | 0.1509 | 0.3031 | 0.3662 |  0.4429 |
|     1      | 0.2871 | 0.1855 | 0.3346 | 0.3984 |  0.485  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:32:52,678: Snapshot:2	Epoch:0	Loss:31.353	translation_Loss:23.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.608                                                   	MRR:19.1	Hits@10:34.59	Best:19.1
2024-12-27 17:33:03,124: Snapshot:2	Epoch:1	Loss:20.008	translation_Loss:13.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.648                                                   	MRR:19.76	Hits@10:35.28	Best:19.76
2024-12-27 17:33:13,480: Snapshot:2	Epoch:2	Loss:18.445	translation_Loss:11.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.512                                                   	MRR:19.88	Hits@10:35.53	Best:19.88
2024-12-27 17:33:23,741: Snapshot:2	Epoch:3	Loss:17.887	translation_Loss:11.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.478                                                   	MRR:19.83	Hits@10:35.54	Best:19.88
2024-12-27 17:33:34,141: Snapshot:2	Epoch:4	Loss:17.467	translation_Loss:11.049	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.418                                                   	MRR:20.11	Hits@10:35.9	Best:20.11
2024-12-27 17:33:44,504: Snapshot:2	Epoch:5	Loss:17.242	translation_Loss:10.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.402                                                   	MRR:20.21	Hits@10:35.9	Best:20.21
2024-12-27 17:33:54,804: Snapshot:2	Epoch:6	Loss:17.067	translation_Loss:10.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.391                                                   	MRR:20.35	Hits@10:35.89	Best:20.35
2024-12-27 17:34:05,147: Snapshot:2	Epoch:7	Loss:16.937	translation_Loss:10.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.384                                                   	MRR:20.35	Hits@10:35.88	Best:20.35
2024-12-27 17:34:15,607: Snapshot:2	Epoch:8	Loss:16.909	translation_Loss:10.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.4                                                   	MRR:20.29	Hits@10:35.94	Best:20.35
2024-12-27 17:34:25,828: Snapshot:2	Epoch:9	Loss:16.771	translation_Loss:10.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.368                                                   	MRR:20.35	Hits@10:35.92	Best:20.35
2024-12-27 17:34:36,221: Snapshot:2	Epoch:10	Loss:16.664	translation_Loss:10.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.341                                                   	MRR:20.41	Hits@10:36.01	Best:20.41
2024-12-27 17:34:46,562: Snapshot:2	Epoch:11	Loss:16.6	translation_Loss:10.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.337                                                   	MRR:20.43	Hits@10:35.83	Best:20.43
2024-12-27 17:34:56,860: Snapshot:2	Epoch:12	Loss:16.642	translation_Loss:10.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.379                                                   	MRR:20.42	Hits@10:36.22	Best:20.43
2024-12-27 17:35:07,628: Snapshot:2	Epoch:13	Loss:16.534	translation_Loss:10.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.355                                                   	MRR:20.37	Hits@10:36.03	Best:20.43
2024-12-27 17:35:17,855: Snapshot:2	Epoch:14	Loss:16.487	translation_Loss:10.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.37                                                   	MRR:20.33	Hits@10:35.92	Best:20.43
2024-12-27 17:35:28,104: Snapshot:2	Epoch:15	Loss:16.413	translation_Loss:10.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.329                                                   	MRR:20.45	Hits@10:36.17	Best:20.45
2024-12-27 17:35:38,396: Snapshot:2	Epoch:16	Loss:16.395	translation_Loss:10.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.31                                                   	MRR:20.59	Hits@10:36.07	Best:20.59
2024-12-27 17:35:48,714: Snapshot:2	Epoch:17	Loss:16.366	translation_Loss:10.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.333                                                   	MRR:20.4	Hits@10:35.86	Best:20.59
2024-12-27 17:35:59,119: Snapshot:2	Epoch:18	Loss:16.345	translation_Loss:10.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.311                                                   	MRR:20.47	Hits@10:36.29	Best:20.59
2024-12-27 17:36:09,385: Snapshot:2	Epoch:19	Loss:16.296	translation_Loss:9.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.323                                                   	MRR:20.48	Hits@10:36.01	Best:20.59
2024-12-27 17:36:19,609: Snapshot:2	Epoch:20	Loss:16.314	translation_Loss:9.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.329                                                   	MRR:20.39	Hits@10:36.17	Best:20.59
2024-12-27 17:36:29,816: Early Stopping! Snapshot: 2 Epoch: 21 Best Results: 20.59
2024-12-27 17:36:29,816: Start to training tokens! Snapshot: 2 Epoch: 21 Loss:16.312 MRR:20.53 Best Results: 20.59
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:36:29,816: Snapshot:2	Epoch:21	Loss:16.312	translation_Loss:9.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.364                                                   	MRR:20.53	Hits@10:36.18	Best:20.59
2024-12-27 17:36:39,891: Snapshot:2	Epoch:22	Loss:57.229	translation_Loss:57.193	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.53	Hits@10:36.18	Best:20.59
2024-12-27 17:36:50,370: End of token training: 2 Epoch: 23 Loss:57.203 MRR:20.53 Best Results: 20.59
2024-12-27 17:36:50,370: Snapshot:2	Epoch:23	Loss:57.203	translation_Loss:57.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.53	Hits@10:36.18	Best:20.59
2024-12-27 17:36:50,644: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_1000/2model_best.tar'
2024-12-27 17:36:58,740: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2433 | 0.1392 | 0.2942 | 0.3587 |  0.4415 |
|     1      | 0.2749 | 0.1768 | 0.3145 | 0.3813 |  0.4704 |
|     2      | 0.2028 | 0.1226 |  0.23  | 0.2834 |  0.3606 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:37:39,612: Snapshot:3	Epoch:0	Loss:34.649	translation_Loss:24.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.661                                                   	MRR:18.47	Hits@10:34.0	Best:18.47
2024-12-27 17:37:52,275: Snapshot:3	Epoch:1	Loss:25.648	translation_Loss:16.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.789                                                   	MRR:18.84	Hits@10:34.4	Best:18.84
2024-12-27 17:38:04,757: Snapshot:3	Epoch:2	Loss:24.711	translation_Loss:15.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.746                                                   	MRR:18.84	Hits@10:34.49	Best:18.84
2024-12-27 17:38:17,325: Snapshot:3	Epoch:3	Loss:24.259	translation_Loss:15.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.731                                                   	MRR:19.16	Hits@10:34.98	Best:19.16
2024-12-27 17:38:29,806: Snapshot:3	Epoch:4	Loss:23.986	translation_Loss:15.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.715                                                   	MRR:19.02	Hits@10:34.83	Best:19.16
2024-12-27 17:38:42,458: Snapshot:3	Epoch:5	Loss:23.837	translation_Loss:15.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.71                                                   	MRR:19.16	Hits@10:35.0	Best:19.16
2024-12-27 17:38:55,032: Snapshot:3	Epoch:6	Loss:23.696	translation_Loss:14.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.704                                                   	MRR:19.2	Hits@10:35.08	Best:19.2
2024-12-27 17:39:07,669: Snapshot:3	Epoch:7	Loss:23.572	translation_Loss:14.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.689                                                   	MRR:19.22	Hits@10:35.12	Best:19.22
2024-12-27 17:39:20,386: Snapshot:3	Epoch:8	Loss:23.478	translation_Loss:14.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.681                                                   	MRR:19.29	Hits@10:35.1	Best:19.29
2024-12-27 17:39:32,991: Snapshot:3	Epoch:9	Loss:23.456	translation_Loss:14.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.697                                                   	MRR:19.35	Hits@10:35.2	Best:19.35
2024-12-27 17:39:45,616: Snapshot:3	Epoch:10	Loss:23.333	translation_Loss:14.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.649                                                   	MRR:19.36	Hits@10:35.19	Best:19.36
2024-12-27 17:39:58,394: Snapshot:3	Epoch:11	Loss:23.293	translation_Loss:14.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.657                                                   	MRR:19.38	Hits@10:35.15	Best:19.38
2024-12-27 17:40:11,115: Snapshot:3	Epoch:12	Loss:23.266	translation_Loss:14.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.672                                                   	MRR:19.43	Hits@10:35.13	Best:19.43
2024-12-27 17:40:23,669: Snapshot:3	Epoch:13	Loss:23.207	translation_Loss:14.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.664                                                   	MRR:19.35	Hits@10:35.16	Best:19.43
2024-12-27 17:40:36,300: Snapshot:3	Epoch:14	Loss:23.239	translation_Loss:14.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.678                                                   	MRR:19.33	Hits@10:35.15	Best:19.43
2024-12-27 17:40:48,762: Snapshot:3	Epoch:15	Loss:23.104	translation_Loss:14.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.638                                                   	MRR:19.29	Hits@10:35.11	Best:19.43
2024-12-27 17:41:01,344: Snapshot:3	Epoch:16	Loss:23.151	translation_Loss:14.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.674                                                   	MRR:19.41	Hits@10:35.25	Best:19.43
2024-12-27 17:41:13,942: Snapshot:3	Epoch:17	Loss:23.115	translation_Loss:14.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.642                                                   	MRR:19.47	Hits@10:35.22	Best:19.47
2024-12-27 17:41:26,551: Snapshot:3	Epoch:18	Loss:23.082	translation_Loss:14.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.652                                                   	MRR:19.44	Hits@10:35.27	Best:19.47
2024-12-27 17:41:39,067: Snapshot:3	Epoch:19	Loss:23.113	translation_Loss:14.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.673                                                   	MRR:19.45	Hits@10:35.37	Best:19.47
2024-12-27 17:41:51,715: Snapshot:3	Epoch:20	Loss:23.096	translation_Loss:14.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.683                                                   	MRR:19.56	Hits@10:35.28	Best:19.56
2024-12-27 17:42:04,798: Snapshot:3	Epoch:21	Loss:23.057	translation_Loss:14.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.669                                                   	MRR:19.46	Hits@10:35.35	Best:19.56
2024-12-27 17:42:17,272: Snapshot:3	Epoch:22	Loss:23.017	translation_Loss:14.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.645                                                   	MRR:19.46	Hits@10:35.28	Best:19.56
2024-12-27 17:42:29,776: Snapshot:3	Epoch:23	Loss:23.048	translation_Loss:14.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.679                                                   	MRR:19.39	Hits@10:35.26	Best:19.56
2024-12-27 17:42:42,323: Snapshot:3	Epoch:24	Loss:23.017	translation_Loss:14.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.676                                                   	MRR:19.42	Hits@10:35.1	Best:19.56
2024-12-27 17:42:54,815: Early Stopping! Snapshot: 3 Epoch: 25 Best Results: 19.56
2024-12-27 17:42:54,816: Start to training tokens! Snapshot: 3 Epoch: 25 Loss:22.993 MRR:19.38 Best Results: 19.56
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:42:54,816: Snapshot:3	Epoch:25	Loss:22.993	translation_Loss:14.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.669                                                   	MRR:19.38	Hits@10:35.28	Best:19.56
2024-12-27 17:43:07,117: Snapshot:3	Epoch:26	Loss:63.157	translation_Loss:63.12	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.38	Hits@10:35.28	Best:19.56
2024-12-27 17:43:19,405: End of token training: 3 Epoch: 27 Loss:63.057 MRR:19.38 Best Results: 19.56
2024-12-27 17:43:19,406: Snapshot:3	Epoch:27	Loss:63.057	translation_Loss:63.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.38	Hits@10:35.28	Best:19.56
2024-12-27 17:43:19,728: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_1000/3model_best.tar'
2024-12-27 17:43:33,795: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2455 | 0.1407 | 0.299  | 0.3639 |  0.4416 |
|     1      | 0.271  | 0.1704 | 0.3133 | 0.3793 |  0.4684 |
|     2      | 0.2028 | 0.1218 | 0.2308 | 0.2855 |  0.3633 |
|     3      | 0.1939 | 0.1105 | 0.2285 | 0.2828 |  0.3526 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:43:52,442: Snapshot:4	Epoch:0	Loss:15.733	translation_Loss:11.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.278                                                   	MRR:19.59	Hits@10:31.46	Best:19.59
2024-12-27 17:43:57,588: Snapshot:4	Epoch:1	Loss:12.749	translation_Loss:8.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.108                                                   	MRR:19.67	Hits@10:31.32	Best:19.67
2024-12-27 17:44:02,661: Snapshot:4	Epoch:2	Loss:12.705	translation_Loss:8.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.121                                                   	MRR:19.59	Hits@10:31.35	Best:19.67
2024-12-27 17:44:07,832: Snapshot:4	Epoch:3	Loss:12.688	translation_Loss:8.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.151                                                   	MRR:19.43	Hits@10:31.42	Best:19.67
2024-12-27 17:44:12,983: Snapshot:4	Epoch:4	Loss:12.71	translation_Loss:8.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.174                                                   	MRR:19.62	Hits@10:31.34	Best:19.67
2024-12-27 17:44:18,149: Snapshot:4	Epoch:5	Loss:12.673	translation_Loss:8.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.171                                                   	MRR:19.66	Hits@10:31.38	Best:19.67
2024-12-27 17:44:23,209: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 19.67
2024-12-27 17:44:23,209: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:12.702 MRR:19.65 Best Results: 19.67
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:44:23,210: Snapshot:4	Epoch:6	Loss:12.702	translation_Loss:8.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.164                                                   	MRR:19.65	Hits@10:31.65	Best:19.67
2024-12-27 17:44:28,197: Snapshot:4	Epoch:7	Loss:30.971	translation_Loss:30.934	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.65	Hits@10:31.65	Best:19.67
2024-12-27 17:44:33,215: End of token training: 4 Epoch: 8 Loss:30.917 MRR:19.65 Best Results: 19.67
2024-12-27 17:44:33,215: Snapshot:4	Epoch:8	Loss:30.917	translation_Loss:30.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.65	Hits@10:31.65	Best:19.67
2024-12-27 17:44:33,422: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_1000/4model_best.tar'
2024-12-27 17:44:49,791: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2283 | 0.1295 | 0.2737 | 0.3361 |  0.4134 |
|     1      | 0.2694 | 0.1717 | 0.3105 | 0.3713 |  0.4635 |
|     2      | 0.1993 | 0.1199 | 0.2267 | 0.2794 |  0.3553 |
|     3      | 0.1905 | 0.1067 | 0.2253 | 0.2794 |  0.3488 |
|     4      | 0.1952 | 0.1346 | 0.2162 | 0.2581 |  0.3128 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 17:44:49,793: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2528 |  0.15  | 0.3047 | 0.3685 |  0.444  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2526 | 0.1509 | 0.3031 | 0.3662 |  0.4429 |
|     1      | 0.2871 | 0.1855 | 0.3346 | 0.3984 |  0.485  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2433 | 0.1392 | 0.2942 | 0.3587 |  0.4415 |
|     1      | 0.2749 | 0.1768 | 0.3145 | 0.3813 |  0.4704 |
|     2      | 0.2028 | 0.1226 |  0.23  | 0.2834 |  0.3606 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2455 | 0.1407 | 0.299  | 0.3639 |  0.4416 |
|     1      | 0.271  | 0.1704 | 0.3133 | 0.3793 |  0.4684 |
|     2      | 0.2028 | 0.1218 | 0.2308 | 0.2855 |  0.3633 |
|     3      | 0.1939 | 0.1105 | 0.2285 | 0.2828 |  0.3526 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2283 | 0.1295 | 0.2737 | 0.3361 |  0.4134 |
|     1      | 0.2694 | 0.1717 | 0.3105 | 0.3713 |  0.4635 |
|     2      | 0.1993 | 0.1199 | 0.2267 | 0.2794 |  0.3553 |
|     3      | 0.1905 | 0.1067 | 0.2253 | 0.2794 |  0.3488 |
|     4      | 0.1952 | 0.1346 | 0.2162 | 0.2581 |  0.3128 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 17:44:49,794: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 71.14638566970825  |   0.253   |     0.15     |    0.305     |     0.444     |
|    1     | 49.71114754676819  |   0.262   |     0.16     |    0.311     |     0.454     |
|    2     | 267.4196982383728  |   0.226   |    0.135     |    0.263     |     0.402     |
|    3     | 375.40200686454773 |   0.214   |    0.125     |    0.251     |     0.384     |
|    4     | 56.83386588096619  |   0.206   |    0.123     |    0.239     |     0.366     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 17:44:49,794: Sum_Training_Time:820.5131042003632
2024-12-27 17:44:49,794: Every_Training_Time:[71.14638566970825, 49.71114754676819, 267.4196982383728, 375.40200686454773, 56.83386588096619]
2024-12-27 17:44:49,794: Forward transfer: 0.0383 Backward transfer: -0.012275000000000015
2024-12-27 17:45:23,474: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227174453/HYBRIDHYBRID_0.01_1024_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_1024_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_1024_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 17:45:33,144: Snapshot:0	Epoch:0	Loss:35.025	translation_Loss:35.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.86	Hits@10:36.8	Best:18.86
2024-12-27 17:45:39,243: Snapshot:0	Epoch:1	Loss:15.112	translation_Loss:15.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.03	Hits@10:43.72	Best:24.03
2024-12-27 17:45:45,230: Snapshot:0	Epoch:2	Loss:6.993	translation_Loss:6.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.08	Best:25.33
2024-12-27 17:45:51,261: Snapshot:0	Epoch:3	Loss:3.635	translation_Loss:3.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.17	Best:25.33
2024-12-27 17:45:57,257: Snapshot:0	Epoch:4	Loss:2.297	translation_Loss:2.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.29	Hits@10:45.14	Best:25.33
2024-12-27 17:46:03,256: Snapshot:0	Epoch:5	Loss:1.756	translation_Loss:1.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:44.69	Best:25.33
2024-12-27 17:46:09,699: Snapshot:0	Epoch:6	Loss:1.48	translation_Loss:1.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.7	Hits@10:44.35	Best:25.33
2024-12-27 17:46:15,681: Early Stopping! Snapshot: 0 Epoch: 7 Best Results: 25.33
2024-12-27 17:46:15,681: Start to training tokens! Snapshot: 0 Epoch: 7 Loss:1.295 MRR:24.63 Best Results: 25.33
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:46:15,682: Snapshot:0	Epoch:7	Loss:1.295	translation_Loss:1.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:44.23	Best:25.33
2024-12-27 17:46:22,332: Snapshot:0	Epoch:8	Loss:34.486	translation_Loss:34.45	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:44.23	Best:25.33
2024-12-27 17:46:28,404: End of token training: 0 Epoch: 9 Loss:34.465 MRR:24.63 Best Results: 25.33
2024-12-27 17:46:28,405: Snapshot:0	Epoch:9	Loss:34.465	translation_Loss:34.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.63	Hits@10:44.23	Best:25.33
2024-12-27 17:46:28,713: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_5000/0model_best.tar'
2024-12-27 17:46:31,213: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2554 | 0.152  | 0.3092 | 0.3709 |  0.446  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:46:42,276: Snapshot:1	Epoch:0	Loss:13.059	translation_Loss:7.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.271                                                   	MRR:24.65	Hits@10:43.57	Best:24.65
2024-12-27 17:46:44,648: Snapshot:1	Epoch:1	Loss:4.893	translation_Loss:3.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.272                                                   	MRR:27.73	Hits@10:46.79	Best:27.73
2024-12-27 17:46:47,076: Snapshot:1	Epoch:2	Loss:3.683	translation_Loss:2.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.875                                                   	MRR:28.26	Hits@10:47.22	Best:28.26
2024-12-27 17:46:49,434: Snapshot:1	Epoch:3	Loss:3.386	translation_Loss:2.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.815                                                   	MRR:28.62	Hits@10:48.01	Best:28.62
2024-12-27 17:46:51,760: Snapshot:1	Epoch:4	Loss:3.278	translation_Loss:2.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.813                                                   	MRR:28.4	Hits@10:47.92	Best:28.62
2024-12-27 17:46:54,095: Snapshot:1	Epoch:5	Loss:3.196	translation_Loss:2.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.81                                                   	MRR:28.5	Hits@10:48.17	Best:28.62
2024-12-27 17:46:56,443: Snapshot:1	Epoch:6	Loss:3.156	translation_Loss:2.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.806                                                   	MRR:28.34	Hits@10:47.88	Best:28.62
2024-12-27 17:46:58,797: Snapshot:1	Epoch:7	Loss:3.124	translation_Loss:2.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.811                                                   	MRR:28.45	Hits@10:48.14	Best:28.62
2024-12-27 17:47:01,179: Snapshot:1	Epoch:8	Loss:3.104	translation_Loss:2.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.807                                                   	MRR:28.66	Hits@10:48.25	Best:28.66
2024-12-27 17:47:03,511: Snapshot:1	Epoch:9	Loss:3.094	translation_Loss:2.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.814                                                   	MRR:28.4	Hits@10:48.36	Best:28.66
2024-12-27 17:47:06,171: Snapshot:1	Epoch:10	Loss:3.061	translation_Loss:2.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.811                                                   	MRR:28.45	Hits@10:48.43	Best:28.66
2024-12-27 17:47:08,509: Snapshot:1	Epoch:11	Loss:3.052	translation_Loss:2.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.811                                                   	MRR:28.56	Hits@10:48.12	Best:28.66
2024-12-27 17:47:10,875: Snapshot:1	Epoch:12	Loss:3.033	translation_Loss:2.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.812                                                   	MRR:28.76	Hits@10:48.4	Best:28.76
2024-12-27 17:47:13,233: Snapshot:1	Epoch:13	Loss:3.025	translation_Loss:2.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.813                                                   	MRR:28.83	Hits@10:48.3	Best:28.83
2024-12-27 17:47:15,553: Snapshot:1	Epoch:14	Loss:3.006	translation_Loss:2.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.812                                                   	MRR:28.49	Hits@10:48.55	Best:28.83
2024-12-27 17:47:17,924: Snapshot:1	Epoch:15	Loss:3.005	translation_Loss:2.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.812                                                   	MRR:28.55	Hits@10:48.72	Best:28.83
2024-12-27 17:47:20,273: Snapshot:1	Epoch:16	Loss:2.989	translation_Loss:2.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.812                                                   	MRR:28.46	Hits@10:48.34	Best:28.83
2024-12-27 17:47:22,645: Snapshot:1	Epoch:17	Loss:2.978	translation_Loss:2.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.81                                                   	MRR:28.95	Hits@10:48.6	Best:28.95
2024-12-27 17:47:24,977: Snapshot:1	Epoch:18	Loss:2.996	translation_Loss:2.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.824                                                   	MRR:28.67	Hits@10:48.68	Best:28.95
2024-12-27 17:47:27,291: Snapshot:1	Epoch:19	Loss:2.974	translation_Loss:2.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:28.83	Hits@10:48.56	Best:28.95
2024-12-27 17:47:29,617: Snapshot:1	Epoch:20	Loss:2.963	translation_Loss:2.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.81                                                   	MRR:28.92	Hits@10:49.2	Best:28.95
2024-12-27 17:47:31,964: Snapshot:1	Epoch:21	Loss:2.976	translation_Loss:2.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.819                                                   	MRR:29.27	Hits@10:49.15	Best:29.27
2024-12-27 17:47:34,306: Snapshot:1	Epoch:22	Loss:2.952	translation_Loss:2.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.816                                                   	MRR:28.71	Hits@10:48.74	Best:29.27
2024-12-27 17:47:36,621: Snapshot:1	Epoch:23	Loss:2.958	translation_Loss:2.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.818                                                   	MRR:28.75	Hits@10:49.1	Best:29.27
2024-12-27 17:47:38,978: Snapshot:1	Epoch:24	Loss:2.944	translation_Loss:2.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.817                                                   	MRR:28.81	Hits@10:48.98	Best:29.27
2024-12-27 17:47:41,324: Snapshot:1	Epoch:25	Loss:2.934	translation_Loss:2.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.813                                                   	MRR:28.78	Hits@10:48.79	Best:29.27
2024-12-27 17:47:43,656: Early Stopping! Snapshot: 1 Epoch: 26 Best Results: 29.27
2024-12-27 17:47:43,656: Start to training tokens! Snapshot: 1 Epoch: 26 Loss:2.934 MRR:29.07 Best Results: 29.27
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:47:43,656: Snapshot:1	Epoch:26	Loss:2.934	translation_Loss:2.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.814                                                   	MRR:29.07	Hits@10:49.03	Best:29.27
2024-12-27 17:47:45,971: Snapshot:1	Epoch:27	Loss:13.581	translation_Loss:13.545	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.07	Hits@10:49.03	Best:29.27
2024-12-27 17:47:48,248: End of token training: 1 Epoch: 28 Loss:13.547 MRR:29.07 Best Results: 29.27
2024-12-27 17:47:48,248: Snapshot:1	Epoch:28	Loss:13.547	translation_Loss:13.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.07	Hits@10:49.03	Best:29.27
2024-12-27 17:47:48,578: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_5000/1model_best.tar'
2024-12-27 17:47:52,609: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2546 | 0.1515 | 0.3078 | 0.3703 |  0.4445 |
|     1      | 0.2849 | 0.1823 | 0.3343 | 0.3986 |  0.4816 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:48:26,156: Snapshot:2	Epoch:0	Loss:36.838	translation_Loss:27.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.711                                                   	MRR:18.91	Hits@10:33.59	Best:18.91
2024-12-27 17:48:36,531: Snapshot:2	Epoch:1	Loss:22.577	translation_Loss:16.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.865                                                   	MRR:19.61	Hits@10:34.73	Best:19.61
2024-12-27 17:48:46,823: Snapshot:2	Epoch:2	Loss:20.945	translation_Loss:15.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.767                                                   	MRR:19.84	Hits@10:35.21	Best:19.84
2024-12-27 17:48:57,137: Snapshot:2	Epoch:3	Loss:20.22	translation_Loss:14.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.752                                                   	MRR:19.8	Hits@10:35.27	Best:19.84
2024-12-27 17:49:07,467: Snapshot:2	Epoch:4	Loss:19.812	translation_Loss:14.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.713                                                   	MRR:19.99	Hits@10:35.3	Best:19.99
2024-12-27 17:49:18,232: Snapshot:2	Epoch:5	Loss:19.552	translation_Loss:13.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.72                                                   	MRR:20.0	Hits@10:35.34	Best:20.0
2024-12-27 17:49:28,561: Snapshot:2	Epoch:6	Loss:19.359	translation_Loss:13.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.69                                                   	MRR:19.95	Hits@10:35.51	Best:20.0
2024-12-27 17:49:38,918: Snapshot:2	Epoch:7	Loss:19.191	translation_Loss:13.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.69                                                   	MRR:20.09	Hits@10:35.46	Best:20.09
2024-12-27 17:49:49,321: Snapshot:2	Epoch:8	Loss:19.091	translation_Loss:13.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.675                                                   	MRR:20.1	Hits@10:35.48	Best:20.1
2024-12-27 17:49:59,627: Snapshot:2	Epoch:9	Loss:19.025	translation_Loss:13.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.698                                                   	MRR:20.17	Hits@10:35.55	Best:20.17
2024-12-27 17:50:10,123: Snapshot:2	Epoch:10	Loss:18.902	translation_Loss:13.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.672                                                   	MRR:20.17	Hits@10:35.6	Best:20.17
2024-12-27 17:50:20,886: Snapshot:2	Epoch:11	Loss:18.902	translation_Loss:13.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.717                                                   	MRR:20.17	Hits@10:35.7	Best:20.17
2024-12-27 17:50:31,252: Snapshot:2	Epoch:12	Loss:18.781	translation_Loss:13.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.678                                                   	MRR:20.17	Hits@10:35.77	Best:20.17
2024-12-27 17:50:41,732: Snapshot:2	Epoch:13	Loss:18.729	translation_Loss:13.036	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.693                                                   	MRR:20.32	Hits@10:35.76	Best:20.32
2024-12-27 17:50:52,052: Snapshot:2	Epoch:14	Loss:18.691	translation_Loss:13.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.678                                                   	MRR:20.23	Hits@10:35.74	Best:20.32
2024-12-27 17:51:02,358: Snapshot:2	Epoch:15	Loss:18.702	translation_Loss:12.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.709                                                   	MRR:20.37	Hits@10:35.7	Best:20.37
2024-12-27 17:51:12,831: Snapshot:2	Epoch:16	Loss:18.587	translation_Loss:12.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.679                                                   	MRR:20.35	Hits@10:35.75	Best:20.37
2024-12-27 17:51:23,314: Snapshot:2	Epoch:17	Loss:18.592	translation_Loss:12.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.693                                                   	MRR:20.34	Hits@10:35.65	Best:20.37
2024-12-27 17:51:33,625: Snapshot:2	Epoch:18	Loss:18.56	translation_Loss:12.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.678                                                   	MRR:20.21	Hits@10:35.56	Best:20.37
2024-12-27 17:51:43,979: Snapshot:2	Epoch:19	Loss:18.519	translation_Loss:12.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.683                                                   	MRR:20.28	Hits@10:35.68	Best:20.37
2024-12-27 17:51:54,275: Early Stopping! Snapshot: 2 Epoch: 20 Best Results: 20.37
2024-12-27 17:51:54,275: Start to training tokens! Snapshot: 2 Epoch: 20 Loss:18.493 MRR:20.26 Best Results: 20.37
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:51:54,275: Snapshot:2	Epoch:20	Loss:18.493	translation_Loss:12.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.686                                                   	MRR:20.26	Hits@10:35.54	Best:20.37
2024-12-27 17:52:04,497: Snapshot:2	Epoch:21	Loss:59.12	translation_Loss:59.084	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.26	Hits@10:35.54	Best:20.37
2024-12-27 17:52:14,639: End of token training: 2 Epoch: 22 Loss:59.096 MRR:20.26 Best Results: 20.37
2024-12-27 17:52:14,639: Snapshot:2	Epoch:22	Loss:59.096	translation_Loss:59.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.26	Hits@10:35.54	Best:20.37
2024-12-27 17:52:14,964: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_5000/2model_best.tar'
2024-12-27 17:52:24,341: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2534 | 0.1501 | 0.3067 | 0.3693 |  0.4441 |
|     1      | 0.2817 | 0.1774 | 0.3305 | 0.3974 |  0.4801 |
|     2      | 0.2011 | 0.1226 | 0.2285 | 0.2825 |  0.3534 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:53:04,403: Snapshot:3	Epoch:0	Loss:40.766	translation_Loss:29.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.42                                                   	MRR:18.05	Hits@10:33.21	Best:18.05
2024-12-27 17:53:17,018: Snapshot:3	Epoch:1	Loss:28.875	translation_Loss:21.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.833                                                   	MRR:18.35	Hits@10:33.8	Best:18.35
2024-12-27 17:53:29,683: Snapshot:3	Epoch:2	Loss:27.88	translation_Loss:20.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.794                                                   	MRR:18.53	Hits@10:34.11	Best:18.53
2024-12-27 17:53:42,335: Snapshot:3	Epoch:3	Loss:27.392	translation_Loss:19.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.766                                                   	MRR:18.74	Hits@10:34.36	Best:18.74
2024-12-27 17:53:54,865: Snapshot:3	Epoch:4	Loss:27.066	translation_Loss:19.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.736                                                   	MRR:18.74	Hits@10:34.2	Best:18.74
2024-12-27 17:54:07,481: Snapshot:3	Epoch:5	Loss:26.874	translation_Loss:19.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.755                                                   	MRR:18.79	Hits@10:34.33	Best:18.79
2024-12-27 17:54:20,161: Snapshot:3	Epoch:6	Loss:26.633	translation_Loss:18.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.707                                                   	MRR:18.92	Hits@10:34.4	Best:18.92
2024-12-27 17:54:32,782: Snapshot:3	Epoch:7	Loss:26.56	translation_Loss:18.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.722                                                   	MRR:18.97	Hits@10:34.49	Best:18.97
2024-12-27 17:54:45,399: Snapshot:3	Epoch:8	Loss:26.506	translation_Loss:18.729	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.777                                                   	MRR:18.83	Hits@10:34.43	Best:18.97
2024-12-27 17:54:58,051: Snapshot:3	Epoch:9	Loss:26.429	translation_Loss:18.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.762                                                   	MRR:18.98	Hits@10:34.44	Best:18.98
2024-12-27 17:55:10,678: Snapshot:3	Epoch:10	Loss:26.312	translation_Loss:18.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.699                                                   	MRR:18.99	Hits@10:34.62	Best:18.99
2024-12-27 17:55:23,310: Snapshot:3	Epoch:11	Loss:26.286	translation_Loss:18.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.73                                                   	MRR:18.97	Hits@10:34.52	Best:18.99
2024-12-27 17:55:35,987: Snapshot:3	Epoch:12	Loss:26.203	translation_Loss:18.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.726                                                   	MRR:19.04	Hits@10:34.51	Best:19.04
2024-12-27 17:55:48,507: Snapshot:3	Epoch:13	Loss:26.252	translation_Loss:18.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.78                                                   	MRR:19.04	Hits@10:34.49	Best:19.04
2024-12-27 17:56:01,187: Snapshot:3	Epoch:14	Loss:26.092	translation_Loss:18.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.707                                                   	MRR:19.05	Hits@10:34.56	Best:19.05
2024-12-27 17:56:13,847: Snapshot:3	Epoch:15	Loss:26.173	translation_Loss:18.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.779                                                   	MRR:19.04	Hits@10:34.53	Best:19.05
2024-12-27 17:56:26,405: Snapshot:3	Epoch:16	Loss:26.138	translation_Loss:18.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.781                                                   	MRR:19.07	Hits@10:34.55	Best:19.07
2024-12-27 17:56:39,088: Snapshot:3	Epoch:17	Loss:26.142	translation_Loss:18.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.786                                                   	MRR:19.15	Hits@10:34.59	Best:19.15
2024-12-27 17:56:51,794: Snapshot:3	Epoch:18	Loss:25.999	translation_Loss:18.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.698                                                   	MRR:19.15	Hits@10:34.59	Best:19.15
2024-12-27 17:57:04,438: Snapshot:3	Epoch:19	Loss:26.061	translation_Loss:18.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.753                                                   	MRR:19.18	Hits@10:34.6	Best:19.18
2024-12-27 17:57:16,954: Snapshot:3	Epoch:20	Loss:25.995	translation_Loss:18.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.719                                                   	MRR:19.1	Hits@10:34.52	Best:19.18
2024-12-27 17:57:29,574: Snapshot:3	Epoch:21	Loss:25.999	translation_Loss:18.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.762                                                   	MRR:19.19	Hits@10:34.59	Best:19.19
2024-12-27 17:57:42,146: Snapshot:3	Epoch:22	Loss:25.966	translation_Loss:18.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.758                                                   	MRR:19.09	Hits@10:34.56	Best:19.19
2024-12-27 17:57:54,815: Snapshot:3	Epoch:23	Loss:25.928	translation_Loss:18.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.736                                                   	MRR:19.14	Hits@10:34.51	Best:19.19
2024-12-27 17:58:07,400: Snapshot:3	Epoch:24	Loss:25.967	translation_Loss:18.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.77                                                   	MRR:19.14	Hits@10:34.59	Best:19.19
2024-12-27 17:58:19,901: Snapshot:3	Epoch:25	Loss:25.909	translation_Loss:18.182	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.726                                                   	MRR:19.12	Hits@10:34.64	Best:19.19
2024-12-27 17:58:32,465: Early Stopping! Snapshot: 3 Epoch: 26 Best Results: 19.19
2024-12-27 17:58:32,466: Start to training tokens! Snapshot: 3 Epoch: 26 Loss:25.876 MRR:19.09 Best Results: 19.19
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:58:32,466: Snapshot:3	Epoch:26	Loss:25.876	translation_Loss:18.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.734                                                   	MRR:19.09	Hits@10:34.62	Best:19.19
2024-12-27 17:58:44,715: Snapshot:3	Epoch:27	Loss:65.903	translation_Loss:65.867	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.09	Hits@10:34.62	Best:19.19
2024-12-27 17:58:57,028: End of token training: 3 Epoch: 28 Loss:65.874 MRR:19.09 Best Results: 19.19
2024-12-27 17:58:57,028: Snapshot:3	Epoch:28	Loss:65.874	translation_Loss:65.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.09	Hits@10:34.62	Best:19.19
2024-12-27 17:58:57,363: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_5000/3model_best.tar'
2024-12-27 17:59:11,695: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1479 | 0.3065 | 0.3693 |  0.444  |
|     1      | 0.2813 | 0.1768 | 0.3316 | 0.3957 |  0.4818 |
|     2      | 0.1995 | 0.1194 | 0.2282 | 0.2814 |  0.354  |
|     3      | 0.1917 | 0.1095 | 0.2258 | 0.2796 |  0.3467 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:59:30,361: Snapshot:4	Epoch:0	Loss:19.263	translation_Loss:13.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.689                                                   	MRR:18.47	Hits@10:29.33	Best:18.47
2024-12-27 17:59:35,575: Snapshot:4	Epoch:1	Loss:14.858	translation_Loss:11.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.638                                                   	MRR:18.48	Hits@10:29.48	Best:18.48
2024-12-27 17:59:40,664: Snapshot:4	Epoch:2	Loss:14.786	translation_Loss:11.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.611                                                   	MRR:18.48	Hits@10:29.57	Best:18.48
2024-12-27 17:59:45,910: Snapshot:4	Epoch:3	Loss:14.775	translation_Loss:11.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.635                                                   	MRR:18.52	Hits@10:29.77	Best:18.52
2024-12-27 17:59:51,120: Snapshot:4	Epoch:4	Loss:14.742	translation_Loss:11.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.633                                                   	MRR:18.62	Hits@10:29.5	Best:18.62
2024-12-27 17:59:56,356: Snapshot:4	Epoch:5	Loss:14.753	translation_Loss:11.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.633                                                   	MRR:18.68	Hits@10:29.82	Best:18.68
2024-12-27 18:00:01,541: Snapshot:4	Epoch:6	Loss:14.732	translation_Loss:11.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.633                                                   	MRR:18.62	Hits@10:29.49	Best:18.68
2024-12-27 18:00:06,745: Snapshot:4	Epoch:7	Loss:14.761	translation_Loss:11.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.649                                                   	MRR:18.63	Hits@10:29.8	Best:18.68
2024-12-27 18:00:12,011: Snapshot:4	Epoch:8	Loss:14.753	translation_Loss:11.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.659                                                   	MRR:18.73	Hits@10:29.56	Best:18.73
2024-12-27 18:00:17,169: Snapshot:4	Epoch:9	Loss:14.739	translation_Loss:11.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.654                                                   	MRR:18.62	Hits@10:29.75	Best:18.73
2024-12-27 18:00:22,291: Snapshot:4	Epoch:10	Loss:14.767	translation_Loss:11.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.651                                                   	MRR:18.75	Hits@10:29.59	Best:18.75
2024-12-27 18:00:27,497: Snapshot:4	Epoch:11	Loss:14.72	translation_Loss:11.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.648                                                   	MRR:18.63	Hits@10:29.67	Best:18.75
2024-12-27 18:00:32,624: Snapshot:4	Epoch:12	Loss:14.75	translation_Loss:11.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.652                                                   	MRR:18.57	Hits@10:29.63	Best:18.75
2024-12-27 18:00:37,720: Snapshot:4	Epoch:13	Loss:14.747	translation_Loss:11.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.656                                                   	MRR:18.68	Hits@10:29.59	Best:18.75
2024-12-27 18:00:42,808: Snapshot:4	Epoch:14	Loss:14.763	translation_Loss:11.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.66                                                   	MRR:18.48	Hits@10:29.54	Best:18.75
2024-12-27 18:00:47,919: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 18.75
2024-12-27 18:00:47,920: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:14.739 MRR:18.63 Best Results: 18.75
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:00:47,920: Snapshot:4	Epoch:15	Loss:14.739	translation_Loss:11.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.672                                                   	MRR:18.63	Hits@10:29.52	Best:18.75
2024-12-27 18:00:53,368: Snapshot:4	Epoch:16	Loss:32.609	translation_Loss:32.571	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.63	Hits@10:29.52	Best:18.75
2024-12-27 18:00:58,425: End of token training: 4 Epoch: 17 Loss:32.572 MRR:18.63 Best Results: 18.75
2024-12-27 18:00:58,425: Snapshot:4	Epoch:17	Loss:32.572	translation_Loss:32.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.63	Hits@10:29.52	Best:18.75
2024-12-27 18:00:58,686: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_5000/4model_best.tar'
2024-12-27 18:01:15,138: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2428 | 0.1385 | 0.2931 | 0.3606 |  0.4407 |
|     1      | 0.2801 | 0.1762 | 0.329  | 0.3916 |  0.483  |
|     2      | 0.1976 | 0.1182 | 0.2253 | 0.2781 |  0.3516 |
|     3      | 0.1908 | 0.1085 | 0.2242 | 0.2791 |  0.3465 |
|     4      | 0.186  | 0.1288 | 0.2061 | 0.2432 |  0.2957 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:01:15,141: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2554 | 0.152  | 0.3092 | 0.3709 |  0.446  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2546 | 0.1515 | 0.3078 | 0.3703 |  0.4445 |
|     1      | 0.2849 | 0.1823 | 0.3343 | 0.3986 |  0.4816 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2534 | 0.1501 | 0.3067 | 0.3693 |  0.4441 |
|     1      | 0.2817 | 0.1774 | 0.3305 | 0.3974 |  0.4801 |
|     2      | 0.2011 | 0.1226 | 0.2285 | 0.2825 |  0.3534 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1479 | 0.3065 | 0.3693 |  0.444  |
|     1      | 0.2813 | 0.1768 | 0.3316 | 0.3957 |  0.4818 |
|     2      | 0.1995 | 0.1194 | 0.2282 | 0.2814 |  0.354  |
|     3      | 0.1917 | 0.1095 | 0.2258 | 0.2796 |  0.3467 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2428 | 0.1385 | 0.2931 | 0.3606 |  0.4407 |
|     1      | 0.2801 | 0.1762 | 0.329  | 0.3916 |  0.483  |
|     2      | 0.1976 | 0.1182 | 0.2253 | 0.2781 |  0.3516 |
|     3      | 0.1908 | 0.1085 | 0.2242 | 0.2791 |  0.3465 |
|     4      | 0.186  | 0.1288 | 0.2061 | 0.2432 |  0.2957 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:01:15,142: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  64.9302978515625  |   0.255   |    0.152     |    0.309     |     0.446     |
|    1     |  75.7864944934845  |   0.263   |     0.16     |    0.315     |     0.454     |
|    2     | 257.78167390823364 |   0.229   |    0.139     |    0.268     |     0.401     |
|    3     | 387.3792943954468  |   0.214   |    0.126     |    0.252     |      0.38     |
|    4     | 104.17649459838867 |   0.208   |    0.124     |    0.242     |     0.368     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:01:15,142: Sum_Training_Time:890.0542552471161
2024-12-27 18:01:15,142: Every_Training_Time:[64.9302978515625, 75.7864944934845, 257.78167390823364, 387.3792943954468, 104.17649459838867]
2024-12-27 18:01:15,142: Forward transfer: 0.039349999999999996 Backward transfer: -0.0054500000000000035
2024-12-27 18:01:48,600: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227180118/HYBRIDHYBRID_0.01_1024_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_1024_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_1024_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:01:58,197: Snapshot:0	Epoch:0	Loss:35.025	translation_Loss:35.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.86	Hits@10:36.86	Best:18.86
2024-12-27 18:02:04,177: Snapshot:0	Epoch:1	Loss:15.112	translation_Loss:15.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:43.74	Best:23.95
2024-12-27 18:02:10,224: Snapshot:0	Epoch:2	Loss:6.998	translation_Loss:6.998	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.25	Hits@10:44.88	Best:25.25
2024-12-27 18:02:16,224: Snapshot:0	Epoch:3	Loss:3.627	translation_Loss:3.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.3	Hits@10:45.23	Best:25.3
2024-12-27 18:02:22,206: Snapshot:0	Epoch:4	Loss:2.287	translation_Loss:2.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:44.89	Best:25.3
2024-12-27 18:02:28,140: Snapshot:0	Epoch:5	Loss:1.757	translation_Loss:1.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:44.68	Best:25.3
2024-12-27 18:02:34,548: Snapshot:0	Epoch:6	Loss:1.482	translation_Loss:1.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:44.25	Best:25.3
2024-12-27 18:02:40,581: Snapshot:0	Epoch:7	Loss:1.297	translation_Loss:1.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:44.26	Best:25.3
2024-12-27 18:02:46,596: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 25.3
2024-12-27 18:02:46,596: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:1.176 MRR:24.36 Best Results: 25.3
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:02:46,596: Snapshot:0	Epoch:8	Loss:1.176	translation_Loss:1.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:44.07	Best:25.3
2024-12-27 18:02:53,202: Snapshot:0	Epoch:9	Loss:34.533	translation_Loss:34.496	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:44.07	Best:25.3
2024-12-27 18:02:59,317: End of token training: 0 Epoch: 10 Loss:34.513 MRR:24.36 Best Results: 25.3
2024-12-27 18:02:59,317: Snapshot:0	Epoch:10	Loss:34.513	translation_Loss:34.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.36	Hits@10:44.07	Best:25.3
2024-12-27 18:02:59,623: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_10000/0model_best.tar'
2024-12-27 18:03:02,623: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2504 | 0.1459 | 0.3027 | 0.3667 |  0.4449 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:03:13,770: Snapshot:1	Epoch:0	Loss:16.682	translation_Loss:7.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.273                                                   	MRR:24.01	Hits@10:42.31	Best:24.01
2024-12-27 18:03:16,140: Snapshot:1	Epoch:1	Loss:4.421	translation_Loss:3.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.287                                                   	MRR:26.89	Hits@10:45.56	Best:26.89
2024-12-27 18:03:18,575: Snapshot:1	Epoch:2	Loss:3.015	translation_Loss:2.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.647                                                   	MRR:27.65	Hits@10:46.51	Best:27.65
2024-12-27 18:03:20,906: Snapshot:1	Epoch:3	Loss:2.733	translation_Loss:2.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.568                                                   	MRR:27.61	Hits@10:46.99	Best:27.65
2024-12-27 18:03:23,270: Snapshot:1	Epoch:4	Loss:2.64	translation_Loss:2.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.565                                                   	MRR:27.77	Hits@10:47.27	Best:27.77
2024-12-27 18:03:25,691: Snapshot:1	Epoch:5	Loss:2.582	translation_Loss:2.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.562                                                   	MRR:28.05	Hits@10:47.52	Best:28.05
2024-12-27 18:03:28,117: Snapshot:1	Epoch:6	Loss:2.549	translation_Loss:1.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.568                                                   	MRR:28.17	Hits@10:47.57	Best:28.17
2024-12-27 18:03:30,436: Snapshot:1	Epoch:7	Loss:2.533	translation_Loss:1.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.579                                                   	MRR:27.86	Hits@10:47.63	Best:28.17
2024-12-27 18:03:32,752: Snapshot:1	Epoch:8	Loss:2.517	translation_Loss:1.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.58                                                   	MRR:27.98	Hits@10:47.19	Best:28.17
2024-12-27 18:03:35,068: Snapshot:1	Epoch:9	Loss:2.498	translation_Loss:1.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.578                                                   	MRR:28.16	Hits@10:47.7	Best:28.17
2024-12-27 18:03:37,375: Snapshot:1	Epoch:10	Loss:2.508	translation_Loss:1.921	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.587                                                   	MRR:28.04	Hits@10:47.2	Best:28.17
2024-12-27 18:03:39,676: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 28.17
2024-12-27 18:03:39,676: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:2.478 MRR:28.03 Best Results: 28.17
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:03:39,677: Snapshot:1	Epoch:11	Loss:2.478	translation_Loss:1.896	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.582                                                   	MRR:28.03	Hits@10:47.7	Best:28.17
2024-12-27 18:03:41,944: Snapshot:1	Epoch:12	Loss:13.389	translation_Loss:13.353	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.03	Hits@10:47.7	Best:28.17
2024-12-27 18:03:44,205: End of token training: 1 Epoch: 13 Loss:13.374 MRR:28.03 Best Results: 28.17
2024-12-27 18:03:44,206: Snapshot:1	Epoch:13	Loss:13.374	translation_Loss:13.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.03	Hits@10:47.7	Best:28.17
2024-12-27 18:03:44,407: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_10000/1model_best.tar'
2024-12-27 18:03:48,468: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1469 | 0.303  | 0.3676 |  0.4451 |
|     1      | 0.2721 | 0.1746 | 0.3128 | 0.376  |  0.4624 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:04:22,549: Snapshot:2	Epoch:0	Loss:37.551	translation_Loss:25.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.51                                                   	MRR:19.13	Hits@10:33.92	Best:19.13
2024-12-27 18:04:32,833: Snapshot:2	Epoch:1	Loss:19.344	translation_Loss:14.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.659                                                   	MRR:19.57	Hits@10:34.75	Best:19.57
2024-12-27 18:04:43,103: Snapshot:2	Epoch:2	Loss:17.834	translation_Loss:13.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.657                                                   	MRR:19.7	Hits@10:34.75	Best:19.7
2024-12-27 18:04:53,328: Snapshot:2	Epoch:3	Loss:17.244	translation_Loss:12.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.649                                                   	MRR:19.77	Hits@10:34.94	Best:19.77
2024-12-27 18:05:03,624: Snapshot:2	Epoch:4	Loss:16.967	translation_Loss:12.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.706                                                   	MRR:19.77	Hits@10:35.11	Best:19.77
2024-12-27 18:05:13,870: Snapshot:2	Epoch:5	Loss:16.727	translation_Loss:12.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.673                                                   	MRR:19.82	Hits@10:35.28	Best:19.82
2024-12-27 18:05:24,196: Snapshot:2	Epoch:6	Loss:16.563	translation_Loss:11.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.68                                                   	MRR:19.91	Hits@10:35.34	Best:19.91
2024-12-27 18:05:34,463: Snapshot:2	Epoch:7	Loss:16.379	translation_Loss:11.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.653                                                   	MRR:19.73	Hits@10:35.25	Best:19.91
2024-12-27 18:05:44,682: Snapshot:2	Epoch:8	Loss:16.339	translation_Loss:11.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.69                                                   	MRR:19.82	Hits@10:35.17	Best:19.91
2024-12-27 18:05:54,911: Snapshot:2	Epoch:9	Loss:16.25	translation_Loss:11.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.677                                                   	MRR:19.95	Hits@10:35.25	Best:19.95
2024-12-27 18:06:05,179: Snapshot:2	Epoch:10	Loss:16.213	translation_Loss:11.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.686                                                   	MRR:19.88	Hits@10:35.26	Best:19.95
2024-12-27 18:06:15,338: Snapshot:2	Epoch:11	Loss:16.147	translation_Loss:11.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.695                                                   	MRR:19.82	Hits@10:35.28	Best:19.95
2024-12-27 18:06:25,578: Snapshot:2	Epoch:12	Loss:16.058	translation_Loss:11.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.702                                                   	MRR:19.87	Hits@10:35.28	Best:19.95
2024-12-27 18:06:35,816: Snapshot:2	Epoch:13	Loss:16.027	translation_Loss:11.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.695                                                   	MRR:19.92	Hits@10:35.3	Best:19.95
2024-12-27 18:06:45,952: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 19.95
2024-12-27 18:06:45,952: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:16.037 MRR:19.89 Best Results: 19.95
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:06:45,953: Snapshot:2	Epoch:14	Loss:16.037	translation_Loss:11.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.708                                                   	MRR:19.89	Hits@10:35.36	Best:19.95
2024-12-27 18:06:56,003: Snapshot:2	Epoch:15	Loss:58.216	translation_Loss:58.181	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.89	Hits@10:35.36	Best:19.95
2024-12-27 18:07:06,031: End of token training: 2 Epoch: 16 Loss:58.132 MRR:19.89 Best Results: 19.95
2024-12-27 18:07:06,031: Snapshot:2	Epoch:16	Loss:58.132	translation_Loss:58.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.89	Hits@10:35.36	Best:19.95
2024-12-27 18:07:06,318: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_10000/2model_best.tar'
2024-12-27 18:07:14,245: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1441 | 0.3022 | 0.3668 |  0.445  |
|     1      | 0.2719 | 0.1747 | 0.3116 | 0.3768 |  0.4626 |
|     2      |  0.2   | 0.1213 | 0.2251 | 0.2801 |  0.3525 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:07:53,813: Snapshot:3	Epoch:0	Loss:41.081	translation_Loss:27.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.012                                                   	MRR:18.07	Hits@10:33.21	Best:18.07
2024-12-27 18:08:06,289: Snapshot:3	Epoch:1	Loss:25.489	translation_Loss:18.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.66                                                   	MRR:18.48	Hits@10:33.99	Best:18.48
2024-12-27 18:08:18,714: Snapshot:3	Epoch:2	Loss:24.561	translation_Loss:17.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.658                                                   	MRR:18.58	Hits@10:34.19	Best:18.58
2024-12-27 18:08:31,228: Snapshot:3	Epoch:3	Loss:24.113	translation_Loss:17.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.675                                                   	MRR:18.84	Hits@10:34.44	Best:18.84
2024-12-27 18:08:43,751: Snapshot:3	Epoch:4	Loss:23.911	translation_Loss:17.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.71                                                   	MRR:18.86	Hits@10:34.51	Best:18.86
2024-12-27 18:08:56,179: Snapshot:3	Epoch:5	Loss:23.757	translation_Loss:17.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.733                                                   	MRR:18.9	Hits@10:34.45	Best:18.9
2024-12-27 18:09:08,818: Snapshot:3	Epoch:6	Loss:23.608	translation_Loss:16.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.705                                                   	MRR:19.08	Hits@10:34.7	Best:19.08
2024-12-27 18:09:21,336: Snapshot:3	Epoch:7	Loss:23.532	translation_Loss:16.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.732                                                   	MRR:19.07	Hits@10:34.76	Best:19.08
2024-12-27 18:09:33,769: Snapshot:3	Epoch:8	Loss:23.514	translation_Loss:16.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.778                                                   	MRR:19.05	Hits@10:34.62	Best:19.08
2024-12-27 18:09:46,185: Snapshot:3	Epoch:9	Loss:23.436	translation_Loss:16.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.757                                                   	MRR:19.17	Hits@10:34.76	Best:19.17
2024-12-27 18:09:58,653: Snapshot:3	Epoch:10	Loss:23.35	translation_Loss:16.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.758                                                   	MRR:19.22	Hits@10:34.78	Best:19.22
2024-12-27 18:10:11,300: Snapshot:3	Epoch:11	Loss:23.319	translation_Loss:16.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.771                                                   	MRR:19.16	Hits@10:34.79	Best:19.22
2024-12-27 18:10:23,804: Snapshot:3	Epoch:12	Loss:23.256	translation_Loss:16.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.789                                                   	MRR:19.23	Hits@10:34.7	Best:19.23
2024-12-27 18:10:36,350: Snapshot:3	Epoch:13	Loss:23.218	translation_Loss:16.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.776                                                   	MRR:19.25	Hits@10:34.8	Best:19.25
2024-12-27 18:10:49,040: Snapshot:3	Epoch:14	Loss:23.163	translation_Loss:16.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.771                                                   	MRR:19.26	Hits@10:34.77	Best:19.26
2024-12-27 18:11:01,908: Snapshot:3	Epoch:15	Loss:23.152	translation_Loss:16.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.783                                                   	MRR:19.23	Hits@10:34.79	Best:19.26
2024-12-27 18:11:14,376: Snapshot:3	Epoch:16	Loss:23.175	translation_Loss:16.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.825                                                   	MRR:19.25	Hits@10:34.67	Best:19.26
2024-12-27 18:11:27,052: Snapshot:3	Epoch:17	Loss:23.187	translation_Loss:16.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.84                                                   	MRR:19.32	Hits@10:34.82	Best:19.32
2024-12-27 18:11:39,655: Snapshot:3	Epoch:18	Loss:23.074	translation_Loss:16.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.798                                                   	MRR:19.27	Hits@10:34.84	Best:19.32
2024-12-27 18:11:52,164: Snapshot:3	Epoch:19	Loss:23.088	translation_Loss:16.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.795                                                   	MRR:19.33	Hits@10:34.83	Best:19.33
2024-12-27 18:12:05,144: Snapshot:3	Epoch:20	Loss:23.072	translation_Loss:16.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.808                                                   	MRR:19.23	Hits@10:34.73	Best:19.33
2024-12-27 18:12:17,725: Snapshot:3	Epoch:21	Loss:23.05	translation_Loss:16.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.806                                                   	MRR:19.28	Hits@10:34.71	Best:19.33
2024-12-27 18:12:30,379: Snapshot:3	Epoch:22	Loss:23.026	translation_Loss:16.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.802                                                   	MRR:19.27	Hits@10:34.93	Best:19.33
2024-12-27 18:12:42,809: Snapshot:3	Epoch:23	Loss:22.997	translation_Loss:16.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.77                                                   	MRR:19.22	Hits@10:34.77	Best:19.33
2024-12-27 18:12:55,334: Early Stopping! Snapshot: 3 Epoch: 24 Best Results: 19.33
2024-12-27 18:12:55,335: Start to training tokens! Snapshot: 3 Epoch: 24 Loss:23.027 MRR:19.28 Best Results: 19.33
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:12:55,335: Snapshot:3	Epoch:24	Loss:23.027	translation_Loss:16.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.836                                                   	MRR:19.28	Hits@10:34.8	Best:19.33
2024-12-27 18:13:07,639: Snapshot:3	Epoch:25	Loss:64.87	translation_Loss:64.833	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.28	Hits@10:34.8	Best:19.33
2024-12-27 18:13:19,790: End of token training: 3 Epoch: 26 Loss:64.766 MRR:19.28 Best Results: 19.33
2024-12-27 18:13:19,791: Snapshot:3	Epoch:26	Loss:64.766	translation_Loss:64.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.28	Hits@10:34.8	Best:19.33
2024-12-27 18:13:20,075: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_10000/3model_best.tar'
2024-12-27 18:13:34,102: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2485 | 0.1429 | 0.3031 | 0.3673 |  0.4447 |
|     1      | 0.2712 | 0.1743 | 0.3095 | 0.3749 |  0.4624 |
|     2      | 0.1997 | 0.1206 | 0.2249 | 0.2801 |  0.3538 |
|     3      | 0.1919 | 0.1108 | 0.2238 | 0.2789 |  0.3477 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:13:52,518: Snapshot:4	Epoch:0	Loss:20.486	translation_Loss:13.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.407                                                   	MRR:18.18	Hits@10:29.78	Best:18.18
2024-12-27 18:13:57,716: Snapshot:4	Epoch:1	Loss:13.767	translation_Loss:10.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.072                                                   	MRR:18.13	Hits@10:29.69	Best:18.18
2024-12-27 18:14:02,873: Snapshot:4	Epoch:2	Loss:13.727	translation_Loss:10.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.102                                                   	MRR:18.22	Hits@10:29.78	Best:18.22
2024-12-27 18:14:07,943: Snapshot:4	Epoch:3	Loss:13.759	translation_Loss:10.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.139                                                   	MRR:18.2	Hits@10:29.8	Best:18.22
2024-12-27 18:14:13,065: Snapshot:4	Epoch:4	Loss:13.75	translation_Loss:10.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.161                                                   	MRR:18.14	Hits@10:29.73	Best:18.22
2024-12-27 18:14:18,237: Snapshot:4	Epoch:5	Loss:13.774	translation_Loss:10.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.184                                                   	MRR:18.25	Hits@10:29.62	Best:18.25
2024-12-27 18:14:23,495: Snapshot:4	Epoch:6	Loss:13.786	translation_Loss:10.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.207                                                   	MRR:18.26	Hits@10:29.82	Best:18.26
2024-12-27 18:14:28,655: Snapshot:4	Epoch:7	Loss:13.802	translation_Loss:10.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.22                                                   	MRR:18.1	Hits@10:29.74	Best:18.26
2024-12-27 18:14:33,742: Snapshot:4	Epoch:8	Loss:13.77	translation_Loss:10.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.223                                                   	MRR:18.14	Hits@10:29.74	Best:18.26
2024-12-27 18:14:38,859: Snapshot:4	Epoch:9	Loss:13.819	translation_Loss:10.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.225                                                   	MRR:18.29	Hits@10:29.96	Best:18.29
2024-12-27 18:14:44,005: Snapshot:4	Epoch:10	Loss:13.806	translation_Loss:10.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.232                                                   	MRR:18.2	Hits@10:29.69	Best:18.29
2024-12-27 18:14:49,070: Snapshot:4	Epoch:11	Loss:13.786	translation_Loss:10.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.242                                                   	MRR:18.18	Hits@10:29.88	Best:18.29
2024-12-27 18:14:54,191: Snapshot:4	Epoch:12	Loss:13.807	translation_Loss:10.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.259                                                   	MRR:18.21	Hits@10:29.69	Best:18.29
2024-12-27 18:14:59,314: Snapshot:4	Epoch:13	Loss:13.813	translation_Loss:10.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.288                                                   	MRR:18.19	Hits@10:29.77	Best:18.29
2024-12-27 18:15:04,458: Early Stopping! Snapshot: 4 Epoch: 14 Best Results: 18.29
2024-12-27 18:15:04,459: Start to training tokens! Snapshot: 4 Epoch: 14 Loss:13.803 MRR:18.13 Best Results: 18.29
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:15:04,459: Snapshot:4	Epoch:14	Loss:13.803	translation_Loss:10.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.27                                                   	MRR:18.13	Hits@10:29.92	Best:18.29
2024-12-27 18:15:09,455: Snapshot:4	Epoch:15	Loss:32.633	translation_Loss:32.596	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.13	Hits@10:29.92	Best:18.29
2024-12-27 18:15:14,898: End of token training: 4 Epoch: 16 Loss:32.592 MRR:18.13 Best Results: 18.29
2024-12-27 18:15:14,898: Snapshot:4	Epoch:16	Loss:32.592	translation_Loss:32.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.13	Hits@10:29.92	Best:18.29
2024-12-27 18:15:15,142: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_1024_10000/4model_best.tar'
2024-12-27 18:15:31,408: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2429 | 0.1358 | 0.2967 | 0.3639 |  0.4445 |
|     1      | 0.2699 | 0.1722 | 0.3089 | 0.3725 |  0.463  |
|     2      | 0.1975 | 0.1179 | 0.2232 | 0.2793 |  0.3532 |
|     3      | 0.1913 | 0.1097 | 0.2228 | 0.2795 |  0.3495 |
|     4      | 0.1812 | 0.1237 | 0.1983 | 0.2439 |  0.2969 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:15:31,410: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2504 | 0.1459 | 0.3027 | 0.3667 |  0.4449 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1469 | 0.303  | 0.3676 |  0.4451 |
|     1      | 0.2721 | 0.1746 | 0.3128 | 0.376  |  0.4624 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1441 | 0.3022 | 0.3668 |  0.445  |
|     1      | 0.2719 | 0.1747 | 0.3116 | 0.3768 |  0.4626 |
|     2      |  0.2   | 0.1213 | 0.2251 | 0.2801 |  0.3525 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2485 | 0.1429 | 0.3031 | 0.3673 |  0.4447 |
|     1      | 0.2712 | 0.1743 | 0.3095 | 0.3749 |  0.4624 |
|     2      | 0.1997 | 0.1206 | 0.2249 | 0.2801 |  0.3538 |
|     3      | 0.1919 | 0.1108 | 0.2238 | 0.2789 |  0.3477 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2429 | 0.1358 | 0.2967 | 0.3639 |  0.4445 |
|     1      | 0.2699 | 0.1722 | 0.3089 | 0.3725 |  0.463  |
|     2      | 0.1975 | 0.1179 | 0.2232 | 0.2793 |  0.3532 |
|     3      | 0.1913 | 0.1097 | 0.2228 | 0.2795 |  0.3495 |
|     4      | 0.1812 | 0.1237 | 0.1983 | 0.2439 |  0.2969 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:15:31,411: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 70.71655321121216  |    0.25   |    0.146     |    0.303     |     0.445     |
|    1     | 40.05744791030884  |   0.257   |    0.154     |    0.306     |      0.45     |
|    2     | 193.34965324401855 |   0.226   |    0.136     |    0.263     |     0.398     |
|    3     | 360.2756698131561  |   0.213   |    0.126     |    0.248     |     0.379     |
|    4     | 98.25315737724304  |   0.207   |    0.123     |    0.239     |     0.369     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:15:31,411: Sum_Training_Time:762.6524815559387
2024-12-27 18:15:31,411: Every_Training_Time:[70.71655321121216, 40.05744791030884, 193.34965324401855, 360.2756698131561, 98.25315737724304]
2024-12-27 18:15:31,411: Forward transfer: 0.03845 Backward transfer: -0.0032000000000000084
2024-12-27 18:16:04,612: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227181535/HYBRIDHYBRID_0.01_2048_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_2048_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_2048_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:16:14,290: Snapshot:0	Epoch:0	Loss:18.537	translation_Loss:18.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.38	Hits@10:34.53	Best:17.38
2024-12-27 18:16:20,189: Snapshot:0	Epoch:1	Loss:8.452	translation_Loss:8.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.78	Hits@10:43.17	Best:23.78
2024-12-27 18:16:26,428: Snapshot:0	Epoch:2	Loss:3.951	translation_Loss:3.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:45.19	Best:25.31
2024-12-27 18:16:32,328: Snapshot:0	Epoch:3	Loss:2.078	translation_Loss:2.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.91	Hits@10:45.56	Best:25.91
2024-12-27 18:16:38,255: Snapshot:0	Epoch:4	Loss:1.311	translation_Loss:1.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:45.64	Best:25.91
2024-12-27 18:16:44,116: Snapshot:0	Epoch:5	Loss:0.963	translation_Loss:0.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:45.29	Best:25.91
2024-12-27 18:16:50,049: Snapshot:0	Epoch:6	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.42	Hits@10:45.11	Best:25.91
2024-12-27 18:16:56,411: Snapshot:0	Epoch:7	Loss:0.654	translation_Loss:0.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.27	Hits@10:44.83	Best:25.91
2024-12-27 18:17:02,292: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 25.91
2024-12-27 18:17:02,292: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:0.56 MRR:25.03 Best Results: 25.91
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:17:02,292: Snapshot:0	Epoch:8	Loss:0.56	translation_Loss:0.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.03	Hits@10:44.64	Best:25.91
2024-12-27 18:17:08,724: Snapshot:0	Epoch:9	Loss:17.414	translation_Loss:17.377	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.03	Hits@10:44.64	Best:25.91
2024-12-27 18:17:14,583: End of token training: 0 Epoch: 10 Loss:17.385 MRR:25.03 Best Results: 25.91
2024-12-27 18:17:14,583: Snapshot:0	Epoch:10	Loss:17.385	translation_Loss:17.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.03	Hits@10:44.64	Best:25.91
2024-12-27 18:17:14,860: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_1000/0model_best.tar'
2024-12-27 18:17:17,628: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2601 | 0.1561 | 0.3156 | 0.3777 |  0.4512 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:17:28,641: Snapshot:1	Epoch:0	Loss:5.524	translation_Loss:4.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.353                                                   	MRR:25.31	Hits@10:44.44	Best:25.31
2024-12-27 18:17:30,941: Snapshot:1	Epoch:1	Loss:2.543	translation_Loss:1.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.88                                                   	MRR:28.23	Hits@10:47.81	Best:28.23
2024-12-27 18:17:33,200: Snapshot:1	Epoch:2	Loss:1.726	translation_Loss:1.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.548                                                   	MRR:29.75	Hits@10:49.53	Best:29.75
2024-12-27 18:17:35,516: Snapshot:1	Epoch:3	Loss:1.448	translation_Loss:1.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:30.14	Hits@10:49.81	Best:30.14
2024-12-27 18:17:37,790: Snapshot:1	Epoch:4	Loss:1.334	translation_Loss:0.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:30.04	Hits@10:50.28	Best:30.14
2024-12-27 18:17:40,091: Snapshot:1	Epoch:5	Loss:1.277	translation_Loss:0.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.379                                                   	MRR:30.52	Hits@10:50.32	Best:30.52
2024-12-27 18:17:42,501: Snapshot:1	Epoch:6	Loss:1.256	translation_Loss:0.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.383                                                   	MRR:30.57	Hits@10:50.53	Best:30.57
2024-12-27 18:17:44,810: Snapshot:1	Epoch:7	Loss:1.237	translation_Loss:0.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.383                                                   	MRR:30.69	Hits@10:50.44	Best:30.69
2024-12-27 18:17:47,054: Snapshot:1	Epoch:8	Loss:1.227	translation_Loss:0.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.382                                                   	MRR:30.29	Hits@10:50.41	Best:30.69
2024-12-27 18:17:49,312: Snapshot:1	Epoch:9	Loss:1.222	translation_Loss:0.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.387                                                   	MRR:30.61	Hits@10:50.71	Best:30.69
2024-12-27 18:17:51,566: Snapshot:1	Epoch:10	Loss:1.219	translation_Loss:0.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:30.68	Hits@10:50.81	Best:30.69
2024-12-27 18:17:53,866: Snapshot:1	Epoch:11	Loss:1.208	translation_Loss:0.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.385                                                   	MRR:30.89	Hits@10:50.84	Best:30.89
2024-12-27 18:17:56,131: Snapshot:1	Epoch:12	Loss:1.203	translation_Loss:0.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.388                                                   	MRR:30.69	Hits@10:50.77	Best:30.89
2024-12-27 18:17:58,387: Snapshot:1	Epoch:13	Loss:1.202	translation_Loss:0.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.394                                                   	MRR:30.87	Hits@10:51.03	Best:30.89
2024-12-27 18:18:00,682: Snapshot:1	Epoch:14	Loss:1.189	translation_Loss:0.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.39                                                   	MRR:30.92	Hits@10:50.92	Best:30.92
2024-12-27 18:18:02,942: Snapshot:1	Epoch:15	Loss:1.195	translation_Loss:0.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:30.67	Hits@10:50.91	Best:30.92
2024-12-27 18:18:05,184: Snapshot:1	Epoch:16	Loss:1.189	translation_Loss:0.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:30.87	Hits@10:50.85	Best:30.92
2024-12-27 18:18:07,495: Snapshot:1	Epoch:17	Loss:1.181	translation_Loss:0.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.388                                                   	MRR:30.97	Hits@10:50.99	Best:30.97
2024-12-27 18:18:09,757: Snapshot:1	Epoch:18	Loss:1.18	translation_Loss:0.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.386                                                   	MRR:30.87	Hits@10:50.93	Best:30.97
2024-12-27 18:18:12,416: Snapshot:1	Epoch:19	Loss:1.186	translation_Loss:0.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.39                                                   	MRR:30.51	Hits@10:50.72	Best:30.97
2024-12-27 18:18:14,675: Snapshot:1	Epoch:20	Loss:1.175	translation_Loss:0.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.393                                                   	MRR:30.72	Hits@10:51.21	Best:30.97
2024-12-27 18:18:16,930: Snapshot:1	Epoch:21	Loss:1.175	translation_Loss:0.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.388                                                   	MRR:30.81	Hits@10:50.91	Best:30.97
2024-12-27 18:18:19,165: Early Stopping! Snapshot: 1 Epoch: 22 Best Results: 30.97
2024-12-27 18:18:19,166: Start to training tokens! Snapshot: 1 Epoch: 22 Loss:1.176 MRR:30.59 Best Results: 30.97
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:18:19,166: Snapshot:1	Epoch:22	Loss:1.176	translation_Loss:0.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.387                                                   	MRR:30.59	Hits@10:50.92	Best:30.97
2024-12-27 18:18:21,396: Snapshot:1	Epoch:23	Loss:6.9	translation_Loss:6.865	multi_layer_Loss:0.034	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.59	Hits@10:50.92	Best:30.97
2024-12-27 18:18:23,607: End of token training: 1 Epoch: 24 Loss:6.869 MRR:30.59 Best Results: 30.97
2024-12-27 18:18:23,607: Snapshot:1	Epoch:24	Loss:6.869	translation_Loss:6.867	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.59	Hits@10:50.92	Best:30.97
2024-12-27 18:18:23,890: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_1000/1model_best.tar'
2024-12-27 18:18:27,917: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1546 | 0.3133 | 0.376  |  0.4518 |
|     1      | 0.3031 | 0.2012 | 0.3473 | 0.4189 |  0.5022 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:19:00,716: Snapshot:2	Epoch:0	Loss:17.517	translation_Loss:13.426	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.09                                                   	MRR:19.65	Hits@10:35.01	Best:19.65
2024-12-27 18:19:10,772: Snapshot:2	Epoch:1	Loss:10.105	translation_Loss:7.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.947                                                   	MRR:20.45	Hits@10:36.16	Best:20.45
2024-12-27 18:19:20,842: Snapshot:2	Epoch:2	Loss:9.257	translation_Loss:6.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.882                                                   	MRR:20.35	Hits@10:36.38	Best:20.45
2024-12-27 18:19:30,842: Snapshot:2	Epoch:3	Loss:8.921	translation_Loss:6.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.857                                                   	MRR:20.44	Hits@10:36.37	Best:20.45
2024-12-27 18:19:40,712: Snapshot:2	Epoch:4	Loss:8.785	translation_Loss:5.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.856                                                   	MRR:20.34	Hits@10:36.34	Best:20.45
2024-12-27 18:19:50,723: Snapshot:2	Epoch:5	Loss:8.648	translation_Loss:5.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.853                                                   	MRR:20.47	Hits@10:36.64	Best:20.47
2024-12-27 18:20:01,034: Snapshot:2	Epoch:6	Loss:8.566	translation_Loss:5.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.832                                                   	MRR:20.61	Hits@10:36.57	Best:20.61
2024-12-27 18:20:11,152: Snapshot:2	Epoch:7	Loss:8.521	translation_Loss:5.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.838                                                   	MRR:20.67	Hits@10:36.78	Best:20.67
2024-12-27 18:20:21,045: Snapshot:2	Epoch:8	Loss:8.475	translation_Loss:5.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.843                                                   	MRR:20.6	Hits@10:36.66	Best:20.67
2024-12-27 18:20:31,246: Snapshot:2	Epoch:9	Loss:8.421	translation_Loss:5.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.834                                                   	MRR:20.53	Hits@10:36.73	Best:20.67
2024-12-27 18:20:41,208: Snapshot:2	Epoch:10	Loss:8.384	translation_Loss:5.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.829                                                   	MRR:20.66	Hits@10:36.64	Best:20.67
2024-12-27 18:20:51,296: Snapshot:2	Epoch:11	Loss:8.35	translation_Loss:5.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.831                                                   	MRR:20.73	Hits@10:36.66	Best:20.73
2024-12-27 18:21:01,569: Snapshot:2	Epoch:12	Loss:8.324	translation_Loss:5.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.82                                                   	MRR:20.66	Hits@10:36.71	Best:20.73
2024-12-27 18:21:11,467: Snapshot:2	Epoch:13	Loss:8.29	translation_Loss:5.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.816                                                   	MRR:20.66	Hits@10:36.55	Best:20.73
2024-12-27 18:21:21,457: Snapshot:2	Epoch:14	Loss:8.277	translation_Loss:5.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.82                                                   	MRR:20.75	Hits@10:36.77	Best:20.75
2024-12-27 18:21:31,347: Snapshot:2	Epoch:15	Loss:8.259	translation_Loss:5.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.829                                                   	MRR:20.69	Hits@10:36.67	Best:20.75
2024-12-27 18:21:41,701: Snapshot:2	Epoch:16	Loss:8.231	translation_Loss:5.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.814                                                   	MRR:20.56	Hits@10:36.66	Best:20.75
2024-12-27 18:21:51,564: Snapshot:2	Epoch:17	Loss:8.206	translation_Loss:5.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.811                                                   	MRR:20.72	Hits@10:36.7	Best:20.75
2024-12-27 18:22:01,456: Snapshot:2	Epoch:18	Loss:8.178	translation_Loss:5.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.808                                                   	MRR:20.75	Hits@10:36.75	Best:20.75
2024-12-27 18:22:11,819: Early Stopping! Snapshot: 2 Epoch: 19 Best Results: 20.75
2024-12-27 18:22:11,819: Start to training tokens! Snapshot: 2 Epoch: 19 Loss:8.176 MRR:20.61 Best Results: 20.75
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:22:11,820: Snapshot:2	Epoch:19	Loss:8.176	translation_Loss:5.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.806                                                   	MRR:20.61	Hits@10:36.72	Best:20.75
2024-12-27 18:22:21,544: Snapshot:2	Epoch:20	Loss:29.099	translation_Loss:29.063	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.61	Hits@10:36.72	Best:20.75
2024-12-27 18:22:31,420: End of token training: 2 Epoch: 21 Loss:29.103 MRR:20.61 Best Results: 20.75
2024-12-27 18:22:31,420: Snapshot:2	Epoch:21	Loss:29.103	translation_Loss:29.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.61	Hits@10:36.72	Best:20.75
2024-12-27 18:22:31,651: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_1000/2model_best.tar'
2024-12-27 18:22:39,984: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2517 | 0.1448 | 0.3061 | 0.3728 |  0.4499 |
|     1      | 0.2862 | 0.1813 | 0.3321 | 0.4023 |  0.4959 |
|     2      | 0.205  | 0.1234 | 0.2325 | 0.2883 |  0.3678 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:23:19,385: Snapshot:3	Epoch:0	Loss:19.061	translation_Loss:13.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.137                                                   	MRR:18.84	Hits@10:34.37	Best:18.84
2024-12-27 18:23:31,619: Snapshot:3	Epoch:1	Loss:12.89	translation_Loss:8.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.036                                                   	MRR:19.26	Hits@10:35.24	Best:19.26
2024-12-27 18:23:43,728: Snapshot:3	Epoch:2	Loss:12.316	translation_Loss:8.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.977                                                   	MRR:19.35	Hits@10:35.51	Best:19.35
2024-12-27 18:23:55,841: Snapshot:3	Epoch:3	Loss:12.121	translation_Loss:8.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.964                                                   	MRR:19.45	Hits@10:35.57	Best:19.45
2024-12-27 18:24:08,134: Snapshot:3	Epoch:4	Loss:11.96	translation_Loss:8.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.948                                                   	MRR:19.58	Hits@10:35.86	Best:19.58
2024-12-27 18:24:20,458: Snapshot:3	Epoch:5	Loss:11.874	translation_Loss:7.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.95                                                   	MRR:19.61	Hits@10:35.9	Best:19.61
2024-12-27 18:24:32,858: Snapshot:3	Epoch:6	Loss:11.769	translation_Loss:7.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.936                                                   	MRR:19.6	Hits@10:35.71	Best:19.61
2024-12-27 18:24:44,863: Snapshot:3	Epoch:7	Loss:11.709	translation_Loss:7.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.93                                                   	MRR:19.59	Hits@10:35.92	Best:19.61
2024-12-27 18:24:56,962: Snapshot:3	Epoch:8	Loss:11.677	translation_Loss:7.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.941                                                   	MRR:19.64	Hits@10:35.96	Best:19.64
2024-12-27 18:25:09,083: Snapshot:3	Epoch:9	Loss:11.671	translation_Loss:7.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.937                                                   	MRR:19.59	Hits@10:35.97	Best:19.64
2024-12-27 18:25:21,602: Snapshot:3	Epoch:10	Loss:11.62	translation_Loss:7.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.946                                                   	MRR:19.7	Hits@10:35.96	Best:19.7
2024-12-27 18:25:33,899: Snapshot:3	Epoch:11	Loss:11.595	translation_Loss:7.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.938                                                   	MRR:19.74	Hits@10:35.93	Best:19.74
2024-12-27 18:25:45,980: Snapshot:3	Epoch:12	Loss:11.569	translation_Loss:7.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.924                                                   	MRR:19.71	Hits@10:35.98	Best:19.74
2024-12-27 18:25:58,383: Snapshot:3	Epoch:13	Loss:11.551	translation_Loss:7.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.919                                                   	MRR:19.65	Hits@10:36.0	Best:19.74
2024-12-27 18:26:10,478: Snapshot:3	Epoch:14	Loss:11.512	translation_Loss:7.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.919                                                   	MRR:19.78	Hits@10:36.02	Best:19.78
2024-12-27 18:26:22,625: Snapshot:3	Epoch:15	Loss:11.508	translation_Loss:7.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.928                                                   	MRR:19.81	Hits@10:36.01	Best:19.81
2024-12-27 18:26:35,297: Snapshot:3	Epoch:16	Loss:11.486	translation_Loss:7.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.921                                                   	MRR:19.87	Hits@10:36.18	Best:19.87
2024-12-27 18:26:47,439: Snapshot:3	Epoch:17	Loss:11.484	translation_Loss:7.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.927                                                   	MRR:19.77	Hits@10:35.9	Best:19.87
2024-12-27 18:26:59,532: Snapshot:3	Epoch:18	Loss:11.456	translation_Loss:7.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.919                                                   	MRR:19.83	Hits@10:35.96	Best:19.87
2024-12-27 18:27:12,015: Snapshot:3	Epoch:19	Loss:11.482	translation_Loss:7.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.917                                                   	MRR:19.87	Hits@10:36.13	Best:19.87
2024-12-27 18:27:24,053: Snapshot:3	Epoch:20	Loss:11.447	translation_Loss:7.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.924                                                   	MRR:19.77	Hits@10:35.94	Best:19.87
2024-12-27 18:27:36,440: Early Stopping! Snapshot: 3 Epoch: 21 Best Results: 19.87
2024-12-27 18:27:36,441: Start to training tokens! Snapshot: 3 Epoch: 21 Loss:11.439 MRR:19.82 Best Results: 19.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:27:36,441: Snapshot:3	Epoch:21	Loss:11.439	translation_Loss:7.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.921                                                   	MRR:19.82	Hits@10:35.99	Best:19.87
2024-12-27 18:27:48,300: Snapshot:3	Epoch:22	Loss:31.706	translation_Loss:31.669	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.82	Hits@10:35.99	Best:19.87
2024-12-27 18:28:00,204: End of token training: 3 Epoch: 23 Loss:31.67 MRR:19.82 Best Results: 19.87
2024-12-27 18:28:00,205: Snapshot:3	Epoch:23	Loss:31.67	translation_Loss:31.67	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.82	Hits@10:35.99	Best:19.87
2024-12-27 18:28:00,481: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_1000/3model_best.tar'
2024-12-27 18:28:14,454: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.255  | 0.1488 | 0.3099 | 0.3743 |  0.4513 |
|     1      | 0.2827 | 0.1784 | 0.3278 | 0.3981 |  0.491  |
|     2      | 0.2037 | 0.1211 | 0.2314 | 0.2868 |  0.3676 |
|     3      | 0.1985 | 0.113  | 0.2342 | 0.289  |  0.3588 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:28:32,628: Snapshot:4	Epoch:0	Loss:8.68	translation_Loss:6.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.284                                                   	MRR:20.3	Hits@10:32.75	Best:20.3
2024-12-27 18:28:37,561: Snapshot:4	Epoch:1	Loss:6.595	translation_Loss:4.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.967                                                   	MRR:20.36	Hits@10:32.82	Best:20.36
2024-12-27 18:28:42,595: Snapshot:4	Epoch:2	Loss:6.285	translation_Loss:4.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.782                                                   	MRR:20.47	Hits@10:32.76	Best:20.47
2024-12-27 18:28:47,606: Snapshot:4	Epoch:3	Loss:6.229	translation_Loss:4.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.734                                                   	MRR:20.12	Hits@10:32.62	Best:20.47
2024-12-27 18:28:52,631: Snapshot:4	Epoch:4	Loss:6.211	translation_Loss:4.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.722                                                   	MRR:20.64	Hits@10:32.99	Best:20.64
2024-12-27 18:28:57,671: Snapshot:4	Epoch:5	Loss:6.206	translation_Loss:4.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.717                                                   	MRR:20.23	Hits@10:32.63	Best:20.64
2024-12-27 18:29:02,978: Snapshot:4	Epoch:6	Loss:6.2	translation_Loss:4.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:20.45	Hits@10:33.23	Best:20.64
2024-12-27 18:29:07,949: Snapshot:4	Epoch:7	Loss:6.188	translation_Loss:4.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.708                                                   	MRR:20.3	Hits@10:32.87	Best:20.64
2024-12-27 18:29:12,836: Snapshot:4	Epoch:8	Loss:6.199	translation_Loss:4.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.71                                                   	MRR:20.29	Hits@10:32.74	Best:20.64
2024-12-27 18:29:17,812: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 20.64
2024-12-27 18:29:17,812: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:6.209 MRR:20.35 Best Results: 20.64
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:29:17,812: Snapshot:4	Epoch:9	Loss:6.209	translation_Loss:4.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.715                                                   	MRR:20.35	Hits@10:32.98	Best:20.64
2024-12-27 18:29:22,614: Snapshot:4	Epoch:10	Loss:14.957	translation_Loss:14.92	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.35	Hits@10:32.98	Best:20.64
2024-12-27 18:29:27,493: End of token training: 4 Epoch: 11 Loss:14.922 MRR:20.35 Best Results: 20.64
2024-12-27 18:29:27,494: Snapshot:4	Epoch:11	Loss:14.922	translation_Loss:14.921	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.35	Hits@10:32.98	Best:20.64
2024-12-27 18:29:27,732: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_1000/4model_best.tar'
2024-12-27 18:29:43,991: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2341 | 0.1357 | 0.2795 | 0.3406 |  0.4164 |
|     1      | 0.2822 | 0.1812 | 0.3253 | 0.3908 |  0.4836 |
|     2      | 0.1997 | 0.1202 | 0.2244 | 0.278  |  0.3568 |
|     3      | 0.1934 | 0.1078 | 0.228  | 0.2848 |  0.3551 |
|     4      | 0.2065 | 0.1435 | 0.2286 | 0.2755 |  0.3308 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:29:43,993: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2601 | 0.1561 | 0.3156 | 0.3777 |  0.4512 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1546 | 0.3133 | 0.376  |  0.4518 |
|     1      | 0.3031 | 0.2012 | 0.3473 | 0.4189 |  0.5022 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2517 | 0.1448 | 0.3061 | 0.3728 |  0.4499 |
|     1      | 0.2862 | 0.1813 | 0.3321 | 0.4023 |  0.4959 |
|     2      | 0.205  | 0.1234 | 0.2325 | 0.2883 |  0.3678 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.255  | 0.1488 | 0.3099 | 0.3743 |  0.4513 |
|     1      | 0.2827 | 0.1784 | 0.3278 | 0.3981 |  0.491  |
|     2      | 0.2037 | 0.1211 | 0.2314 | 0.2868 |  0.3676 |
|     3      | 0.1985 | 0.113  | 0.2342 | 0.289  |  0.3588 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2341 | 0.1357 | 0.2795 | 0.3406 |  0.4164 |
|     1      | 0.2822 | 0.1812 | 0.3253 | 0.3908 |  0.4836 |
|     2      | 0.1997 | 0.1202 | 0.2244 | 0.278  |  0.3568 |
|     3      | 0.1934 | 0.1078 | 0.228  | 0.2848 |  0.3551 |
|     4      | 0.2065 | 0.1435 | 0.2286 | 0.2755 |  0.3308 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:29:43,994: Report Result:
+----------+-------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time       | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+-------------------+-----------+--------------+--------------+---------------+
|    0     | 69.97008275985718 |    0.26   |    0.156     |    0.316     |     0.451     |
|    1     | 64.71982145309448 |   0.271   |    0.167     |    0.322     |     0.465     |
|    2     | 239.2516484260559 |   0.231   |    0.138     |     0.27     |     0.412     |
|    3     | 314.7505712509155 |   0.219   |    0.128     |    0.257     |     0.392     |
|    4     | 70.30026721954346 |    0.21   |    0.126     |    0.243     |     0.372     |
+----------+-------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:29:43,994: Sum_Training_Time:758.9923911094666
2024-12-27 18:29:43,994: Every_Training_Time:[69.97008275985718, 64.71982145309448, 239.2516484260559, 314.7505712509155, 70.30026721954346]
2024-12-27 18:29:43,994: Forward transfer: 0.0402 Backward transfer: -0.014324999999999997
2024-12-27 18:30:17,141: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227182947/HYBRIDHYBRID_0.01_2048_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_2048_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_2048_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:30:26,824: Snapshot:0	Epoch:0	Loss:18.538	translation_Loss:18.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.38	Hits@10:34.51	Best:17.38
2024-12-27 18:30:32,873: Snapshot:0	Epoch:1	Loss:8.452	translation_Loss:8.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.73	Hits@10:43.1	Best:23.73
2024-12-27 18:30:39,258: Snapshot:0	Epoch:2	Loss:3.95	translation_Loss:3.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.4	Hits@10:45.16	Best:25.4
2024-12-27 18:30:45,320: Snapshot:0	Epoch:3	Loss:2.076	translation_Loss:2.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.98	Hits@10:45.66	Best:25.98
2024-12-27 18:30:51,327: Snapshot:0	Epoch:4	Loss:1.311	translation_Loss:1.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:45.68	Best:25.98
2024-12-27 18:30:57,276: Snapshot:0	Epoch:5	Loss:0.963	translation_Loss:0.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:45.48	Best:25.98
2024-12-27 18:31:03,288: Snapshot:0	Epoch:6	Loss:0.771	translation_Loss:0.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:45.08	Best:25.98
2024-12-27 18:31:09,757: Snapshot:0	Epoch:7	Loss:0.652	translation_Loss:0.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.23	Hits@10:44.93	Best:25.98
2024-12-27 18:31:15,787: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 25.98
2024-12-27 18:31:15,787: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:0.558 MRR:25.04 Best Results: 25.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:31:15,787: Snapshot:0	Epoch:8	Loss:0.558	translation_Loss:0.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:44.83	Best:25.98
2024-12-27 18:31:22,438: Snapshot:0	Epoch:9	Loss:17.421	translation_Loss:17.384	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:44.83	Best:25.98
2024-12-27 18:31:28,381: End of token training: 0 Epoch: 10 Loss:17.393 MRR:25.04 Best Results: 25.98
2024-12-27 18:31:28,382: Snapshot:0	Epoch:10	Loss:17.393	translation_Loss:17.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.04	Hits@10:44.83	Best:25.98
2024-12-27 18:31:28,662: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_5000/0model_best.tar'
2024-12-27 18:31:31,427: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2608 | 0.1575 | 0.3144 | 0.3782 |  0.4519 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:31:42,591: Snapshot:1	Epoch:0	Loss:8.445	translation_Loss:4.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.136                                                   	MRR:23.75	Hits@10:41.9	Best:23.75
2024-12-27 18:31:44,914: Snapshot:1	Epoch:1	Loss:3.207	translation_Loss:1.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.309                                                   	MRR:27.05	Hits@10:45.84	Best:27.05
2024-12-27 18:31:47,262: Snapshot:1	Epoch:2	Loss:2.12	translation_Loss:1.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.689                                                   	MRR:28.52	Hits@10:48.11	Best:28.52
2024-12-27 18:31:49,586: Snapshot:1	Epoch:3	Loss:1.739	translation_Loss:1.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.47                                                   	MRR:28.92	Hits@10:48.51	Best:28.92
2024-12-27 18:31:51,938: Snapshot:1	Epoch:4	Loss:1.587	translation_Loss:1.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:29.14	Hits@10:48.73	Best:29.14
2024-12-27 18:31:54,272: Snapshot:1	Epoch:5	Loss:1.522	translation_Loss:1.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.375                                                   	MRR:29.21	Hits@10:48.74	Best:29.21
2024-12-27 18:31:56,622: Snapshot:1	Epoch:6	Loss:1.478	translation_Loss:1.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.365                                                   	MRR:29.41	Hits@10:48.92	Best:29.41
2024-12-27 18:31:58,919: Snapshot:1	Epoch:7	Loss:1.453	translation_Loss:1.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.358                                                   	MRR:29.29	Hits@10:48.86	Best:29.41
2024-12-27 18:32:01,220: Snapshot:1	Epoch:8	Loss:1.439	translation_Loss:1.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.353                                                   	MRR:29.33	Hits@10:48.92	Best:29.41
2024-12-27 18:32:03,562: Snapshot:1	Epoch:9	Loss:1.429	translation_Loss:1.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:29.55	Hits@10:49.15	Best:29.55
2024-12-27 18:32:05,931: Snapshot:1	Epoch:10	Loss:1.426	translation_Loss:1.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:29.74	Hits@10:49.33	Best:29.74
2024-12-27 18:32:08,269: Snapshot:1	Epoch:11	Loss:1.41	translation_Loss:1.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:29.88	Hits@10:49.6	Best:29.88
2024-12-27 18:32:10,574: Snapshot:1	Epoch:12	Loss:1.406	translation_Loss:1.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.354                                                   	MRR:29.86	Hits@10:49.27	Best:29.88
2024-12-27 18:32:12,868: Snapshot:1	Epoch:13	Loss:1.399	translation_Loss:1.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:29.82	Hits@10:49.39	Best:29.88
2024-12-27 18:32:15,192: Snapshot:1	Epoch:14	Loss:1.382	translation_Loss:1.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.351                                                   	MRR:29.69	Hits@10:49.44	Best:29.88
2024-12-27 18:32:17,469: Snapshot:1	Epoch:15	Loss:1.391	translation_Loss:1.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.349                                                   	MRR:29.57	Hits@10:49.45	Best:29.88
2024-12-27 18:32:19,772: Early Stopping! Snapshot: 1 Epoch: 16 Best Results: 29.88
2024-12-27 18:32:19,772: Start to training tokens! Snapshot: 1 Epoch: 16 Loss:1.384 MRR:29.58 Best Results: 29.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:32:19,773: Snapshot:1	Epoch:16	Loss:1.384	translation_Loss:1.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.351                                                   	MRR:29.58	Hits@10:49.66	Best:29.88
2024-12-27 18:32:22,036: Snapshot:1	Epoch:17	Loss:7.074	translation_Loss:7.039	multi_layer_Loss:0.034	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.58	Hits@10:49.66	Best:29.88
2024-12-27 18:32:24,267: End of token training: 1 Epoch: 18 Loss:7.038 MRR:29.58 Best Results: 29.88
2024-12-27 18:32:24,267: Snapshot:1	Epoch:18	Loss:7.038	translation_Loss:7.036	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.58	Hits@10:49.66	Best:29.88
2024-12-27 18:32:24,574: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_5000/1model_best.tar'
2024-12-27 18:32:28,615: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2607 | 0.1572 | 0.3152 | 0.378  |  0.4506 |
|     1      | 0.2911 | 0.189  | 0.3399 | 0.4059 |  0.4892 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:33:02,188: Snapshot:2	Epoch:0	Loss:20.905	translation_Loss:14.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.842                                                   	MRR:19.42	Hits@10:34.34	Best:19.42
2024-12-27 18:33:12,242: Snapshot:2	Epoch:1	Loss:10.113	translation_Loss:7.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.318                                                   	MRR:20.44	Hits@10:35.53	Best:20.44
2024-12-27 18:33:22,254: Snapshot:2	Epoch:2	Loss:9.286	translation_Loss:7.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.285                                                   	MRR:20.36	Hits@10:35.45	Best:20.44
2024-12-27 18:33:32,329: Snapshot:2	Epoch:3	Loss:8.941	translation_Loss:6.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.275                                                   	MRR:20.37	Hits@10:35.91	Best:20.44
2024-12-27 18:33:42,503: Snapshot:2	Epoch:4	Loss:8.792	translation_Loss:6.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.28                                                   	MRR:20.33	Hits@10:35.81	Best:20.44
2024-12-27 18:33:52,537: Snapshot:2	Epoch:5	Loss:8.703	translation_Loss:6.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.286                                                   	MRR:20.42	Hits@10:35.8	Best:20.44
2024-12-27 18:34:02,979: Snapshot:2	Epoch:6	Loss:8.628	translation_Loss:6.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.291                                                   	MRR:20.45	Hits@10:35.74	Best:20.45
2024-12-27 18:34:13,160: Snapshot:2	Epoch:7	Loss:8.567	translation_Loss:6.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.285                                                   	MRR:20.33	Hits@10:35.88	Best:20.45
2024-12-27 18:34:23,204: Snapshot:2	Epoch:8	Loss:8.503	translation_Loss:6.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.282                                                   	MRR:20.52	Hits@10:35.96	Best:20.52
2024-12-27 18:34:33,705: Snapshot:2	Epoch:9	Loss:8.458	translation_Loss:6.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.283                                                   	MRR:20.49	Hits@10:35.85	Best:20.52
2024-12-27 18:34:43,679: Snapshot:2	Epoch:10	Loss:8.424	translation_Loss:6.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.287                                                   	MRR:20.48	Hits@10:36.05	Best:20.52
2024-12-27 18:34:53,711: Snapshot:2	Epoch:11	Loss:8.39	translation_Loss:6.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.288                                                   	MRR:20.41	Hits@10:35.93	Best:20.52
2024-12-27 18:35:03,801: Snapshot:2	Epoch:12	Loss:8.378	translation_Loss:6.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.287                                                   	MRR:20.44	Hits@10:35.88	Best:20.52
2024-12-27 18:35:14,268: Snapshot:2	Epoch:13	Loss:8.341	translation_Loss:6.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.283                                                   	MRR:20.63	Hits@10:35.94	Best:20.63
2024-12-27 18:35:24,494: Snapshot:2	Epoch:14	Loss:8.308	translation_Loss:6.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.279                                                   	MRR:20.6	Hits@10:36.04	Best:20.63
2024-12-27 18:35:34,681: Snapshot:2	Epoch:15	Loss:8.319	translation_Loss:6.027	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.293                                                   	MRR:20.7	Hits@10:36.22	Best:20.7
2024-12-27 18:35:45,183: Snapshot:2	Epoch:16	Loss:8.301	translation_Loss:6.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.287                                                   	MRR:20.74	Hits@10:36.06	Best:20.74
2024-12-27 18:35:55,383: Snapshot:2	Epoch:17	Loss:8.289	translation_Loss:6.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.281                                                   	MRR:20.61	Hits@10:36.08	Best:20.74
2024-12-27 18:36:05,531: Snapshot:2	Epoch:18	Loss:8.255	translation_Loss:5.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.282                                                   	MRR:20.58	Hits@10:36.02	Best:20.74
2024-12-27 18:36:16,099: Snapshot:2	Epoch:19	Loss:8.248	translation_Loss:5.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.283                                                   	MRR:20.6	Hits@10:36.06	Best:20.74
2024-12-27 18:36:26,205: Snapshot:2	Epoch:20	Loss:8.246	translation_Loss:5.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.297                                                   	MRR:20.48	Hits@10:35.98	Best:20.74
2024-12-27 18:36:36,340: Early Stopping! Snapshot: 2 Epoch: 21 Best Results: 20.74
2024-12-27 18:36:36,341: Start to training tokens! Snapshot: 2 Epoch: 21 Loss:8.252 MRR:20.63 Best Results: 20.74
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:36:36,341: Snapshot:2	Epoch:21	Loss:8.252	translation_Loss:5.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.297                                                   	MRR:20.63	Hits@10:36.02	Best:20.74
2024-12-27 18:36:46,591: Snapshot:2	Epoch:22	Loss:29.722	translation_Loss:29.686	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.63	Hits@10:36.02	Best:20.74
2024-12-27 18:36:56,423: End of token training: 2 Epoch: 23 Loss:29.682 MRR:20.63 Best Results: 20.74
2024-12-27 18:36:56,423: Snapshot:2	Epoch:23	Loss:29.682	translation_Loss:29.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.63	Hits@10:36.02	Best:20.74
2024-12-27 18:36:56,660: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_5000/2model_best.tar'
2024-12-27 18:37:04,794: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2596 | 0.156  | 0.3137 | 0.3765 |  0.4491 |
|     1      | 0.2875 | 0.1854 | 0.3334 | 0.4007 |  0.4884 |
|     2      | 0.2037 | 0.1242 | 0.2309 | 0.2844 |  0.3585 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:37:43,719: Snapshot:3	Epoch:0	Loss:22.638	translation_Loss:15.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.483                                                   	MRR:18.42	Hits@10:33.63	Best:18.42
2024-12-27 18:37:56,005: Snapshot:3	Epoch:1	Loss:13.227	translation_Loss:10.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.175                                                   	MRR:18.91	Hits@10:34.68	Best:18.91
2024-12-27 18:38:08,252: Snapshot:3	Epoch:2	Loss:12.709	translation_Loss:9.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.176                                                   	MRR:19.17	Hits@10:34.87	Best:19.17
2024-12-27 18:38:20,641: Snapshot:3	Epoch:3	Loss:12.501	translation_Loss:9.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.182                                                   	MRR:19.23	Hits@10:34.83	Best:19.23
2024-12-27 18:38:33,415: Snapshot:3	Epoch:4	Loss:12.386	translation_Loss:9.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.193                                                   	MRR:19.24	Hits@10:34.91	Best:19.24
2024-12-27 18:38:45,848: Snapshot:3	Epoch:5	Loss:12.272	translation_Loss:9.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.194                                                   	MRR:19.28	Hits@10:34.97	Best:19.28
2024-12-27 18:38:58,254: Snapshot:3	Epoch:6	Loss:12.221	translation_Loss:9.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.193                                                   	MRR:19.33	Hits@10:34.85	Best:19.33
2024-12-27 18:39:10,925: Snapshot:3	Epoch:7	Loss:12.157	translation_Loss:8.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.197                                                   	MRR:19.4	Hits@10:34.96	Best:19.4
2024-12-27 18:39:23,548: Snapshot:3	Epoch:8	Loss:12.11	translation_Loss:8.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.2                                                   	MRR:19.46	Hits@10:35.04	Best:19.46
2024-12-27 18:39:36,282: Snapshot:3	Epoch:9	Loss:12.086	translation_Loss:8.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.211                                                   	MRR:19.36	Hits@10:34.92	Best:19.46
2024-12-27 18:39:48,488: Snapshot:3	Epoch:10	Loss:12.047	translation_Loss:8.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.197                                                   	MRR:19.43	Hits@10:34.9	Best:19.46
2024-12-27 18:40:00,736: Snapshot:3	Epoch:11	Loss:12.022	translation_Loss:8.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.21                                                   	MRR:19.54	Hits@10:35.03	Best:19.54
2024-12-27 18:40:13,558: Snapshot:3	Epoch:12	Loss:11.997	translation_Loss:8.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.199                                                   	MRR:19.41	Hits@10:34.82	Best:19.54
2024-12-27 18:40:25,788: Snapshot:3	Epoch:13	Loss:11.961	translation_Loss:8.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.196                                                   	MRR:19.47	Hits@10:35.05	Best:19.54
2024-12-27 18:40:38,067: Snapshot:3	Epoch:14	Loss:11.968	translation_Loss:8.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.208                                                   	MRR:19.41	Hits@10:34.96	Best:19.54
2024-12-27 18:40:50,700: Snapshot:3	Epoch:15	Loss:11.923	translation_Loss:8.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.207                                                   	MRR:19.51	Hits@10:35.04	Best:19.54
2024-12-27 18:41:02,958: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 19.54
2024-12-27 18:41:02,958: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:11.956 MRR:19.47 Best Results: 19.54
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:41:02,959: Snapshot:3	Epoch:16	Loss:11.956	translation_Loss:8.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.215                                                   	MRR:19.47	Hits@10:35.02	Best:19.54
2024-12-27 18:41:14,998: Snapshot:3	Epoch:17	Loss:32.758	translation_Loss:32.721	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.47	Hits@10:35.02	Best:19.54
2024-12-27 18:41:27,551: End of token training: 3 Epoch: 18 Loss:32.746 MRR:19.47 Best Results: 19.54
2024-12-27 18:41:27,552: Snapshot:3	Epoch:18	Loss:32.746	translation_Loss:32.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.47	Hits@10:35.02	Best:19.54
2024-12-27 18:41:27,792: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_5000/3model_best.tar'
2024-12-27 18:41:41,415: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2612 | 0.1572 | 0.3164 | 0.3776 |  0.4508 |
|     1      | 0.2881 | 0.1871 | 0.3332 | 0.3989 |  0.4856 |
|     2      | 0.2035 | 0.1235 | 0.2303 | 0.2852 |  0.3591 |
|     3      | 0.1957 | 0.1131 | 0.2295 | 0.2837 |  0.3507 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:41:59,962: Snapshot:4	Epoch:0	Loss:11.093	translation_Loss:7.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.978                                                   	MRR:18.88	Hits@10:29.96	Best:18.88
2024-12-27 18:42:04,978: Snapshot:4	Epoch:1	Loss:6.916	translation_Loss:5.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.729                                                   	MRR:18.72	Hits@10:30.11	Best:18.88
2024-12-27 18:42:09,966: Snapshot:4	Epoch:2	Loss:6.621	translation_Loss:5.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.513                                                   	MRR:18.84	Hits@10:30.21	Best:18.88
2024-12-27 18:42:14,973: Snapshot:4	Epoch:3	Loss:6.61	translation_Loss:5.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.499                                                   	MRR:18.81	Hits@10:30.21	Best:18.88
2024-12-27 18:42:20,009: Snapshot:4	Epoch:4	Loss:6.608	translation_Loss:5.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.505                                                   	MRR:18.74	Hits@10:30.17	Best:18.88
2024-12-27 18:42:25,016: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 18.88
2024-12-27 18:42:25,017: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:6.619 MRR:18.86 Best Results: 18.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:42:25,017: Snapshot:4	Epoch:5	Loss:6.619	translation_Loss:5.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.513                                                   	MRR:18.86	Hits@10:30.03	Best:18.88
2024-12-27 18:42:29,929: Snapshot:4	Epoch:6	Loss:15.899	translation_Loss:15.862	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.86	Hits@10:30.03	Best:18.88
2024-12-27 18:42:34,890: End of token training: 4 Epoch: 7 Loss:15.842 MRR:18.86 Best Results: 18.88
2024-12-27 18:42:34,890: Snapshot:4	Epoch:7	Loss:15.842	translation_Loss:15.842	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.86	Hits@10:30.03	Best:18.88
2024-12-27 18:42:35,142: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_5000/4model_best.tar'
2024-12-27 18:42:51,456: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1409 | 0.2987 | 0.3612 |  0.4384 |
|     1      | 0.2824 | 0.1808 | 0.3265 | 0.3941 |  0.4804 |
|     2      | 0.1999 | 0.1205 | 0.2264 | 0.2801 |  0.3547 |
|     3      | 0.1913 | 0.1077 | 0.2255 | 0.2797 |  0.3494 |
|     4      | 0.1883 | 0.1312 | 0.2086 | 0.2488 |  0.2998 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:42:51,458: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2608 | 0.1575 | 0.3144 | 0.3782 |  0.4519 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2607 | 0.1572 | 0.3152 | 0.378  |  0.4506 |
|     1      | 0.2911 | 0.189  | 0.3399 | 0.4059 |  0.4892 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2596 | 0.156  | 0.3137 | 0.3765 |  0.4491 |
|     1      | 0.2875 | 0.1854 | 0.3334 | 0.4007 |  0.4884 |
|     2      | 0.2037 | 0.1242 | 0.2309 | 0.2844 |  0.3585 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2612 | 0.1572 | 0.3164 | 0.3776 |  0.4508 |
|     1      | 0.2881 | 0.1871 | 0.3332 | 0.3989 |  0.4856 |
|     2      | 0.2035 | 0.1235 | 0.2303 | 0.2852 |  0.3591 |
|     3      | 0.1957 | 0.1131 | 0.2295 | 0.2837 |  0.3507 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1409 | 0.2987 | 0.3612 |  0.4384 |
|     1      | 0.2824 | 0.1808 | 0.3265 | 0.3941 |  0.4804 |
|     2      | 0.1999 | 0.1205 | 0.2264 | 0.2801 |  0.3547 |
|     3      | 0.1913 | 0.1077 | 0.2255 | 0.2797 |  0.3494 |
|     4      | 0.1883 | 0.1312 | 0.2086 | 0.2488 |  0.2998 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:42:51,459: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 71.24024677276611  |   0.261   |    0.158     |    0.314     |     0.452     |
|    1     |  51.5649893283844  |   0.269   |    0.166     |    0.322     |     0.461     |
|    2     | 263.3379740715027  |   0.234   |    0.143     |    0.272     |     0.406     |
|    3     | 257.52439761161804 |   0.219   |    0.132     |    0.256     |     0.385     |
|    4     | 50.95272159576416  |    0.21   |    0.125     |    0.244     |      0.37     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:42:51,459: Sum_Training_Time:694.6203293800354
2024-12-27 18:42:51,459: Every_Training_Time:[71.24024677276611, 51.5649893283844, 263.3379740715027, 257.52439761161804, 50.95272159576416]
2024-12-27 18:42:51,459: Forward transfer: 0.0392 Backward transfer: -0.00810000000000001
2024-12-27 18:43:25,095: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227184255/HYBRIDHYBRID_0.01_2048_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.01_2048_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.01_2048_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:43:34,826: Snapshot:0	Epoch:0	Loss:18.538	translation_Loss:18.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.38	Hits@10:34.51	Best:17.38
2024-12-27 18:43:40,741: Snapshot:0	Epoch:1	Loss:8.453	translation_Loss:8.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:43.14	Best:23.75
2024-12-27 18:43:47,047: Snapshot:0	Epoch:2	Loss:3.949	translation_Loss:3.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.46	Hits@10:45.31	Best:25.46
2024-12-27 18:43:52,950: Snapshot:0	Epoch:3	Loss:2.077	translation_Loss:2.077	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:45.61	Best:25.85
2024-12-27 18:43:58,914: Snapshot:0	Epoch:4	Loss:1.311	translation_Loss:1.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.88	Hits@10:45.66	Best:25.88
2024-12-27 18:44:04,797: Snapshot:0	Epoch:5	Loss:0.962	translation_Loss:0.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:45.34	Best:25.88
2024-12-27 18:44:10,703: Snapshot:0	Epoch:6	Loss:0.769	translation_Loss:0.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.37	Hits@10:45.26	Best:25.88
2024-12-27 18:44:17,103: Snapshot:0	Epoch:7	Loss:0.655	translation_Loss:0.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.36	Hits@10:45.03	Best:25.88
2024-12-27 18:44:22,969: Snapshot:0	Epoch:8	Loss:0.559	translation_Loss:0.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:44.72	Best:25.88
2024-12-27 18:44:28,892: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 25.88
2024-12-27 18:44:28,892: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:0.502 MRR:25.07 Best Results: 25.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:44:28,893: Snapshot:0	Epoch:9	Loss:0.502	translation_Loss:0.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:44.73	Best:25.88
2024-12-27 18:44:35,431: Snapshot:0	Epoch:10	Loss:17.47	translation_Loss:17.433	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:44.73	Best:25.88
2024-12-27 18:44:41,698: End of token training: 0 Epoch: 11 Loss:17.407 MRR:25.07 Best Results: 25.88
2024-12-27 18:44:41,699: Snapshot:0	Epoch:11	Loss:17.407	translation_Loss:17.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.07	Hits@10:44.73	Best:25.88
2024-12-27 18:44:41,953: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_10000/0model_best.tar'
2024-12-27 18:44:44,548: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2583 | 0.1547 | 0.312  | 0.3751 |  0.4492 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:44:55,669: Snapshot:1	Epoch:0	Loss:11.972	translation_Loss:4.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.751                                                   	MRR:23.32	Hits@10:40.74	Best:23.32
2024-12-27 18:44:57,975: Snapshot:1	Epoch:1	Loss:3.722	translation_Loss:1.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.978                                                   	MRR:26.11	Hits@10:44.89	Best:26.11
2024-12-27 18:45:00,305: Snapshot:1	Epoch:2	Loss:2.106	translation_Loss:1.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.83                                                   	MRR:27.27	Hits@10:46.23	Best:27.27
2024-12-27 18:45:02,693: Snapshot:1	Epoch:3	Loss:1.55	translation_Loss:1.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.438                                                   	MRR:27.72	Hits@10:46.85	Best:27.72
2024-12-27 18:45:05,066: Snapshot:1	Epoch:4	Loss:1.364	translation_Loss:1.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.325                                                   	MRR:28.2	Hits@10:46.97	Best:28.2
2024-12-27 18:45:07,400: Snapshot:1	Epoch:5	Loss:1.285	translation_Loss:1.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.276                                                   	MRR:28.17	Hits@10:47.39	Best:28.2
2024-12-27 18:45:09,654: Snapshot:1	Epoch:6	Loss:1.236	translation_Loss:0.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:28.2	Hits@10:47.63	Best:28.2
2024-12-27 18:45:11,969: Snapshot:1	Epoch:7	Loss:1.217	translation_Loss:0.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:28.44	Hits@10:47.5	Best:28.44
2024-12-27 18:45:14,220: Snapshot:1	Epoch:8	Loss:1.201	translation_Loss:0.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:28.18	Hits@10:47.76	Best:28.44
2024-12-27 18:45:16,502: Snapshot:1	Epoch:9	Loss:1.184	translation_Loss:0.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:28.39	Hits@10:47.84	Best:28.44
2024-12-27 18:45:18,820: Snapshot:1	Epoch:10	Loss:1.189	translation_Loss:0.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.238                                                   	MRR:28.45	Hits@10:48.3	Best:28.45
2024-12-27 18:45:21,101: Snapshot:1	Epoch:11	Loss:1.172	translation_Loss:0.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.235                                                   	MRR:28.45	Hits@10:47.89	Best:28.45
2024-12-27 18:45:23,376: Snapshot:1	Epoch:12	Loss:1.172	translation_Loss:0.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.234                                                   	MRR:28.46	Hits@10:47.95	Best:28.46
2024-12-27 18:45:25,696: Snapshot:1	Epoch:13	Loss:1.167	translation_Loss:0.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.236                                                   	MRR:28.6	Hits@10:48.11	Best:28.6
2024-12-27 18:45:27,942: Snapshot:1	Epoch:14	Loss:1.171	translation_Loss:0.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.24                                                   	MRR:28.58	Hits@10:48.0	Best:28.6
2024-12-27 18:45:30,257: Snapshot:1	Epoch:15	Loss:1.17	translation_Loss:0.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:28.72	Hits@10:48.12	Best:28.72
2024-12-27 18:45:32,572: Snapshot:1	Epoch:16	Loss:1.167	translation_Loss:0.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.24                                                   	MRR:28.83	Hits@10:48.2	Best:28.83
2024-12-27 18:45:34,839: Snapshot:1	Epoch:17	Loss:1.164	translation_Loss:0.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:28.59	Hits@10:47.88	Best:28.83
2024-12-27 18:45:37,131: Snapshot:1	Epoch:18	Loss:1.166	translation_Loss:0.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.243                                                   	MRR:28.48	Hits@10:48.04	Best:28.83
2024-12-27 18:45:39,802: Snapshot:1	Epoch:19	Loss:1.169	translation_Loss:0.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.246                                                   	MRR:28.6	Hits@10:48.05	Best:28.83
2024-12-27 18:45:42,038: Snapshot:1	Epoch:20	Loss:1.152	translation_Loss:0.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:28.68	Hits@10:48.01	Best:28.83
2024-12-27 18:45:44,316: Early Stopping! Snapshot: 1 Epoch: 21 Best Results: 28.83
2024-12-27 18:45:44,317: Start to training tokens! Snapshot: 1 Epoch: 21 Loss:1.161 MRR:28.49 Best Results: 28.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:45:44,317: Snapshot:1	Epoch:21	Loss:1.161	translation_Loss:0.917	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.244                                                   	MRR:28.49	Hits@10:48.06	Best:28.83
2024-12-27 18:45:46,565: Snapshot:1	Epoch:22	Loss:7.054	translation_Loss:7.019	multi_layer_Loss:0.034	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.49	Hits@10:48.06	Best:28.83
2024-12-27 18:45:48,811: End of token training: 1 Epoch: 23 Loss:7.027 MRR:28.49 Best Results: 28.83
2024-12-27 18:45:48,811: Snapshot:1	Epoch:23	Loss:7.027	translation_Loss:7.026	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.49	Hits@10:48.06	Best:28.83
2024-12-27 18:45:49,056: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_10000/1model_best.tar'
2024-12-27 18:45:52,751: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.1553 | 0.3118 | 0.3757 |  0.4492 |
|     1      |  0.28  | 0.1781 | 0.3255 | 0.3921 |  0.4777 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:46:25,925: Snapshot:2	Epoch:0	Loss:24.218	translation_Loss:13.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.661                                                   	MRR:19.12	Hits@10:33.9	Best:19.12
2024-12-27 18:46:35,928: Snapshot:2	Epoch:1	Loss:8.874	translation_Loss:7.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.646                                                   	MRR:19.74	Hits@10:34.99	Best:19.74
2024-12-27 18:46:45,923: Snapshot:2	Epoch:2	Loss:8.118	translation_Loss:6.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.681                                                   	MRR:19.95	Hits@10:35.31	Best:19.95
2024-12-27 18:46:56,051: Snapshot:2	Epoch:3	Loss:7.869	translation_Loss:6.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.723                                                   	MRR:19.97	Hits@10:35.51	Best:19.97
2024-12-27 18:47:06,443: Snapshot:2	Epoch:4	Loss:7.743	translation_Loss:5.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.747                                                   	MRR:19.96	Hits@10:35.46	Best:19.97
2024-12-27 18:47:16,497: Snapshot:2	Epoch:5	Loss:7.675	translation_Loss:5.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.768                                                   	MRR:20.05	Hits@10:35.5	Best:20.05
2024-12-27 18:47:26,481: Snapshot:2	Epoch:6	Loss:7.624	translation_Loss:5.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.79                                                   	MRR:19.98	Hits@10:35.63	Best:20.05
2024-12-27 18:47:36,948: Snapshot:2	Epoch:7	Loss:7.544	translation_Loss:5.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.787                                                   	MRR:20.03	Hits@10:35.59	Best:20.05
2024-12-27 18:47:47,024: Snapshot:2	Epoch:8	Loss:7.534	translation_Loss:5.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.801                                                   	MRR:19.98	Hits@10:35.61	Best:20.05
2024-12-27 18:47:57,022: Snapshot:2	Epoch:9	Loss:7.494	translation_Loss:5.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.809                                                   	MRR:20.09	Hits@10:35.82	Best:20.09
2024-12-27 18:48:07,351: Snapshot:2	Epoch:10	Loss:7.463	translation_Loss:5.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.809                                                   	MRR:20.12	Hits@10:35.79	Best:20.12
2024-12-27 18:48:17,317: Snapshot:2	Epoch:11	Loss:7.443	translation_Loss:5.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.818                                                   	MRR:20.17	Hits@10:35.58	Best:20.17
2024-12-27 18:48:27,226: Snapshot:2	Epoch:12	Loss:7.395	translation_Loss:5.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.803                                                   	MRR:20.16	Hits@10:35.8	Best:20.17
2024-12-27 18:48:37,272: Snapshot:2	Epoch:13	Loss:7.394	translation_Loss:5.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.817                                                   	MRR:20.19	Hits@10:35.57	Best:20.19
2024-12-27 18:48:47,694: Snapshot:2	Epoch:14	Loss:7.371	translation_Loss:5.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.818                                                   	MRR:20.21	Hits@10:35.7	Best:20.21
2024-12-27 18:48:57,611: Snapshot:2	Epoch:15	Loss:7.36	translation_Loss:5.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.821                                                   	MRR:20.17	Hits@10:35.59	Best:20.21
2024-12-27 18:49:07,608: Snapshot:2	Epoch:16	Loss:7.309	translation_Loss:5.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.813                                                   	MRR:20.23	Hits@10:35.63	Best:20.23
2024-12-27 18:49:18,087: Snapshot:2	Epoch:17	Loss:7.345	translation_Loss:5.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.831                                                   	MRR:20.18	Hits@10:35.68	Best:20.23
2024-12-27 18:49:28,197: Snapshot:2	Epoch:18	Loss:7.33	translation_Loss:5.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.831                                                   	MRR:20.13	Hits@10:35.71	Best:20.23
2024-12-27 18:49:38,308: Snapshot:2	Epoch:19	Loss:7.313	translation_Loss:5.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.829                                                   	MRR:20.03	Hits@10:35.7	Best:20.23
2024-12-27 18:49:48,715: Snapshot:2	Epoch:20	Loss:7.288	translation_Loss:5.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.826                                                   	MRR:20.12	Hits@10:35.63	Best:20.23
2024-12-27 18:49:58,868: Snapshot:2	Epoch:21	Loss:7.272	translation_Loss:5.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.823                                                   	MRR:20.24	Hits@10:35.83	Best:20.24
2024-12-27 18:50:09,034: Snapshot:2	Epoch:22	Loss:7.28	translation_Loss:5.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.829                                                   	MRR:20.09	Hits@10:35.82	Best:20.24
2024-12-27 18:50:19,321: Snapshot:2	Epoch:23	Loss:7.28	translation_Loss:5.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.832                                                   	MRR:20.23	Hits@10:35.83	Best:20.24
2024-12-27 18:50:29,291: Snapshot:2	Epoch:24	Loss:7.273	translation_Loss:5.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.832                                                   	MRR:20.24	Hits@10:35.84	Best:20.24
2024-12-27 18:50:39,357: Snapshot:2	Epoch:25	Loss:7.244	translation_Loss:5.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.828                                                   	MRR:20.4	Hits@10:35.89	Best:20.4
2024-12-27 18:50:49,793: Snapshot:2	Epoch:26	Loss:7.233	translation_Loss:5.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.827                                                   	MRR:20.28	Hits@10:35.79	Best:20.4
2024-12-27 18:50:59,827: Snapshot:2	Epoch:27	Loss:7.242	translation_Loss:5.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.839                                                   	MRR:20.32	Hits@10:35.82	Best:20.4
2024-12-27 18:51:09,782: Snapshot:2	Epoch:28	Loss:7.214	translation_Loss:5.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.825                                                   	MRR:20.25	Hits@10:35.73	Best:20.4
2024-12-27 18:51:19,742: Snapshot:2	Epoch:29	Loss:7.238	translation_Loss:5.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.838                                                   	MRR:20.21	Hits@10:35.78	Best:20.4
2024-12-27 18:51:30,078: Early Stopping! Snapshot: 2 Epoch: 30 Best Results: 20.4
2024-12-27 18:51:30,079: Start to training tokens! Snapshot: 2 Epoch: 30 Loss:7.25 MRR:20.23 Best Results: 20.4
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:51:30,079: Snapshot:2	Epoch:30	Loss:7.25	translation_Loss:5.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.845                                                   	MRR:20.23	Hits@10:35.77	Best:20.4
2024-12-27 18:51:40,010: Snapshot:2	Epoch:31	Loss:29.626	translation_Loss:29.59	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.23	Hits@10:35.77	Best:20.4
2024-12-27 18:51:49,865: End of token training: 2 Epoch: 32 Loss:29.592 MRR:20.23 Best Results: 20.4
2024-12-27 18:51:49,866: Snapshot:2	Epoch:32	Loss:29.592	translation_Loss:29.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.23	Hits@10:35.77	Best:20.4
2024-12-27 18:51:50,133: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_10000/2model_best.tar'
2024-12-27 18:51:58,298: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.259  | 0.1556 | 0.3129 | 0.3749 |  0.4491 |
|     1      | 0.2788 | 0.1775 | 0.3223 | 0.3916 |  0.4776 |
|     2      | 0.2016 | 0.1224 | 0.2272 | 0.2809 |  0.3571 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:52:37,668: Snapshot:3	Epoch:0	Loss:25.384	translation_Loss:14.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.936                                                   	MRR:18.44	Hits@10:33.32	Best:18.44
2024-12-27 18:52:49,911: Snapshot:3	Epoch:1	Loss:11.643	translation_Loss:9.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.372                                                   	MRR:18.86	Hits@10:34.18	Best:18.86
2024-12-27 18:53:02,196: Snapshot:3	Epoch:2	Loss:11.207	translation_Loss:8.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.45                                                   	MRR:19.0	Hits@10:34.32	Best:19.0
2024-12-27 18:53:14,365: Snapshot:3	Epoch:3	Loss:11.035	translation_Loss:8.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.494                                                   	MRR:19.04	Hits@10:34.5	Best:19.04
2024-12-27 18:53:26,678: Snapshot:3	Epoch:4	Loss:10.908	translation_Loss:8.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.512                                                   	MRR:19.16	Hits@10:34.62	Best:19.16
2024-12-27 18:53:39,040: Snapshot:3	Epoch:5	Loss:10.866	translation_Loss:8.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.539                                                   	MRR:18.95	Hits@10:34.52	Best:19.16
2024-12-27 18:53:51,323: Snapshot:3	Epoch:6	Loss:10.789	translation_Loss:8.245	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.544                                                   	MRR:19.2	Hits@10:34.54	Best:19.2
2024-12-27 18:54:03,927: Snapshot:3	Epoch:7	Loss:10.729	translation_Loss:8.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.554                                                   	MRR:19.21	Hits@10:34.6	Best:19.21
2024-12-27 18:54:16,104: Snapshot:3	Epoch:8	Loss:10.708	translation_Loss:8.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.555                                                   	MRR:19.21	Hits@10:34.68	Best:19.21
2024-12-27 18:54:28,457: Snapshot:3	Epoch:9	Loss:10.682	translation_Loss:8.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.58                                                   	MRR:19.24	Hits@10:34.81	Best:19.24
2024-12-27 18:54:41,075: Snapshot:3	Epoch:10	Loss:10.683	translation_Loss:8.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.582                                                   	MRR:19.17	Hits@10:34.75	Best:19.24
2024-12-27 18:54:53,228: Snapshot:3	Epoch:11	Loss:10.651	translation_Loss:8.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.578                                                   	MRR:19.24	Hits@10:34.67	Best:19.24
2024-12-27 18:55:05,939: Snapshot:3	Epoch:12	Loss:10.621	translation_Loss:8.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.575                                                   	MRR:19.25	Hits@10:34.75	Best:19.25
2024-12-27 18:55:18,095: Snapshot:3	Epoch:13	Loss:10.631	translation_Loss:8.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.588                                                   	MRR:19.19	Hits@10:34.63	Best:19.25
2024-12-27 18:55:30,314: Snapshot:3	Epoch:14	Loss:10.626	translation_Loss:8.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.592                                                   	MRR:19.18	Hits@10:34.77	Best:19.25
2024-12-27 18:55:42,778: Snapshot:3	Epoch:15	Loss:10.578	translation_Loss:7.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.586                                                   	MRR:19.23	Hits@10:34.66	Best:19.25
2024-12-27 18:55:54,935: Snapshot:3	Epoch:16	Loss:10.579	translation_Loss:7.991	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.588                                                   	MRR:19.31	Hits@10:34.75	Best:19.31
2024-12-27 18:56:07,279: Snapshot:3	Epoch:17	Loss:10.578	translation_Loss:7.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.599                                                   	MRR:19.23	Hits@10:34.79	Best:19.31
2024-12-27 18:56:19,977: Snapshot:3	Epoch:18	Loss:10.547	translation_Loss:7.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.589                                                   	MRR:19.28	Hits@10:34.73	Best:19.31
2024-12-27 18:56:32,055: Snapshot:3	Epoch:19	Loss:10.543	translation_Loss:7.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.596                                                   	MRR:19.25	Hits@10:34.7	Best:19.31
2024-12-27 18:56:44,615: Snapshot:3	Epoch:20	Loss:10.543	translation_Loss:7.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.596                                                   	MRR:19.34	Hits@10:34.68	Best:19.34
2024-12-27 18:56:57,022: Snapshot:3	Epoch:21	Loss:10.554	translation_Loss:7.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.603                                                   	MRR:19.37	Hits@10:34.7	Best:19.37
2024-12-27 18:57:09,204: Snapshot:3	Epoch:22	Loss:10.522	translation_Loss:7.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.602                                                   	MRR:19.34	Hits@10:34.74	Best:19.37
2024-12-27 18:57:21,773: Snapshot:3	Epoch:23	Loss:10.511	translation_Loss:7.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.605                                                   	MRR:19.29	Hits@10:34.83	Best:19.37
2024-12-27 18:57:33,874: Snapshot:3	Epoch:24	Loss:10.526	translation_Loss:7.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.607                                                   	MRR:19.35	Hits@10:34.73	Best:19.37
2024-12-27 18:57:46,045: Snapshot:3	Epoch:25	Loss:10.496	translation_Loss:7.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.609                                                   	MRR:19.38	Hits@10:34.69	Best:19.38
2024-12-27 18:57:58,619: Snapshot:3	Epoch:26	Loss:10.527	translation_Loss:7.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.622                                                   	MRR:19.35	Hits@10:34.77	Best:19.38
2024-12-27 18:58:10,729: Snapshot:3	Epoch:27	Loss:10.511	translation_Loss:7.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.624                                                   	MRR:19.31	Hits@10:34.66	Best:19.38
2024-12-27 18:58:23,017: Snapshot:3	Epoch:28	Loss:10.501	translation_Loss:7.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.63                                                   	MRR:19.33	Hits@10:34.81	Best:19.38
2024-12-27 18:58:35,617: Snapshot:3	Epoch:29	Loss:10.513	translation_Loss:7.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.623                                                   	MRR:19.35	Hits@10:34.66	Best:19.38
2024-12-27 18:58:47,762: Early Stopping! Snapshot: 3 Epoch: 30 Best Results: 19.38
2024-12-27 18:58:47,763: Start to training tokens! Snapshot: 3 Epoch: 30 Loss:10.506 MRR:19.26 Best Results: 19.38
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:58:47,763: Snapshot:3	Epoch:30	Loss:10.506	translation_Loss:7.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.634                                                   	MRR:19.26	Hits@10:34.6	Best:19.38
2024-12-27 18:59:00,047: Snapshot:3	Epoch:31	Loss:32.59	translation_Loss:32.553	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.26	Hits@10:34.6	Best:19.38
2024-12-27 18:59:12,015: End of token training: 3 Epoch: 32 Loss:32.537 MRR:19.26 Best Results: 19.38
2024-12-27 18:59:12,015: Snapshot:3	Epoch:32	Loss:32.537	translation_Loss:32.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.26	Hits@10:34.6	Best:19.38
2024-12-27 18:59:12,254: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_10000/3model_best.tar'
2024-12-27 18:59:26,292: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2601 | 0.1569 | 0.3133 | 0.3756 |  0.4504 |
|     1      | 0.2779 | 0.1777 | 0.3195 | 0.3893 |  0.4744 |
|     2      | 0.2017 | 0.1222 | 0.2274 | 0.2804 |  0.3574 |
|     3      | 0.1924 | 0.1105 | 0.2257 | 0.2801 |  0.348  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:59:44,179: Snapshot:4	Epoch:0	Loss:13.404	translation_Loss:7.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.318                                                   	MRR:18.52	Hits@10:29.47	Best:18.52
2024-12-27 18:59:49,515: Snapshot:4	Epoch:1	Loss:6.778	translation_Loss:5.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.553                                                   	MRR:18.48	Hits@10:29.35	Best:18.52
2024-12-27 18:59:54,431: Snapshot:4	Epoch:2	Loss:6.341	translation_Loss:5.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.209                                                   	MRR:18.44	Hits@10:29.37	Best:18.52
2024-12-27 18:59:59,346: Snapshot:4	Epoch:3	Loss:6.311	translation_Loss:5.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.186                                                   	MRR:18.49	Hits@10:29.36	Best:18.52
2024-12-27 19:00:04,353: Snapshot:4	Epoch:4	Loss:6.342	translation_Loss:5.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.193                                                   	MRR:18.38	Hits@10:29.41	Best:18.52
2024-12-27 19:00:09,381: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 18.52
2024-12-27 19:00:09,382: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:6.337 MRR:18.45 Best Results: 18.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:00:09,382: Snapshot:4	Epoch:5	Loss:6.337	translation_Loss:5.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.209                                                   	MRR:18.45	Hits@10:29.35	Best:18.52
2024-12-27 19:00:14,316: Snapshot:4	Epoch:6	Loss:16.246	translation_Loss:16.21	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.45	Hits@10:29.35	Best:18.52
2024-12-27 19:00:19,213: End of token training: 4 Epoch: 7 Loss:16.223 MRR:18.45 Best Results: 18.52
2024-12-27 19:00:19,214: Snapshot:4	Epoch:7	Loss:16.223	translation_Loss:16.223	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.45	Hits@10:29.35	Best:18.52
2024-12-27 19:00:19,415: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.01_2048_10000/4model_best.tar'
2024-12-27 19:00:35,813: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2513 | 0.1455 | 0.3061 | 0.3715 |  0.4496 |
|     1      | 0.2754 | 0.1737 | 0.3154 | 0.388  |  0.4744 |
|     2      | 0.1993 | 0.1194 | 0.2255 | 0.2794 |  0.3561 |
|     3      | 0.1917 | 0.109  | 0.2245 | 0.2799 |  0.349  |
|     4      | 0.1828 | 0.1275 | 0.1996 | 0.237  |  0.2905 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:00:35,814: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2583 | 0.1547 | 0.312  | 0.3751 |  0.4492 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.1553 | 0.3118 | 0.3757 |  0.4492 |
|     1      |  0.28  | 0.1781 | 0.3255 | 0.3921 |  0.4777 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.259  | 0.1556 | 0.3129 | 0.3749 |  0.4491 |
|     1      | 0.2788 | 0.1775 | 0.3223 | 0.3916 |  0.4776 |
|     2      | 0.2016 | 0.1224 | 0.2272 | 0.2809 |  0.3571 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2601 | 0.1569 | 0.3133 | 0.3756 |  0.4504 |
|     1      | 0.2779 | 0.1777 | 0.3195 | 0.3893 |  0.4744 |
|     2      | 0.2017 | 0.1222 | 0.2274 | 0.2804 |  0.3574 |
|     3      | 0.1924 | 0.1105 | 0.2257 | 0.2801 |  0.348  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2513 | 0.1455 | 0.3061 | 0.3715 |  0.4496 |
|     1      | 0.2754 | 0.1737 | 0.3154 | 0.388  |  0.4744 |
|     2      | 0.1993 | 0.1194 | 0.2255 | 0.2794 |  0.3561 |
|     3      | 0.1917 | 0.109  | 0.2245 | 0.2799 |  0.349  |
|     4      | 0.1828 | 0.1275 | 0.1996 | 0.237  |  0.2905 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:00:35,815: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 76.60237407684326  |   0.258   |    0.155     |    0.312     |     0.449     |
|    1     | 62.989540338516235 |   0.264   |    0.161     |    0.315     |     0.457     |
|    2     | 352.6795651912689  |   0.231   |    0.141     |    0.269     |     0.404     |
|    3     | 428.2145268917084  |   0.216   |    0.129     |    0.252     |     0.383     |
|    4     | 50.41777682304382  |   0.209   |    0.125     |    0.243     |     0.371     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:00:35,815: Sum_Training_Time:970.9037833213806
2024-12-27 19:00:35,815: Every_Training_Time:[76.60237407684326, 62.989540338516235, 352.6795651912689, 428.2145268917084, 50.41777682304382]
2024-12-27 19:00:35,815: Forward transfer: 0.037774999999999996 Backward transfer: -0.0036499999999999935
2024-12-27 19:01:09,401: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227190039/HYBRIDHYBRID_0.001_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:01:19,247: Snapshot:0	Epoch:0	Loss:78.158	translation_Loss:78.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.42	Hits@10:34.08	Best:16.42
2024-12-27 19:01:25,362: Snapshot:0	Epoch:1	Loss:35.712	translation_Loss:35.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:43.24	Best:23.24
2024-12-27 19:01:31,532: Snapshot:0	Epoch:2	Loss:17.295	translation_Loss:17.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:45.18	Best:24.8
2024-12-27 19:01:37,691: Snapshot:0	Epoch:3	Loss:9.266	translation_Loss:9.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.51	Hits@10:45.58	Best:25.51
2024-12-27 19:01:43,828: Snapshot:0	Epoch:4	Loss:5.746	translation_Loss:5.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.52	Hits@10:45.44	Best:25.52
2024-12-27 19:01:49,852: Snapshot:0	Epoch:5	Loss:4.16	translation_Loss:4.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:45.68	Best:25.52
2024-12-27 19:01:55,966: Snapshot:0	Epoch:6	Loss:3.275	translation_Loss:3.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.27	Hits@10:45.17	Best:25.52
2024-12-27 19:02:01,987: Snapshot:0	Epoch:7	Loss:2.777	translation_Loss:2.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:45.3	Best:25.52
2024-12-27 19:02:08,489: Snapshot:0	Epoch:8	Loss:2.458	translation_Loss:2.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.22	Hits@10:44.76	Best:25.52
2024-12-27 19:02:14,490: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 25.52
2024-12-27 19:02:14,490: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:2.268 MRR:25.14 Best Results: 25.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:02:14,490: Snapshot:0	Epoch:9	Loss:2.268	translation_Loss:2.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.14	Hits@10:44.72	Best:25.52
2024-12-27 19:02:21,205: Snapshot:0	Epoch:10	Loss:68.163	translation_Loss:68.009	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.14	Hits@10:44.72	Best:25.52
2024-12-27 19:02:27,539: End of token training: 0 Epoch: 11 Loss:67.911 MRR:25.14 Best Results: 25.52
2024-12-27 19:02:27,540: Snapshot:0	Epoch:11	Loss:67.911	translation_Loss:67.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.14	Hits@10:44.72	Best:25.52
2024-12-27 19:02:27,789: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_1000/0model_best.tar'
2024-12-27 19:02:30,335: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2544 | 0.1487 | 0.3087 | 0.3731 |  0.4481 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:02:41,712: Snapshot:1	Epoch:0	Loss:19.744	translation_Loss:18.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.976                                                   	MRR:23.51	Hits@10:41.03	Best:23.51
2024-12-27 19:02:44,188: Snapshot:1	Epoch:1	Loss:7.4	translation_Loss:5.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.509                                                   	MRR:27.41	Hits@10:47.33	Best:27.41
2024-12-27 19:02:46,634: Snapshot:1	Epoch:2	Loss:4.415	translation_Loss:2.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.621                                                   	MRR:28.26	Hits@10:47.94	Best:28.26
2024-12-27 19:02:49,113: Snapshot:1	Epoch:3	Loss:3.335	translation_Loss:1.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.597                                                   	MRR:28.8	Hits@10:48.17	Best:28.8
2024-12-27 19:02:51,567: Snapshot:1	Epoch:4	Loss:2.927	translation_Loss:1.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.569                                                   	MRR:29.33	Hits@10:48.74	Best:29.33
2024-12-27 19:02:53,995: Snapshot:1	Epoch:5	Loss:2.709	translation_Loss:1.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.541                                                   	MRR:29.02	Hits@10:49.13	Best:29.33
2024-12-27 19:02:56,416: Snapshot:1	Epoch:6	Loss:2.585	translation_Loss:1.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.516                                                   	MRR:29.56	Hits@10:50.38	Best:29.56
2024-12-27 19:02:58,881: Snapshot:1	Epoch:7	Loss:2.47	translation_Loss:0.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.478                                                   	MRR:29.64	Hits@10:49.81	Best:29.64
2024-12-27 19:03:01,348: Snapshot:1	Epoch:8	Loss:2.411	translation_Loss:0.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.454                                                   	MRR:29.83	Hits@10:50.2	Best:29.83
2024-12-27 19:03:03,736: Snapshot:1	Epoch:9	Loss:2.359	translation_Loss:0.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.451                                                   	MRR:29.83	Hits@10:50.06	Best:29.83
2024-12-27 19:03:06,164: Snapshot:1	Epoch:10	Loss:2.33	translation_Loss:0.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.431                                                   	MRR:29.63	Hits@10:50.69	Best:29.83
2024-12-27 19:03:08,575: Snapshot:1	Epoch:11	Loss:2.258	translation_Loss:0.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.405                                                   	MRR:29.98	Hits@10:50.44	Best:29.98
2024-12-27 19:03:11,062: Snapshot:1	Epoch:12	Loss:2.267	translation_Loss:0.86	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.407                                                   	MRR:30.21	Hits@10:50.59	Best:30.21
2024-12-27 19:03:13,477: Snapshot:1	Epoch:13	Loss:2.238	translation_Loss:0.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.405                                                   	MRR:29.69	Hits@10:50.72	Best:30.21
2024-12-27 19:03:15,894: Snapshot:1	Epoch:14	Loss:2.221	translation_Loss:0.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.401                                                   	MRR:29.9	Hits@10:51.05	Best:30.21
2024-12-27 19:03:18,359: Snapshot:1	Epoch:15	Loss:2.209	translation_Loss:0.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.389                                                   	MRR:29.92	Hits@10:50.35	Best:30.21
2024-12-27 19:03:20,747: Snapshot:1	Epoch:16	Loss:2.192	translation_Loss:0.792	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.4                                                   	MRR:30.07	Hits@10:50.83	Best:30.21
2024-12-27 19:03:23,156: Early Stopping! Snapshot: 1 Epoch: 17 Best Results: 30.21
2024-12-27 19:03:23,156: Start to training tokens! Snapshot: 1 Epoch: 17 Loss:2.176 MRR:30.21 Best Results: 30.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:03:23,156: Snapshot:1	Epoch:17	Loss:2.176	translation_Loss:0.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.382                                                   	MRR:30.21	Hits@10:50.91	Best:30.21
2024-12-27 19:03:25,505: Snapshot:1	Epoch:18	Loss:23.547	translation_Loss:23.395	multi_layer_Loss:0.152	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.21	Hits@10:50.91	Best:30.21
2024-12-27 19:03:27,856: End of token training: 1 Epoch: 19 Loss:23.36 MRR:30.21 Best Results: 30.21
2024-12-27 19:03:27,857: Snapshot:1	Epoch:19	Loss:23.36	translation_Loss:23.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.21	Hits@10:50.91	Best:30.21
2024-12-27 19:03:28,180: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_1000/1model_best.tar'
2024-12-27 19:03:32,389: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2429 | 0.1395 | 0.2939 | 0.3586 |  0.4418 |
|     1      | 0.2966 | 0.1901 | 0.3461 | 0.4147 |  0.5054 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:04:07,167: Snapshot:2	Epoch:0	Loss:60.575	translation_Loss:53.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.17                                                   	MRR:19.5	Hits@10:35.76	Best:19.5
2024-12-27 19:04:17,878: Snapshot:2	Epoch:1	Loss:27.71	translation_Loss:18.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.895                                                   	MRR:20.27	Hits@10:37.11	Best:20.27
2024-12-27 19:04:28,593: Snapshot:2	Epoch:2	Loss:24.066	translation_Loss:14.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.07                                                   	MRR:20.69	Hits@10:37.36	Best:20.69
2024-12-27 19:04:39,360: Snapshot:2	Epoch:3	Loss:22.752	translation_Loss:13.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.112                                                   	MRR:20.67	Hits@10:37.42	Best:20.69
2024-12-27 19:04:50,069: Snapshot:2	Epoch:4	Loss:22.256	translation_Loss:13.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.151                                                   	MRR:21.05	Hits@10:37.74	Best:21.05
2024-12-27 19:05:00,755: Snapshot:2	Epoch:5	Loss:21.906	translation_Loss:12.708	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.198                                                   	MRR:21.05	Hits@10:37.78	Best:21.05
2024-12-27 19:05:11,445: Snapshot:2	Epoch:6	Loss:21.671	translation_Loss:12.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.179                                                   	MRR:20.8	Hits@10:37.52	Best:21.05
2024-12-27 19:05:22,206: Snapshot:2	Epoch:7	Loss:21.534	translation_Loss:12.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.185                                                   	MRR:20.85	Hits@10:37.66	Best:21.05
2024-12-27 19:05:32,872: Snapshot:2	Epoch:8	Loss:21.406	translation_Loss:12.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.189                                                   	MRR:20.91	Hits@10:37.82	Best:21.05
2024-12-27 19:05:43,439: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 21.05
2024-12-27 19:05:43,439: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:21.317 MRR:20.77 Best Results: 21.05
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:05:43,439: Snapshot:2	Epoch:9	Loss:21.317	translation_Loss:12.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.215                                                   	MRR:20.77	Hits@10:37.8	Best:21.05
2024-12-27 19:05:53,893: Snapshot:2	Epoch:10	Loss:104.011	translation_Loss:103.865	multi_layer_Loss:0.146	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.77	Hits@10:37.8	Best:21.05
2024-12-27 19:06:04,313: End of token training: 2 Epoch: 11 Loss:103.919 MRR:20.77 Best Results: 21.05
2024-12-27 19:06:04,314: Snapshot:2	Epoch:11	Loss:103.919	translation_Loss:103.919	multi_layer_Loss:-0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.77	Hits@10:37.8	Best:21.05
2024-12-27 19:06:04,634: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_1000/2model_best.tar'
2024-12-27 19:06:12,882: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2362 | 0.1292 | 0.2857 | 0.3535 |  0.4429 |
|     1      | 0.2746 | 0.1749 | 0.3107 | 0.3752 |  0.4691 |
|     2      | 0.2076 | 0.1226 | 0.2362 | 0.2936 |  0.3756 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:06:54,605: Snapshot:3	Epoch:0	Loss:56.179	translation_Loss:46.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.425                                                   	MRR:19.66	Hits@10:36.11	Best:19.66
2024-12-27 19:07:07,696: Snapshot:3	Epoch:1	Loss:27.936	translation_Loss:16.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.074                                                   	MRR:20.27	Hits@10:36.88	Best:20.27
2024-12-27 19:07:20,870: Snapshot:3	Epoch:2	Loss:25.957	translation_Loss:14.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.207                                                   	MRR:20.44	Hits@10:36.89	Best:20.44
2024-12-27 19:07:34,021: Snapshot:3	Epoch:3	Loss:25.499	translation_Loss:14.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.372                                                   	MRR:20.47	Hits@10:37.11	Best:20.47
2024-12-27 19:07:46,922: Snapshot:3	Epoch:4	Loss:25.239	translation_Loss:13.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.458                                                   	MRR:20.44	Hits@10:37.32	Best:20.47
2024-12-27 19:07:59,842: Snapshot:3	Epoch:5	Loss:25.072	translation_Loss:13.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.509                                                   	MRR:20.46	Hits@10:37.27	Best:20.47
2024-12-27 19:08:12,859: Snapshot:3	Epoch:6	Loss:25.078	translation_Loss:13.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.614                                                   	MRR:20.59	Hits@10:37.31	Best:20.59
2024-12-27 19:08:25,834: Snapshot:3	Epoch:7	Loss:24.919	translation_Loss:13.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.609                                                   	MRR:20.36	Hits@10:37.17	Best:20.59
2024-12-27 19:08:38,807: Snapshot:3	Epoch:8	Loss:24.806	translation_Loss:13.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.605                                                   	MRR:20.56	Hits@10:37.36	Best:20.59
2024-12-27 19:08:51,856: Snapshot:3	Epoch:9	Loss:24.79	translation_Loss:13.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.637                                                   	MRR:20.59	Hits@10:37.55	Best:20.59
2024-12-27 19:09:04,874: Snapshot:3	Epoch:10	Loss:24.678	translation_Loss:13.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.626                                                   	MRR:20.4	Hits@10:37.2	Best:20.59
2024-12-27 19:09:17,889: Snapshot:3	Epoch:11	Loss:24.674	translation_Loss:13.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.669                                                   	MRR:20.6	Hits@10:37.21	Best:20.6
2024-12-27 19:09:31,022: Snapshot:3	Epoch:12	Loss:24.722	translation_Loss:13.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.663                                                   	MRR:20.68	Hits@10:37.34	Best:20.68
2024-12-27 19:09:44,058: Snapshot:3	Epoch:13	Loss:24.606	translation_Loss:12.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.681                                                   	MRR:20.75	Hits@10:37.35	Best:20.75
2024-12-27 19:09:57,124: Snapshot:3	Epoch:14	Loss:24.528	translation_Loss:12.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.627                                                   	MRR:20.83	Hits@10:37.4	Best:20.83
2024-12-27 19:10:10,203: Snapshot:3	Epoch:15	Loss:24.515	translation_Loss:12.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.685                                                   	MRR:20.81	Hits@10:37.48	Best:20.83
2024-12-27 19:10:23,161: Snapshot:3	Epoch:16	Loss:24.488	translation_Loss:12.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.652                                                   	MRR:20.74	Hits@10:37.66	Best:20.83
2024-12-27 19:10:36,153: Snapshot:3	Epoch:17	Loss:24.456	translation_Loss:12.801	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.655                                                   	MRR:20.76	Hits@10:37.66	Best:20.83
2024-12-27 19:10:49,064: Snapshot:3	Epoch:18	Loss:24.35	translation_Loss:12.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.595                                                   	MRR:20.63	Hits@10:37.45	Best:20.83
2024-12-27 19:11:02,100: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 20.83
2024-12-27 19:11:02,101: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:24.363 MRR:20.81 Best Results: 20.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:11:02,101: Snapshot:3	Epoch:19	Loss:24.363	translation_Loss:12.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.643                                                   	MRR:20.81	Hits@10:37.61	Best:20.83
2024-12-27 19:11:14,697: Snapshot:3	Epoch:20	Loss:110.154	translation_Loss:110.0	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.81	Hits@10:37.61	Best:20.83
2024-12-27 19:11:27,343: End of token training: 3 Epoch: 21 Loss:109.989 MRR:20.81 Best Results: 20.83
2024-12-27 19:11:27,343: Snapshot:3	Epoch:21	Loss:109.989	translation_Loss:109.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.81	Hits@10:37.61	Best:20.83
2024-12-27 19:11:27,658: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_1000/3model_best.tar'
2024-12-27 19:11:41,484: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2411 | 0.1408 | 0.2849 | 0.3503 |  0.4361 |
|     1      | 0.2665 | 0.1702 | 0.3014 | 0.3632 |  0.456  |
|     2      | 0.1993 | 0.116  | 0.2252 | 0.2813 |  0.3661 |
|     3      | 0.2087 | 0.1206 | 0.2405 | 0.3001 |  0.3757 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:12:01,283: Snapshot:4	Epoch:0	Loss:27.158	translation_Loss:24.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.425                                                   	MRR:20.78	Hits@10:35.36	Best:20.78
2024-12-27 19:12:06,656: Snapshot:4	Epoch:1	Loss:11.452	translation_Loss:7.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.132                                                   	MRR:22.2	Hits@10:36.29	Best:22.2
2024-12-27 19:12:11,955: Snapshot:4	Epoch:2	Loss:9.195	translation_Loss:4.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.605                                                   	MRR:21.92	Hits@10:36.13	Best:22.2
2024-12-27 19:12:17,299: Snapshot:4	Epoch:3	Loss:8.666	translation_Loss:3.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.798                                                   	MRR:21.71	Hits@10:36.38	Best:22.2
2024-12-27 19:12:22,622: Snapshot:4	Epoch:4	Loss:8.598	translation_Loss:3.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.948                                                   	MRR:21.99	Hits@10:36.73	Best:22.2
2024-12-27 19:12:27,954: Snapshot:4	Epoch:5	Loss:8.576	translation_Loss:3.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.033                                                   	MRR:21.92	Hits@10:36.67	Best:22.2
2024-12-27 19:12:33,250: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 22.2
2024-12-27 19:12:33,250: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:8.559 MRR:21.82 Best Results: 22.2
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:12:33,251: Snapshot:4	Epoch:6	Loss:8.559	translation_Loss:3.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.094                                                   	MRR:21.82	Hits@10:35.8	Best:22.2
2024-12-27 19:12:38,420: Snapshot:4	Epoch:7	Loss:51.318	translation_Loss:51.156	multi_layer_Loss:0.162	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.82	Hits@10:35.8	Best:22.2
2024-12-27 19:12:43,582: End of token training: 4 Epoch: 8 Loss:51.156 MRR:21.82 Best Results: 22.2
2024-12-27 19:12:43,582: Snapshot:4	Epoch:8	Loss:51.156	translation_Loss:51.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.82	Hits@10:35.8	Best:22.2
2024-12-27 19:12:43,789: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_1000/4model_best.tar'
2024-12-27 19:12:59,851: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2027 | 0.1127 | 0.2358 | 0.2976 |  0.3794 |
|     1      | 0.2479 | 0.1556 | 0.2768 | 0.3413 |  0.4319 |
|     2      | 0.1814 | 0.1047 | 0.2007 | 0.2538 |  0.3327 |
|     3      | 0.1834 | 0.0995 | 0.2115 | 0.2651 |  0.343  |
|     4      | 0.2213 | 0.1495 | 0.246  | 0.2964 |  0.3554 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:12:59,853: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2544 | 0.1487 | 0.3087 | 0.3731 |  0.4481 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2429 | 0.1395 | 0.2939 | 0.3586 |  0.4418 |
|     1      | 0.2966 | 0.1901 | 0.3461 | 0.4147 |  0.5054 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2362 | 0.1292 | 0.2857 | 0.3535 |  0.4429 |
|     1      | 0.2746 | 0.1749 | 0.3107 | 0.3752 |  0.4691 |
|     2      | 0.2076 | 0.1226 | 0.2362 | 0.2936 |  0.3756 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2411 | 0.1408 | 0.2849 | 0.3503 |  0.4361 |
|     1      | 0.2665 | 0.1702 | 0.3014 | 0.3632 |  0.456  |
|     2      | 0.1993 | 0.116  | 0.2252 | 0.2813 |  0.3661 |
|     3      | 0.2087 | 0.1206 | 0.2405 | 0.3001 |  0.3757 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2027 | 0.1127 | 0.2358 | 0.2976 |  0.3794 |
|     1      | 0.2479 | 0.1556 | 0.2768 | 0.3413 |  0.4319 |
|     2      | 0.1814 | 0.1047 | 0.2007 | 0.2538 |  0.3327 |
|     3      | 0.1834 | 0.0995 | 0.2115 | 0.2651 |  0.343  |
|     4      | 0.2213 | 0.1495 | 0.246  | 0.2964 |  0.3554 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:12:59,854: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 78.13824081420898  |   0.254   |    0.149     |    0.309     |     0.448     |
|    1     | 56.279693365097046 |   0.257   |    0.153     |    0.308     |     0.459     |
|    2     | 147.6611099243164  |   0.226   |    0.131     |    0.263     |     0.411     |
|    3     | 309.17094707489014 |   0.217   |    0.127     |     0.25     |     0.392     |
|    4     | 59.516358613967896 |   0.196   |    0.114     |    0.222     |     0.354     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:12:59,854: Sum_Training_Time:650.7663497924805
2024-12-27 19:12:59,854: Every_Training_Time:[78.13824081420898, 56.279693365097046, 147.6611099243164, 309.17094707489014, 59.516358613967896]
2024-12-27 19:12:59,854: Forward transfer: 0.042249999999999996 Backward transfer: -0.037974999999999995
2024-12-27 19:13:33,621: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227191303/HYBRIDHYBRID_0.001_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:13:43,364: Snapshot:0	Epoch:0	Loss:78.158	translation_Loss:78.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.42	Hits@10:34.09	Best:16.42
2024-12-27 19:13:49,470: Snapshot:0	Epoch:1	Loss:35.714	translation_Loss:35.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.18	Hits@10:43.16	Best:23.18
2024-12-27 19:13:55,641: Snapshot:0	Epoch:2	Loss:17.301	translation_Loss:17.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.22	Best:24.9
2024-12-27 19:14:01,756: Snapshot:0	Epoch:3	Loss:9.268	translation_Loss:9.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:45.55	Best:25.53
2024-12-27 19:14:07,934: Snapshot:0	Epoch:4	Loss:5.748	translation_Loss:5.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:45.55	Best:25.6
2024-12-27 19:14:14,081: Snapshot:0	Epoch:5	Loss:4.167	translation_Loss:4.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:45.36	Best:25.6
2024-12-27 19:14:20,117: Snapshot:0	Epoch:6	Loss:3.275	translation_Loss:3.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:45.07	Best:25.6
2024-12-27 19:14:26,277: Snapshot:0	Epoch:7	Loss:2.771	translation_Loss:2.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.26	Hits@10:45.05	Best:25.6
2024-12-27 19:14:32,820: Snapshot:0	Epoch:8	Loss:2.448	translation_Loss:2.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.03	Hits@10:44.68	Best:25.6
2024-12-27 19:14:38,870: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 25.6
2024-12-27 19:14:38,871: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:2.237 MRR:25.02 Best Results: 25.6
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:14:38,871: Snapshot:0	Epoch:9	Loss:2.237	translation_Loss:2.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.02	Hits@10:44.62	Best:25.6
2024-12-27 19:14:45,693: Snapshot:0	Epoch:10	Loss:68.2	translation_Loss:68.045	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.02	Hits@10:44.62	Best:25.6
2024-12-27 19:14:51,972: End of token training: 0 Epoch: 11 Loss:67.952 MRR:25.02 Best Results: 25.6
2024-12-27 19:14:51,973: Snapshot:0	Epoch:11	Loss:67.952	translation_Loss:67.952	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.02	Hits@10:44.62	Best:25.6
2024-12-27 19:14:52,230: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_5000/0model_best.tar'
2024-12-27 19:14:54,908: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2572 | 0.1528 | 0.3116 | 0.3738 |  0.4484 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:15:06,398: Snapshot:1	Epoch:0	Loss:20.558	translation_Loss:18.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.67                                                   	MRR:21.8	Hits@10:39.09	Best:21.8
2024-12-27 19:15:08,919: Snapshot:1	Epoch:1	Loss:9.121	translation_Loss:7.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.064                                                   	MRR:26.49	Hits@10:46.67	Best:26.49
2024-12-27 19:15:11,451: Snapshot:1	Epoch:2	Loss:6.263	translation_Loss:4.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.808                                                   	MRR:28.5	Hits@10:47.96	Best:28.5
2024-12-27 19:15:13,930: Snapshot:1	Epoch:3	Loss:5.192	translation_Loss:3.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.702                                                   	MRR:28.89	Hits@10:49.07	Best:28.89
2024-12-27 19:15:16,417: Snapshot:1	Epoch:4	Loss:4.695	translation_Loss:3.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.64                                                   	MRR:29.38	Hits@10:48.95	Best:29.38
2024-12-27 19:15:18,941: Snapshot:1	Epoch:5	Loss:4.399	translation_Loss:2.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.588                                                   	MRR:29.87	Hits@10:49.28	Best:29.87
2024-12-27 19:15:21,376: Snapshot:1	Epoch:6	Loss:4.233	translation_Loss:2.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.57                                                   	MRR:29.51	Hits@10:49.27	Best:29.87
2024-12-27 19:15:23,799: Snapshot:1	Epoch:7	Loss:4.108	translation_Loss:2.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.549                                                   	MRR:29.49	Hits@10:49.71	Best:29.87
2024-12-27 19:15:26,242: Snapshot:1	Epoch:8	Loss:4.036	translation_Loss:2.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.54                                                   	MRR:29.77	Hits@10:49.62	Best:29.87
2024-12-27 19:15:28,681: Snapshot:1	Epoch:9	Loss:3.948	translation_Loss:2.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.536                                                   	MRR:29.75	Hits@10:49.71	Best:29.87
2024-12-27 19:15:31,092: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 29.87
2024-12-27 19:15:31,092: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:3.915 MRR:29.76 Best Results: 29.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:15:31,092: Snapshot:1	Epoch:10	Loss:3.915	translation_Loss:2.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.526                                                   	MRR:29.76	Hits@10:50.29	Best:29.87
2024-12-27 19:15:33,420: Snapshot:1	Epoch:11	Loss:24.835	translation_Loss:24.683	multi_layer_Loss:0.152	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.76	Hits@10:50.29	Best:29.87
2024-12-27 19:15:35,766: End of token training: 1 Epoch: 12 Loss:24.658 MRR:29.76 Best Results: 29.87
2024-12-27 19:15:35,766: Snapshot:1	Epoch:12	Loss:24.658	translation_Loss:24.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.76	Hits@10:50.29	Best:29.87
2024-12-27 19:15:36,038: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_5000/1model_best.tar'
2024-12-27 19:15:39,831: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2567 | 0.153  | 0.3088 | 0.3744 |  0.4499 |
|     1      | 0.2956 | 0.1938 |  0.34  | 0.4084 |  0.4951 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:16:14,914: Snapshot:2	Epoch:0	Loss:66.257	translation_Loss:56.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.822                                                   	MRR:19.02	Hits@10:34.93	Best:19.02
2024-12-27 19:16:25,623: Snapshot:2	Epoch:1	Loss:34.88	translation_Loss:25.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.623                                                   	MRR:19.97	Hits@10:36.45	Best:19.97
2024-12-27 19:16:36,369: Snapshot:2	Epoch:2	Loss:31.263	translation_Loss:21.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.482                                                   	MRR:20.23	Hits@10:36.53	Best:20.23
2024-12-27 19:16:47,009: Snapshot:2	Epoch:3	Loss:29.753	translation_Loss:20.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.417                                                   	MRR:20.13	Hits@10:36.55	Best:20.23
2024-12-27 19:16:57,787: Snapshot:2	Epoch:4	Loss:29.198	translation_Loss:19.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.443                                                   	MRR:20.32	Hits@10:36.67	Best:20.32
2024-12-27 19:17:08,557: Snapshot:2	Epoch:5	Loss:28.655	translation_Loss:19.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.407                                                   	MRR:20.22	Hits@10:36.67	Best:20.32
2024-12-27 19:17:19,211: Snapshot:2	Epoch:6	Loss:28.463	translation_Loss:19.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.432                                                   	MRR:20.29	Hits@10:36.76	Best:20.32
2024-12-27 19:17:29,840: Snapshot:2	Epoch:7	Loss:28.16	translation_Loss:18.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.403                                                   	MRR:20.3	Hits@10:36.45	Best:20.32
2024-12-27 19:17:40,680: Snapshot:2	Epoch:8	Loss:28.084	translation_Loss:18.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.438                                                   	MRR:20.38	Hits@10:36.59	Best:20.38
2024-12-27 19:17:51,425: Snapshot:2	Epoch:9	Loss:27.893	translation_Loss:18.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.438                                                   	MRR:20.55	Hits@10:36.74	Best:20.55
2024-12-27 19:18:02,213: Snapshot:2	Epoch:10	Loss:27.729	translation_Loss:18.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.419                                                   	MRR:20.43	Hits@10:37.05	Best:20.55
2024-12-27 19:18:12,874: Snapshot:2	Epoch:11	Loss:27.661	translation_Loss:18.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.409                                                   	MRR:20.41	Hits@10:36.67	Best:20.55
2024-12-27 19:18:23,579: Snapshot:2	Epoch:12	Loss:27.612	translation_Loss:18.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.466                                                   	MRR:20.29	Hits@10:36.51	Best:20.55
2024-12-27 19:18:34,238: Snapshot:2	Epoch:13	Loss:27.432	translation_Loss:18.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.405                                                   	MRR:20.5	Hits@10:36.86	Best:20.55
2024-12-27 19:18:44,900: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 20.55
2024-12-27 19:18:44,900: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:27.323 MRR:20.33 Best Results: 20.55
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:18:44,900: Snapshot:2	Epoch:14	Loss:27.323	translation_Loss:17.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.394                                                   	MRR:20.33	Hits@10:36.7	Best:20.55
2024-12-27 19:18:55,285: Snapshot:2	Epoch:15	Loss:110.64	translation_Loss:110.494	multi_layer_Loss:0.146	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.33	Hits@10:36.7	Best:20.55
2024-12-27 19:19:05,705: End of token training: 2 Epoch: 16 Loss:110.527 MRR:20.33 Best Results: 20.55
2024-12-27 19:19:05,705: Snapshot:2	Epoch:16	Loss:110.527	translation_Loss:110.527	multi_layer_Loss:-0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.33	Hits@10:36.7	Best:20.55
2024-12-27 19:19:06,037: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_5000/2model_best.tar'
2024-12-27 19:19:14,430: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1384 | 0.3006 | 0.3659 |  0.4493 |
|     1      | 0.2755 | 0.1733 | 0.3166 | 0.3842 |  0.4727 |
|     2      | 0.2033 | 0.1204 | 0.2313 | 0.2892 |  0.3691 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:19:56,604: Snapshot:3	Epoch:0	Loss:68.06	translation_Loss:54.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.659                                                   	MRR:19.04	Hits@10:35.06	Best:19.04
2024-12-27 19:20:09,900: Snapshot:3	Epoch:1	Loss:41.017	translation_Loss:27.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.788                                                   	MRR:19.66	Hits@10:35.55	Best:19.66
2024-12-27 19:20:22,976: Snapshot:3	Epoch:2	Loss:38.834	translation_Loss:25.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.655                                                   	MRR:19.58	Hits@10:35.45	Best:19.66
2024-12-27 19:20:36,122: Snapshot:3	Epoch:3	Loss:38.066	translation_Loss:24.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.66                                                   	MRR:19.61	Hits@10:35.69	Best:19.66
2024-12-27 19:20:49,189: Snapshot:3	Epoch:4	Loss:37.53	translation_Loss:23.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.639                                                   	MRR:19.78	Hits@10:36.02	Best:19.78
2024-12-27 19:21:02,408: Snapshot:3	Epoch:5	Loss:37.188	translation_Loss:23.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.652                                                   	MRR:19.79	Hits@10:36.12	Best:19.79
2024-12-27 19:21:15,560: Snapshot:3	Epoch:6	Loss:37.011	translation_Loss:23.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.691                                                   	MRR:19.75	Hits@10:35.74	Best:19.79
2024-12-27 19:21:28,742: Snapshot:3	Epoch:7	Loss:36.951	translation_Loss:23.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.717                                                   	MRR:19.83	Hits@10:35.86	Best:19.83
2024-12-27 19:21:41,874: Snapshot:3	Epoch:8	Loss:36.784	translation_Loss:23.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.72                                                   	MRR:19.85	Hits@10:35.92	Best:19.85
2024-12-27 19:21:54,941: Snapshot:3	Epoch:9	Loss:36.593	translation_Loss:22.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.698                                                   	MRR:19.81	Hits@10:35.93	Best:19.85
2024-12-27 19:22:08,116: Snapshot:3	Epoch:10	Loss:36.552	translation_Loss:22.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.7                                                   	MRR:19.87	Hits@10:35.93	Best:19.87
2024-12-27 19:22:21,320: Snapshot:3	Epoch:11	Loss:36.439	translation_Loss:22.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.657                                                   	MRR:19.94	Hits@10:36.0	Best:19.94
2024-12-27 19:22:34,485: Snapshot:3	Epoch:12	Loss:36.41	translation_Loss:22.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.689                                                   	MRR:19.79	Hits@10:35.92	Best:19.94
2024-12-27 19:22:47,528: Snapshot:3	Epoch:13	Loss:36.282	translation_Loss:22.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.656                                                   	MRR:19.87	Hits@10:35.9	Best:19.94
2024-12-27 19:23:00,561: Snapshot:3	Epoch:14	Loss:36.326	translation_Loss:22.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.745                                                   	MRR:20.06	Hits@10:36.07	Best:20.06
2024-12-27 19:23:13,600: Snapshot:3	Epoch:15	Loss:36.292	translation_Loss:22.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.706                                                   	MRR:19.98	Hits@10:36.17	Best:20.06
2024-12-27 19:23:26,649: Snapshot:3	Epoch:16	Loss:36.202	translation_Loss:22.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.708                                                   	MRR:19.88	Hits@10:36.2	Best:20.06
2024-12-27 19:23:39,719: Snapshot:3	Epoch:17	Loss:36.174	translation_Loss:22.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.723                                                   	MRR:19.88	Hits@10:35.84	Best:20.06
2024-12-27 19:23:52,763: Snapshot:3	Epoch:18	Loss:36.113	translation_Loss:22.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.704                                                   	MRR:19.95	Hits@10:36.03	Best:20.06
2024-12-27 19:24:05,912: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 20.06
2024-12-27 19:24:05,912: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:36.049 MRR:19.96 Best Results: 20.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:24:05,912: Snapshot:3	Epoch:19	Loss:36.049	translation_Loss:22.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.657                                                   	MRR:19.96	Hits@10:35.98	Best:20.06
2024-12-27 19:24:18,488: Snapshot:3	Epoch:20	Loss:120.721	translation_Loss:120.567	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.96	Hits@10:35.98	Best:20.06
2024-12-27 19:24:31,207: End of token training: 3 Epoch: 21 Loss:120.488 MRR:19.96 Best Results: 20.06
2024-12-27 19:24:31,207: Snapshot:3	Epoch:21	Loss:120.488	translation_Loss:120.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.96	Hits@10:35.98	Best:20.06
2024-12-27 19:24:31,538: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_5000/3model_best.tar'
2024-12-27 19:24:44,984: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2507 | 0.1469 | 0.3014 | 0.3678 |  0.4494 |
|     1      | 0.2725 | 0.1709 | 0.3154 | 0.3854 |  0.4716 |
|     2      | 0.2031 | 0.1192 | 0.2329 | 0.2886 |  0.3682 |
|     3      | 0.2006 | 0.1159 | 0.2332 | 0.2892 |  0.3602 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:25:04,830: Snapshot:4	Epoch:0	Loss:32.314	translation_Loss:27.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.558                                                   	MRR:20.67	Hits@10:34.4	Best:20.67
2024-12-27 19:25:10,309: Snapshot:4	Epoch:1	Loss:19.815	translation_Loss:13.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.176                                                   	MRR:20.88	Hits@10:34.42	Best:20.88
2024-12-27 19:25:15,707: Snapshot:4	Epoch:2	Loss:18.402	translation_Loss:12.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.263                                                   	MRR:21.01	Hits@10:34.47	Best:21.01
2024-12-27 19:25:21,064: Snapshot:4	Epoch:3	Loss:18.061	translation_Loss:11.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.279                                                   	MRR:20.89	Hits@10:34.88	Best:21.01
2024-12-27 19:25:26,320: Snapshot:4	Epoch:4	Loss:17.958	translation_Loss:11.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.308                                                   	MRR:21.0	Hits@10:34.5	Best:21.01
2024-12-27 19:25:31,629: Snapshot:4	Epoch:5	Loss:17.958	translation_Loss:11.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.33                                                   	MRR:20.67	Hits@10:34.21	Best:21.01
2024-12-27 19:25:36,990: Snapshot:4	Epoch:6	Loss:17.897	translation_Loss:11.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.367                                                   	MRR:20.79	Hits@10:34.59	Best:21.01
2024-12-27 19:25:42,353: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 21.01
2024-12-27 19:25:42,353: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:17.877 MRR:20.84 Best Results: 21.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:25:42,353: Snapshot:4	Epoch:7	Loss:17.877	translation_Loss:11.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.348                                                   	MRR:20.84	Hits@10:34.26	Best:21.01
2024-12-27 19:25:47,497: Snapshot:4	Epoch:8	Loss:57.547	translation_Loss:57.385	multi_layer_Loss:0.162	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.84	Hits@10:34.26	Best:21.01
2024-12-27 19:25:52,670: End of token training: 4 Epoch: 9 Loss:57.463 MRR:20.84 Best Results: 21.01
2024-12-27 19:25:52,670: Snapshot:4	Epoch:9	Loss:57.463	translation_Loss:57.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.84	Hits@10:34.26	Best:21.01
2024-12-27 19:25:52,923: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_5000/4model_best.tar'
2024-12-27 19:26:09,689: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2243 | 0.1267 | 0.2677 | 0.3307 |  0.4081 |
|     1      | 0.2653 | 0.1667 | 0.3066 | 0.3703 |  0.4564 |
|     2      | 0.1955 | 0.114  | 0.2234 | 0.2765 |  0.3548 |
|     3      | 0.1898 | 0.1051 | 0.2209 | 0.2778 |  0.3511 |
|     4      | 0.2127 | 0.1433 | 0.2395 | 0.2866 |  0.3442 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:26:09,692: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2572 | 0.1528 | 0.3116 | 0.3738 |  0.4484 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2567 | 0.153  | 0.3088 | 0.3744 |  0.4499 |
|     1      | 0.2956 | 0.1938 |  0.34  | 0.4084 |  0.4951 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1384 | 0.3006 | 0.3659 |  0.4493 |
|     1      | 0.2755 | 0.1733 | 0.3166 | 0.3842 |  0.4727 |
|     2      | 0.2033 | 0.1204 | 0.2313 | 0.2892 |  0.3691 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2507 | 0.1469 | 0.3014 | 0.3678 |  0.4494 |
|     1      | 0.2725 | 0.1709 | 0.3154 | 0.3854 |  0.4716 |
|     2      | 0.2031 | 0.1192 | 0.2329 | 0.2886 |  0.3682 |
|     3      | 0.2006 | 0.1159 | 0.2332 | 0.2892 |  0.3602 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2243 | 0.1267 | 0.2677 | 0.3307 |  0.4081 |
|     1      | 0.2653 | 0.1667 | 0.3066 | 0.3703 |  0.4564 |
|     2      | 0.1955 | 0.114  | 0.2234 | 0.2765 |  0.3548 |
|     3      | 0.1898 | 0.1051 | 0.2209 | 0.2778 |  0.3511 |
|     4      | 0.2127 | 0.1433 | 0.2395 | 0.2866 |  0.3442 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:26:09,692: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  78.3514518737793  |   0.257   |    0.153     |    0.312     |     0.448     |
|    1     | 39.589014291763306 |   0.267   |    0.164     |    0.317     |     0.462     |
|    2     | 201.40453147888184 |   0.227   |    0.133     |    0.266     |      0.41     |
|    3     | 311.46801376342773 |   0.218   |    0.128     |    0.254     |      0.39     |
|    4     | 64.84908270835876  |   0.206   |    0.121     |    0.238     |     0.369     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:26:09,692: Sum_Training_Time:695.6620941162109
2024-12-27 19:26:09,692: Every_Training_Time:[78.3514518737793, 39.589014291763306, 201.40453147888184, 311.46801376342773, 64.84908270835876]
2024-12-27 19:26:09,692: Forward transfer: 0.041675000000000004 Backward transfer: -0.020449999999999996
2024-12-27 19:26:42,975: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227192613/HYBRIDHYBRID_0.001_512_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_512_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_512_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:26:52,770: Snapshot:0	Epoch:0	Loss:78.158	translation_Loss:78.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.42	Hits@10:34.08	Best:16.42
2024-12-27 19:26:58,885: Snapshot:0	Epoch:1	Loss:35.712	translation_Loss:35.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:43.19	Best:23.22
2024-12-27 19:27:05,067: Snapshot:0	Epoch:2	Loss:17.292	translation_Loss:17.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:45.21	Best:24.82
2024-12-27 19:27:11,137: Snapshot:0	Epoch:3	Loss:9.259	translation_Loss:9.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:45.62	Best:25.7
2024-12-27 19:27:17,257: Snapshot:0	Epoch:4	Loss:5.743	translation_Loss:5.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:45.67	Best:25.7
2024-12-27 19:27:23,319: Snapshot:0	Epoch:5	Loss:4.164	translation_Loss:4.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:45.27	Best:25.7
2024-12-27 19:27:29,436: Snapshot:0	Epoch:6	Loss:3.274	translation_Loss:3.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.26	Hits@10:45.03	Best:25.7
2024-12-27 19:27:35,522: Snapshot:0	Epoch:7	Loss:2.778	translation_Loss:2.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:45.04	Best:25.7
2024-12-27 19:27:42,063: Early Stopping! Snapshot: 0 Epoch: 8 Best Results: 25.7
2024-12-27 19:27:42,063: Start to training tokens! Snapshot: 0 Epoch: 8 Loss:2.439 MRR:25.39 Best Results: 25.7
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:27:42,063: Snapshot:0	Epoch:8	Loss:2.439	translation_Loss:2.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.39	Hits@10:44.77	Best:25.7
2024-12-27 19:27:48,817: Snapshot:0	Epoch:9	Loss:68.478	translation_Loss:68.324	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.39	Hits@10:44.77	Best:25.7
2024-12-27 19:27:55,108: End of token training: 0 Epoch: 10 Loss:68.346 MRR:25.39 Best Results: 25.7
2024-12-27 19:27:55,108: Snapshot:0	Epoch:10	Loss:68.346	translation_Loss:68.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.39	Hits@10:44.77	Best:25.7
2024-12-27 19:27:55,364: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_10000/0model_best.tar'
2024-12-27 19:27:57,881: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2552 | 0.149  | 0.3123 | 0.374  |  0.4462 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:28:09,287: Snapshot:1	Epoch:0	Loss:21.383	translation_Loss:19.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.001                                                   	MRR:20.88	Hits@10:38.05	Best:20.88
2024-12-27 19:28:11,753: Snapshot:1	Epoch:1	Loss:10.466	translation_Loss:8.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.149                                                   	MRR:26.28	Hits@10:46.43	Best:26.28
2024-12-27 19:28:14,218: Snapshot:1	Epoch:2	Loss:7.608	translation_Loss:5.75	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.858                                                   	MRR:28.22	Hits@10:48.24	Best:28.22
2024-12-27 19:28:16,684: Snapshot:1	Epoch:3	Loss:6.481	translation_Loss:4.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.759                                                   	MRR:28.67	Hits@10:49.05	Best:28.67
2024-12-27 19:28:19,151: Snapshot:1	Epoch:4	Loss:5.958	translation_Loss:4.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.707                                                   	MRR:28.92	Hits@10:49.34	Best:28.92
2024-12-27 19:28:21,608: Snapshot:1	Epoch:5	Loss:5.635	translation_Loss:3.967	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.668                                                   	MRR:29.19	Hits@10:49.64	Best:29.19
2024-12-27 19:28:24,143: Snapshot:1	Epoch:6	Loss:5.431	translation_Loss:3.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.646                                                   	MRR:29.4	Hits@10:49.84	Best:29.4
2024-12-27 19:28:26,688: Snapshot:1	Epoch:7	Loss:5.271	translation_Loss:3.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.627                                                   	MRR:29.42	Hits@10:50.29	Best:29.42
2024-12-27 19:28:29,213: Snapshot:1	Epoch:8	Loss:5.19	translation_Loss:3.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.617                                                   	MRR:29.51	Hits@10:50.14	Best:29.51
2024-12-27 19:28:31,740: Snapshot:1	Epoch:9	Loss:5.1	translation_Loss:3.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.605                                                   	MRR:29.98	Hits@10:50.06	Best:29.98
2024-12-27 19:28:34,184: Snapshot:1	Epoch:10	Loss:5.094	translation_Loss:3.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.626                                                   	MRR:29.69	Hits@10:49.88	Best:29.98
2024-12-27 19:28:36,608: Snapshot:1	Epoch:11	Loss:4.994	translation_Loss:3.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.589                                                   	MRR:29.88	Hits@10:50.32	Best:29.98
2024-12-27 19:28:39,048: Snapshot:1	Epoch:12	Loss:4.956	translation_Loss:3.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.584                                                   	MRR:29.58	Hits@10:50.06	Best:29.98
2024-12-27 19:28:41,483: Snapshot:1	Epoch:13	Loss:4.897	translation_Loss:3.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.598                                                   	MRR:29.7	Hits@10:50.31	Best:29.98
2024-12-27 19:28:43,888: Early Stopping! Snapshot: 1 Epoch: 14 Best Results: 29.98
2024-12-27 19:28:43,888: Start to training tokens! Snapshot: 1 Epoch: 14 Loss:4.839 MRR:29.7 Best Results: 29.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:28:43,889: Snapshot:1	Epoch:14	Loss:4.839	translation_Loss:3.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.573                                                   	MRR:29.7	Hits@10:50.11	Best:29.98
2024-12-27 19:28:46,235: Snapshot:1	Epoch:15	Loss:25.556	translation_Loss:25.404	multi_layer_Loss:0.152	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.7	Hits@10:50.11	Best:29.98
2024-12-27 19:28:48,595: End of token training: 1 Epoch: 16 Loss:25.406 MRR:29.7 Best Results: 29.98
2024-12-27 19:28:48,596: Snapshot:1	Epoch:16	Loss:25.406	translation_Loss:25.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.7	Hits@10:50.11	Best:29.98
2024-12-27 19:28:48,924: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_10000/1model_best.tar'
2024-12-27 19:28:52,748: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.254  | 0.149  | 0.3087 | 0.3724 |  0.4463 |
|     1      | 0.2915 | 0.1869 | 0.3394 | 0.4054 |  0.4947 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:29:27,917: Snapshot:2	Epoch:0	Loss:70.709	translation_Loss:60.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.5                                                   	MRR:18.67	Hits@10:34.6	Best:18.67
2024-12-27 19:29:38,604: Snapshot:2	Epoch:1	Loss:39.509	translation_Loss:29.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.076                                                   	MRR:19.67	Hits@10:35.82	Best:19.67
2024-12-27 19:29:49,418: Snapshot:2	Epoch:2	Loss:35.69	translation_Loss:25.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.964                                                   	MRR:19.88	Hits@10:35.97	Best:19.88
2024-12-27 19:30:00,183: Snapshot:2	Epoch:3	Loss:34.141	translation_Loss:24.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.897                                                   	MRR:19.97	Hits@10:36.21	Best:19.97
2024-12-27 19:30:11,201: Snapshot:2	Epoch:4	Loss:33.366	translation_Loss:23.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.906                                                   	MRR:19.95	Hits@10:36.11	Best:19.97
2024-12-27 19:30:21,943: Snapshot:2	Epoch:5	Loss:32.857	translation_Loss:22.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.894                                                   	MRR:20.03	Hits@10:36.06	Best:20.03
2024-12-27 19:30:32,647: Snapshot:2	Epoch:6	Loss:32.433	translation_Loss:22.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.84                                                   	MRR:20.06	Hits@10:36.29	Best:20.06
2024-12-27 19:30:43,431: Snapshot:2	Epoch:7	Loss:32.178	translation_Loss:22.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.819                                                   	MRR:20.13	Hits@10:36.37	Best:20.13
2024-12-27 19:30:54,243: Snapshot:2	Epoch:8	Loss:32.058	translation_Loss:22.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.848                                                   	MRR:20.19	Hits@10:36.42	Best:20.19
2024-12-27 19:31:05,008: Snapshot:2	Epoch:9	Loss:31.867	translation_Loss:21.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.873                                                   	MRR:20.15	Hits@10:36.34	Best:20.19
2024-12-27 19:31:15,717: Snapshot:2	Epoch:10	Loss:31.657	translation_Loss:21.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.848                                                   	MRR:20.06	Hits@10:36.31	Best:20.19
2024-12-27 19:31:26,484: Snapshot:2	Epoch:11	Loss:31.577	translation_Loss:21.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.832                                                   	MRR:20.12	Hits@10:36.22	Best:20.19
2024-12-27 19:31:37,237: Snapshot:2	Epoch:12	Loss:31.445	translation_Loss:21.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.86                                                   	MRR:20.21	Hits@10:36.27	Best:20.21
2024-12-27 19:31:47,984: Snapshot:2	Epoch:13	Loss:31.37	translation_Loss:21.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.852                                                   	MRR:20.17	Hits@10:36.26	Best:20.21
2024-12-27 19:31:58,610: Snapshot:2	Epoch:14	Loss:31.232	translation_Loss:21.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.819                                                   	MRR:19.98	Hits@10:36.14	Best:20.21
2024-12-27 19:32:09,239: Snapshot:2	Epoch:15	Loss:31.21	translation_Loss:21.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.827                                                   	MRR:20.08	Hits@10:36.2	Best:20.21
2024-12-27 19:32:19,876: Snapshot:2	Epoch:16	Loss:31.17	translation_Loss:21.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.825                                                   	MRR:20.07	Hits@10:36.38	Best:20.21
2024-12-27 19:32:30,517: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 20.21
2024-12-27 19:32:30,517: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:31.115 MRR:20.08 Best Results: 20.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:32:30,517: Snapshot:2	Epoch:17	Loss:31.115	translation_Loss:21.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.814                                                   	MRR:20.08	Hits@10:36.41	Best:20.21
2024-12-27 19:32:40,996: Snapshot:2	Epoch:18	Loss:114.344	translation_Loss:114.199	multi_layer_Loss:0.146	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.08	Hits@10:36.41	Best:20.21
2024-12-27 19:32:51,444: End of token training: 2 Epoch: 19 Loss:114.163 MRR:20.08 Best Results: 20.21
2024-12-27 19:32:51,445: Snapshot:2	Epoch:19	Loss:114.163	translation_Loss:114.163	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.08	Hits@10:36.41	Best:20.21
2024-12-27 19:32:51,774: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_10000/2model_best.tar'
2024-12-27 19:33:00,509: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2439 | 0.1353 | 0.299  | 0.3665 |  0.4451 |
|     1      | 0.2795 | 0.1738 | 0.3263 | 0.3952 |  0.4895 |
|     2      | 0.2007 | 0.1186 | 0.2299 | 0.2855 |  0.3619 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:33:41,752: Snapshot:3	Epoch:0	Loss:75.302	translation_Loss:60.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.389                                                   	MRR:18.43	Hits@10:34.04	Best:18.43
2024-12-27 19:33:54,847: Snapshot:3	Epoch:1	Loss:48.572	translation_Loss:34.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.273                                                   	MRR:18.94	Hits@10:34.68	Best:18.94
2024-12-27 19:34:08,403: Snapshot:3	Epoch:2	Loss:46.101	translation_Loss:31.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.173                                                   	MRR:19.11	Hits@10:34.94	Best:19.11
2024-12-27 19:34:21,607: Snapshot:3	Epoch:3	Loss:45.052	translation_Loss:30.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.147                                                   	MRR:19.21	Hits@10:35.22	Best:19.21
2024-12-27 19:34:34,812: Snapshot:3	Epoch:4	Loss:44.537	translation_Loss:30.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.137                                                   	MRR:19.33	Hits@10:35.13	Best:19.33
2024-12-27 19:34:48,022: Snapshot:3	Epoch:5	Loss:44.188	translation_Loss:30.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.11                                                   	MRR:19.35	Hits@10:35.15	Best:19.35
2024-12-27 19:35:01,163: Snapshot:3	Epoch:6	Loss:43.938	translation_Loss:29.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.1                                                   	MRR:19.32	Hits@10:35.01	Best:19.35
2024-12-27 19:35:14,373: Snapshot:3	Epoch:7	Loss:43.646	translation_Loss:29.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.061                                                   	MRR:19.42	Hits@10:35.15	Best:19.42
2024-12-27 19:35:27,449: Snapshot:3	Epoch:8	Loss:43.593	translation_Loss:29.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.105                                                   	MRR:19.35	Hits@10:35.16	Best:19.42
2024-12-27 19:35:40,583: Snapshot:3	Epoch:9	Loss:43.442	translation_Loss:29.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.086                                                   	MRR:19.46	Hits@10:35.25	Best:19.46
2024-12-27 19:35:53,697: Snapshot:3	Epoch:10	Loss:43.269	translation_Loss:29.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.085                                                   	MRR:19.43	Hits@10:35.19	Best:19.46
2024-12-27 19:36:06,921: Snapshot:3	Epoch:11	Loss:43.233	translation_Loss:29.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.137                                                   	MRR:19.38	Hits@10:35.04	Best:19.46
2024-12-27 19:36:20,419: Snapshot:3	Epoch:12	Loss:43.166	translation_Loss:29.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.108                                                   	MRR:19.41	Hits@10:35.36	Best:19.46
2024-12-27 19:36:33,451: Snapshot:3	Epoch:13	Loss:43.095	translation_Loss:29.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.089                                                   	MRR:19.39	Hits@10:35.47	Best:19.46
2024-12-27 19:36:46,486: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 19.46
2024-12-27 19:36:46,486: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:42.948 MRR:19.43 Best Results: 19.46
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:36:46,487: Snapshot:3	Epoch:14	Loss:42.948	translation_Loss:28.864	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.085                                                   	MRR:19.43	Hits@10:35.2	Best:19.46
2024-12-27 19:36:59,111: Snapshot:3	Epoch:15	Loss:126.091	translation_Loss:125.937	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.43	Hits@10:35.2	Best:19.46
2024-12-27 19:37:11,840: End of token training: 3 Epoch: 16 Loss:125.867 MRR:19.43 Best Results: 19.46
2024-12-27 19:37:11,840: Snapshot:3	Epoch:16	Loss:125.867	translation_Loss:125.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.43	Hits@10:35.2	Best:19.46
2024-12-27 19:37:12,124: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_10000/3model_best.tar'
2024-12-27 19:37:26,160: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1444 | 0.3034 | 0.3683 |  0.4444 |
|     1      | 0.2792 | 0.1749 | 0.3231 | 0.3934 |  0.4829 |
|     2      | 0.2006 | 0.1175 | 0.2295 | 0.2861 |  0.3632 |
|     3      | 0.1951 | 0.1103 | 0.2299 | 0.2854 |  0.3538 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:37:45,809: Snapshot:4	Epoch:0	Loss:34.983	translation_Loss:29.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.394                                                   	MRR:19.76	Hits@10:33.29	Best:19.76
2024-12-27 19:37:51,173: Snapshot:4	Epoch:1	Loss:23.642	translation_Loss:17.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.596                                                   	MRR:20.95	Hits@10:33.65	Best:20.95
2024-12-27 19:37:56,537: Snapshot:4	Epoch:2	Loss:22.26	translation_Loss:15.678	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.582                                                   	MRR:20.88	Hits@10:33.59	Best:20.95
2024-12-27 19:38:01,856: Snapshot:4	Epoch:3	Loss:22.022	translation_Loss:15.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.569                                                   	MRR:20.51	Hits@10:33.24	Best:20.95
2024-12-27 19:38:07,148: Snapshot:4	Epoch:4	Loss:21.891	translation_Loss:15.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.563                                                   	MRR:20.49	Hits@10:33.27	Best:20.95
2024-12-27 19:38:12,441: Snapshot:4	Epoch:5	Loss:21.888	translation_Loss:15.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.581                                                   	MRR:20.73	Hits@10:33.41	Best:20.95
2024-12-27 19:38:17,791: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 20.95
2024-12-27 19:38:17,791: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:21.83 MRR:20.82 Best Results: 20.95
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:38:17,792: Snapshot:4	Epoch:6	Loss:21.83	translation_Loss:15.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.596                                                   	MRR:20.82	Hits@10:33.46	Best:20.95
2024-12-27 19:38:22,902: Snapshot:4	Epoch:7	Loss:60.193	translation_Loss:60.031	multi_layer_Loss:0.162	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.82	Hits@10:33.46	Best:20.95
2024-12-27 19:38:28,060: End of token training: 4 Epoch: 8 Loss:60.03 MRR:20.82 Best Results: 20.95
2024-12-27 19:38:28,061: Snapshot:4	Epoch:8	Loss:60.03	translation_Loss:60.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.82	Hits@10:33.46	Best:20.95
2024-12-27 19:38:28,392: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_512_10000/4model_best.tar'
2024-12-27 19:38:44,679: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2261 | 0.1262 | 0.2723 | 0.3358 |  0.4094 |
|     1      | 0.2748 | 0.1743 | 0.3163 | 0.3818 |  0.4713 |
|     2      | 0.195  | 0.1142 | 0.2225 | 0.2763 |  0.3518 |
|     3      | 0.185  | 0.0998 | 0.2179 | 0.2747 |  0.3458 |
|     4      |  0.21  | 0.1427 | 0.238  | 0.2861 |  0.3385 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:38:44,682: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2552 | 0.149  | 0.3123 | 0.374  |  0.4462 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.254  | 0.149  | 0.3087 | 0.3724 |  0.4463 |
|     1      | 0.2915 | 0.1869 | 0.3394 | 0.4054 |  0.4947 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2439 | 0.1353 | 0.299  | 0.3665 |  0.4451 |
|     1      | 0.2795 | 0.1738 | 0.3263 | 0.3952 |  0.4895 |
|     2      | 0.2007 | 0.1186 | 0.2299 | 0.2855 |  0.3619 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1444 | 0.3034 | 0.3683 |  0.4444 |
|     1      | 0.2792 | 0.1749 | 0.3231 | 0.3934 |  0.4829 |
|     2      | 0.2006 | 0.1175 | 0.2295 | 0.2861 |  0.3632 |
|     3      | 0.1951 | 0.1103 | 0.2299 | 0.2854 |  0.3538 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2261 | 0.1262 | 0.2723 | 0.3358 |  0.4094 |
|     1      | 0.2748 | 0.1743 | 0.3163 | 0.3818 |  0.4713 |
|     2      | 0.195  | 0.1142 | 0.2225 | 0.2763 |  0.3518 |
|     3      | 0.185  | 0.0998 | 0.2179 | 0.2747 |  0.3458 |
|     4      |  0.21  | 0.1427 | 0.238  | 0.2861 |  0.3385 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:38:44,683: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  72.132488489151   |   0.255   |    0.149     |    0.312     |     0.446     |
|    1     | 49.450557231903076 |   0.264   |    0.159     |    0.317     |     0.459     |
|    2     | 234.2361512184143  |   0.226   |    0.131     |    0.266     |     0.407     |
|    3     | 246.0529761314392  |   0.215   |    0.125     |    0.253     |     0.386     |
|    4     | 59.355271339416504 |   0.205   |    0.119     |    0.239     |     0.367     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:38:44,683: Sum_Training_Time:661.2274444103241
2024-12-27 19:38:44,683: Every_Training_Time:[72.132488489151, 49.450557231903076, 234.2361512184143, 246.0529761314392, 59.355271339416504]
2024-12-27 19:38:44,683: Forward transfer: 0.04252499999999999 Backward transfer: -0.01539999999999999
2024-12-27 19:39:18,535: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227193848/HYBRIDHYBRID_0.001_1024_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_1024_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_1024_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:39:28,110: Snapshot:0	Epoch:0	Loss:42.202	translation_Loss:42.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.75	Hits@10:28.15	Best:11.75
2024-12-27 19:39:34,148: Snapshot:0	Epoch:1	Loss:22.665	translation_Loss:22.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.73	Hits@10:41.15	Best:21.73
2024-12-27 19:39:40,100: Snapshot:0	Epoch:2	Loss:11.298	translation_Loss:11.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.17	Hits@10:44.78	Best:24.17
2024-12-27 19:39:46,082: Snapshot:0	Epoch:3	Loss:6.107	translation_Loss:6.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.81	Best:25.33
2024-12-27 19:39:52,074: Snapshot:0	Epoch:4	Loss:3.713	translation_Loss:3.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.09	Best:25.69
2024-12-27 19:39:58,016: Snapshot:0	Epoch:5	Loss:2.583	translation_Loss:2.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.3	Best:25.78
2024-12-27 19:40:04,454: Snapshot:0	Epoch:6	Loss:1.947	translation_Loss:1.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:45.99	Best:25.84
2024-12-27 19:40:10,456: Snapshot:0	Epoch:7	Loss:1.59	translation_Loss:1.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:45.89	Best:25.84
2024-12-27 19:40:16,474: Snapshot:0	Epoch:8	Loss:1.355	translation_Loss:1.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:45.77	Best:25.84
2024-12-27 19:40:22,399: Snapshot:0	Epoch:9	Loss:1.216	translation_Loss:1.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:45.33	Best:25.84
2024-12-27 19:40:28,367: Snapshot:0	Epoch:10	Loss:1.089	translation_Loss:1.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:45.38	Best:25.84
2024-12-27 19:40:34,279: Early Stopping! Snapshot: 0 Epoch: 11 Best Results: 25.84
2024-12-27 19:40:34,280: Start to training tokens! Snapshot: 0 Epoch: 11 Loss:0.997 MRR:25.5 Best Results: 25.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:40:34,280: Snapshot:0	Epoch:11	Loss:0.997	translation_Loss:0.997	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.5	Hits@10:45.11	Best:25.84
2024-12-27 19:40:40,944: Snapshot:0	Epoch:12	Loss:34.267	translation_Loss:34.113	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.5	Hits@10:45.11	Best:25.84
2024-12-27 19:40:46,994: End of token training: 0 Epoch: 13 Loss:34.123 MRR:25.5 Best Results: 25.84
2024-12-27 19:40:46,994: Snapshot:0	Epoch:13	Loss:34.123	translation_Loss:34.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.5	Hits@10:45.11	Best:25.84
2024-12-27 19:40:47,305: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_1000/0model_best.tar'
2024-12-27 19:40:50,094: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2577 | 0.1518 | 0.315  | 0.3771 |  0.4507 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:41:00,994: Snapshot:1	Epoch:0	Loss:11.498	translation_Loss:11.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.383                                                   	MRR:17.23	Hits@10:31.68	Best:17.23
2024-12-27 19:41:03,315: Snapshot:1	Epoch:1	Loss:4.476	translation_Loss:3.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.69                                                   	MRR:25.85	Hits@10:44.53	Best:25.85
2024-12-27 19:41:05,662: Snapshot:1	Epoch:2	Loss:2.439	translation_Loss:1.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.8                                                   	MRR:27.0	Hits@10:46.67	Best:27.0
2024-12-27 19:41:07,977: Snapshot:1	Epoch:3	Loss:1.673	translation_Loss:0.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.784                                                   	MRR:27.89	Hits@10:47.04	Best:27.89
2024-12-27 19:41:10,345: Snapshot:1	Epoch:4	Loss:1.355	translation_Loss:0.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.738                                                   	MRR:28.03	Hits@10:47.75	Best:28.03
2024-12-27 19:41:12,736: Snapshot:1	Epoch:5	Loss:1.193	translation_Loss:0.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.703                                                   	MRR:28.38	Hits@10:48.11	Best:28.38
2024-12-27 19:41:15,077: Snapshot:1	Epoch:6	Loss:1.105	translation_Loss:0.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.676                                                   	MRR:29.03	Hits@10:49.02	Best:29.03
2024-12-27 19:41:17,458: Snapshot:1	Epoch:7	Loss:1.055	translation_Loss:0.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.651                                                   	MRR:29.54	Hits@10:49.46	Best:29.54
2024-12-27 19:41:19,793: Snapshot:1	Epoch:8	Loss:0.99	translation_Loss:0.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.627                                                   	MRR:29.68	Hits@10:49.82	Best:29.68
2024-12-27 19:41:22,163: Snapshot:1	Epoch:9	Loss:0.96	translation_Loss:0.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.614                                                   	MRR:30.03	Hits@10:50.17	Best:30.03
2024-12-27 19:41:24,530: Snapshot:1	Epoch:10	Loss:0.929	translation_Loss:0.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.597                                                   	MRR:30.38	Hits@10:50.42	Best:30.38
2024-12-27 19:41:26,863: Snapshot:1	Epoch:11	Loss:0.907	translation_Loss:0.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.586                                                   	MRR:30.16	Hits@10:50.78	Best:30.38
2024-12-27 19:41:29,609: Snapshot:1	Epoch:12	Loss:0.899	translation_Loss:0.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.579                                                   	MRR:30.37	Hits@10:50.47	Best:30.38
2024-12-27 19:41:31,937: Snapshot:1	Epoch:13	Loss:0.875	translation_Loss:0.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.572                                                   	MRR:30.45	Hits@10:51.07	Best:30.45
2024-12-27 19:41:34,268: Snapshot:1	Epoch:14	Loss:0.86	translation_Loss:0.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.56                                                   	MRR:30.44	Hits@10:51.09	Best:30.45
2024-12-27 19:41:36,553: Snapshot:1	Epoch:15	Loss:0.851	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.559                                                   	MRR:30.13	Hits@10:50.74	Best:30.45
2024-12-27 19:41:38,899: Snapshot:1	Epoch:16	Loss:0.84	translation_Loss:0.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.55                                                   	MRR:30.72	Hits@10:51.21	Best:30.72
2024-12-27 19:41:41,215: Snapshot:1	Epoch:17	Loss:0.835	translation_Loss:0.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.547                                                   	MRR:30.61	Hits@10:50.85	Best:30.72
2024-12-27 19:41:43,547: Snapshot:1	Epoch:18	Loss:0.833	translation_Loss:0.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.544                                                   	MRR:30.44	Hits@10:50.71	Best:30.72
2024-12-27 19:41:45,868: Snapshot:1	Epoch:19	Loss:0.823	translation_Loss:0.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.537                                                   	MRR:30.38	Hits@10:50.78	Best:30.72
2024-12-27 19:41:48,144: Snapshot:1	Epoch:20	Loss:0.825	translation_Loss:0.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.542                                                   	MRR:30.59	Hits@10:51.16	Best:30.72
2024-12-27 19:41:50,419: Early Stopping! Snapshot: 1 Epoch: 21 Best Results: 30.72
2024-12-27 19:41:50,419: Start to training tokens! Snapshot: 1 Epoch: 21 Loss:0.802 MRR:30.39 Best Results: 30.72
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:41:50,420: Snapshot:1	Epoch:21	Loss:0.802	translation_Loss:0.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.537                                                   	MRR:30.39	Hits@10:51.53	Best:30.72
2024-12-27 19:41:52,698: Snapshot:1	Epoch:22	Loss:11.876	translation_Loss:11.727	multi_layer_Loss:0.149	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.39	Hits@10:51.53	Best:30.72
2024-12-27 19:41:54,965: End of token training: 1 Epoch: 23 Loss:11.733 MRR:30.39 Best Results: 30.72
2024-12-27 19:41:54,965: Snapshot:1	Epoch:23	Loss:11.733	translation_Loss:11.729	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.39	Hits@10:51.53	Best:30.72
2024-12-27 19:41:55,224: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_1000/1model_best.tar'
2024-12-27 19:41:58,999: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2487 | 0.1423 | 0.3021 | 0.3682 |  0.4454 |
|     1      | 0.3054 | 0.199  | 0.3546 | 0.423  |  0.5119 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:42:32,870: Snapshot:2	Epoch:0	Loss:34.91	translation_Loss:31.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.951                                                   	MRR:19.17	Hits@10:35.7	Best:19.17
2024-12-27 19:42:43,035: Snapshot:2	Epoch:1	Loss:13.427	translation_Loss:9.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.92                                                   	MRR:20.66	Hits@10:37.26	Best:20.66
2024-12-27 19:42:53,217: Snapshot:2	Epoch:2	Loss:10.508	translation_Loss:6.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.88                                                   	MRR:21.03	Hits@10:37.95	Best:21.03
2024-12-27 19:43:03,454: Snapshot:2	Epoch:3	Loss:9.741	translation_Loss:5.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.897                                                   	MRR:21.14	Hits@10:37.99	Best:21.14
2024-12-27 19:43:13,634: Snapshot:2	Epoch:4	Loss:9.358	translation_Loss:5.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.887                                                   	MRR:21.29	Hits@10:37.98	Best:21.29
2024-12-27 19:43:23,839: Snapshot:2	Epoch:5	Loss:9.159	translation_Loss:5.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.898                                                   	MRR:21.36	Hits@10:38.37	Best:21.36
2024-12-27 19:43:34,027: Snapshot:2	Epoch:6	Loss:9.04	translation_Loss:5.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.904                                                   	MRR:21.47	Hits@10:38.32	Best:21.47
2024-12-27 19:43:44,168: Snapshot:2	Epoch:7	Loss:8.964	translation_Loss:5.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.907                                                   	MRR:21.42	Hits@10:38.09	Best:21.47
2024-12-27 19:43:54,467: Snapshot:2	Epoch:8	Loss:8.88	translation_Loss:4.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.909                                                   	MRR:21.25	Hits@10:38.22	Best:21.47
2024-12-27 19:44:04,689: Snapshot:2	Epoch:9	Loss:8.838	translation_Loss:4.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.897                                                   	MRR:21.43	Hits@10:38.16	Best:21.47
2024-12-27 19:44:15,248: Snapshot:2	Epoch:10	Loss:8.784	translation_Loss:4.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.904                                                   	MRR:21.43	Hits@10:38.18	Best:21.47
2024-12-27 19:44:25,412: Snapshot:2	Epoch:11	Loss:8.709	translation_Loss:4.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.882                                                   	MRR:21.49	Hits@10:38.43	Best:21.49
2024-12-27 19:44:35,718: Snapshot:2	Epoch:12	Loss:8.699	translation_Loss:4.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.915                                                   	MRR:21.5	Hits@10:38.4	Best:21.5
2024-12-27 19:44:45,926: Snapshot:2	Epoch:13	Loss:8.742	translation_Loss:4.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.947                                                   	MRR:21.42	Hits@10:38.19	Best:21.5
2024-12-27 19:44:56,111: Snapshot:2	Epoch:14	Loss:8.642	translation_Loss:4.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.908                                                   	MRR:21.33	Hits@10:38.22	Best:21.5
2024-12-27 19:45:06,445: Snapshot:2	Epoch:15	Loss:8.626	translation_Loss:4.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.928                                                   	MRR:21.34	Hits@10:38.08	Best:21.5
2024-12-27 19:45:17,096: Snapshot:2	Epoch:16	Loss:8.606	translation_Loss:4.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.906                                                   	MRR:21.49	Hits@10:38.32	Best:21.5
2024-12-27 19:45:27,235: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 21.5
2024-12-27 19:45:27,235: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:8.613 MRR:21.44 Best Results: 21.5
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:45:27,235: Snapshot:2	Epoch:17	Loss:8.613	translation_Loss:4.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.925                                                   	MRR:21.44	Hits@10:38.58	Best:21.5
2024-12-27 19:45:37,225: Snapshot:2	Epoch:18	Loss:51.38	translation_Loss:51.234	multi_layer_Loss:0.146	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.44	Hits@10:38.58	Best:21.5
2024-12-27 19:45:47,187: End of token training: 2 Epoch: 19 Loss:51.272 MRR:21.44 Best Results: 21.5
2024-12-27 19:45:47,187: Snapshot:2	Epoch:19	Loss:51.272	translation_Loss:51.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.44	Hits@10:38.58	Best:21.5
2024-12-27 19:45:47,445: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_1000/2model_best.tar'
2024-12-27 19:45:55,699: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2369 | 0.1317 | 0.2827 | 0.3532 |  0.4421 |
|     1      | 0.2725 | 0.175  | 0.3093 | 0.3709 |  0.4643 |
|     2      | 0.216  | 0.1292 | 0.2453 | 0.307  |  0.386  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:46:35,572: Snapshot:3	Epoch:0	Loss:31.494	translation_Loss:27.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.82                                                   	MRR:19.88	Hits@10:36.92	Best:19.88
2024-12-27 19:46:47,970: Snapshot:3	Epoch:1	Loss:12.514	translation_Loss:7.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.613                                                   	MRR:20.89	Hits@10:38.01	Best:20.89
2024-12-27 19:47:00,461: Snapshot:3	Epoch:2	Loss:10.77	translation_Loss:6.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.51                                                   	MRR:21.13	Hits@10:38.15	Best:21.13
2024-12-27 19:47:12,856: Snapshot:3	Epoch:3	Loss:10.325	translation_Loss:5.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.528                                                   	MRR:20.98	Hits@10:38.34	Best:21.13
2024-12-27 19:47:25,274: Snapshot:3	Epoch:4	Loss:10.141	translation_Loss:5.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.548                                                   	MRR:20.9	Hits@10:38.16	Best:21.13
2024-12-27 19:47:37,718: Snapshot:3	Epoch:5	Loss:10.028	translation_Loss:5.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.561                                                   	MRR:20.99	Hits@10:38.1	Best:21.13
2024-12-27 19:47:50,168: Snapshot:3	Epoch:6	Loss:9.885	translation_Loss:5.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.549                                                   	MRR:20.93	Hits@10:38.13	Best:21.13
2024-12-27 19:48:02,641: Early Stopping! Snapshot: 3 Epoch: 7 Best Results: 21.13
2024-12-27 19:48:02,641: Start to training tokens! Snapshot: 3 Epoch: 7 Loss:9.85 MRR:21.1 Best Results: 21.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:48:02,642: Snapshot:3	Epoch:7	Loss:9.85	translation_Loss:5.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.566                                                   	MRR:21.1	Hits@10:38.2	Best:21.13
2024-12-27 19:48:14,734: Snapshot:3	Epoch:8	Loss:55.037	translation_Loss:54.883	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.1	Hits@10:38.2	Best:21.13
2024-12-27 19:48:26,859: End of token training: 3 Epoch: 9 Loss:54.974 MRR:21.1 Best Results: 21.13
2024-12-27 19:48:26,860: Snapshot:3	Epoch:9	Loss:54.974	translation_Loss:54.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.1	Hits@10:38.2	Best:21.13
2024-12-27 19:48:27,186: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_1000/3model_best.tar'
2024-12-27 19:48:41,318: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1474 | 0.296  | 0.3589 |  0.4426 |
|     1      | 0.2694 | 0.1722 | 0.3072 | 0.3651 |  0.4566 |
|     2      | 0.2121 | 0.1253 | 0.2406 | 0.3014 |  0.3832 |
|     3      | 0.2106 | 0.1203 | 0.2452 | 0.3048 |  0.3818 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:49:00,210: Snapshot:4	Epoch:0	Loss:15.071	translation_Loss:14.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.939                                                   	MRR:18.06	Hits@10:34.1	Best:18.06
2024-12-27 19:49:05,373: Snapshot:4	Epoch:1	Loss:6.14	translation_Loss:4.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.74                                                   	MRR:21.46	Hits@10:35.84	Best:21.46
2024-12-27 19:49:10,460: Snapshot:4	Epoch:2	Loss:3.959	translation_Loss:1.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.978                                                   	MRR:22.08	Hits@10:36.72	Best:22.08
2024-12-27 19:49:15,590: Snapshot:4	Epoch:3	Loss:3.376	translation_Loss:1.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.986                                                   	MRR:22.05	Hits@10:36.72	Best:22.08
2024-12-27 19:49:20,726: Snapshot:4	Epoch:4	Loss:3.19	translation_Loss:1.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.974                                                   	MRR:21.79	Hits@10:36.9	Best:22.08
2024-12-27 19:49:25,852: Snapshot:4	Epoch:5	Loss:3.104	translation_Loss:1.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.951                                                   	MRR:21.74	Hits@10:37.08	Best:22.08
2024-12-27 19:49:30,899: Snapshot:4	Epoch:6	Loss:3.091	translation_Loss:1.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.957                                                   	MRR:21.08	Hits@10:36.97	Best:22.08
2024-12-27 19:49:35,980: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 22.08
2024-12-27 19:49:35,980: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:3.072 MRR:21.84 Best Results: 22.08
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:49:35,980: Snapshot:4	Epoch:7	Loss:3.072	translation_Loss:1.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.965                                                   	MRR:21.84	Hits@10:37.2	Best:22.08
2024-12-27 19:49:40,970: Snapshot:4	Epoch:8	Loss:25.223	translation_Loss:25.062	multi_layer_Loss:0.161	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.84	Hits@10:37.2	Best:22.08
2024-12-27 19:49:45,901: End of token training: 4 Epoch: 9 Loss:25.069 MRR:21.84 Best Results: 22.08
2024-12-27 19:49:45,902: Snapshot:4	Epoch:9	Loss:25.069	translation_Loss:25.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.84	Hits@10:37.2	Best:22.08
2024-12-27 19:49:46,103: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_1000/4model_best.tar'
2024-12-27 19:50:02,272: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2036 | 0.1128 | 0.2403 | 0.2978 |  0.3761 |
|     1      | 0.2438 | 0.1547 | 0.2698 | 0.3365 |  0.426  |
|     2      | 0.1928 | 0.1129 | 0.2177 | 0.2696 |  0.3479 |
|     3      | 0.1708 | 0.0896 | 0.1974 |  0.25  |  0.3244 |
|     4      | 0.2203 | 0.1428 | 0.249  | 0.2983 |  0.3644 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:50:02,274: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2577 | 0.1518 | 0.315  | 0.3771 |  0.4507 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2487 | 0.1423 | 0.3021 | 0.3682 |  0.4454 |
|     1      | 0.3054 | 0.199  | 0.3546 | 0.423  |  0.5119 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2369 | 0.1317 | 0.2827 | 0.3532 |  0.4421 |
|     1      | 0.2725 | 0.175  | 0.3093 | 0.3709 |  0.4643 |
|     2      | 0.216  | 0.1292 | 0.2453 | 0.307  |  0.386  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1474 | 0.296  | 0.3589 |  0.4426 |
|     1      | 0.2694 | 0.1722 | 0.3072 | 0.3651 |  0.4566 |
|     2      | 0.2121 | 0.1253 | 0.2406 | 0.3014 |  0.3832 |
|     3      | 0.2106 | 0.1203 | 0.2452 | 0.3048 |  0.3818 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2036 | 0.1128 | 0.2403 | 0.2978 |  0.3761 |
|     1      | 0.2438 | 0.1547 | 0.2698 | 0.3365 |  0.426  |
|     2      | 0.1928 | 0.1129 | 0.2177 | 0.2696 |  0.3479 |
|     3      | 0.1708 | 0.0896 | 0.1974 |  0.25  |  0.3244 |
|     4      | 0.2203 | 0.1428 | 0.249  | 0.2983 |  0.3644 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:50:02,275: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 88.45778751373291  |   0.258   |    0.152     |    0.315     |     0.451     |
|    1     | 63.58432054519653  |   0.264   |    0.157     |    0.316     |     0.463     |
|    2     | 223.74901700019836 |    0.23   |    0.136     |    0.266     |     0.415     |
|    3     | 145.8682200908661  |   0.224   |    0.132     |    0.259     |     0.401     |
|    4     | 61.75466990470886  |   0.194   |    0.112     |    0.223     |     0.353     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:50:02,275: Sum_Training_Time:583.4140150547028
2024-12-27 19:50:02,275: Every_Training_Time:[88.45778751373291, 63.58432054519653, 223.74901700019836, 145.8682200908661, 61.75466990470886]
2024-12-27 19:50:02,275: Forward transfer: 0.043025 Backward transfer: -0.044675
2024-12-27 19:50:38,633: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227195008/HYBRIDHYBRID_0.001_1024_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_1024_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_1024_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:50:48,334: Snapshot:0	Epoch:0	Loss:42.202	translation_Loss:42.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.75	Hits@10:28.15	Best:11.75
2024-12-27 19:50:54,370: Snapshot:0	Epoch:1	Loss:22.665	translation_Loss:22.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.69	Hits@10:41.15	Best:21.69
2024-12-27 19:51:00,355: Snapshot:0	Epoch:2	Loss:11.299	translation_Loss:11.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:44.81	Best:24.11
2024-12-27 19:51:06,396: Snapshot:0	Epoch:3	Loss:6.109	translation_Loss:6.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.88	Best:25.33
2024-12-27 19:51:12,421: Snapshot:0	Epoch:4	Loss:3.715	translation_Loss:3.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.89	Hits@10:46.22	Best:25.89
2024-12-27 19:51:18,454: Snapshot:0	Epoch:5	Loss:2.581	translation_Loss:2.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.23	Best:25.89
2024-12-27 19:51:24,898: Snapshot:0	Epoch:6	Loss:1.944	translation_Loss:1.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.88	Hits@10:46.09	Best:25.89
2024-12-27 19:51:30,870: Snapshot:0	Epoch:7	Loss:1.589	translation_Loss:1.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.03	Best:25.89
2024-12-27 19:51:36,850: Snapshot:0	Epoch:8	Loss:1.358	translation_Loss:1.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:45.61	Best:25.89
2024-12-27 19:51:42,786: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 25.89
2024-12-27 19:51:42,786: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:1.221 MRR:25.44 Best Results: 25.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:51:42,786: Snapshot:0	Epoch:9	Loss:1.221	translation_Loss:1.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:45.47	Best:25.89
2024-12-27 19:51:49,339: Snapshot:0	Epoch:10	Loss:34.427	translation_Loss:34.273	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:45.47	Best:25.89
2024-12-27 19:51:55,389: End of token training: 0 Epoch: 11 Loss:34.232 MRR:25.44 Best Results: 25.89
2024-12-27 19:51:55,389: Snapshot:0	Epoch:11	Loss:34.232	translation_Loss:34.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.44	Hits@10:45.47	Best:25.89
2024-12-27 19:51:55,700: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_5000/0model_best.tar'
2024-12-27 19:51:58,208: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1524 | 0.3151 | 0.3787 |  0.4504 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:52:09,324: Snapshot:1	Epoch:0	Loss:12.134	translation_Loss:11.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.707                                                   	MRR:17.04	Hits@10:31.46	Best:17.04
2024-12-27 19:52:11,654: Snapshot:1	Epoch:1	Loss:5.768	translation_Loss:4.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.136                                                   	MRR:24.37	Hits@10:43.11	Best:24.37
2024-12-27 19:52:13,991: Snapshot:1	Epoch:2	Loss:3.746	translation_Loss:2.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.062                                                   	MRR:26.43	Hits@10:46.38	Best:26.43
2024-12-27 19:52:16,324: Snapshot:1	Epoch:3	Loss:2.938	translation_Loss:1.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.959                                                   	MRR:28.53	Hits@10:49.35	Best:28.53
2024-12-27 19:52:18,680: Snapshot:1	Epoch:4	Loss:2.57	translation_Loss:1.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.909                                                   	MRR:29.02	Hits@10:49.33	Best:29.02
2024-12-27 19:52:21,026: Snapshot:1	Epoch:5	Loss:2.351	translation_Loss:1.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.87                                                   	MRR:29.72	Hits@10:50.51	Best:29.72
2024-12-27 19:52:23,359: Snapshot:1	Epoch:6	Loss:2.212	translation_Loss:1.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.85                                                   	MRR:29.67	Hits@10:50.83	Best:29.72
2024-12-27 19:52:25,698: Snapshot:1	Epoch:7	Loss:2.113	translation_Loss:1.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.825                                                   	MRR:30.46	Hits@10:51.02	Best:30.46
2024-12-27 19:52:28,092: Snapshot:1	Epoch:8	Loss:2.05	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.81                                                   	MRR:30.05	Hits@10:51.22	Best:30.46
2024-12-27 19:52:30,376: Snapshot:1	Epoch:9	Loss:1.983	translation_Loss:1.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.804                                                   	MRR:29.99	Hits@10:50.97	Best:30.46
2024-12-27 19:52:33,091: Snapshot:1	Epoch:10	Loss:1.949	translation_Loss:1.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.793                                                   	MRR:30.5	Hits@10:51.59	Best:30.5
2024-12-27 19:52:35,436: Snapshot:1	Epoch:11	Loss:1.89	translation_Loss:1.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.778                                                   	MRR:30.38	Hits@10:51.2	Best:30.5
2024-12-27 19:52:37,789: Snapshot:1	Epoch:12	Loss:1.883	translation_Loss:1.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.776                                                   	MRR:30.61	Hits@10:51.47	Best:30.61
2024-12-27 19:52:40,132: Snapshot:1	Epoch:13	Loss:1.853	translation_Loss:1.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.778                                                   	MRR:30.37	Hits@10:51.1	Best:30.61
2024-12-27 19:52:42,430: Snapshot:1	Epoch:14	Loss:1.837	translation_Loss:1.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.773                                                   	MRR:30.36	Hits@10:51.4	Best:30.61
2024-12-27 19:52:44,748: Snapshot:1	Epoch:15	Loss:1.82	translation_Loss:1.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:30.2	Hits@10:51.1	Best:30.61
2024-12-27 19:52:47,064: Snapshot:1	Epoch:16	Loss:1.812	translation_Loss:1.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.764                                                   	MRR:30.54	Hits@10:51.72	Best:30.61
2024-12-27 19:52:49,367: Early Stopping! Snapshot: 1 Epoch: 17 Best Results: 30.61
2024-12-27 19:52:49,368: Start to training tokens! Snapshot: 1 Epoch: 17 Loss:1.799 MRR:30.52 Best Results: 30.61
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:52:49,368: Snapshot:1	Epoch:17	Loss:1.799	translation_Loss:1.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.758                                                   	MRR:30.52	Hits@10:51.83	Best:30.61
2024-12-27 19:52:51,668: Snapshot:1	Epoch:18	Loss:12.702	translation_Loss:12.554	multi_layer_Loss:0.149	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.52	Hits@10:51.83	Best:30.61
2024-12-27 19:52:53,954: End of token training: 1 Epoch: 19 Loss:12.542 MRR:30.52 Best Results: 30.61
2024-12-27 19:52:53,954: Snapshot:1	Epoch:19	Loss:12.542	translation_Loss:12.539	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.52	Hits@10:51.83	Best:30.61
2024-12-27 19:52:54,287: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_5000/1model_best.tar'
2024-12-27 19:52:58,107: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2531 | 0.1457 | 0.3094 | 0.3759 |  0.4505 |
|     1      | 0.2995 | 0.1892 | 0.3557 | 0.4259 |  0.508  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:53:31,887: Snapshot:2	Epoch:0	Loss:39.245	translation_Loss:34.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.605                                                   	MRR:18.32	Hits@10:34.59	Best:18.32
2024-12-27 19:53:42,121: Snapshot:2	Epoch:1	Loss:19.086	translation_Loss:14.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.591                                                   	MRR:20.22	Hits@10:36.85	Best:20.22
2024-12-27 19:53:52,437: Snapshot:2	Epoch:2	Loss:16.324	translation_Loss:11.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.428                                                   	MRR:20.65	Hits@10:37.01	Best:20.65
2024-12-27 19:54:02,659: Snapshot:2	Epoch:3	Loss:15.313	translation_Loss:10.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.36                                                   	MRR:20.82	Hits@10:37.19	Best:20.82
2024-12-27 19:54:12,938: Snapshot:2	Epoch:4	Loss:14.888	translation_Loss:10.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.359                                                   	MRR:20.85	Hits@10:37.36	Best:20.85
2024-12-27 19:54:23,127: Snapshot:2	Epoch:5	Loss:14.572	translation_Loss:10.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.337                                                   	MRR:20.85	Hits@10:37.09	Best:20.85
2024-12-27 19:54:33,328: Snapshot:2	Epoch:6	Loss:14.387	translation_Loss:10.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.326                                                   	MRR:20.78	Hits@10:37.14	Best:20.85
2024-12-27 19:54:43,544: Snapshot:2	Epoch:7	Loss:14.254	translation_Loss:9.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.313                                                   	MRR:20.99	Hits@10:37.45	Best:20.99
2024-12-27 19:54:53,766: Snapshot:2	Epoch:8	Loss:14.192	translation_Loss:9.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.341                                                   	MRR:20.81	Hits@10:37.34	Best:20.99
2024-12-27 19:55:03,960: Snapshot:2	Epoch:9	Loss:14.016	translation_Loss:9.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.296                                                   	MRR:20.69	Hits@10:37.26	Best:20.99
2024-12-27 19:55:14,218: Snapshot:2	Epoch:10	Loss:13.958	translation_Loss:9.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.325                                                   	MRR:20.82	Hits@10:37.3	Best:20.99
2024-12-27 19:55:24,563: Snapshot:2	Epoch:11	Loss:13.901	translation_Loss:9.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.332                                                   	MRR:20.81	Hits@10:37.43	Best:20.99
2024-12-27 19:55:34,770: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 20.99
2024-12-27 19:55:34,771: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:13.847 MRR:20.75 Best Results: 20.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:55:34,771: Snapshot:2	Epoch:12	Loss:13.847	translation_Loss:9.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.306                                                   	MRR:20.75	Hits@10:37.29	Best:20.99
2024-12-27 19:55:44,879: Snapshot:2	Epoch:13	Loss:55.787	translation_Loss:55.641	multi_layer_Loss:0.146	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.75	Hits@10:37.29	Best:20.99
2024-12-27 19:55:54,950: End of token training: 2 Epoch: 14 Loss:55.642 MRR:20.75 Best Results: 20.99
2024-12-27 19:55:54,950: Snapshot:2	Epoch:14	Loss:55.642	translation_Loss:55.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.75	Hits@10:37.29	Best:20.99
2024-12-27 19:55:55,275: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_5000/2model_best.tar'
2024-12-27 19:56:03,638: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2461 | 0.1362 | 0.3019 | 0.3705 |  0.4508 |
|     1      | 0.2852 | 0.1778 | 0.3358 | 0.4002 |  0.488  |
|     2      | 0.2076 | 0.1221 | 0.2387 | 0.2971 |  0.3749 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:56:43,458: Snapshot:3	Epoch:0	Loss:39.132	translation_Loss:32.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.473                                                   	MRR:19.07	Hits@10:35.86	Best:19.07
2024-12-27 19:56:55,925: Snapshot:3	Epoch:1	Loss:21.242	translation_Loss:14.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.634                                                   	MRR:20.06	Hits@10:36.7	Best:20.06
2024-12-27 19:57:08,367: Snapshot:3	Epoch:2	Loss:19.259	translation_Loss:12.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.343                                                   	MRR:20.27	Hits@10:36.7	Best:20.27
2024-12-27 19:57:20,812: Snapshot:3	Epoch:3	Loss:18.64	translation_Loss:12.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.289                                                   	MRR:20.2	Hits@10:36.85	Best:20.27
2024-12-27 19:57:33,195: Snapshot:3	Epoch:4	Loss:18.34	translation_Loss:12.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.266                                                   	MRR:20.15	Hits@10:36.83	Best:20.27
2024-12-27 19:57:45,558: Snapshot:3	Epoch:5	Loss:18.236	translation_Loss:11.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.318                                                   	MRR:20.15	Hits@10:36.77	Best:20.27
2024-12-27 19:57:57,994: Snapshot:3	Epoch:6	Loss:18.038	translation_Loss:11.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.269                                                   	MRR:20.25	Hits@10:36.68	Best:20.27
2024-12-27 19:58:10,406: Snapshot:3	Epoch:7	Loss:17.987	translation_Loss:11.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.299                                                   	MRR:20.3	Hits@10:36.85	Best:20.3
2024-12-27 19:58:23,404: Snapshot:3	Epoch:8	Loss:17.957	translation_Loss:11.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.312                                                   	MRR:20.4	Hits@10:37.08	Best:20.4
2024-12-27 19:58:35,870: Snapshot:3	Epoch:9	Loss:17.832	translation_Loss:11.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.279                                                   	MRR:20.33	Hits@10:37.09	Best:20.4
2024-12-27 19:58:48,287: Snapshot:3	Epoch:10	Loss:17.795	translation_Loss:11.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.295                                                   	MRR:20.39	Hits@10:37.05	Best:20.4
2024-12-27 19:59:00,799: Snapshot:3	Epoch:11	Loss:17.698	translation_Loss:11.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.291                                                   	MRR:20.43	Hits@10:37.04	Best:20.43
2024-12-27 19:59:13,380: Snapshot:3	Epoch:12	Loss:17.712	translation_Loss:11.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.295                                                   	MRR:20.48	Hits@10:37.02	Best:20.48
2024-12-27 19:59:25,987: Snapshot:3	Epoch:13	Loss:17.67	translation_Loss:11.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.281                                                   	MRR:20.43	Hits@10:36.99	Best:20.48
2024-12-27 19:59:38,435: Snapshot:3	Epoch:14	Loss:17.59	translation_Loss:11.308	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.283                                                   	MRR:20.44	Hits@10:37.09	Best:20.48
2024-12-27 19:59:50,986: Snapshot:3	Epoch:15	Loss:17.583	translation_Loss:11.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.28                                                   	MRR:20.49	Hits@10:37.04	Best:20.49
2024-12-27 20:00:03,599: Snapshot:3	Epoch:16	Loss:17.549	translation_Loss:11.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.292                                                   	MRR:20.37	Hits@10:37.02	Best:20.49
2024-12-27 20:00:16,123: Snapshot:3	Epoch:17	Loss:17.517	translation_Loss:11.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.288                                                   	MRR:20.45	Hits@10:37.05	Best:20.49
2024-12-27 20:00:29,067: Snapshot:3	Epoch:18	Loss:17.522	translation_Loss:11.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.296                                                   	MRR:20.4	Hits@10:36.87	Best:20.49
2024-12-27 20:00:41,600: Snapshot:3	Epoch:19	Loss:17.487	translation_Loss:11.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.301                                                   	MRR:20.32	Hits@10:36.75	Best:20.49
2024-12-27 20:00:54,006: Early Stopping! Snapshot: 3 Epoch: 20 Best Results: 20.49
2024-12-27 20:00:54,006: Start to training tokens! Snapshot: 3 Epoch: 20 Loss:17.49 MRR:20.26 Best Results: 20.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:00:54,006: Snapshot:3	Epoch:20	Loss:17.49	translation_Loss:11.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.306                                                   	MRR:20.26	Hits@10:36.81	Best:20.49
2024-12-27 20:01:06,352: Snapshot:3	Epoch:21	Loss:60.112	translation_Loss:59.957	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.26	Hits@10:36.81	Best:20.49
2024-12-27 20:01:18,549: End of token training: 3 Epoch: 22 Loss:59.958 MRR:20.26 Best Results: 20.49
2024-12-27 20:01:18,550: Snapshot:3	Epoch:22	Loss:59.958	translation_Loss:59.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.26	Hits@10:36.81	Best:20.49
2024-12-27 20:01:18,876: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_5000/3model_best.tar'
2024-12-27 20:01:32,986: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2497 | 0.1425 | 0.3044 | 0.3692 |  0.4494 |
|     1      | 0.2834 | 0.178  | 0.3307 | 0.3945 |  0.4838 |
|     2      | 0.2067 | 0.121  | 0.2363 | 0.2935 |  0.3746 |
|     3      | 0.2046 | 0.1166 | 0.2393 | 0.2964 |  0.3698 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:01:51,666: Snapshot:4	Epoch:0	Loss:18.172	translation_Loss:16.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.926                                                   	MRR:16.75	Hits@10:33.65	Best:16.75
2024-12-27 20:01:56,854: Snapshot:4	Epoch:1	Loss:10.847	translation_Loss:7.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.904                                                   	MRR:20.67	Hits@10:36.11	Best:20.67
2024-12-27 20:02:02,045: Snapshot:4	Epoch:2	Loss:8.843	translation_Loss:5.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.975                                                   	MRR:21.7	Hits@10:35.76	Best:21.7
2024-12-27 20:02:07,137: Snapshot:4	Epoch:3	Loss:8.262	translation_Loss:5.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.969                                                   	MRR:21.63	Hits@10:35.81	Best:21.7
2024-12-27 20:02:12,341: Snapshot:4	Epoch:4	Loss:8.05	translation_Loss:5.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.962                                                   	MRR:21.84	Hits@10:35.91	Best:21.84
2024-12-27 20:02:17,565: Snapshot:4	Epoch:5	Loss:7.949	translation_Loss:5.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.94                                                   	MRR:21.98	Hits@10:35.51	Best:21.98
2024-12-27 20:02:22,731: Snapshot:4	Epoch:6	Loss:7.882	translation_Loss:4.954	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.929                                                   	MRR:21.85	Hits@10:35.47	Best:21.98
2024-12-27 20:02:27,930: Snapshot:4	Epoch:7	Loss:7.879	translation_Loss:4.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.939                                                   	MRR:22.03	Hits@10:35.79	Best:22.03
2024-12-27 20:02:33,068: Snapshot:4	Epoch:8	Loss:7.853	translation_Loss:4.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.945                                                   	MRR:21.75	Hits@10:35.77	Best:22.03
2024-12-27 20:02:38,176: Snapshot:4	Epoch:9	Loss:7.847	translation_Loss:4.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.945                                                   	MRR:22.02	Hits@10:35.97	Best:22.03
2024-12-27 20:02:43,277: Snapshot:4	Epoch:10	Loss:7.81	translation_Loss:4.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.956                                                   	MRR:21.76	Hits@10:35.9	Best:22.03
2024-12-27 20:02:48,414: Snapshot:4	Epoch:11	Loss:7.817	translation_Loss:4.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.942                                                   	MRR:21.63	Hits@10:35.99	Best:22.03
2024-12-27 20:02:53,498: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 22.03
2024-12-27 20:02:53,498: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:7.816 MRR:21.73 Best Results: 22.03
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:02:53,498: Snapshot:4	Epoch:12	Loss:7.816	translation_Loss:4.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.963                                                   	MRR:21.73	Hits@10:35.78	Best:22.03
2024-12-27 20:02:58,539: Snapshot:4	Epoch:13	Loss:28.526	translation_Loss:28.364	multi_layer_Loss:0.161	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.73	Hits@10:35.78	Best:22.03
2024-12-27 20:03:03,560: End of token training: 4 Epoch: 14 Loss:28.358 MRR:21.73 Best Results: 22.03
2024-12-27 20:03:03,560: Snapshot:4	Epoch:14	Loss:28.358	translation_Loss:28.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.73	Hits@10:35.78	Best:22.03
2024-12-27 20:03:03,898: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_5000/4model_best.tar'
2024-12-27 20:03:20,810: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2232 | 0.1241 | 0.2705 | 0.3298 |  0.4065 |
|     1      | 0.2756 | 0.1764 | 0.3152 | 0.3789 |  0.4653 |
|     2      | 0.1985 | 0.1171 | 0.2241 | 0.2806 |  0.3588 |
|     3      | 0.1923 | 0.1069 | 0.2214 | 0.2784 |  0.3562 |
|     4      | 0.2199 | 0.1462 | 0.2501 | 0.2988 |  0.3606 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:03:20,812: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1524 | 0.3151 | 0.3787 |  0.4504 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2531 | 0.1457 | 0.3094 | 0.3759 |  0.4505 |
|     1      | 0.2995 | 0.1892 | 0.3557 | 0.4259 |  0.508  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2461 | 0.1362 | 0.3019 | 0.3705 |  0.4508 |
|     1      | 0.2852 | 0.1778 | 0.3358 | 0.4002 |  0.488  |
|     2      | 0.2076 | 0.1221 | 0.2387 | 0.2971 |  0.3749 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2497 | 0.1425 | 0.3044 | 0.3692 |  0.4494 |
|     1      | 0.2834 | 0.178  | 0.3307 | 0.3945 |  0.4838 |
|     2      | 0.2067 | 0.121  | 0.2363 | 0.2935 |  0.3746 |
|     3      | 0.2046 | 0.1166 | 0.2393 | 0.2964 |  0.3698 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2232 | 0.1241 | 0.2705 | 0.3298 |  0.4065 |
|     1      | 0.2756 | 0.1764 | 0.3152 | 0.3789 |  0.4653 |
|     2      | 0.1985 | 0.1171 | 0.2241 | 0.2806 |  0.3588 |
|     3      | 0.1923 | 0.1069 | 0.2214 | 0.2784 |  0.3562 |
|     4      | 0.2199 | 0.1462 | 0.2501 | 0.2988 |  0.3606 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:03:20,813: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  76.7553505897522  |   0.258   |    0.152     |    0.315     |      0.45     |
|    1     |  54.2271294593811  |   0.265   |    0.157     |    0.322     |     0.466     |
|    2     | 172.3869173526764  |   0.231   |    0.134     |    0.273     |     0.415     |
|    3     | 309.63044333457947 |   0.221   |    0.128     |    0.259     |     0.397     |
|    4     | 88.00426506996155  |   0.209   |    0.123     |    0.241     |     0.374     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:03:20,813: Sum_Training_Time:701.0041058063507
2024-12-27 20:03:20,813: Every_Training_Time:[76.7553505897522, 54.2271294593811, 172.3869173526764, 309.63044333457947, 88.00426506996155]
2024-12-27 20:03:20,813: Forward transfer: 0.043725 Backward transfer: -0.02004999999999999
2024-12-27 20:03:54,184: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227200324/HYBRIDHYBRID_0.001_1024_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_1024_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_1024_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:04:03,802: Snapshot:0	Epoch:0	Loss:42.202	translation_Loss:42.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.75	Hits@10:28.16	Best:11.75
2024-12-27 20:04:09,856: Snapshot:0	Epoch:1	Loss:22.665	translation_Loss:22.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.7	Hits@10:41.15	Best:21.7
2024-12-27 20:04:15,823: Snapshot:0	Epoch:2	Loss:11.299	translation_Loss:11.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:44.72	Best:24.14
2024-12-27 20:04:21,781: Snapshot:0	Epoch:3	Loss:6.109	translation_Loss:6.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:45.74	Best:25.35
2024-12-27 20:04:27,841: Snapshot:0	Epoch:4	Loss:3.716	translation_Loss:3.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.11	Best:25.85
2024-12-27 20:04:33,877: Snapshot:0	Epoch:5	Loss:2.577	translation_Loss:2.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.37	Best:25.85
2024-12-27 20:04:40,288: Snapshot:0	Epoch:6	Loss:1.947	translation_Loss:1.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:45.98	Best:25.85
2024-12-27 20:04:46,218: Snapshot:0	Epoch:7	Loss:1.593	translation_Loss:1.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:45.95	Best:25.85
2024-12-27 20:04:52,169: Snapshot:0	Epoch:8	Loss:1.355	translation_Loss:1.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:45.82	Best:25.85
2024-12-27 20:04:58,200: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 25.85
2024-12-27 20:04:58,200: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:1.22 MRR:25.52 Best Results: 25.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:04:58,201: Snapshot:0	Epoch:9	Loss:1.22	translation_Loss:1.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.52	Hits@10:45.57	Best:25.85
2024-12-27 20:05:04,848: Snapshot:0	Epoch:10	Loss:34.514	translation_Loss:34.359	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.52	Hits@10:45.57	Best:25.85
2024-12-27 20:05:10,941: End of token training: 0 Epoch: 11 Loss:34.312 MRR:25.52 Best Results: 25.85
2024-12-27 20:05:10,941: Snapshot:0	Epoch:11	Loss:34.312	translation_Loss:34.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.52	Hits@10:45.57	Best:25.85
2024-12-27 20:05:11,248: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_10000/0model_best.tar'
2024-12-27 20:05:13,837: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2577 | 0.1507 | 0.315  | 0.3806 |  0.4501 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:05:25,017: Snapshot:1	Epoch:0	Loss:12.337	translation_Loss:11.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.877                                                   	MRR:16.69	Hits@10:30.72	Best:16.69
2024-12-27 20:05:27,416: Snapshot:1	Epoch:1	Loss:6.188	translation_Loss:5.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.15                                                   	MRR:23.86	Hits@10:42.49	Best:23.86
2024-12-27 20:05:29,796: Snapshot:1	Epoch:2	Loss:4.148	translation_Loss:3.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.962                                                   	MRR:26.62	Hits@10:47.17	Best:26.62
2024-12-27 20:05:32,191: Snapshot:1	Epoch:3	Loss:3.325	translation_Loss:2.469	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.856                                                   	MRR:28.21	Hits@10:48.96	Best:28.21
2024-12-27 20:05:34,598: Snapshot:1	Epoch:4	Loss:2.929	translation_Loss:2.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.813                                                   	MRR:28.94	Hits@10:49.66	Best:28.94
2024-12-27 20:05:36,971: Snapshot:1	Epoch:5	Loss:2.691	translation_Loss:1.917	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.774                                                   	MRR:29.36	Hits@10:49.65	Best:29.36
2024-12-27 20:05:39,352: Snapshot:1	Epoch:6	Loss:2.534	translation_Loss:1.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.752                                                   	MRR:29.54	Hits@10:50.25	Best:29.54
2024-12-27 20:05:41,776: Snapshot:1	Epoch:7	Loss:2.431	translation_Loss:1.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.733                                                   	MRR:29.86	Hits@10:50.91	Best:29.86
2024-12-27 20:05:44,195: Snapshot:1	Epoch:8	Loss:2.363	translation_Loss:1.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.718                                                   	MRR:30.26	Hits@10:51.03	Best:30.26
2024-12-27 20:05:46,534: Snapshot:1	Epoch:9	Loss:2.291	translation_Loss:1.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.711                                                   	MRR:30.04	Hits@10:51.03	Best:30.26
2024-12-27 20:05:49,251: Snapshot:1	Epoch:10	Loss:2.257	translation_Loss:1.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.705                                                   	MRR:30.35	Hits@10:51.32	Best:30.35
2024-12-27 20:05:51,635: Snapshot:1	Epoch:11	Loss:2.204	translation_Loss:1.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.697                                                   	MRR:30.18	Hits@10:51.09	Best:30.35
2024-12-27 20:05:53,974: Snapshot:1	Epoch:12	Loss:2.19	translation_Loss:1.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.693                                                   	MRR:30.29	Hits@10:51.44	Best:30.35
2024-12-27 20:05:56,264: Snapshot:1	Epoch:13	Loss:2.163	translation_Loss:1.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.694                                                   	MRR:30.17	Hits@10:51.29	Best:30.35
2024-12-27 20:05:58,601: Snapshot:1	Epoch:14	Loss:2.143	translation_Loss:1.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.689                                                   	MRR:30.28	Hits@10:51.33	Best:30.35
2024-12-27 20:06:00,934: Early Stopping! Snapshot: 1 Epoch: 15 Best Results: 30.35
2024-12-27 20:06:00,934: Start to training tokens! Snapshot: 1 Epoch: 15 Loss:2.131 MRR:29.93 Best Results: 30.35
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:06:00,934: Snapshot:1	Epoch:15	Loss:2.131	translation_Loss:1.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.686                                                   	MRR:29.93	Hits@10:51.07	Best:30.35
2024-12-27 20:06:03,226: Snapshot:1	Epoch:16	Loss:12.953	translation_Loss:12.804	multi_layer_Loss:0.149	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.93	Hits@10:51.07	Best:30.35
2024-12-27 20:06:05,475: End of token training: 1 Epoch: 17 Loss:12.813 MRR:29.93 Best Results: 30.35
2024-12-27 20:06:05,475: Snapshot:1	Epoch:17	Loss:12.813	translation_Loss:12.81	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.93	Hits@10:51.07	Best:30.35
2024-12-27 20:06:05,791: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_10000/1model_best.tar'
2024-12-27 20:06:09,582: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1485 | 0.3139 | 0.3788 |  0.4503 |
|     1      | 0.2979 | 0.1898 | 0.3516 | 0.4212 |  0.5072 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:06:43,660: Snapshot:2	Epoch:0	Loss:40.402	translation_Loss:35.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.877                                                   	MRR:18.04	Hits@10:34.28	Best:18.04
2024-12-27 20:06:53,863: Snapshot:2	Epoch:1	Loss:20.232	translation_Loss:15.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.497                                                   	MRR:20.1	Hits@10:36.24	Best:20.1
2024-12-27 20:07:04,193: Snapshot:2	Epoch:2	Loss:17.397	translation_Loss:13.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.316                                                   	MRR:20.37	Hits@10:36.63	Best:20.37
2024-12-27 20:07:14,626: Snapshot:2	Epoch:3	Loss:16.442	translation_Loss:12.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.264                                                   	MRR:20.57	Hits@10:36.68	Best:20.57
2024-12-27 20:07:24,974: Snapshot:2	Epoch:4	Loss:15.941	translation_Loss:11.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.24                                                   	MRR:20.47	Hits@10:36.89	Best:20.57
2024-12-27 20:07:35,232: Snapshot:2	Epoch:5	Loss:15.668	translation_Loss:11.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.217                                                   	MRR:20.51	Hits@10:36.95	Best:20.57
2024-12-27 20:07:45,455: Snapshot:2	Epoch:6	Loss:15.462	translation_Loss:11.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.215                                                   	MRR:20.63	Hits@10:36.85	Best:20.63
2024-12-27 20:07:55,607: Snapshot:2	Epoch:7	Loss:15.323	translation_Loss:11.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.218                                                   	MRR:20.39	Hits@10:36.78	Best:20.63
2024-12-27 20:08:05,774: Snapshot:2	Epoch:8	Loss:15.194	translation_Loss:10.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.2                                                   	MRR:20.57	Hits@10:36.98	Best:20.63
2024-12-27 20:08:15,980: Snapshot:2	Epoch:9	Loss:15.108	translation_Loss:10.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.21                                                   	MRR:20.56	Hits@10:36.75	Best:20.63
2024-12-27 20:08:26,280: Snapshot:2	Epoch:10	Loss:15.027	translation_Loss:10.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.202                                                   	MRR:20.61	Hits@10:36.98	Best:20.63
2024-12-27 20:08:36,696: Snapshot:2	Epoch:11	Loss:14.965	translation_Loss:10.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.198                                                   	MRR:20.72	Hits@10:37.06	Best:20.72
2024-12-27 20:08:47,029: Snapshot:2	Epoch:12	Loss:14.906	translation_Loss:10.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.195                                                   	MRR:20.62	Hits@10:37.13	Best:20.72
2024-12-27 20:08:57,306: Snapshot:2	Epoch:13	Loss:14.85	translation_Loss:10.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.203                                                   	MRR:20.76	Hits@10:36.98	Best:20.76
2024-12-27 20:09:07,554: Snapshot:2	Epoch:14	Loss:14.839	translation_Loss:10.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.202                                                   	MRR:20.75	Hits@10:36.89	Best:20.76
2024-12-27 20:09:17,783: Snapshot:2	Epoch:15	Loss:14.776	translation_Loss:10.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.202                                                   	MRR:20.6	Hits@10:37.05	Best:20.76
2024-12-27 20:09:27,972: Snapshot:2	Epoch:16	Loss:14.73	translation_Loss:10.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.199                                                   	MRR:20.62	Hits@10:36.89	Best:20.76
2024-12-27 20:09:38,352: Snapshot:2	Epoch:17	Loss:14.696	translation_Loss:10.508	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.188                                                   	MRR:20.71	Hits@10:37.02	Best:20.76
2024-12-27 20:09:48,800: Early Stopping! Snapshot: 2 Epoch: 18 Best Results: 20.76
2024-12-27 20:09:48,800: Start to training tokens! Snapshot: 2 Epoch: 18 Loss:14.675 MRR:20.67 Best Results: 20.76
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:09:48,800: Snapshot:2	Epoch:18	Loss:14.675	translation_Loss:10.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.204                                                   	MRR:20.67	Hits@10:36.93	Best:20.76
2024-12-27 20:09:58,817: Snapshot:2	Epoch:19	Loss:56.928	translation_Loss:56.783	multi_layer_Loss:0.146	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.67	Hits@10:36.93	Best:20.76
2024-12-27 20:10:09,034: End of token training: 2 Epoch: 20 Loss:56.743 MRR:20.67 Best Results: 20.76
2024-12-27 20:10:09,034: Snapshot:2	Epoch:20	Loss:56.743	translation_Loss:56.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.67	Hits@10:36.93	Best:20.76
2024-12-27 20:10:09,364: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_10000/2model_best.tar'
2024-12-27 20:10:17,761: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.246  | 0.1347 | 0.3044 | 0.3735 |  0.4513 |
|     1      | 0.2861 | 0.1806 | 0.3327 | 0.4005 |  0.4908 |
|     2      | 0.2062 | 0.1228 | 0.2364 | 0.294  |  0.3688 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:10:57,562: Snapshot:3	Epoch:0	Loss:41.622	translation_Loss:34.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.752                                                   	MRR:18.53	Hits@10:34.91	Best:18.53
2024-12-27 20:11:10,036: Snapshot:3	Epoch:1	Loss:23.682	translation_Loss:17.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.485                                                   	MRR:19.64	Hits@10:35.8	Best:19.64
2024-12-27 20:11:22,715: Snapshot:3	Epoch:2	Loss:21.693	translation_Loss:15.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.275                                                   	MRR:19.7	Hits@10:35.95	Best:19.7
2024-12-27 20:11:35,261: Snapshot:3	Epoch:3	Loss:21.082	translation_Loss:14.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.237                                                   	MRR:19.9	Hits@10:35.85	Best:19.9
2024-12-27 20:11:47,691: Snapshot:3	Epoch:4	Loss:20.778	translation_Loss:14.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.22                                                   	MRR:19.74	Hits@10:35.83	Best:19.9
2024-12-27 20:12:00,188: Snapshot:3	Epoch:5	Loss:20.543	translation_Loss:14.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.196                                                   	MRR:19.98	Hits@10:36.17	Best:19.98
2024-12-27 20:12:12,732: Snapshot:3	Epoch:6	Loss:20.373	translation_Loss:14.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.19                                                   	MRR:19.88	Hits@10:36.02	Best:19.98
2024-12-27 20:12:25,207: Snapshot:3	Epoch:7	Loss:20.246	translation_Loss:14.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.199                                                   	MRR:19.99	Hits@10:36.09	Best:19.99
2024-12-27 20:12:38,241: Snapshot:3	Epoch:8	Loss:20.178	translation_Loss:13.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.186                                                   	MRR:19.94	Hits@10:36.15	Best:19.99
2024-12-27 20:12:50,660: Snapshot:3	Epoch:9	Loss:20.102	translation_Loss:13.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.172                                                   	MRR:19.94	Hits@10:36.07	Best:19.99
2024-12-27 20:13:03,075: Snapshot:3	Epoch:10	Loss:20.058	translation_Loss:13.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.181                                                   	MRR:19.85	Hits@10:36.14	Best:19.99
2024-12-27 20:13:15,607: Snapshot:3	Epoch:11	Loss:20.038	translation_Loss:13.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.204                                                   	MRR:19.93	Hits@10:36.11	Best:19.99
2024-12-27 20:13:28,145: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 19.99
2024-12-27 20:13:28,147: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:19.948 MRR:19.94 Best Results: 19.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:13:28,147: Snapshot:3	Epoch:12	Loss:19.948	translation_Loss:13.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.182                                                   	MRR:19.94	Hits@10:36.28	Best:19.99
2024-12-27 20:13:40,368: Snapshot:3	Epoch:13	Loss:62.146	translation_Loss:61.992	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.94	Hits@10:36.28	Best:19.99
2024-12-27 20:13:53,016: End of token training: 3 Epoch: 14 Loss:62.002 MRR:19.94 Best Results: 19.99
2024-12-27 20:13:53,017: Snapshot:3	Epoch:14	Loss:62.002	translation_Loss:62.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.94	Hits@10:36.28	Best:19.99
2024-12-27 20:13:53,274: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_10000/3model_best.tar'
2024-12-27 20:14:07,164: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1412 | 0.3083 | 0.3752 |  0.4504 |
|     1      | 0.2869 | 0.1822 | 0.335  | 0.4012 |  0.4811 |
|     2      | 0.2066 | 0.1218 | 0.2375 | 0.2944 |  0.3698 |
|     3      | 0.1984 | 0.1119 | 0.2342 | 0.2904 |  0.3614 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:14:25,610: Snapshot:4	Epoch:0	Loss:19.172	translation_Loss:16.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.374                                                   	MRR:16.65	Hits@10:32.91	Best:16.65
2024-12-27 20:14:30,759: Snapshot:4	Epoch:1	Loss:12.755	translation_Loss:9.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.058                                                   	MRR:20.85	Hits@10:35.36	Best:20.85
2024-12-27 20:14:36,052: Snapshot:4	Epoch:2	Loss:10.837	translation_Loss:7.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.963                                                   	MRR:21.56	Hits@10:35.06	Best:21.56
2024-12-27 20:14:41,509: Snapshot:4	Epoch:3	Loss:10.266	translation_Loss:7.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.914                                                   	MRR:21.32	Hits@10:34.93	Best:21.56
2024-12-27 20:14:46,633: Snapshot:4	Epoch:4	Loss:10.091	translation_Loss:7.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.869                                                   	MRR:21.49	Hits@10:34.85	Best:21.56
2024-12-27 20:14:51,748: Snapshot:4	Epoch:5	Loss:9.978	translation_Loss:7.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.845                                                   	MRR:21.81	Hits@10:34.96	Best:21.81
2024-12-27 20:14:56,888: Snapshot:4	Epoch:6	Loss:9.933	translation_Loss:7.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.844                                                   	MRR:21.22	Hits@10:34.7	Best:21.81
2024-12-27 20:15:02,016: Snapshot:4	Epoch:7	Loss:9.899	translation_Loss:7.051	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.848                                                   	MRR:21.45	Hits@10:34.94	Best:21.81
2024-12-27 20:15:07,207: Snapshot:4	Epoch:8	Loss:9.872	translation_Loss:7.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.833                                                   	MRR:21.53	Hits@10:34.65	Best:21.81
2024-12-27 20:15:12,270: Snapshot:4	Epoch:9	Loss:9.898	translation_Loss:7.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.845                                                   	MRR:21.19	Hits@10:34.65	Best:21.81
2024-12-27 20:15:17,414: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 21.81
2024-12-27 20:15:17,414: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:9.858 MRR:21.32 Best Results: 21.81
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:15:17,414: Snapshot:4	Epoch:10	Loss:9.858	translation_Loss:7.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.844                                                   	MRR:21.32	Hits@10:34.69	Best:21.81
2024-12-27 20:15:22,369: Snapshot:4	Epoch:11	Loss:29.514	translation_Loss:29.353	multi_layer_Loss:0.161	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.32	Hits@10:34.69	Best:21.81
2024-12-27 20:15:27,319: End of token training: 4 Epoch: 12 Loss:29.343 MRR:21.32 Best Results: 21.81
2024-12-27 20:15:27,320: Snapshot:4	Epoch:12	Loss:29.343	translation_Loss:29.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.32	Hits@10:34.69	Best:21.81
2024-12-27 20:15:27,580: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_1024_10000/4model_best.tar'
2024-12-27 20:15:44,228: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2279 | 0.1262 | 0.2763 | 0.3387 |  0.4136 |
|     1      | 0.2808 | 0.1796 | 0.3255 | 0.3893 |  0.4697 |
|     2      | 0.2015 | 0.1198 | 0.2295 | 0.2836 |  0.359  |
|     3      | 0.1889 | 0.1034 | 0.2208 | 0.2775 |  0.3509 |
|     4      | 0.2162 | 0.1453 | 0.2475 | 0.2946 |  0.3492 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:15:44,231: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2577 | 0.1507 | 0.315  | 0.3806 |  0.4501 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2558 | 0.1485 | 0.3139 | 0.3788 |  0.4503 |
|     1      | 0.2979 | 0.1898 | 0.3516 | 0.4212 |  0.5072 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.246  | 0.1347 | 0.3044 | 0.3735 |  0.4513 |
|     1      | 0.2861 | 0.1806 | 0.3327 | 0.4005 |  0.4908 |
|     2      | 0.2062 | 0.1228 | 0.2364 | 0.294  |  0.3688 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1412 | 0.3083 | 0.3752 |  0.4504 |
|     1      | 0.2869 | 0.1822 | 0.335  | 0.4012 |  0.4811 |
|     2      | 0.2066 | 0.1218 | 0.2375 | 0.2944 |  0.3698 |
|     3      | 0.1984 | 0.1119 | 0.2342 | 0.2904 |  0.3614 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2279 | 0.1262 | 0.2763 | 0.3387 |  0.4136 |
|     1      | 0.2808 | 0.1796 | 0.3255 | 0.3893 |  0.4697 |
|     2      | 0.2015 | 0.1198 | 0.2295 | 0.2836 |  0.359  |
|     3      | 0.1889 | 0.1034 | 0.2208 | 0.2775 |  0.3509 |
|     4      | 0.2162 | 0.1453 | 0.2475 | 0.2946 |  0.3492 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:15:44,231: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 76.75710153579712  |   0.258   |    0.151     |    0.315     |      0.45     |
|    1     | 50.117127656936646 |   0.267   |    0.159     |    0.324     |     0.465     |
|    2     | 234.9720697402954  |    0.23   |    0.134     |    0.272     |     0.413     |
|    3     | 209.98860335350037 |   0.219   |    0.127     |    0.259     |     0.392     |
|    4     | 77.60580587387085  |   0.209   |    0.123     |    0.244     |     0.373     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:15:44,231: Sum_Training_Time:649.4407081604004
2024-12-27 20:15:44,232: Every_Training_Time:[76.75710153579712, 50.117127656936646, 234.9720697402954, 209.98860335350037, 77.60580587387085]
2024-12-27 20:15:44,232: Forward transfer: 0.043125 Backward transfer: -0.01527499999999999
2024-12-27 20:16:17,966: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227201548/HYBRIDHYBRID_0.001_2048_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_2048_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_2048_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:16:27,651: Snapshot:0	Epoch:0	Loss:22.251	translation_Loss:22.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.47	Hits@10:21.0	Best:8.47
2024-12-27 20:16:33,515: Snapshot:0	Epoch:1	Loss:14.345	translation_Loss:14.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.18	Hits@10:35.97	Best:16.18
2024-12-27 20:16:39,804: Snapshot:0	Epoch:2	Loss:8.168	translation_Loss:8.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.18	Hits@10:42.61	Best:22.18
2024-12-27 20:16:45,688: Snapshot:0	Epoch:3	Loss:4.491	translation_Loss:4.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.39	Hits@10:45.06	Best:24.39
2024-12-27 20:16:51,527: Snapshot:0	Epoch:4	Loss:2.638	translation_Loss:2.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.17	Hits@10:45.92	Best:25.17
2024-12-27 20:16:57,469: Snapshot:0	Epoch:5	Loss:1.728	translation_Loss:1.728	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.41	Best:25.7
2024-12-27 20:17:03,315: Snapshot:0	Epoch:6	Loss:1.237	translation_Loss:1.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.45	Best:25.7
2024-12-27 20:17:09,653: Snapshot:0	Epoch:7	Loss:0.967	translation_Loss:0.967	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.43	Best:25.78
2024-12-27 20:17:15,584: Snapshot:0	Epoch:8	Loss:0.794	translation_Loss:0.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.27	Best:25.78
2024-12-27 20:17:21,564: Snapshot:0	Epoch:9	Loss:0.692	translation_Loss:0.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.15	Best:25.78
2024-12-27 20:17:27,455: Snapshot:0	Epoch:10	Loss:0.603	translation_Loss:0.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.91	Hits@10:45.94	Best:25.91
2024-12-27 20:17:33,810: Snapshot:0	Epoch:11	Loss:0.541	translation_Loss:0.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.93	Hits@10:46.07	Best:25.93
2024-12-27 20:17:39,743: Snapshot:0	Epoch:12	Loss:0.489	translation_Loss:0.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:45.87	Best:25.93
2024-12-27 20:17:45,659: Snapshot:0	Epoch:13	Loss:0.441	translation_Loss:0.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:45.75	Best:25.93
2024-12-27 20:17:51,475: Snapshot:0	Epoch:14	Loss:0.411	translation_Loss:0.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.83	Best:25.93
2024-12-27 20:17:57,329: Snapshot:0	Epoch:15	Loss:0.381	translation_Loss:0.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:45.59	Best:25.93
2024-12-27 20:18:03,617: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 25.93
2024-12-27 20:18:03,618: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.358 MRR:25.55 Best Results: 25.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:18:03,618: Snapshot:0	Epoch:16	Loss:0.358	translation_Loss:0.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:45.9	Best:25.93
2024-12-27 20:18:10,038: Snapshot:0	Epoch:17	Loss:17.326	translation_Loss:17.173	multi_layer_Loss:0.153	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:45.9	Best:25.93
2024-12-27 20:18:15,966: End of token training: 0 Epoch: 18 Loss:17.143 MRR:25.55 Best Results: 25.93
2024-12-27 20:18:15,966: Snapshot:0	Epoch:18	Loss:17.143	translation_Loss:17.141	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.55	Hits@10:45.9	Best:25.93
2024-12-27 20:18:16,251: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_1000/0model_best.tar'
2024-12-27 20:18:18,851: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.1517 | 0.3123 | 0.3754 |  0.4513 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:18:29,984: Snapshot:1	Epoch:0	Loss:6.605	translation_Loss:6.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.132                                                   	MRR:12.21	Hits@10:23.09	Best:12.21
2024-12-27 20:18:32,288: Snapshot:1	Epoch:1	Loss:3.117	translation_Loss:2.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.304                                                   	MRR:20.29	Hits@10:36.44	Best:20.29
2024-12-27 20:18:34,592: Snapshot:1	Epoch:2	Loss:1.62	translation_Loss:1.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.403                                                   	MRR:24.94	Hits@10:42.9	Best:24.94
2024-12-27 20:18:36,986: Snapshot:1	Epoch:3	Loss:1.05	translation_Loss:0.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.435                                                   	MRR:26.09	Hits@10:44.6	Best:26.09
2024-12-27 20:18:39,281: Snapshot:1	Epoch:4	Loss:0.778	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.415                                                   	MRR:26.4	Hits@10:45.2	Best:26.4
2024-12-27 20:18:41,570: Snapshot:1	Epoch:5	Loss:0.622	translation_Loss:0.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.376                                                   	MRR:26.69	Hits@10:45.63	Best:26.69
2024-12-27 20:18:44,210: Snapshot:1	Epoch:6	Loss:0.544	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.34                                                   	MRR:27.4	Hits@10:46.38	Best:27.4
2024-12-27 20:18:46,501: Snapshot:1	Epoch:7	Loss:0.498	translation_Loss:0.182	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.316                                                   	MRR:28.03	Hits@10:46.79	Best:28.03
2024-12-27 20:18:48,768: Snapshot:1	Epoch:8	Loss:0.465	translation_Loss:0.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.3                                                   	MRR:28.34	Hits@10:47.39	Best:28.34
2024-12-27 20:18:51,093: Snapshot:1	Epoch:9	Loss:0.434	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.287                                                   	MRR:28.69	Hits@10:48.18	Best:28.69
2024-12-27 20:18:53,451: Snapshot:1	Epoch:10	Loss:0.412	translation_Loss:0.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.272                                                   	MRR:28.98	Hits@10:48.95	Best:28.98
2024-12-27 20:18:55,778: Snapshot:1	Epoch:11	Loss:0.391	translation_Loss:0.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:29.04	Hits@10:49.31	Best:29.04
2024-12-27 20:18:58,085: Snapshot:1	Epoch:12	Loss:0.382	translation_Loss:0.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.254                                                   	MRR:29.46	Hits@10:49.99	Best:29.46
2024-12-27 20:19:00,399: Snapshot:1	Epoch:13	Loss:0.368	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:29.71	Hits@10:50.26	Best:29.71
2024-12-27 20:19:02,705: Snapshot:1	Epoch:14	Loss:0.361	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.24                                                   	MRR:30.0	Hits@10:50.26	Best:30.0
2024-12-27 20:19:05,093: Snapshot:1	Epoch:15	Loss:0.352	translation_Loss:0.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.235                                                   	MRR:30.18	Hits@10:50.36	Best:30.18
2024-12-27 20:19:07,348: Snapshot:1	Epoch:16	Loss:0.347	translation_Loss:0.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:29.92	Hits@10:50.6	Best:30.18
2024-12-27 20:19:09,581: Snapshot:1	Epoch:17	Loss:0.34	translation_Loss:0.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.227                                                   	MRR:30.18	Hits@10:50.44	Best:30.18
2024-12-27 20:19:11,834: Snapshot:1	Epoch:18	Loss:0.331	translation_Loss:0.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:30.53	Hits@10:50.66	Best:30.53
2024-12-27 20:19:14,455: Snapshot:1	Epoch:19	Loss:0.33	translation_Loss:0.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.222                                                   	MRR:30.65	Hits@10:50.74	Best:30.65
2024-12-27 20:19:16,764: Snapshot:1	Epoch:20	Loss:0.328	translation_Loss:0.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.218                                                   	MRR:30.82	Hits@10:50.68	Best:30.82
2024-12-27 20:19:19,052: Snapshot:1	Epoch:21	Loss:0.318	translation_Loss:0.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.215                                                   	MRR:30.57	Hits@10:50.73	Best:30.82
2024-12-27 20:19:21,293: Snapshot:1	Epoch:22	Loss:0.318	translation_Loss:0.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:30.74	Hits@10:50.84	Best:30.82
2024-12-27 20:19:23,586: Snapshot:1	Epoch:23	Loss:0.31	translation_Loss:0.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:30.98	Hits@10:51.13	Best:30.98
2024-12-27 20:19:25,848: Snapshot:1	Epoch:24	Loss:0.309	translation_Loss:0.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:30.87	Hits@10:51.23	Best:30.98
2024-12-27 20:19:28,071: Snapshot:1	Epoch:25	Loss:0.307	translation_Loss:0.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:30.65	Hits@10:51.27	Best:30.98
2024-12-27 20:19:30,354: Snapshot:1	Epoch:26	Loss:0.305	translation_Loss:0.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.207                                                   	MRR:30.67	Hits@10:51.33	Best:30.98
2024-12-27 20:19:32,619: Snapshot:1	Epoch:27	Loss:0.302	translation_Loss:0.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.204                                                   	MRR:30.54	Hits@10:51.28	Best:30.98
2024-12-27 20:19:34,909: Early Stopping! Snapshot: 1 Epoch: 28 Best Results: 30.98
2024-12-27 20:19:34,909: Start to training tokens! Snapshot: 1 Epoch: 28 Loss:0.3 MRR:30.7 Best Results: 30.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:19:34,910: Snapshot:1	Epoch:28	Loss:0.3	translation_Loss:0.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.201                                                   	MRR:30.7	Hits@10:50.87	Best:30.98
2024-12-27 20:19:37,138: Snapshot:1	Epoch:29	Loss:6.197	translation_Loss:6.062	multi_layer_Loss:0.135	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.7	Hits@10:50.87	Best:30.98
2024-12-27 20:19:39,333: End of token training: 1 Epoch: 30 Loss:6.068 MRR:30.7 Best Results: 30.98
2024-12-27 20:19:39,333: Snapshot:1	Epoch:30	Loss:6.068	translation_Loss:6.055	multi_layer_Loss:0.014	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.7	Hits@10:50.87	Best:30.98
2024-12-27 20:19:39,658: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_1000/1model_best.tar'
2024-12-27 20:19:43,697: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2516 | 0.1469 | 0.3034 | 0.3701 |  0.4486 |
|     1      | 0.3019 | 0.1949 | 0.3502 | 0.4214 |  0.5081 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:20:17,245: Snapshot:2	Epoch:0	Loss:20.415	translation_Loss:19.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.149                                                   	MRR:17.46	Hits@10:33.02	Best:17.46
2024-12-27 20:20:27,317: Snapshot:2	Epoch:1	Loss:7.325	translation_Loss:5.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.812                                                   	MRR:20.47	Hits@10:37.4	Best:20.47
2024-12-27 20:20:37,397: Snapshot:2	Epoch:2	Loss:4.707	translation_Loss:2.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.716                                                   	MRR:21.45	Hits@10:38.54	Best:21.45
2024-12-27 20:20:47,391: Snapshot:2	Epoch:3	Loss:3.987	translation_Loss:2.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.632                                                   	MRR:21.55	Hits@10:38.64	Best:21.55
2024-12-27 20:20:57,313: Snapshot:2	Epoch:4	Loss:3.73	translation_Loss:2.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.603                                                   	MRR:21.85	Hits@10:38.59	Best:21.85
2024-12-27 20:21:07,235: Snapshot:2	Epoch:5	Loss:3.573	translation_Loss:1.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.577                                                   	MRR:21.79	Hits@10:38.61	Best:21.85
2024-12-27 20:21:17,156: Snapshot:2	Epoch:6	Loss:3.498	translation_Loss:1.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.565                                                   	MRR:21.61	Hits@10:38.51	Best:21.85
2024-12-27 20:21:27,086: Snapshot:2	Epoch:7	Loss:3.452	translation_Loss:1.89	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.562                                                   	MRR:21.71	Hits@10:38.8	Best:21.85
2024-12-27 20:21:37,430: Snapshot:2	Epoch:8	Loss:3.413	translation_Loss:1.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.56                                                   	MRR:21.67	Hits@10:38.62	Best:21.85
2024-12-27 20:21:47,339: Snapshot:2	Epoch:9	Loss:3.384	translation_Loss:1.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.558                                                   	MRR:21.92	Hits@10:38.82	Best:21.92
2024-12-27 20:21:57,201: Snapshot:2	Epoch:10	Loss:3.372	translation_Loss:1.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.561                                                   	MRR:21.87	Hits@10:38.82	Best:21.92
2024-12-27 20:22:07,475: Snapshot:2	Epoch:11	Loss:3.339	translation_Loss:1.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.555                                                   	MRR:21.65	Hits@10:38.74	Best:21.92
2024-12-27 20:22:17,523: Snapshot:2	Epoch:12	Loss:3.327	translation_Loss:1.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.554                                                   	MRR:21.75	Hits@10:38.98	Best:21.92
2024-12-27 20:22:27,384: Snapshot:2	Epoch:13	Loss:3.313	translation_Loss:1.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.552                                                   	MRR:21.65	Hits@10:38.73	Best:21.92
2024-12-27 20:22:37,731: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 21.92
2024-12-27 20:22:37,731: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:3.3 MRR:21.63 Best Results: 21.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:22:37,731: Snapshot:2	Epoch:14	Loss:3.3	translation_Loss:1.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.546                                                   	MRR:21.63	Hits@10:38.64	Best:21.92
2024-12-27 20:22:47,560: Snapshot:2	Epoch:15	Loss:25.694	translation_Loss:25.548	multi_layer_Loss:0.145	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.63	Hits@10:38.64	Best:21.92
2024-12-27 20:22:57,290: End of token training: 2 Epoch: 16 Loss:25.529 MRR:21.63 Best Results: 21.92
2024-12-27 20:22:57,290: Snapshot:2	Epoch:16	Loss:25.529	translation_Loss:25.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.63	Hits@10:38.64	Best:21.92
2024-12-27 20:22:57,548: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_1000/2model_best.tar'
2024-12-27 20:23:05,773: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2455 | 0.1365 | 0.2958 | 0.3693 |  0.4558 |
|     1      | 0.2773 | 0.1764 | 0.3158 | 0.3824 |  0.4719 |
|     2      | 0.2206 | 0.1335 | 0.2507 | 0.3114 |  0.3927 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:23:45,528: Snapshot:3	Epoch:0	Loss:18.411	translation_Loss:16.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.543                                                   	MRR:18.15	Hits@10:35.12	Best:18.15
2024-12-27 20:23:57,704: Snapshot:3	Epoch:1	Loss:6.36	translation_Loss:4.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.125                                                   	MRR:20.73	Hits@10:38.23	Best:20.73
2024-12-27 20:24:09,762: Snapshot:3	Epoch:2	Loss:4.592	translation_Loss:2.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.916                                                   	MRR:21.16	Hits@10:38.71	Best:21.16
2024-12-27 20:24:21,880: Snapshot:3	Epoch:3	Loss:4.141	translation_Loss:2.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.854                                                   	MRR:21.24	Hits@10:38.76	Best:21.24
2024-12-27 20:24:34,070: Snapshot:3	Epoch:4	Loss:3.958	translation_Loss:2.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.825                                                   	MRR:21.44	Hits@10:38.93	Best:21.44
2024-12-27 20:24:46,248: Snapshot:3	Epoch:5	Loss:3.836	translation_Loss:2.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.804                                                   	MRR:21.49	Hits@10:39.24	Best:21.49
2024-12-27 20:24:58,470: Snapshot:3	Epoch:6	Loss:3.814	translation_Loss:2.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.812                                                   	MRR:21.43	Hits@10:39.02	Best:21.49
2024-12-27 20:25:10,737: Snapshot:3	Epoch:7	Loss:3.773	translation_Loss:1.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.805                                                   	MRR:21.61	Hits@10:38.9	Best:21.61
2024-12-27 20:25:22,930: Snapshot:3	Epoch:8	Loss:3.755	translation_Loss:1.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.809                                                   	MRR:21.61	Hits@10:39.14	Best:21.61
2024-12-27 20:25:35,492: Snapshot:3	Epoch:9	Loss:3.741	translation_Loss:1.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.814                                                   	MRR:21.35	Hits@10:38.87	Best:21.61
2024-12-27 20:25:47,545: Snapshot:3	Epoch:10	Loss:3.728	translation_Loss:1.917	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.811                                                   	MRR:21.41	Hits@10:38.9	Best:21.61
2024-12-27 20:25:59,714: Snapshot:3	Epoch:11	Loss:3.719	translation_Loss:1.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.812                                                   	MRR:21.36	Hits@10:39.11	Best:21.61
2024-12-27 20:26:12,322: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 21.61
2024-12-27 20:26:12,323: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:3.695 MRR:21.32 Best Results: 21.61
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:26:12,323: Snapshot:3	Epoch:12	Loss:3.695	translation_Loss:1.884	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.811                                                   	MRR:21.32	Hits@10:39.05	Best:21.61
2024-12-27 20:26:24,190: Snapshot:3	Epoch:13	Loss:27.324	translation_Loss:27.17	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.32	Hits@10:39.05	Best:21.61
2024-12-27 20:26:36,529: End of token training: 3 Epoch: 14 Loss:27.158 MRR:21.32 Best Results: 21.61
2024-12-27 20:26:36,530: Snapshot:3	Epoch:14	Loss:27.158	translation_Loss:27.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.32	Hits@10:39.05	Best:21.61
2024-12-27 20:26:36,783: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_1000/3model_best.tar'
2024-12-27 20:26:50,722: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2569 | 0.1543 | 0.3032 |  0.37  |  0.4514 |
|     1      | 0.2697 | 0.1729 | 0.3044 | 0.3638 |  0.4607 |
|     2      | 0.2138 | 0.1282 | 0.2405 | 0.3026 |  0.3845 |
|     3      | 0.216  | 0.1243 | 0.2497 | 0.3107 |  0.3908 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:27:09,137: Snapshot:4	Epoch:0	Loss:8.563	translation_Loss:8.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:12.01	Hits@10:26.72	Best:12.01
2024-12-27 20:27:14,176: Snapshot:4	Epoch:1	Loss:3.808	translation_Loss:3.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.708                                                   	MRR:19.14	Hits@10:35.46	Best:19.14
2024-12-27 20:27:19,245: Snapshot:4	Epoch:2	Loss:2.175	translation_Loss:1.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.902                                                   	MRR:21.29	Hits@10:36.86	Best:21.29
2024-12-27 20:27:24,196: Snapshot:4	Epoch:3	Loss:1.58	translation_Loss:0.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.921                                                   	MRR:21.96	Hits@10:36.8	Best:21.96
2024-12-27 20:27:29,279: Snapshot:4	Epoch:4	Loss:1.335	translation_Loss:0.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.864                                                   	MRR:22.14	Hits@10:36.72	Best:22.14
2024-12-27 20:27:34,329: Snapshot:4	Epoch:5	Loss:1.227	translation_Loss:0.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.817                                                   	MRR:22.75	Hits@10:37.29	Best:22.75
2024-12-27 20:27:39,249: Snapshot:4	Epoch:6	Loss:1.158	translation_Loss:0.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.782                                                   	MRR:22.44	Hits@10:37.57	Best:22.75
2024-12-27 20:27:44,221: Snapshot:4	Epoch:7	Loss:1.123	translation_Loss:0.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:22.57	Hits@10:37.83	Best:22.75
2024-12-27 20:27:49,115: Snapshot:4	Epoch:8	Loss:1.095	translation_Loss:0.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.741                                                   	MRR:22.39	Hits@10:37.16	Best:22.75
2024-12-27 20:27:54,011: Snapshot:4	Epoch:9	Loss:1.089	translation_Loss:0.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.728                                                   	MRR:22.34	Hits@10:37.46	Best:22.75
2024-12-27 20:27:58,962: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 22.75
2024-12-27 20:27:58,962: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:1.077 MRR:22.69 Best Results: 22.75
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:27:58,963: Snapshot:4	Epoch:10	Loss:1.077	translation_Loss:0.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.724                                                   	MRR:22.69	Hits@10:37.79	Best:22.75
2024-12-27 20:28:04,235: Snapshot:4	Epoch:11	Loss:12.36	translation_Loss:12.202	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:37.79	Best:22.75
2024-12-27 20:28:09,126: End of token training: 4 Epoch: 12 Loss:12.21 MRR:22.69 Best Results: 22.75
2024-12-27 20:28:09,126: Snapshot:4	Epoch:12	Loss:12.21	translation_Loss:12.206	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.69	Hits@10:37.79	Best:22.75
2024-12-27 20:28:09,402: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_1000/4model_best.tar'
2024-12-27 20:28:25,643: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.207  | 0.1152 | 0.2442 | 0.3043 |  0.3821 |
|     1      | 0.2445 | 0.156  | 0.2697 | 0.3319 |  0.4243 |
|     2      | 0.1898 | 0.1116 | 0.2128 | 0.2651 |  0.3431 |
|     3      | 0.177  | 0.0935 | 0.2044 | 0.2612 |  0.3375 |
|     4      | 0.2274 | 0.1498 | 0.2555 | 0.3029 |  0.3751 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:28:25,646: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.1517 | 0.3123 | 0.3754 |  0.4513 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2516 | 0.1469 | 0.3034 | 0.3701 |  0.4486 |
|     1      | 0.3019 | 0.1949 | 0.3502 | 0.4214 |  0.5081 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2455 | 0.1365 | 0.2958 | 0.3693 |  0.4558 |
|     1      | 0.2773 | 0.1764 | 0.3158 | 0.3824 |  0.4719 |
|     2      | 0.2206 | 0.1335 | 0.2507 | 0.3114 |  0.3927 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2569 | 0.1543 | 0.3032 |  0.37  |  0.4514 |
|     1      | 0.2697 | 0.1729 | 0.3044 | 0.3638 |  0.4607 |
|     2      | 0.2138 | 0.1282 | 0.2405 | 0.3026 |  0.3845 |
|     3      | 0.216  | 0.1243 | 0.2497 | 0.3107 |  0.3908 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.207  | 0.1152 | 0.2442 | 0.3043 |  0.3821 |
|     1      | 0.2445 | 0.156  | 0.2697 | 0.3319 |  0.4243 |
|     2      | 0.1898 | 0.1116 | 0.2128 | 0.2651 |  0.3431 |
|     3      | 0.177  | 0.0935 | 0.2044 | 0.2612 |  0.3375 |
|     4      | 0.2274 | 0.1498 | 0.2555 | 0.3029 |  0.3751 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:28:25,647: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 117.9995949268341  |   0.258   |    0.152     |    0.312     |     0.451     |
|    1     | 78.94453120231628  |   0.265   |     0.16     |    0.316     |     0.464     |
|    2     | 189.40230917930603 |   0.236   |     0.14     |    0.274     |     0.424     |
|    3     | 205.23230528831482 |   0.228   |    0.136     |    0.262     |     0.407     |
|    4     | 75.81847167015076  |   0.197   |    0.114     |    0.225     |     0.358     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:28:25,647: Sum_Training_Time:667.397212266922
2024-12-27 20:28:25,647: Every_Training_Time:[117.9995949268341, 78.94453120231628, 189.40230917930603, 205.23230528831482, 75.81847167015076]
2024-12-27 20:28:25,647: Forward transfer: 0.042575 Backward transfer: -0.04445
2024-12-27 20:28:59,322: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227202829/HYBRIDHYBRID_0.001_2048_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_2048_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_2048_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:29:08,975: Snapshot:0	Epoch:0	Loss:22.251	translation_Loss:22.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.47	Hits@10:21.0	Best:8.47
2024-12-27 20:29:14,938: Snapshot:0	Epoch:1	Loss:14.345	translation_Loss:14.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.18	Hits@10:35.95	Best:16.18
2024-12-27 20:29:21,404: Snapshot:0	Epoch:2	Loss:8.169	translation_Loss:8.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.19	Hits@10:42.56	Best:22.19
2024-12-27 20:29:27,341: Snapshot:0	Epoch:3	Loss:4.491	translation_Loss:4.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.45	Hits@10:44.96	Best:24.45
2024-12-27 20:29:33,412: Snapshot:0	Epoch:4	Loss:2.638	translation_Loss:2.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.12	Hits@10:45.87	Best:25.12
2024-12-27 20:29:39,487: Snapshot:0	Epoch:5	Loss:1.73	translation_Loss:1.73	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.5	Best:25.63
2024-12-27 20:29:45,497: Snapshot:0	Epoch:6	Loss:1.236	translation_Loss:1.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.43	Best:25.7
2024-12-27 20:29:51,901: Snapshot:0	Epoch:7	Loss:0.968	translation_Loss:0.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.43	Best:25.8
2024-12-27 20:29:57,859: Snapshot:0	Epoch:8	Loss:0.795	translation_Loss:0.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.9	Hits@10:46.35	Best:25.9
2024-12-27 20:30:03,809: Snapshot:0	Epoch:9	Loss:0.69	translation_Loss:0.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.89	Hits@10:46.15	Best:25.9
2024-12-27 20:30:09,845: Snapshot:0	Epoch:10	Loss:0.603	translation_Loss:0.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.03	Best:25.9
2024-12-27 20:30:16,189: Snapshot:0	Epoch:11	Loss:0.544	translation_Loss:0.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:45.85	Best:25.9
2024-12-27 20:30:22,099: Snapshot:0	Epoch:12	Loss:0.489	translation_Loss:0.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:45.58	Best:25.9
2024-12-27 20:30:28,038: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 25.9
2024-12-27 20:30:28,038: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.442 MRR:25.75 Best Results: 25.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:30:28,039: Snapshot:0	Epoch:13	Loss:0.442	translation_Loss:0.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:45.59	Best:25.9
2024-12-27 20:30:34,560: Snapshot:0	Epoch:14	Loss:17.27	translation_Loss:17.118	multi_layer_Loss:0.153	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:45.59	Best:25.9
2024-12-27 20:30:40,631: End of token training: 0 Epoch: 15 Loss:17.158 MRR:25.75 Best Results: 25.9
2024-12-27 20:30:40,631: Snapshot:0	Epoch:15	Loss:17.158	translation_Loss:17.157	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.75	Hits@10:45.59	Best:25.9
2024-12-27 20:30:40,927: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_5000/0model_best.tar'
2024-12-27 20:30:43,827: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1491 | 0.3145 | 0.3789 |  0.4526 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:30:55,005: Snapshot:1	Epoch:0	Loss:6.851	translation_Loss:6.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.282                                                   	MRR:12.09	Hits@10:23.15	Best:12.09
2024-12-27 20:30:57,364: Snapshot:1	Epoch:1	Loss:3.662	translation_Loss:3.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.557                                                   	MRR:20.03	Hits@10:35.89	Best:20.03
2024-12-27 20:30:59,694: Snapshot:1	Epoch:2	Loss:2.247	translation_Loss:1.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.651                                                   	MRR:24.19	Hits@10:41.59	Best:24.19
2024-12-27 20:31:02,060: Snapshot:1	Epoch:3	Loss:1.61	translation_Loss:1.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.579                                                   	MRR:25.51	Hits@10:44.63	Best:25.51
2024-12-27 20:31:04,420: Snapshot:1	Epoch:4	Loss:1.27	translation_Loss:0.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.489                                                   	MRR:26.84	Hits@10:47.13	Best:26.84
2024-12-27 20:31:06,770: Snapshot:1	Epoch:5	Loss:1.101	translation_Loss:0.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.435                                                   	MRR:28.01	Hits@10:48.57	Best:28.01
2024-12-27 20:31:09,126: Snapshot:1	Epoch:6	Loss:0.993	translation_Loss:0.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.406                                                   	MRR:28.47	Hits@10:49.1	Best:28.47
2024-12-27 20:31:11,464: Snapshot:1	Epoch:7	Loss:0.919	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.386                                                   	MRR:29.08	Hits@10:49.96	Best:29.08
2024-12-27 20:31:13,831: Snapshot:1	Epoch:8	Loss:0.87	translation_Loss:0.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.37                                                   	MRR:29.36	Hits@10:50.4	Best:29.36
2024-12-27 20:31:16,150: Snapshot:1	Epoch:9	Loss:0.831	translation_Loss:0.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.358                                                   	MRR:30.04	Hits@10:50.57	Best:30.04
2024-12-27 20:31:18,520: Snapshot:1	Epoch:10	Loss:0.792	translation_Loss:0.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.349                                                   	MRR:30.21	Hits@10:50.47	Best:30.21
2024-12-27 20:31:20,878: Snapshot:1	Epoch:11	Loss:0.766	translation_Loss:0.429	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.337                                                   	MRR:30.32	Hits@10:50.81	Best:30.32
2024-12-27 20:31:23,167: Snapshot:1	Epoch:12	Loss:0.75	translation_Loss:0.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.332                                                   	MRR:30.3	Hits@10:51.05	Best:30.32
2024-12-27 20:31:25,503: Snapshot:1	Epoch:13	Loss:0.727	translation_Loss:0.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.327                                                   	MRR:30.39	Hits@10:51.16	Best:30.39
2024-12-27 20:31:27,870: Snapshot:1	Epoch:14	Loss:0.714	translation_Loss:0.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.323                                                   	MRR:30.4	Hits@10:51.2	Best:30.4
2024-12-27 20:31:30,183: Snapshot:1	Epoch:15	Loss:0.702	translation_Loss:0.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.32                                                   	MRR:30.13	Hits@10:51.14	Best:30.4
2024-12-27 20:31:32,496: Snapshot:1	Epoch:16	Loss:0.687	translation_Loss:0.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.313                                                   	MRR:30.09	Hits@10:51.4	Best:30.4
2024-12-27 20:31:35,181: Snapshot:1	Epoch:17	Loss:0.677	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.311                                                   	MRR:30.45	Hits@10:51.35	Best:30.45
2024-12-27 20:31:37,532: Snapshot:1	Epoch:18	Loss:0.669	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.307                                                   	MRR:30.56	Hits@10:51.1	Best:30.56
2024-12-27 20:31:39,900: Snapshot:1	Epoch:19	Loss:0.659	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.304                                                   	MRR:30.78	Hits@10:51.53	Best:30.78
2024-12-27 20:31:42,214: Snapshot:1	Epoch:20	Loss:0.655	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.298                                                   	MRR:30.74	Hits@10:51.1	Best:30.78
2024-12-27 20:31:44,501: Snapshot:1	Epoch:21	Loss:0.652	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.301                                                   	MRR:30.31	Hits@10:51.45	Best:30.78
2024-12-27 20:31:46,804: Snapshot:1	Epoch:22	Loss:0.643	translation_Loss:0.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.3                                                   	MRR:30.16	Hits@10:51.38	Best:30.78
2024-12-27 20:31:49,101: Snapshot:1	Epoch:23	Loss:0.641	translation_Loss:0.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.297                                                   	MRR:30.27	Hits@10:51.44	Best:30.78
2024-12-27 20:31:51,386: Early Stopping! Snapshot: 1 Epoch: 24 Best Results: 30.78
2024-12-27 20:31:51,386: Start to training tokens! Snapshot: 1 Epoch: 24 Loss:0.643 MRR:30.34 Best Results: 30.78
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:31:51,386: Snapshot:1	Epoch:24	Loss:0.643	translation_Loss:0.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.297                                                   	MRR:30.34	Hits@10:51.56	Best:30.78
2024-12-27 20:31:53,647: Snapshot:1	Epoch:25	Loss:6.547	translation_Loss:6.412	multi_layer_Loss:0.135	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.34	Hits@10:51.56	Best:30.78
2024-12-27 20:31:55,920: End of token training: 1 Epoch: 26 Loss:6.424 MRR:30.34 Best Results: 30.78
2024-12-27 20:31:55,920: Snapshot:1	Epoch:26	Loss:6.424	translation_Loss:6.41	multi_layer_Loss:0.014	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.34	Hits@10:51.56	Best:30.78
2024-12-27 20:31:56,125: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_5000/1model_best.tar'
2024-12-27 20:32:00,229: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.255  | 0.1465 | 0.3126 | 0.3787 |  0.4543 |
|     1      | 0.3036 | 0.1957 | 0.3553 | 0.426  |  0.5156 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:32:33,474: Snapshot:2	Epoch:0	Loss:22.114	translation_Loss:20.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.015                                                   	MRR:16.45	Hits@10:32.18	Best:16.45
2024-12-27 20:32:44,001: Snapshot:2	Epoch:1	Loss:10.002	translation_Loss:7.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.143                                                   	MRR:19.81	Hits@10:36.51	Best:19.81
2024-12-27 20:32:54,134: Snapshot:2	Epoch:2	Loss:7.305	translation_Loss:5.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.813                                                   	MRR:20.74	Hits@10:37.28	Best:20.74
2024-12-27 20:33:04,235: Snapshot:2	Epoch:3	Loss:6.521	translation_Loss:4.813	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:21.18	Hits@10:37.68	Best:21.18
2024-12-27 20:33:14,655: Snapshot:2	Epoch:4	Loss:6.212	translation_Loss:4.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.667                                                   	MRR:21.25	Hits@10:37.72	Best:21.25
2024-12-27 20:33:24,975: Snapshot:2	Epoch:5	Loss:6.006	translation_Loss:4.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.645                                                   	MRR:21.26	Hits@10:37.6	Best:21.26
2024-12-27 20:33:35,141: Snapshot:2	Epoch:6	Loss:5.928	translation_Loss:4.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.64                                                   	MRR:21.29	Hits@10:37.64	Best:21.29
2024-12-27 20:33:45,812: Snapshot:2	Epoch:7	Loss:5.852	translation_Loss:4.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.637                                                   	MRR:21.34	Hits@10:37.97	Best:21.34
2024-12-27 20:33:55,927: Snapshot:2	Epoch:8	Loss:5.792	translation_Loss:4.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.63                                                   	MRR:21.04	Hits@10:37.87	Best:21.34
2024-12-27 20:34:05,981: Snapshot:2	Epoch:9	Loss:5.766	translation_Loss:4.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.635                                                   	MRR:21.18	Hits@10:37.62	Best:21.34
2024-12-27 20:34:16,191: Snapshot:2	Epoch:10	Loss:5.729	translation_Loss:4.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.629                                                   	MRR:21.3	Hits@10:37.89	Best:21.34
2024-12-27 20:34:26,697: Snapshot:2	Epoch:11	Loss:5.688	translation_Loss:4.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.631                                                   	MRR:21.15	Hits@10:37.75	Best:21.34
2024-12-27 20:34:36,826: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 21.34
2024-12-27 20:34:36,826: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:5.665 MRR:21.12 Best Results: 21.34
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:34:36,826: Snapshot:2	Epoch:12	Loss:5.665	translation_Loss:4.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.635                                                   	MRR:21.12	Hits@10:37.84	Best:21.34
2024-12-27 20:34:46,875: Snapshot:2	Epoch:13	Loss:27.513	translation_Loss:27.367	multi_layer_Loss:0.145	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.12	Hits@10:37.84	Best:21.34
2024-12-27 20:34:57,351: End of token training: 2 Epoch: 14 Loss:27.398 MRR:21.12 Best Results: 21.34
2024-12-27 20:34:57,351: Snapshot:2	Epoch:14	Loss:27.398	translation_Loss:27.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.12	Hits@10:37.84	Best:21.34
2024-12-27 20:34:57,651: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_5000/2model_best.tar'
2024-12-27 20:35:05,806: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1355 | 0.3064 | 0.3775 |  0.4569 |
|     1      |  0.29  | 0.184  | 0.3349 | 0.4056 |  0.4981 |
|     2      | 0.2123 | 0.1274 | 0.2407 | 0.3012 |  0.3812 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:35:45,743: Snapshot:3	Epoch:0	Loss:21.501	translation_Loss:18.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.853                                                   	MRR:17.35	Hits@10:34.02	Best:17.35
2024-12-27 20:35:58,183: Snapshot:3	Epoch:1	Loss:10.376	translation_Loss:7.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.03                                                   	MRR:20.24	Hits@10:37.29	Best:20.24
2024-12-27 20:36:10,489: Snapshot:3	Epoch:2	Loss:8.38	translation_Loss:5.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.658                                                   	MRR:20.61	Hits@10:37.42	Best:20.61
2024-12-27 20:36:22,943: Snapshot:3	Epoch:3	Loss:7.794	translation_Loss:5.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.511                                                   	MRR:20.64	Hits@10:37.49	Best:20.64
2024-12-27 20:36:35,359: Snapshot:3	Epoch:4	Loss:7.546	translation_Loss:5.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.45                                                   	MRR:20.83	Hits@10:37.54	Best:20.83
2024-12-27 20:36:47,757: Snapshot:3	Epoch:5	Loss:7.439	translation_Loss:5.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.437                                                   	MRR:20.86	Hits@10:37.61	Best:20.86
2024-12-27 20:37:00,584: Snapshot:3	Epoch:6	Loss:7.369	translation_Loss:4.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.429                                                   	MRR:20.78	Hits@10:37.82	Best:20.86
2024-12-27 20:37:13,063: Snapshot:3	Epoch:7	Loss:7.328	translation_Loss:4.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.426                                                   	MRR:20.95	Hits@10:37.68	Best:20.95
2024-12-27 20:37:25,488: Snapshot:3	Epoch:8	Loss:7.264	translation_Loss:4.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.427                                                   	MRR:20.93	Hits@10:37.57	Best:20.95
2024-12-27 20:37:38,205: Snapshot:3	Epoch:9	Loss:7.229	translation_Loss:4.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.429                                                   	MRR:20.83	Hits@10:37.65	Best:20.95
2024-12-27 20:37:50,469: Snapshot:3	Epoch:10	Loss:7.198	translation_Loss:4.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.416                                                   	MRR:20.84	Hits@10:37.43	Best:20.95
2024-12-27 20:38:02,754: Snapshot:3	Epoch:11	Loss:7.205	translation_Loss:4.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.421                                                   	MRR:20.82	Hits@10:37.76	Best:20.95
2024-12-27 20:38:15,341: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 20.95
2024-12-27 20:38:15,341: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:7.178 MRR:20.82 Best Results: 20.95
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:38:15,341: Snapshot:3	Epoch:12	Loss:7.178	translation_Loss:4.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.432                                                   	MRR:20.82	Hits@10:37.72	Best:20.95
2024-12-27 20:38:27,423: Snapshot:3	Epoch:13	Loss:29.66	translation_Loss:29.506	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.82	Hits@10:37.72	Best:20.95
2024-12-27 20:38:39,504: End of token training: 3 Epoch: 14 Loss:29.509 MRR:20.82 Best Results: 20.95
2024-12-27 20:38:39,504: Snapshot:3	Epoch:14	Loss:29.509	translation_Loss:29.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.82	Hits@10:37.72	Best:20.95
2024-12-27 20:38:39,827: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_5000/3model_best.tar'
2024-12-27 20:38:54,168: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2572 | 0.1491 | 0.3129 | 0.3782 |  0.4577 |
|     1      | 0.2859 | 0.1792 | 0.3334 | 0.3977 |  0.4968 |
|     2      | 0.2121 | 0.1263 | 0.2406 | 0.3008 |  0.3819 |
|     3      | 0.2067 | 0.1178 | 0.2406 | 0.2981 |  0.3755 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:39:12,606: Snapshot:4	Epoch:0	Loss:9.592	translation_Loss:8.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.75                                                   	MRR:12.19	Hits@10:27.97	Best:12.19
2024-12-27 20:39:17,691: Snapshot:4	Epoch:1	Loss:5.655	translation_Loss:4.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.328                                                   	MRR:18.36	Hits@10:34.36	Best:18.36
2024-12-27 20:39:22,804: Snapshot:4	Epoch:2	Loss:4.26	translation_Loss:2.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.441                                                   	MRR:21.02	Hits@10:36.13	Best:21.02
2024-12-27 20:39:27,984: Snapshot:4	Epoch:3	Loss:3.562	translation_Loss:2.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.415                                                   	MRR:21.36	Hits@10:36.45	Best:21.36
2024-12-27 20:39:33,032: Snapshot:4	Epoch:4	Loss:3.234	translation_Loss:1.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.364                                                   	MRR:22.42	Hits@10:36.76	Best:22.42
2024-12-27 20:39:38,153: Snapshot:4	Epoch:5	Loss:3.044	translation_Loss:1.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.323                                                   	MRR:22.16	Hits@10:36.77	Best:22.42
2024-12-27 20:39:43,124: Snapshot:4	Epoch:6	Loss:2.959	translation_Loss:1.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.297                                                   	MRR:22.17	Hits@10:36.16	Best:22.42
2024-12-27 20:39:48,143: Snapshot:4	Epoch:7	Loss:2.888	translation_Loss:1.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.27                                                   	MRR:22.21	Hits@10:36.63	Best:22.42
2024-12-27 20:39:53,548: Snapshot:4	Epoch:8	Loss:2.866	translation_Loss:1.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.259                                                   	MRR:22.26	Hits@10:36.67	Best:22.42
2024-12-27 20:39:58,575: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 22.42
2024-12-27 20:39:58,575: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:2.824 MRR:22.33 Best Results: 22.42
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:39:58,576: Snapshot:4	Epoch:9	Loss:2.824	translation_Loss:1.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.246                                                   	MRR:22.33	Hits@10:36.6	Best:22.42
2024-12-27 20:40:03,482: Snapshot:4	Epoch:10	Loss:13.872	translation_Loss:13.714	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.33	Hits@10:36.6	Best:22.42
2024-12-27 20:40:08,501: End of token training: 4 Epoch: 11 Loss:13.705 MRR:22.33 Best Results: 22.42
2024-12-27 20:40:08,501: Snapshot:4	Epoch:11	Loss:13.705	translation_Loss:13.701	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.33	Hits@10:36.6	Best:22.42
2024-12-27 20:40:08,766: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_5000/4model_best.tar'
2024-12-27 20:40:25,359: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2216 | 0.121  | 0.2687 | 0.3319 |  0.408  |
|     1      | 0.2704 | 0.1725 | 0.3067 | 0.3725 |  0.4729 |
|     2      | 0.1972 | 0.1166 | 0.2221 | 0.2787 |  0.358  |
|     3      | 0.1826 | 0.0968 | 0.2125 | 0.2696 |  0.346  |
|     4      | 0.2226 | 0.1454 | 0.2567 | 0.3042 |  0.3668 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:40:25,361: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1491 | 0.3145 | 0.3789 |  0.4526 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.255  | 0.1465 | 0.3126 | 0.3787 |  0.4543 |
|     1      | 0.3036 | 0.1957 | 0.3553 | 0.426  |  0.5156 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1355 | 0.3064 | 0.3775 |  0.4569 |
|     1      |  0.29  | 0.184  | 0.3349 | 0.4056 |  0.4981 |
|     2      | 0.2123 | 0.1274 | 0.2407 | 0.3012 |  0.3812 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2572 | 0.1491 | 0.3129 | 0.3782 |  0.4577 |
|     1      | 0.2859 | 0.1792 | 0.3334 | 0.3977 |  0.4968 |
|     2      | 0.2121 | 0.1263 | 0.2406 | 0.3008 |  0.3819 |
|     3      | 0.2067 | 0.1178 | 0.2406 | 0.2981 |  0.3755 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2216 | 0.121  | 0.2687 | 0.3319 |  0.408  |
|     1      | 0.2704 | 0.1725 | 0.3067 | 0.3725 |  0.4729 |
|     2      | 0.1972 | 0.1166 | 0.2221 | 0.2787 |  0.358  |
|     3      | 0.1826 | 0.0968 | 0.2125 | 0.2696 |  0.346  |
|     4      | 0.2226 | 0.1454 | 0.2567 | 0.3042 |  0.3668 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:40:25,361: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 101.30803322792053 |   0.257   |    0.149     |    0.315     |     0.453     |
|    1     | 70.82540369033813  |   0.268   |     0.16     |    0.324     |     0.471     |
|    2     | 172.89848470687866 |   0.234   |    0.137     |    0.275     |     0.422     |
|    3     | 208.1848108768463  |   0.225   |    0.132     |    0.263     |     0.405     |
|    4     | 71.74089312553406  |   0.205   |    0.118     |    0.238     |     0.372     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:40:25,361: Sum_Training_Time:624.9576256275177
2024-12-27 20:40:25,362: Every_Training_Time:[101.30803322792053, 70.82540369033813, 172.89848470687866, 208.1848108768463, 71.74089312553406]
2024-12-27 20:40:25,362: Forward transfer: 0.043375000000000004 Backward transfer: -0.027050000000000005
2024-12-27 20:40:58,900: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227204029/HYBRIDHYBRID_0.001_2048_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.001_2048_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.001_2048_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:41:08,518: Snapshot:0	Epoch:0	Loss:22.251	translation_Loss:22.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.47	Hits@10:21.0	Best:8.47
2024-12-27 20:41:14,407: Snapshot:0	Epoch:1	Loss:14.345	translation_Loss:14.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.18	Hits@10:35.97	Best:16.18
2024-12-27 20:41:20,792: Snapshot:0	Epoch:2	Loss:8.169	translation_Loss:8.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.18	Hits@10:42.57	Best:22.18
2024-12-27 20:41:26,714: Snapshot:0	Epoch:3	Loss:4.491	translation_Loss:4.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.39	Hits@10:45.07	Best:24.39
2024-12-27 20:41:32,639: Snapshot:0	Epoch:4	Loss:2.638	translation_Loss:2.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.87	Best:25.18
2024-12-27 20:41:38,563: Snapshot:0	Epoch:5	Loss:1.729	translation_Loss:1.729	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.41	Best:25.64
2024-12-27 20:41:44,587: Snapshot:0	Epoch:6	Loss:1.236	translation_Loss:1.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.43	Best:25.75
2024-12-27 20:41:50,957: Snapshot:0	Epoch:7	Loss:0.968	translation_Loss:0.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.9	Hits@10:46.4	Best:25.9
2024-12-27 20:41:56,899: Snapshot:0	Epoch:8	Loss:0.796	translation_Loss:0.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.3	Best:25.9
2024-12-27 20:42:02,795: Snapshot:0	Epoch:9	Loss:0.695	translation_Loss:0.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.31	Best:25.9
2024-12-27 20:42:08,733: Snapshot:0	Epoch:10	Loss:0.604	translation_Loss:0.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.93	Hits@10:46.1	Best:25.93
2024-12-27 20:42:15,013: Snapshot:0	Epoch:11	Loss:0.542	translation_Loss:0.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.89	Hits@10:46.12	Best:25.93
2024-12-27 20:42:20,962: Snapshot:0	Epoch:12	Loss:0.488	translation_Loss:0.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:45.77	Best:25.93
2024-12-27 20:42:26,871: Snapshot:0	Epoch:13	Loss:0.438	translation_Loss:0.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:45.49	Best:25.93
2024-12-27 20:42:32,775: Snapshot:0	Epoch:14	Loss:0.41	translation_Loss:0.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:45.44	Best:25.93
2024-12-27 20:42:38,762: Early Stopping! Snapshot: 0 Epoch: 15 Best Results: 25.93
2024-12-27 20:42:38,763: Start to training tokens! Snapshot: 0 Epoch: 15 Loss:0.379 MRR:25.76 Best Results: 25.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:42:38,763: Snapshot:0	Epoch:15	Loss:0.379	translation_Loss:0.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:45.57	Best:25.93
2024-12-27 20:42:45,722: Snapshot:0	Epoch:16	Loss:17.232	translation_Loss:17.079	multi_layer_Loss:0.153	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:45.57	Best:25.93
2024-12-27 20:42:51,643: End of token training: 0 Epoch: 17 Loss:17.114 MRR:25.76 Best Results: 25.93
2024-12-27 20:42:51,643: Snapshot:0	Epoch:17	Loss:17.114	translation_Loss:17.112	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.76	Hits@10:45.57	Best:25.93
2024-12-27 20:42:51,931: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_10000/0model_best.tar'
2024-12-27 20:42:54,442: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1503 | 0.3135 | 0.3788 |  0.4517 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:43:05,517: Snapshot:1	Epoch:0	Loss:6.901	translation_Loss:6.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.366                                                   	MRR:11.87	Hits@10:22.35	Best:11.87
2024-12-27 20:43:07,861: Snapshot:1	Epoch:1	Loss:3.782	translation_Loss:3.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.601                                                   	MRR:19.6	Hits@10:34.69	Best:19.6
2024-12-27 20:43:10,172: Snapshot:1	Epoch:2	Loss:2.363	translation_Loss:1.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.598                                                   	MRR:23.53	Hits@10:40.44	Best:23.53
2024-12-27 20:43:12,578: Snapshot:1	Epoch:3	Loss:1.709	translation_Loss:1.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.492                                                   	MRR:25.29	Hits@10:44.59	Best:25.29
2024-12-27 20:43:14,905: Snapshot:1	Epoch:4	Loss:1.375	translation_Loss:0.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.417                                                   	MRR:26.77	Hits@10:47.15	Best:26.77
2024-12-27 20:43:17,277: Snapshot:1	Epoch:5	Loss:1.199	translation_Loss:0.823	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.375                                                   	MRR:27.64	Hits@10:48.38	Best:27.64
2024-12-27 20:43:19,575: Snapshot:1	Epoch:6	Loss:1.084	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:28.32	Hits@10:48.9	Best:28.32
2024-12-27 20:43:21,921: Snapshot:1	Epoch:7	Loss:1.007	translation_Loss:0.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.335                                                   	MRR:28.77	Hits@10:49.49	Best:28.77
2024-12-27 20:43:24,277: Snapshot:1	Epoch:8	Loss:0.949	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.318                                                   	MRR:29.24	Hits@10:49.66	Best:29.24
2024-12-27 20:43:26,649: Snapshot:1	Epoch:9	Loss:0.9	translation_Loss:0.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.307                                                   	MRR:29.58	Hits@10:49.95	Best:29.58
2024-12-27 20:43:29,297: Snapshot:1	Epoch:10	Loss:0.871	translation_Loss:0.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.301                                                   	MRR:29.73	Hits@10:49.83	Best:29.73
2024-12-27 20:43:31,594: Snapshot:1	Epoch:11	Loss:0.842	translation_Loss:0.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.293                                                   	MRR:29.71	Hits@10:50.16	Best:29.73
2024-12-27 20:43:33,882: Snapshot:1	Epoch:12	Loss:0.819	translation_Loss:0.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.288                                                   	MRR:29.69	Hits@10:50.28	Best:29.73
2024-12-27 20:43:36,162: Snapshot:1	Epoch:13	Loss:0.796	translation_Loss:0.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.279                                                   	MRR:29.8	Hits@10:50.46	Best:29.8
2024-12-27 20:43:38,498: Snapshot:1	Epoch:14	Loss:0.779	translation_Loss:0.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.275                                                   	MRR:29.92	Hits@10:50.29	Best:29.92
2024-12-27 20:43:40,777: Snapshot:1	Epoch:15	Loss:0.77	translation_Loss:0.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.27                                                   	MRR:30.11	Hits@10:50.6	Best:30.11
2024-12-27 20:43:43,064: Snapshot:1	Epoch:16	Loss:0.754	translation_Loss:0.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.267                                                   	MRR:30.1	Hits@10:50.5	Best:30.11
2024-12-27 20:43:45,353: Snapshot:1	Epoch:17	Loss:0.742	translation_Loss:0.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.265                                                   	MRR:30.04	Hits@10:50.37	Best:30.11
2024-12-27 20:43:47,652: Snapshot:1	Epoch:18	Loss:0.733	translation_Loss:0.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.26                                                   	MRR:30.09	Hits@10:50.53	Best:30.11
2024-12-27 20:43:49,976: Snapshot:1	Epoch:19	Loss:0.735	translation_Loss:0.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:30.26	Hits@10:50.61	Best:30.26
2024-12-27 20:43:52,266: Snapshot:1	Epoch:20	Loss:0.728	translation_Loss:0.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:30.11	Hits@10:51.16	Best:30.26
2024-12-27 20:43:54,565: Snapshot:1	Epoch:21	Loss:0.714	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.259                                                   	MRR:30.14	Hits@10:50.69	Best:30.26
2024-12-27 20:43:56,911: Snapshot:1	Epoch:22	Loss:0.714	translation_Loss:0.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.254                                                   	MRR:30.13	Hits@10:50.8	Best:30.26
2024-12-27 20:43:59,558: Snapshot:1	Epoch:23	Loss:0.715	translation_Loss:0.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:30.16	Hits@10:50.83	Best:30.26
2024-12-27 20:44:01,851: Early Stopping! Snapshot: 1 Epoch: 24 Best Results: 30.26
2024-12-27 20:44:01,851: Start to training tokens! Snapshot: 1 Epoch: 24 Loss:0.708 MRR:30.24 Best Results: 30.26
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:44:01,852: Snapshot:1	Epoch:24	Loss:0.708	translation_Loss:0.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:30.24	Hits@10:50.63	Best:30.26
2024-12-27 20:44:04,101: Snapshot:1	Epoch:25	Loss:6.676	translation_Loss:6.541	multi_layer_Loss:0.135	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.24	Hits@10:50.63	Best:30.26
2024-12-27 20:44:06,347: End of token training: 1 Epoch: 26 Loss:6.548 MRR:30.24 Best Results: 30.26
2024-12-27 20:44:06,347: Snapshot:1	Epoch:26	Loss:6.548	translation_Loss:6.535	multi_layer_Loss:0.014	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.24	Hits@10:50.63	Best:30.26
2024-12-27 20:44:06,548: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_10000/1model_best.tar'
2024-12-27 20:44:10,352: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2584 | 0.1519 | 0.3141 | 0.3808 |  0.4523 |
|     1      | 0.2956 | 0.1884 | 0.3459 | 0.4168 |  0.5054 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:44:43,860: Snapshot:2	Epoch:0	Loss:22.299	translation_Loss:20.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.192                                                   	MRR:15.85	Hits@10:31.42	Best:15.85
2024-12-27 20:44:53,868: Snapshot:2	Epoch:1	Loss:10.099	translation_Loss:8.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.034                                                   	MRR:19.43	Hits@10:35.86	Best:19.43
2024-12-27 20:45:03,907: Snapshot:2	Epoch:2	Loss:7.377	translation_Loss:5.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.708                                                   	MRR:20.43	Hits@10:36.6	Best:20.43
2024-12-27 20:45:14,087: Snapshot:2	Epoch:3	Loss:6.599	translation_Loss:4.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.613                                                   	MRR:20.58	Hits@10:36.97	Best:20.58
2024-12-27 20:45:24,195: Snapshot:2	Epoch:4	Loss:6.282	translation_Loss:4.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.579                                                   	MRR:20.79	Hits@10:36.87	Best:20.79
2024-12-27 20:45:34,336: Snapshot:2	Epoch:5	Loss:6.105	translation_Loss:4.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.565                                                   	MRR:20.69	Hits@10:37.12	Best:20.79
2024-12-27 20:45:44,838: Snapshot:2	Epoch:6	Loss:6.024	translation_Loss:4.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.563                                                   	MRR:20.74	Hits@10:37.12	Best:20.79
2024-12-27 20:45:54,919: Snapshot:2	Epoch:7	Loss:5.927	translation_Loss:4.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.558                                                   	MRR:20.82	Hits@10:37.12	Best:20.82
2024-12-27 20:46:04,992: Snapshot:2	Epoch:8	Loss:5.878	translation_Loss:4.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.549                                                   	MRR:20.78	Hits@10:37.12	Best:20.82
2024-12-27 20:46:15,345: Snapshot:2	Epoch:9	Loss:5.821	translation_Loss:4.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.545                                                   	MRR:20.76	Hits@10:37.11	Best:20.82
2024-12-27 20:46:25,483: Snapshot:2	Epoch:10	Loss:5.811	translation_Loss:4.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.548                                                   	MRR:20.81	Hits@10:37.19	Best:20.82
2024-12-27 20:46:35,548: Snapshot:2	Epoch:11	Loss:5.775	translation_Loss:4.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.555                                                   	MRR:20.85	Hits@10:37.25	Best:20.85
2024-12-27 20:46:45,551: Snapshot:2	Epoch:12	Loss:5.765	translation_Loss:4.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.549                                                   	MRR:20.63	Hits@10:37.26	Best:20.85
2024-12-27 20:46:55,938: Snapshot:2	Epoch:13	Loss:5.734	translation_Loss:4.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.546                                                   	MRR:20.75	Hits@10:37.12	Best:20.85
2024-12-27 20:47:05,897: Snapshot:2	Epoch:14	Loss:5.72	translation_Loss:4.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.554                                                   	MRR:20.79	Hits@10:37.27	Best:20.85
2024-12-27 20:47:15,984: Snapshot:2	Epoch:15	Loss:5.701	translation_Loss:4.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.551                                                   	MRR:20.65	Hits@10:37.15	Best:20.85
2024-12-27 20:47:26,372: Early Stopping! Snapshot: 2 Epoch: 16 Best Results: 20.85
2024-12-27 20:47:26,372: Start to training tokens! Snapshot: 2 Epoch: 16 Loss:5.688 MRR:20.57 Best Results: 20.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:47:26,372: Snapshot:2	Epoch:16	Loss:5.688	translation_Loss:4.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.552                                                   	MRR:20.57	Hits@10:37.28	Best:20.85
2024-12-27 20:47:36,210: Snapshot:2	Epoch:17	Loss:27.941	translation_Loss:27.796	multi_layer_Loss:0.145	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.57	Hits@10:37.28	Best:20.85
2024-12-27 20:47:46,144: End of token training: 2 Epoch: 18 Loss:27.834 MRR:20.57 Best Results: 20.85
2024-12-27 20:47:46,144: Snapshot:2	Epoch:18	Loss:27.834	translation_Loss:27.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.57	Hits@10:37.28	Best:20.85
2024-12-27 20:47:46,424: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_10000/2model_best.tar'
2024-12-27 20:47:54,665: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2502 | 0.1391 | 0.3077 | 0.3778 |  0.4551 |
|     1      | 0.285  | 0.1799 | 0.3278 | 0.4019 |  0.4966 |
|     2      | 0.2069 | 0.1238 | 0.2344 | 0.2934 |  0.3697 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:48:33,967: Snapshot:3	Epoch:0	Loss:22.226	translation_Loss:19.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.077                                                   	MRR:16.63	Hits@10:33.01	Best:16.63
2024-12-27 20:48:46,181: Snapshot:3	Epoch:1	Loss:11.1	translation_Loss:8.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.946                                                   	MRR:19.65	Hits@10:36.07	Best:19.65
2024-12-27 20:48:58,410: Snapshot:3	Epoch:2	Loss:8.958	translation_Loss:6.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.565                                                   	MRR:20.05	Hits@10:36.42	Best:20.05
2024-12-27 20:49:10,645: Snapshot:3	Epoch:3	Loss:8.338	translation_Loss:5.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.441                                                   	MRR:20.24	Hits@10:36.37	Best:20.24
2024-12-27 20:49:23,020: Snapshot:3	Epoch:4	Loss:8.127	translation_Loss:5.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.413                                                   	MRR:20.28	Hits@10:36.42	Best:20.28
2024-12-27 20:49:35,287: Snapshot:3	Epoch:5	Loss:7.994	translation_Loss:5.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.388                                                   	MRR:20.2	Hits@10:36.36	Best:20.28
2024-12-27 20:49:48,069: Snapshot:3	Epoch:6	Loss:7.911	translation_Loss:5.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.391                                                   	MRR:20.33	Hits@10:36.37	Best:20.33
2024-12-27 20:50:00,325: Snapshot:3	Epoch:7	Loss:7.86	translation_Loss:5.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.382                                                   	MRR:20.27	Hits@10:36.36	Best:20.33
2024-12-27 20:50:13,169: Snapshot:3	Epoch:8	Loss:7.802	translation_Loss:5.422	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.379                                                   	MRR:20.21	Hits@10:36.32	Best:20.33
2024-12-27 20:50:25,270: Snapshot:3	Epoch:9	Loss:7.796	translation_Loss:5.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.383                                                   	MRR:20.25	Hits@10:36.38	Best:20.33
2024-12-27 20:50:37,398: Snapshot:3	Epoch:10	Loss:7.796	translation_Loss:5.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.393                                                   	MRR:20.26	Hits@10:36.48	Best:20.33
2024-12-27 20:50:50,060: Early Stopping! Snapshot: 3 Epoch: 11 Best Results: 20.33
2024-12-27 20:50:50,060: Start to training tokens! Snapshot: 3 Epoch: 11 Loss:7.747 MRR:20.22 Best Results: 20.33
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:50:50,061: Snapshot:3	Epoch:11	Loss:7.747	translation_Loss:5.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.387                                                   	MRR:20.22	Hits@10:36.38	Best:20.33
2024-12-27 20:51:02,057: Snapshot:3	Epoch:12	Loss:30.512	translation_Loss:30.358	multi_layer_Loss:0.154	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.22	Hits@10:36.38	Best:20.33
2024-12-27 20:51:13,970: End of token training: 3 Epoch: 13 Loss:30.39 MRR:20.22 Best Results: 20.33
2024-12-27 20:51:13,971: Snapshot:3	Epoch:13	Loss:30.39	translation_Loss:30.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.22	Hits@10:36.38	Best:20.33
2024-12-27 20:51:14,278: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_10000/3model_best.tar'
2024-12-27 20:51:28,310: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2596 | 0.1529 | 0.313  | 0.3802 |  0.4551 |
|     1      | 0.2859 | 0.1794 | 0.3278 | 0.4011 |  0.4983 |
|     2      | 0.2084 | 0.1254 | 0.2359 | 0.2921 |  0.3717 |
|     3      | 0.2012 | 0.1152 | 0.2357 | 0.2911 |  0.3626 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:51:46,471: Snapshot:4	Epoch:0	Loss:10.0	translation_Loss:9.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.961                                                   	MRR:11.75	Hits@10:27.16	Best:11.75
2024-12-27 20:51:51,808: Snapshot:4	Epoch:1	Loss:6.509	translation_Loss:5.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.401                                                   	MRR:18.03	Hits@10:33.68	Best:18.03
2024-12-27 20:51:56,862: Snapshot:4	Epoch:2	Loss:5.133	translation_Loss:3.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.365                                                   	MRR:20.21	Hits@10:35.35	Best:20.21
2024-12-27 20:52:01,977: Snapshot:4	Epoch:3	Loss:4.4	translation_Loss:3.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:21.23	Hits@10:35.69	Best:21.23
2024-12-27 20:52:06,953: Snapshot:4	Epoch:4	Loss:4.006	translation_Loss:2.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.245                                                   	MRR:21.74	Hits@10:35.54	Best:21.74
2024-12-27 20:52:11,970: Snapshot:4	Epoch:5	Loss:3.799	translation_Loss:2.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.21                                                   	MRR:21.31	Hits@10:35.51	Best:21.74
2024-12-27 20:52:16,977: Snapshot:4	Epoch:6	Loss:3.703	translation_Loss:2.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.18                                                   	MRR:21.49	Hits@10:35.44	Best:21.74
2024-12-27 20:52:22,020: Snapshot:4	Epoch:7	Loss:3.643	translation_Loss:2.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.164                                                   	MRR:21.63	Hits@10:35.46	Best:21.74
2024-12-27 20:52:27,333: Snapshot:4	Epoch:8	Loss:3.612	translation_Loss:2.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.154                                                   	MRR:21.4	Hits@10:35.4	Best:21.74
2024-12-27 20:52:32,348: Snapshot:4	Epoch:9	Loss:3.575	translation_Loss:2.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.143                                                   	MRR:21.8	Hits@10:35.69	Best:21.8
2024-12-27 20:52:37,397: Snapshot:4	Epoch:10	Loss:3.578	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.139                                                   	MRR:21.84	Hits@10:35.54	Best:21.84
2024-12-27 20:52:42,411: Snapshot:4	Epoch:11	Loss:3.557	translation_Loss:2.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.129                                                   	MRR:21.72	Hits@10:35.27	Best:21.84
2024-12-27 20:52:47,345: Snapshot:4	Epoch:12	Loss:3.544	translation_Loss:2.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.13                                                   	MRR:21.61	Hits@10:35.24	Best:21.84
2024-12-27 20:52:52,263: Snapshot:4	Epoch:13	Loss:3.543	translation_Loss:2.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.127                                                   	MRR:21.45	Hits@10:35.17	Best:21.84
2024-12-27 20:52:57,223: Snapshot:4	Epoch:14	Loss:3.542	translation_Loss:2.415	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.127                                                   	MRR:21.78	Hits@10:35.35	Best:21.84
2024-12-27 20:53:02,524: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 21.84
2024-12-27 20:53:02,524: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:3.542 MRR:21.74 Best Results: 21.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:53:02,524: Snapshot:4	Epoch:15	Loss:3.542	translation_Loss:2.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.13                                                   	MRR:21.74	Hits@10:35.26	Best:21.84
2024-12-27 20:53:07,456: Snapshot:4	Epoch:16	Loss:14.621	translation_Loss:14.463	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.74	Hits@10:35.26	Best:21.84
2024-12-27 20:53:12,313: End of token training: 4 Epoch: 17 Loss:14.459 MRR:21.74 Best Results: 21.84
2024-12-27 20:53:12,314: Snapshot:4	Epoch:17	Loss:14.459	translation_Loss:14.455	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.74	Hits@10:35.26	Best:21.84
2024-12-27 20:53:12,561: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.001_2048_10000/4model_best.tar'
2024-12-27 20:53:28,603: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2334 | 0.1324 | 0.2817 | 0.3452 |  0.4167 |
|     1      | 0.2772 | 0.1743 | 0.319  | 0.386  |  0.483  |
|     2      | 0.202  | 0.1214 | 0.2274 | 0.283  |  0.3592 |
|     3      | 0.1874 | 0.1023 | 0.2187 | 0.2735 |  0.3501 |
|     4      | 0.2181 | 0.147  | 0.2455 | 0.2936 |  0.3558 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:53:28,605: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1503 | 0.3135 | 0.3788 |  0.4517 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2584 | 0.1519 | 0.3141 | 0.3808 |  0.4523 |
|     1      | 0.2956 | 0.1884 | 0.3459 | 0.4168 |  0.5054 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2502 | 0.1391 | 0.3077 | 0.3778 |  0.4551 |
|     1      | 0.285  | 0.1799 | 0.3278 | 0.4019 |  0.4966 |
|     2      | 0.2069 | 0.1238 | 0.2344 | 0.2934 |  0.3697 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2596 | 0.1529 | 0.313  | 0.3802 |  0.4551 |
|     1      | 0.2859 | 0.1794 | 0.3278 | 0.4011 |  0.4983 |
|     2      | 0.2084 | 0.1254 | 0.2359 | 0.2921 |  0.3717 |
|     3      | 0.2012 | 0.1152 | 0.2357 | 0.2911 |  0.3626 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2334 | 0.1324 | 0.2817 | 0.3452 |  0.4167 |
|     1      | 0.2772 | 0.1743 | 0.319  | 0.386  |  0.483  |
|     2      | 0.202  | 0.1214 | 0.2274 | 0.283  |  0.3592 |
|     3      | 0.1874 | 0.1023 | 0.2187 | 0.2735 |  0.3501 |
|     4      | 0.2181 | 0.147  | 0.2455 | 0.2936 |  0.3558 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:53:28,606: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 112.74308443069458 |   0.258   |     0.15     |    0.314     |     0.452     |
|    1     | 70.63458728790283  |   0.268   |    0.162     |    0.323     |     0.466     |
|    2     | 211.29663681983948 |   0.232   |    0.136     |    0.271     |     0.415     |
|    3     | 193.80141687393188 |   0.223   |    0.132     |    0.259     |     0.396     |
|    4     | 101.48414778709412 |    0.21   |    0.124     |    0.243     |     0.375     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:53:28,606: Sum_Training_Time:689.9598731994629
2024-12-27 20:53:28,606: Every_Training_Time:[112.74308443069458, 70.63458728790283, 211.29663681983948, 193.80141687393188, 101.48414778709412]
2024-12-27 20:53:28,606: Forward transfer: 0.040775 Backward transfer: -0.015299999999999987
2024-12-27 20:54:02,062: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227205332/HYBRIDHYBRID_0.0001_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:54:11,781: Snapshot:0	Epoch:0	Loss:94.862	translation_Loss:94.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.14	Hits@10:6.38	Best:3.14
2024-12-27 20:54:17,816: Snapshot:0	Epoch:1	Loss:85.64	translation_Loss:85.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.83	Hits@10:13.12	Best:5.83
2024-12-27 20:54:23,889: Snapshot:0	Epoch:2	Loss:77.346	translation_Loss:77.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.35	Hits@10:17.43	Best:7.35
2024-12-27 20:54:30,027: Snapshot:0	Epoch:3	Loss:69.519	translation_Loss:69.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.73	Hits@10:21.03	Best:8.73
2024-12-27 20:54:36,070: Snapshot:0	Epoch:4	Loss:61.843	translation_Loss:61.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:25.8	Best:10.96
2024-12-27 20:54:42,162: Snapshot:0	Epoch:5	Loss:54.179	translation_Loss:54.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.47	Hits@10:30.61	Best:13.47
2024-12-27 20:54:48,336: Snapshot:0	Epoch:6	Loss:46.401	translation_Loss:46.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.98	Hits@10:34.4	Best:15.98
2024-12-27 20:54:54,482: Snapshot:0	Epoch:7	Loss:39.052	translation_Loss:39.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.23	Hits@10:37.11	Best:18.23
2024-12-27 20:55:01,037: Snapshot:0	Epoch:8	Loss:32.292	translation_Loss:32.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.06	Hits@10:39.38	Best:20.06
2024-12-27 20:55:07,127: Snapshot:0	Epoch:9	Loss:26.754	translation_Loss:26.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.55	Hits@10:40.96	Best:21.55
2024-12-27 20:55:13,199: Snapshot:0	Epoch:10	Loss:22.209	translation_Loss:22.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.59	Hits@10:42.09	Best:22.59
2024-12-27 20:55:19,332: Snapshot:0	Epoch:11	Loss:18.526	translation_Loss:18.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.25	Hits@10:42.91	Best:23.25
2024-12-27 20:55:25,385: Snapshot:0	Epoch:12	Loss:15.467	translation_Loss:15.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.78	Hits@10:43.69	Best:23.78
2024-12-27 20:55:31,494: Snapshot:0	Epoch:13	Loss:13.082	translation_Loss:13.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:44.12	Best:24.23
2024-12-27 20:55:37,625: Snapshot:0	Epoch:14	Loss:11.144	translation_Loss:11.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:44.53	Best:24.43
2024-12-27 20:55:43,716: Snapshot:0	Epoch:15	Loss:9.517	translation_Loss:9.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:44.9	Best:24.75
2024-12-27 20:55:49,884: Snapshot:0	Epoch:16	Loss:8.253	translation_Loss:8.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:45.09	Best:24.87
2024-12-27 20:55:56,016: Snapshot:0	Epoch:17	Loss:7.2	translation_Loss:7.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.05	Hits@10:45.49	Best:25.05
2024-12-27 20:56:02,101: Snapshot:0	Epoch:18	Loss:6.361	translation_Loss:6.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.25	Hits@10:45.55	Best:25.25
2024-12-27 20:56:08,190: Snapshot:0	Epoch:19	Loss:5.686	translation_Loss:5.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.27	Hits@10:45.8	Best:25.27
2024-12-27 20:56:14,264: Snapshot:0	Epoch:20	Loss:5.021	translation_Loss:5.021	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:45.75	Best:25.35
2024-12-27 20:56:20,336: Snapshot:0	Epoch:21	Loss:4.583	translation_Loss:4.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.38	Hits@10:45.94	Best:25.38
2024-12-27 20:56:26,401: Snapshot:0	Epoch:22	Loss:4.153	translation_Loss:4.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:45.86	Best:25.56
2024-12-27 20:56:32,514: Snapshot:0	Epoch:23	Loss:3.788	translation_Loss:3.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.57	Hits@10:46.09	Best:25.57
2024-12-27 20:56:38,702: Snapshot:0	Epoch:24	Loss:3.533	translation_Loss:3.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.12	Best:25.63
2024-12-27 20:56:44,864: Snapshot:0	Epoch:25	Loss:3.2	translation_Loss:3.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.06	Best:25.65
2024-12-27 20:56:51,405: Snapshot:0	Epoch:26	Loss:2.978	translation_Loss:2.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.12	Best:25.66
2024-12-27 20:56:57,455: Snapshot:0	Epoch:27	Loss:2.782	translation_Loss:2.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.23	Best:25.66
2024-12-27 20:57:03,625: Snapshot:0	Epoch:28	Loss:2.602	translation_Loss:2.602	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.2	Best:25.75
2024-12-27 20:57:09,801: Snapshot:0	Epoch:29	Loss:2.467	translation_Loss:2.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.23	Best:25.81
2024-12-27 20:57:15,889: Snapshot:0	Epoch:30	Loss:2.341	translation_Loss:2.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.32	Best:25.84
2024-12-27 20:57:21,984: Snapshot:0	Epoch:31	Loss:2.189	translation_Loss:2.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.28	Best:25.84
2024-12-27 20:57:28,006: Snapshot:0	Epoch:32	Loss:2.082	translation_Loss:2.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.3	Best:25.84
2024-12-27 20:57:34,050: Snapshot:0	Epoch:33	Loss:1.987	translation_Loss:1.987	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.99	Hits@10:46.32	Best:25.99
2024-12-27 20:57:40,098: Snapshot:0	Epoch:34	Loss:1.906	translation_Loss:1.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:46.2	Best:25.99
2024-12-27 20:57:46,139: Snapshot:0	Epoch:35	Loss:1.817	translation_Loss:1.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:26.01	Hits@10:46.15	Best:26.01
2024-12-27 20:57:52,295: Snapshot:0	Epoch:36	Loss:1.737	translation_Loss:1.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.92	Hits@10:46.15	Best:26.01
2024-12-27 20:57:58,446: Snapshot:0	Epoch:37	Loss:1.675	translation_Loss:1.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.93	Hits@10:46.22	Best:26.01
2024-12-27 20:58:04,584: Snapshot:0	Epoch:38	Loss:1.604	translation_Loss:1.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.97	Hits@10:46.23	Best:26.01
2024-12-27 20:58:10,613: Snapshot:0	Epoch:39	Loss:1.565	translation_Loss:1.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.92	Hits@10:46.1	Best:26.01
2024-12-27 20:58:16,630: Early Stopping! Snapshot: 0 Epoch: 40 Best Results: 26.01
2024-12-27 20:58:16,630: Start to training tokens! Snapshot: 0 Epoch: 40 Loss:1.51 MRR:25.83 Best Results: 26.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:58:16,630: Snapshot:0	Epoch:40	Loss:1.51	translation_Loss:1.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:45.97	Best:26.01
2024-12-27 20:58:23,385: Snapshot:0	Epoch:41	Loss:70.823	translation_Loss:69.609	multi_layer_Loss:1.214	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:45.97	Best:26.01
2024-12-27 20:58:29,612: End of token training: 0 Epoch: 42 Loss:69.716 MRR:25.83 Best Results: 26.01
2024-12-27 20:58:29,612: Snapshot:0	Epoch:42	Loss:69.716	translation_Loss:69.677	multi_layer_Loss:0.039	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.83	Hits@10:45.97	Best:26.01
2024-12-27 20:58:29,895: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_1000/0model_best.tar'
2024-12-27 20:58:32,623: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2591 | 0.1534 | 0.3139 | 0.3789 |  0.452  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:58:43,911: Snapshot:1	Epoch:0	Loss:28.668	translation_Loss:28.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.038                                                   	MRR:6.61	Hits@10:11.01	Best:6.61
2024-12-27 20:58:46,327: Snapshot:1	Epoch:1	Loss:23.885	translation_Loss:23.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.116                                                   	MRR:8.3	Hits@10:14.67	Best:8.3
2024-12-27 20:58:48,814: Snapshot:1	Epoch:2	Loss:19.325	translation_Loss:19.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.203                                                   	MRR:11.49	Hits@10:21.9	Best:11.49
2024-12-27 20:58:51,280: Snapshot:1	Epoch:3	Loss:15.212	translation_Loss:14.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.293                                                   	MRR:14.96	Hits@10:28.12	Best:14.96
2024-12-27 20:58:53,759: Snapshot:1	Epoch:4	Loss:11.881	translation_Loss:11.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:17.44	Hits@10:32.02	Best:17.44
2024-12-27 20:58:56,228: Snapshot:1	Epoch:5	Loss:9.395	translation_Loss:8.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.459                                                   	MRR:19.24	Hits@10:34.77	Best:19.24
2024-12-27 20:58:58,685: Snapshot:1	Epoch:6	Loss:7.548	translation_Loss:7.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.536                                                   	MRR:20.63	Hits@10:36.9	Best:20.63
2024-12-27 20:59:01,144: Snapshot:1	Epoch:7	Loss:6.17	translation_Loss:5.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.604                                                   	MRR:21.96	Hits@10:38.65	Best:21.96
2024-12-27 20:59:03,646: Snapshot:1	Epoch:8	Loss:5.178	translation_Loss:4.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.662                                                   	MRR:23.04	Hits@10:40.11	Best:23.04
2024-12-27 20:59:06,058: Snapshot:1	Epoch:9	Loss:4.4	translation_Loss:3.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.711                                                   	MRR:23.79	Hits@10:41.33	Best:23.79
2024-12-27 20:59:08,521: Snapshot:1	Epoch:10	Loss:3.795	translation_Loss:3.044	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.752                                                   	MRR:24.41	Hits@10:42.22	Best:24.41
2024-12-27 20:59:10,993: Snapshot:1	Epoch:11	Loss:3.299	translation_Loss:2.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.784                                                   	MRR:24.87	Hits@10:42.96	Best:24.87
2024-12-27 20:59:13,446: Snapshot:1	Epoch:12	Loss:2.937	translation_Loss:2.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:25.2	Hits@10:43.54	Best:25.2
2024-12-27 20:59:15,904: Snapshot:1	Epoch:13	Loss:2.616	translation_Loss:1.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.828                                                   	MRR:25.49	Hits@10:43.94	Best:25.49
2024-12-27 20:59:18,411: Snapshot:1	Epoch:14	Loss:2.375	translation_Loss:1.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.842                                                   	MRR:25.9	Hits@10:44.47	Best:25.9
2024-12-27 20:59:20,868: Snapshot:1	Epoch:15	Loss:2.176	translation_Loss:1.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.852                                                   	MRR:26.41	Hits@10:44.79	Best:26.41
2024-12-27 20:59:23,344: Snapshot:1	Epoch:16	Loss:1.986	translation_Loss:1.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.856                                                   	MRR:26.67	Hits@10:44.96	Best:26.67
2024-12-27 20:59:25,821: Snapshot:1	Epoch:17	Loss:1.849	translation_Loss:0.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.858                                                   	MRR:26.73	Hits@10:45.19	Best:26.73
2024-12-27 20:59:28,295: Snapshot:1	Epoch:18	Loss:1.737	translation_Loss:0.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.857                                                   	MRR:26.86	Hits@10:45.81	Best:26.86
2024-12-27 20:59:30,866: Snapshot:1	Epoch:19	Loss:1.648	translation_Loss:0.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.854                                                   	MRR:27.1	Hits@10:45.89	Best:27.1
2024-12-27 20:59:33,332: Snapshot:1	Epoch:20	Loss:1.562	translation_Loss:0.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.85                                                   	MRR:27.2	Hits@10:46.16	Best:27.2
2024-12-27 20:59:35,826: Snapshot:1	Epoch:21	Loss:1.512	translation_Loss:0.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.845                                                   	MRR:27.35	Hits@10:46.32	Best:27.35
2024-12-27 20:59:38,259: Snapshot:1	Epoch:22	Loss:1.459	translation_Loss:0.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.839                                                   	MRR:27.35	Hits@10:46.55	Best:27.35
2024-12-27 20:59:40,713: Snapshot:1	Epoch:23	Loss:1.378	translation_Loss:0.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.833                                                   	MRR:27.46	Hits@10:46.74	Best:27.46
2024-12-27 20:59:43,174: Snapshot:1	Epoch:24	Loss:1.349	translation_Loss:0.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.825                                                   	MRR:27.6	Hits@10:47.01	Best:27.6
2024-12-27 20:59:45,632: Snapshot:1	Epoch:25	Loss:1.308	translation_Loss:0.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.818                                                   	MRR:27.74	Hits@10:47.09	Best:27.74
2024-12-27 20:59:48,056: Snapshot:1	Epoch:26	Loss:1.259	translation_Loss:0.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.81                                                   	MRR:27.63	Hits@10:47.09	Best:27.74
2024-12-27 20:59:50,455: Snapshot:1	Epoch:27	Loss:1.244	translation_Loss:0.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.801                                                   	MRR:27.59	Hits@10:47.14	Best:27.74
2024-12-27 20:59:52,884: Snapshot:1	Epoch:28	Loss:1.203	translation_Loss:0.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.794                                                   	MRR:27.63	Hits@10:47.4	Best:27.74
2024-12-27 20:59:55,311: Snapshot:1	Epoch:29	Loss:1.177	translation_Loss:0.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.784                                                   	MRR:27.57	Hits@10:47.59	Best:27.74
2024-12-27 20:59:57,792: Snapshot:1	Epoch:30	Loss:1.16	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.776                                                   	MRR:27.76	Hits@10:47.47	Best:27.76
2024-12-27 21:00:00,238: Snapshot:1	Epoch:31	Loss:1.147	translation_Loss:0.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.767                                                   	MRR:27.79	Hits@10:47.7	Best:27.79
2024-12-27 21:00:02,679: Snapshot:1	Epoch:32	Loss:1.103	translation_Loss:0.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:27.77	Hits@10:47.39	Best:27.79
2024-12-27 21:00:05,159: Snapshot:1	Epoch:33	Loss:1.093	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.751                                                   	MRR:27.9	Hits@10:47.47	Best:27.9
2024-12-27 21:00:07,645: Snapshot:1	Epoch:34	Loss:1.074	translation_Loss:0.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.744                                                   	MRR:27.92	Hits@10:47.58	Best:27.92
2024-12-27 21:00:10,132: Snapshot:1	Epoch:35	Loss:1.061	translation_Loss:0.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.734                                                   	MRR:28.07	Hits@10:47.99	Best:28.07
2024-12-27 21:00:12,596: Snapshot:1	Epoch:36	Loss:1.045	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.726                                                   	MRR:28.15	Hits@10:47.93	Best:28.15
2024-12-27 21:00:15,063: Snapshot:1	Epoch:37	Loss:1.04	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.721                                                   	MRR:28.18	Hits@10:48.06	Best:28.18
2024-12-27 21:00:17,605: Snapshot:1	Epoch:38	Loss:1.017	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.715                                                   	MRR:28.25	Hits@10:48.13	Best:28.25
2024-12-27 21:00:20,003: Snapshot:1	Epoch:39	Loss:1.011	translation_Loss:0.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.708                                                   	MRR:28.12	Hits@10:48.04	Best:28.25
2024-12-27 21:00:22,432: Snapshot:1	Epoch:40	Loss:1.008	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.701                                                   	MRR:28.36	Hits@10:48.35	Best:28.36
2024-12-27 21:00:24,863: Snapshot:1	Epoch:41	Loss:0.981	translation_Loss:0.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.694                                                   	MRR:28.49	Hits@10:48.59	Best:28.49
2024-12-27 21:00:27,338: Snapshot:1	Epoch:42	Loss:0.968	translation_Loss:0.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.688                                                   	MRR:28.75	Hits@10:48.71	Best:28.75
2024-12-27 21:00:29,870: Snapshot:1	Epoch:43	Loss:0.966	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.681                                                   	MRR:28.73	Hits@10:48.9	Best:28.75
2024-12-27 21:00:32,406: Snapshot:1	Epoch:44	Loss:0.949	translation_Loss:0.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.676                                                   	MRR:28.81	Hits@10:48.79	Best:28.81
2024-12-27 21:00:34,839: Snapshot:1	Epoch:45	Loss:0.945	translation_Loss:0.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.67                                                   	MRR:28.81	Hits@10:48.78	Best:28.81
2024-12-27 21:00:37,258: Snapshot:1	Epoch:46	Loss:0.939	translation_Loss:0.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.664                                                   	MRR:28.99	Hits@10:49.14	Best:28.99
2024-12-27 21:00:39,737: Snapshot:1	Epoch:47	Loss:0.912	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.659                                                   	MRR:29.08	Hits@10:49.35	Best:29.08
2024-12-27 21:00:42,144: Snapshot:1	Epoch:48	Loss:0.906	translation_Loss:0.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.65                                                   	MRR:28.9	Hits@10:49.27	Best:29.08
2024-12-27 21:00:44,538: Snapshot:1	Epoch:49	Loss:0.909	translation_Loss:0.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.645                                                   	MRR:28.91	Hits@10:49.25	Best:29.08
2024-12-27 21:00:46,937: Snapshot:1	Epoch:50	Loss:0.89	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.64                                                   	MRR:28.85	Hits@10:49.37	Best:29.08
2024-12-27 21:00:49,344: Snapshot:1	Epoch:51	Loss:0.889	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.637                                                   	MRR:28.84	Hits@10:49.19	Best:29.08
2024-12-27 21:00:52,142: Early Stopping! Snapshot: 1 Epoch: 52 Best Results: 29.08
2024-12-27 21:00:52,142: Start to training tokens! Snapshot: 1 Epoch: 52 Loss:0.885 MRR:29.03 Best Results: 29.08
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:00:52,142: Snapshot:1	Epoch:52	Loss:0.885	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.632                                                   	MRR:29.03	Hits@10:49.45	Best:29.08
2024-12-27 21:00:54,510: Snapshot:1	Epoch:53	Loss:23.33	translation_Loss:22.565	multi_layer_Loss:0.765	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.03	Hits@10:49.45	Best:29.08
2024-12-27 21:00:56,879: End of token training: 1 Epoch: 54 Loss:22.897 MRR:29.03 Best Results: 29.08
2024-12-27 21:00:56,879: Snapshot:1	Epoch:54	Loss:22.897	translation_Loss:22.554	multi_layer_Loss:0.343	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.03	Hits@10:49.45	Best:29.08
2024-12-27 21:00:57,095: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_1000/1model_best.tar'
2024-12-27 21:01:00,853: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2445 | 0.1387 | 0.2996 | 0.3651 |  0.4412 |
|     1      | 0.2885 | 0.1838 | 0.3407 | 0.4037 |  0.4887 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:01:36,372: Snapshot:2	Epoch:0	Loss:112.627	translation_Loss:112.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.419                                                   	MRR:8.01	Hits@10:17.4	Best:8.01
2024-12-27 21:01:47,147: Snapshot:2	Epoch:1	Loss:71.798	translation_Loss:70.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.302                                                   	MRR:13.3	Hits@10:26.31	Best:13.3
2024-12-27 21:01:57,942: Snapshot:2	Epoch:2	Loss:46.165	translation_Loss:43.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.291                                                   	MRR:16.61	Hits@10:31.27	Best:16.61
2024-12-27 21:02:08,531: Snapshot:2	Epoch:3	Loss:31.93	translation_Loss:28.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.086                                                   	MRR:18.32	Hits@10:33.96	Best:18.32
2024-12-27 21:02:19,310: Snapshot:2	Epoch:4	Loss:23.858	translation_Loss:20.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.665                                                   	MRR:19.2	Hits@10:35.6	Best:19.2
2024-12-27 21:02:30,086: Snapshot:2	Epoch:5	Loss:19.052	translation_Loss:14.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.067                                                   	MRR:19.81	Hits@10:36.68	Best:19.81
2024-12-27 21:02:40,695: Snapshot:2	Epoch:6	Loss:16.099	translation_Loss:11.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.329                                                   	MRR:20.17	Hits@10:37.52	Best:20.17
2024-12-27 21:02:51,466: Snapshot:2	Epoch:7	Loss:14.18	translation_Loss:9.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.489                                                   	MRR:20.6	Hits@10:38.16	Best:20.6
2024-12-27 21:03:02,058: Snapshot:2	Epoch:8	Loss:12.828	translation_Loss:8.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.584                                                   	MRR:20.85	Hits@10:38.45	Best:20.85
2024-12-27 21:03:12,631: Snapshot:2	Epoch:9	Loss:11.853	translation_Loss:7.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.632                                                   	MRR:21.04	Hits@10:38.44	Best:21.04
2024-12-27 21:03:23,387: Snapshot:2	Epoch:10	Loss:11.152	translation_Loss:6.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.653                                                   	MRR:21.21	Hits@10:38.82	Best:21.21
2024-12-27 21:03:34,179: Snapshot:2	Epoch:11	Loss:10.616	translation_Loss:5.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.653                                                   	MRR:21.3	Hits@10:38.82	Best:21.3
2024-12-27 21:03:44,972: Snapshot:2	Epoch:12	Loss:10.212	translation_Loss:5.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.647                                                   	MRR:21.4	Hits@10:38.9	Best:21.4
2024-12-27 21:03:55,524: Snapshot:2	Epoch:13	Loss:9.887	translation_Loss:5.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.622                                                   	MRR:21.35	Hits@10:38.99	Best:21.4
2024-12-27 21:04:06,139: Snapshot:2	Epoch:14	Loss:9.667	translation_Loss:5.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.608                                                   	MRR:21.52	Hits@10:38.83	Best:21.52
2024-12-27 21:04:16,813: Snapshot:2	Epoch:15	Loss:9.387	translation_Loss:4.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.586                                                   	MRR:21.63	Hits@10:39.11	Best:21.63
2024-12-27 21:04:27,409: Snapshot:2	Epoch:16	Loss:9.192	translation_Loss:4.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.562                                                   	MRR:21.68	Hits@10:39.06	Best:21.68
2024-12-27 21:04:38,138: Snapshot:2	Epoch:17	Loss:9.037	translation_Loss:4.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.534                                                   	MRR:21.57	Hits@10:38.89	Best:21.68
2024-12-27 21:04:48,701: Snapshot:2	Epoch:18	Loss:8.958	translation_Loss:4.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.515                                                   	MRR:21.65	Hits@10:39.18	Best:21.68
2024-12-27 21:04:59,338: Snapshot:2	Epoch:19	Loss:8.845	translation_Loss:4.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.492                                                   	MRR:21.67	Hits@10:39.22	Best:21.68
2024-12-27 21:05:10,087: Snapshot:2	Epoch:20	Loss:8.727	translation_Loss:4.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.47                                                   	MRR:21.78	Hits@10:39.29	Best:21.78
2024-12-27 21:05:20,853: Snapshot:2	Epoch:21	Loss:8.598	translation_Loss:4.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.46                                                   	MRR:21.89	Hits@10:39.26	Best:21.89
2024-12-27 21:05:31,403: Snapshot:2	Epoch:22	Loss:8.532	translation_Loss:4.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.444                                                   	MRR:21.78	Hits@10:39.24	Best:21.89
2024-12-27 21:05:42,174: Snapshot:2	Epoch:23	Loss:8.454	translation_Loss:4.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.42                                                   	MRR:21.72	Hits@10:39.15	Best:21.89
2024-12-27 21:05:52,782: Snapshot:2	Epoch:24	Loss:8.414	translation_Loss:4.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.402                                                   	MRR:21.74	Hits@10:39.33	Best:21.89
2024-12-27 21:06:03,546: Snapshot:2	Epoch:25	Loss:8.355	translation_Loss:3.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.393                                                   	MRR:21.59	Hits@10:39.1	Best:21.89
2024-12-27 21:06:14,144: Early Stopping! Snapshot: 2 Epoch: 26 Best Results: 21.89
2024-12-27 21:06:14,144: Start to training tokens! Snapshot: 2 Epoch: 26 Loss:8.274 MRR:21.69 Best Results: 21.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:06:14,145: Snapshot:2	Epoch:26	Loss:8.274	translation_Loss:3.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.386                                                   	MRR:21.69	Hits@10:39.24	Best:21.89
2024-12-27 21:06:24,435: Snapshot:2	Epoch:27	Loss:100.745	translation_Loss:99.563	multi_layer_Loss:1.182	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.69	Hits@10:39.24	Best:21.89
2024-12-27 21:06:34,866: End of token training: 2 Epoch: 28 Loss:99.538 MRR:21.69 Best Results: 21.89
2024-12-27 21:06:34,866: Snapshot:2	Epoch:28	Loss:99.538	translation_Loss:99.535	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.69	Hits@10:39.24	Best:21.89
2024-12-27 21:06:35,160: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_1000/2model_best.tar'
2024-12-27 21:06:43,407: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1375 | 0.2971 | 0.3665 |  0.4486 |
|     1      | 0.2535 | 0.1486 | 0.3007 | 0.3632 |  0.4442 |
|     2      | 0.2179 | 0.1275 | 0.2509 | 0.3136 |  0.3976 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:07:24,196: Snapshot:3	Epoch:0	Loss:107.561	translation_Loss:106.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.625                                                   	MRR:9.79	Hits@10:21.58	Best:9.79
2024-12-27 21:07:37,412: Snapshot:3	Epoch:1	Loss:54.485	translation_Loss:52.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.841                                                   	MRR:14.74	Hits@10:29.91	Best:14.74
2024-12-27 21:07:50,521: Snapshot:3	Epoch:2	Loss:32.145	translation_Loss:29.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.904                                                   	MRR:17.4	Hits@10:34.42	Best:17.4
2024-12-27 21:08:03,704: Snapshot:3	Epoch:3	Loss:21.973	translation_Loss:18.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.617                                                   	MRR:19.2	Hits@10:36.62	Best:19.2
2024-12-27 21:08:16,716: Snapshot:3	Epoch:4	Loss:16.707	translation_Loss:12.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.043                                                   	MRR:19.85	Hits@10:37.69	Best:19.85
2024-12-27 21:08:29,704: Snapshot:3	Epoch:5	Loss:13.985	translation_Loss:9.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.271                                                   	MRR:20.43	Hits@10:38.33	Best:20.43
2024-12-27 21:08:43,126: Snapshot:3	Epoch:6	Loss:12.291	translation_Loss:7.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.389                                                   	MRR:20.66	Hits@10:38.93	Best:20.66
2024-12-27 21:08:56,192: Snapshot:3	Epoch:7	Loss:11.144	translation_Loss:6.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.424                                                   	MRR:20.89	Hits@10:39.16	Best:20.89
2024-12-27 21:09:09,389: Snapshot:3	Epoch:8	Loss:10.403	translation_Loss:5.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.439                                                   	MRR:21.14	Hits@10:39.39	Best:21.14
2024-12-27 21:09:22,596: Snapshot:3	Epoch:9	Loss:9.841	translation_Loss:5.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.431                                                   	MRR:21.34	Hits@10:39.82	Best:21.34
2024-12-27 21:09:35,939: Snapshot:3	Epoch:10	Loss:9.402	translation_Loss:5.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.395                                                   	MRR:21.5	Hits@10:39.88	Best:21.5
2024-12-27 21:09:49,086: Snapshot:3	Epoch:11	Loss:9.106	translation_Loss:4.73	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.377                                                   	MRR:21.69	Hits@10:40.34	Best:21.69
2024-12-27 21:10:02,239: Snapshot:3	Epoch:12	Loss:8.886	translation_Loss:4.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.342                                                   	MRR:21.72	Hits@10:40.34	Best:21.72
2024-12-27 21:10:15,379: Snapshot:3	Epoch:13	Loss:8.667	translation_Loss:4.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.323                                                   	MRR:21.63	Hits@10:40.45	Best:21.72
2024-12-27 21:10:28,382: Snapshot:3	Epoch:14	Loss:8.449	translation_Loss:4.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.295                                                   	MRR:21.8	Hits@10:40.51	Best:21.8
2024-12-27 21:10:41,625: Snapshot:3	Epoch:15	Loss:8.326	translation_Loss:4.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.262                                                   	MRR:21.85	Hits@10:40.5	Best:21.85
2024-12-27 21:10:55,002: Snapshot:3	Epoch:16	Loss:8.154	translation_Loss:3.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.235                                                   	MRR:21.94	Hits@10:40.61	Best:21.94
2024-12-27 21:11:08,075: Snapshot:3	Epoch:17	Loss:8.074	translation_Loss:3.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.213                                                   	MRR:21.78	Hits@10:40.61	Best:21.94
2024-12-27 21:11:21,203: Snapshot:3	Epoch:18	Loss:7.958	translation_Loss:3.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.183                                                   	MRR:21.86	Hits@10:40.64	Best:21.94
2024-12-27 21:11:34,288: Snapshot:3	Epoch:19	Loss:7.932	translation_Loss:3.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.161                                                   	MRR:21.92	Hits@10:40.77	Best:21.94
2024-12-27 21:11:47,503: Snapshot:3	Epoch:20	Loss:7.807	translation_Loss:3.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.138                                                   	MRR:21.98	Hits@10:40.81	Best:21.98
2024-12-27 21:12:00,696: Snapshot:3	Epoch:21	Loss:7.754	translation_Loss:3.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.115                                                   	MRR:21.98	Hits@10:40.89	Best:21.98
2024-12-27 21:12:13,672: Snapshot:3	Epoch:22	Loss:7.723	translation_Loss:3.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.11                                                   	MRR:21.96	Hits@10:41.05	Best:21.98
2024-12-27 21:12:26,666: Snapshot:3	Epoch:23	Loss:7.639	translation_Loss:3.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.105                                                   	MRR:22.01	Hits@10:40.74	Best:22.01
2024-12-27 21:12:39,924: Snapshot:3	Epoch:24	Loss:7.632	translation_Loss:3.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.078                                                   	MRR:21.87	Hits@10:40.6	Best:22.01
2024-12-27 21:12:52,834: Snapshot:3	Epoch:25	Loss:7.538	translation_Loss:3.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.067                                                   	MRR:21.88	Hits@10:40.7	Best:22.01
2024-12-27 21:13:06,317: Snapshot:3	Epoch:26	Loss:7.489	translation_Loss:3.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.053                                                   	MRR:21.93	Hits@10:40.88	Best:22.01
2024-12-27 21:13:19,379: Snapshot:3	Epoch:27	Loss:7.479	translation_Loss:3.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.04                                                   	MRR:21.97	Hits@10:40.7	Best:22.01
2024-12-27 21:13:32,375: Early Stopping! Snapshot: 3 Epoch: 28 Best Results: 22.01
2024-12-27 21:13:32,376: Start to training tokens! Snapshot: 3 Epoch: 28 Loss:7.457 MRR:21.98 Best Results: 22.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:13:32,376: Snapshot:3	Epoch:28	Loss:7.457	translation_Loss:3.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.027                                                   	MRR:21.98	Hits@10:40.84	Best:22.01
2024-12-27 21:13:45,165: Snapshot:3	Epoch:29	Loss:106.51	translation_Loss:105.259	multi_layer_Loss:1.252	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.98	Hits@10:40.84	Best:22.01
2024-12-27 21:13:57,777: End of token training: 3 Epoch: 30 Loss:105.313 MRR:21.98 Best Results: 22.01
2024-12-27 21:13:57,777: Snapshot:3	Epoch:30	Loss:105.313	translation_Loss:105.312	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.98	Hits@10:40.84	Best:22.01
2024-12-27 21:13:58,064: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_1000/3model_best.tar'
2024-12-27 21:14:11,854: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2467 | 0.1464 | 0.2898 | 0.3556 |  0.4375 |
|     1      | 0.2511 | 0.1577 | 0.2829 | 0.3411 |  0.4354 |
|     2      | 0.2086 | 0.121  | 0.2388 | 0.2965 |  0.3801 |
|     3      | 0.219  | 0.1216 | 0.2575 | 0.3217 |  0.4054 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:14:30,756: Snapshot:4	Epoch:0	Loss:37.53	translation_Loss:37.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.076                                                   	MRR:7.99	Hits@10:16.89	Best:7.99
2024-12-27 21:14:36,217: Snapshot:4	Epoch:1	Loss:24.297	translation_Loss:24.049	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.248                                                   	MRR:15.24	Hits@10:29.87	Best:15.24
2024-12-27 21:14:41,567: Snapshot:4	Epoch:2	Loss:16.244	translation_Loss:15.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.474                                                   	MRR:18.21	Hits@10:33.37	Best:18.21
2024-12-27 21:14:46,923: Snapshot:4	Epoch:3	Loss:11.631	translation_Loss:10.917	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.714                                                   	MRR:19.46	Hits@10:35.25	Best:19.46
2024-12-27 21:14:52,338: Snapshot:4	Epoch:4	Loss:8.637	translation_Loss:7.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.94                                                   	MRR:20.37	Hits@10:36.29	Best:20.37
2024-12-27 21:14:57,765: Snapshot:4	Epoch:5	Loss:6.535	translation_Loss:5.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.131                                                   	MRR:20.53	Hits@10:36.36	Best:20.53
2024-12-27 21:15:03,162: Snapshot:4	Epoch:6	Loss:5.137	translation_Loss:3.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.282                                                   	MRR:20.92	Hits@10:36.75	Best:20.92
2024-12-27 21:15:08,953: Snapshot:4	Epoch:7	Loss:4.206	translation_Loss:2.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.389                                                   	MRR:21.35	Hits@10:37.08	Best:21.35
2024-12-27 21:15:14,374: Snapshot:4	Epoch:8	Loss:3.574	translation_Loss:2.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.461                                                   	MRR:21.78	Hits@10:37.56	Best:21.78
2024-12-27 21:15:19,820: Snapshot:4	Epoch:9	Loss:3.123	translation_Loss:1.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.5                                                   	MRR:21.82	Hits@10:37.41	Best:21.82
2024-12-27 21:15:25,117: Snapshot:4	Epoch:10	Loss:2.83	translation_Loss:1.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.515                                                   	MRR:21.78	Hits@10:37.21	Best:21.82
2024-12-27 21:15:30,461: Snapshot:4	Epoch:11	Loss:2.634	translation_Loss:1.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.517                                                   	MRR:21.89	Hits@10:37.5	Best:21.89
2024-12-27 21:15:35,804: Snapshot:4	Epoch:12	Loss:2.476	translation_Loss:0.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.512                                                   	MRR:21.83	Hits@10:37.61	Best:21.89
2024-12-27 21:15:41,170: Snapshot:4	Epoch:13	Loss:2.374	translation_Loss:0.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.498                                                   	MRR:22.23	Hits@10:37.83	Best:22.23
2024-12-27 21:15:46,559: Snapshot:4	Epoch:14	Loss:2.258	translation_Loss:0.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.486                                                   	MRR:22.33	Hits@10:37.57	Best:22.33
2024-12-27 21:15:51,984: Snapshot:4	Epoch:15	Loss:2.21	translation_Loss:0.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.471                                                   	MRR:22.34	Hits@10:37.99	Best:22.34
2024-12-27 21:15:57,292: Snapshot:4	Epoch:16	Loss:2.139	translation_Loss:0.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.455                                                   	MRR:22.53	Hits@10:38.31	Best:22.53
2024-12-27 21:16:02,680: Snapshot:4	Epoch:17	Loss:2.102	translation_Loss:0.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.44                                                   	MRR:22.37	Hits@10:38.17	Best:22.53
2024-12-27 21:16:08,026: Snapshot:4	Epoch:18	Loss:2.06	translation_Loss:0.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.425                                                   	MRR:22.55	Hits@10:38.35	Best:22.55
2024-12-27 21:16:13,298: Snapshot:4	Epoch:19	Loss:2.016	translation_Loss:0.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.411                                                   	MRR:22.49	Hits@10:38.4	Best:22.55
2024-12-27 21:16:18,666: Snapshot:4	Epoch:20	Loss:1.991	translation_Loss:0.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.397                                                   	MRR:22.62	Hits@10:38.48	Best:22.62
2024-12-27 21:16:24,007: Snapshot:4	Epoch:21	Loss:1.966	translation_Loss:0.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.385                                                   	MRR:22.21	Hits@10:38.49	Best:22.62
2024-12-27 21:16:29,367: Snapshot:4	Epoch:22	Loss:1.919	translation_Loss:0.55	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.369                                                   	MRR:22.42	Hits@10:38.28	Best:22.62
2024-12-27 21:16:34,640: Snapshot:4	Epoch:23	Loss:1.916	translation_Loss:0.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.356                                                   	MRR:22.36	Hits@10:38.15	Best:22.62
2024-12-27 21:16:39,991: Snapshot:4	Epoch:24	Loss:1.894	translation_Loss:0.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.346                                                   	MRR:21.87	Hits@10:38.17	Best:22.62
2024-12-27 21:16:45,303: Early Stopping! Snapshot: 4 Epoch: 25 Best Results: 22.62
2024-12-27 21:16:45,303: Start to training tokens! Snapshot: 4 Epoch: 25 Loss:1.868 MRR:22.17 Best Results: 22.62
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:16:45,303: Snapshot:4	Epoch:25	Loss:1.868	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.333                                                   	MRR:22.17	Hits@10:38.01	Best:22.62
2024-12-27 21:16:50,374: Snapshot:4	Epoch:26	Loss:46.712	translation_Loss:45.559	multi_layer_Loss:1.153	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.17	Hits@10:38.01	Best:22.62
2024-12-27 21:16:55,437: End of token training: 4 Epoch: 27 Loss:45.715 MRR:22.17 Best Results: 22.62
2024-12-27 21:16:55,438: Snapshot:4	Epoch:27	Loss:45.715	translation_Loss:45.57	multi_layer_Loss:0.145	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.17	Hits@10:38.01	Best:22.62
2024-12-27 21:16:55,732: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_1000/4model_best.tar'
2024-12-27 21:17:12,047: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.191  | 0.1017 | 0.2264 | 0.2847 |  0.361  |
|     1      | 0.2167 | 0.1274 | 0.2414 | 0.3047 |  0.4006 |
|     2      | 0.1791 | 0.1024 | 0.2003 | 0.252  |  0.3278 |
|     3      | 0.1718 | 0.0851 | 0.2023 | 0.2594 |  0.3364 |
|     4      | 0.2252 | 0.1424 | 0.2538 |  0.31  |  0.3817 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 21:17:12,049: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2591 | 0.1534 | 0.3139 | 0.3789 |  0.452  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2445 | 0.1387 | 0.2996 | 0.3651 |  0.4412 |
|     1      | 0.2885 | 0.1838 | 0.3407 | 0.4037 |  0.4887 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1375 | 0.2971 | 0.3665 |  0.4486 |
|     1      | 0.2535 | 0.1486 | 0.3007 | 0.3632 |  0.4442 |
|     2      | 0.2179 | 0.1275 | 0.2509 | 0.3136 |  0.3976 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2467 | 0.1464 | 0.2898 | 0.3556 |  0.4375 |
|     1      | 0.2511 | 0.1577 | 0.2829 | 0.3411 |  0.4354 |
|     2      | 0.2086 | 0.121  | 0.2388 | 0.2965 |  0.3801 |
|     3      | 0.219  | 0.1216 | 0.2575 | 0.3217 |  0.4054 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.191  | 0.1017 | 0.2264 | 0.2847 |  0.361  |
|     1      | 0.2167 | 0.1274 | 0.2414 | 0.3047 |  0.4006 |
|     2      | 0.1791 | 0.1024 | 0.2003 | 0.252  |  0.3278 |
|     3      | 0.1718 | 0.0851 | 0.2023 | 0.2594 |  0.3364 |
|     4      | 0.2252 | 0.1424 | 0.2538 |  0.31  |  0.3817 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 21:17:12,050: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  267.549955368042  |   0.259   |    0.153     |    0.314     |     0.452     |
|    1     | 143.02334594726562 |   0.256   |    0.151     |    0.311     |     0.454     |
|    2     | 329.7656059265137  |   0.232   |    0.134     |    0.273     |     0.421     |
|    3     | 429.1045386791229  |   0.224   |    0.129     |     0.26     |     0.406     |
|    4     | 161.05332922935486 |   0.187   |    0.103     |    0.216     |     0.349     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 21:17:12,050: Sum_Training_Time:1330.496775150299
2024-12-27 21:17:12,050: Every_Training_Time:[267.549955368042, 143.02334594726562, 329.7656059265137, 429.1045386791229, 161.05332922935486]
2024-12-27 21:17:12,050: Forward transfer: 0.045175 Backward transfer: -0.05647499999999999
2024-12-27 21:17:48,071: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227211716/HYBRIDHYBRID_0.0001_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 21:17:57,904: Snapshot:0	Epoch:0	Loss:94.862	translation_Loss:94.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.14	Hits@10:6.38	Best:3.14
2024-12-27 21:18:04,051: Snapshot:0	Epoch:1	Loss:85.64	translation_Loss:85.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.83	Hits@10:13.12	Best:5.83
2024-12-27 21:18:10,281: Snapshot:0	Epoch:2	Loss:77.346	translation_Loss:77.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.35	Hits@10:17.43	Best:7.35
2024-12-27 21:18:16,510: Snapshot:0	Epoch:3	Loss:69.519	translation_Loss:69.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.73	Hits@10:21.03	Best:8.73
2024-12-27 21:18:22,773: Snapshot:0	Epoch:4	Loss:61.844	translation_Loss:61.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:25.8	Best:10.96
2024-12-27 21:18:29,031: Snapshot:0	Epoch:5	Loss:54.179	translation_Loss:54.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.47	Hits@10:30.6	Best:13.47
2024-12-27 21:18:35,264: Snapshot:0	Epoch:6	Loss:46.402	translation_Loss:46.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.97	Hits@10:34.43	Best:15.97
2024-12-27 21:18:41,490: Snapshot:0	Epoch:7	Loss:39.053	translation_Loss:39.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.22	Hits@10:37.16	Best:18.22
2024-12-27 21:18:48,107: Snapshot:0	Epoch:8	Loss:32.293	translation_Loss:32.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.05	Hits@10:39.35	Best:20.05
2024-12-27 21:18:54,356: Snapshot:0	Epoch:9	Loss:26.754	translation_Loss:26.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.52	Hits@10:41.0	Best:21.52
2024-12-27 21:19:00,596: Snapshot:0	Epoch:10	Loss:22.209	translation_Loss:22.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.59	Hits@10:42.02	Best:22.59
2024-12-27 21:19:06,752: Snapshot:0	Epoch:11	Loss:18.527	translation_Loss:18.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.28	Hits@10:42.9	Best:23.28
2024-12-27 21:19:13,026: Snapshot:0	Epoch:12	Loss:15.468	translation_Loss:15.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.81	Hits@10:43.72	Best:23.81
2024-12-27 21:19:19,293: Snapshot:0	Epoch:13	Loss:13.083	translation_Loss:13.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.27	Hits@10:44.22	Best:24.27
2024-12-27 21:19:25,568: Snapshot:0	Epoch:14	Loss:11.148	translation_Loss:11.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:44.43	Best:24.41
2024-12-27 21:19:31,770: Snapshot:0	Epoch:15	Loss:9.52	translation_Loss:9.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:44.86	Best:24.76
2024-12-27 21:19:38,065: Snapshot:0	Epoch:16	Loss:8.253	translation_Loss:8.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.1	Best:24.9
2024-12-27 21:19:44,265: Snapshot:0	Epoch:17	Loss:7.201	translation_Loss:7.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.13	Hits@10:45.47	Best:25.13
2024-12-27 21:19:50,462: Snapshot:0	Epoch:18	Loss:6.36	translation_Loss:6.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.19	Hits@10:45.57	Best:25.19
2024-12-27 21:19:56,683: Snapshot:0	Epoch:19	Loss:5.687	translation_Loss:5.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.17	Hits@10:45.7	Best:25.19
2024-12-27 21:20:02,897: Snapshot:0	Epoch:20	Loss:5.017	translation_Loss:5.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:45.8	Best:25.31
2024-12-27 21:20:09,190: Snapshot:0	Epoch:21	Loss:4.584	translation_Loss:4.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.38	Hits@10:45.9	Best:25.38
2024-12-27 21:20:15,359: Snapshot:0	Epoch:22	Loss:4.154	translation_Loss:4.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.47	Hits@10:45.89	Best:25.47
2024-12-27 21:20:21,579: Snapshot:0	Epoch:23	Loss:3.787	translation_Loss:3.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:46.11	Best:25.53
2024-12-27 21:20:27,733: Snapshot:0	Epoch:24	Loss:3.534	translation_Loss:3.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.57	Hits@10:46.13	Best:25.57
2024-12-27 21:20:33,974: Snapshot:0	Epoch:25	Loss:3.196	translation_Loss:3.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.08	Best:25.57
2024-12-27 21:20:40,587: Snapshot:0	Epoch:26	Loss:2.977	translation_Loss:2.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.61	Hits@10:46.08	Best:25.61
2024-12-27 21:20:46,870: Snapshot:0	Epoch:27	Loss:2.782	translation_Loss:2.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:46.28	Best:25.68
2024-12-27 21:20:53,032: Snapshot:0	Epoch:28	Loss:2.603	translation_Loss:2.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:46.17	Best:25.68
2024-12-27 21:20:59,247: Snapshot:0	Epoch:29	Loss:2.466	translation_Loss:2.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.06	Best:25.76
2024-12-27 21:21:05,433: Snapshot:0	Epoch:30	Loss:2.338	translation_Loss:2.338	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.19	Best:25.84
2024-12-27 21:21:11,679: Snapshot:0	Epoch:31	Loss:2.189	translation_Loss:2.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.32	Best:25.85
2024-12-27 21:21:17,858: Snapshot:0	Epoch:32	Loss:2.081	translation_Loss:2.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.37	Best:25.85
2024-12-27 21:21:24,144: Snapshot:0	Epoch:33	Loss:1.99	translation_Loss:1.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.92	Hits@10:46.22	Best:25.92
2024-12-27 21:21:30,350: Snapshot:0	Epoch:34	Loss:1.905	translation_Loss:1.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:46.21	Best:25.92
2024-12-27 21:21:36,549: Snapshot:0	Epoch:35	Loss:1.815	translation_Loss:1.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.89	Hits@10:46.06	Best:25.92
2024-12-27 21:21:42,691: Snapshot:0	Epoch:36	Loss:1.735	translation_Loss:1.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.86	Hits@10:46.05	Best:25.92
2024-12-27 21:21:48,852: Snapshot:0	Epoch:37	Loss:1.672	translation_Loss:1.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.11	Best:25.92
2024-12-27 21:21:55,118: Snapshot:0	Epoch:38	Loss:1.607	translation_Loss:1.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.94	Hits@10:46.14	Best:25.94
2024-12-27 21:22:01,329: Snapshot:0	Epoch:39	Loss:1.565	translation_Loss:1.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:45.99	Best:25.94
2024-12-27 21:22:07,459: Snapshot:0	Epoch:40	Loss:1.507	translation_Loss:1.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.09	Best:25.94
2024-12-27 21:22:13,680: Snapshot:0	Epoch:41	Loss:1.442	translation_Loss:1.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.91	Hits@10:45.98	Best:25.94
2024-12-27 21:22:19,816: Snapshot:0	Epoch:42	Loss:1.39	translation_Loss:1.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:45.96	Best:25.94
2024-12-27 21:22:25,978: Early Stopping! Snapshot: 0 Epoch: 43 Best Results: 25.94
2024-12-27 21:22:25,978: Start to training tokens! Snapshot: 0 Epoch: 43 Loss:1.356 MRR:25.7 Best Results: 25.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:22:25,978: Snapshot:0	Epoch:43	Loss:1.356	translation_Loss:1.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:45.92	Best:25.94
2024-12-27 21:22:33,205: Snapshot:0	Epoch:44	Loss:70.705	translation_Loss:69.491	multi_layer_Loss:1.214	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:45.92	Best:25.94
2024-12-27 21:22:39,569: End of token training: 0 Epoch: 45 Loss:69.539 MRR:25.7 Best Results: 25.94
2024-12-27 21:22:39,570: Snapshot:0	Epoch:45	Loss:69.539	translation_Loss:69.5	multi_layer_Loss:0.039	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.7	Hits@10:45.92	Best:25.94
2024-12-27 21:22:39,799: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_5000/0model_best.tar'
2024-12-27 21:22:42,294: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2585 | 0.1533 | 0.3127 | 0.3778 |  0.4519 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:22:53,633: Snapshot:1	Epoch:0	Loss:28.743	translation_Loss:28.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.103                                                   	MRR:6.58	Hits@10:11.05	Best:6.58
2024-12-27 21:22:56,134: Snapshot:1	Epoch:1	Loss:24.124	translation_Loss:23.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:8.17	Hits@10:14.8	Best:8.17
2024-12-27 21:22:58,646: Snapshot:1	Epoch:2	Loss:19.81	translation_Loss:19.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.468                                                   	MRR:11.32	Hits@10:21.53	Best:11.32
2024-12-27 21:23:01,143: Snapshot:1	Epoch:3	Loss:16.022	translation_Loss:15.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.7                                                   	MRR:14.63	Hits@10:27.51	Best:14.63
2024-12-27 21:23:03,612: Snapshot:1	Epoch:4	Loss:12.989	translation_Loss:12.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.918                                                   	MRR:17.18	Hits@10:31.33	Best:17.18
2024-12-27 21:23:06,109: Snapshot:1	Epoch:5	Loss:10.686	translation_Loss:9.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.102                                                   	MRR:18.97	Hits@10:33.8	Best:18.97
2024-12-27 21:23:08,600: Snapshot:1	Epoch:6	Loss:9.029	translation_Loss:7.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.243                                                   	MRR:20.38	Hits@10:35.92	Best:20.38
2024-12-27 21:23:11,099: Snapshot:1	Epoch:7	Loss:7.77	translation_Loss:6.423	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.346                                                   	MRR:21.72	Hits@10:37.96	Best:21.72
2024-12-27 21:23:13,647: Snapshot:1	Epoch:8	Loss:6.853	translation_Loss:5.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.419                                                   	MRR:22.76	Hits@10:39.49	Best:22.76
2024-12-27 21:23:16,160: Snapshot:1	Epoch:9	Loss:6.123	translation_Loss:4.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.466                                                   	MRR:23.58	Hits@10:41.14	Best:23.58
2024-12-27 21:23:18,663: Snapshot:1	Epoch:10	Loss:5.536	translation_Loss:4.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.495                                                   	MRR:24.18	Hits@10:42.05	Best:24.18
2024-12-27 21:23:21,157: Snapshot:1	Epoch:11	Loss:5.07	translation_Loss:3.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.509                                                   	MRR:24.75	Hits@10:43.03	Best:24.75
2024-12-27 21:23:23,705: Snapshot:1	Epoch:12	Loss:4.627	translation_Loss:3.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.511                                                   	MRR:25.08	Hits@10:43.62	Best:25.08
2024-12-27 21:23:26,182: Snapshot:1	Epoch:13	Loss:4.317	translation_Loss:2.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.504                                                   	MRR:25.58	Hits@10:44.44	Best:25.58
2024-12-27 21:23:28,715: Snapshot:1	Epoch:14	Loss:4.019	translation_Loss:2.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.494                                                   	MRR:25.97	Hits@10:44.99	Best:25.97
2024-12-27 21:23:31,277: Snapshot:1	Epoch:15	Loss:3.781	translation_Loss:2.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.483                                                   	MRR:26.31	Hits@10:45.41	Best:26.31
2024-12-27 21:23:33,724: Snapshot:1	Epoch:16	Loss:3.574	translation_Loss:2.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:26.56	Hits@10:45.81	Best:26.56
2024-12-27 21:23:36,160: Snapshot:1	Epoch:17	Loss:3.401	translation_Loss:1.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.448                                                   	MRR:26.42	Hits@10:46.01	Best:26.56
2024-12-27 21:23:38,613: Snapshot:1	Epoch:18	Loss:3.255	translation_Loss:1.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.428                                                   	MRR:26.66	Hits@10:46.44	Best:26.66
2024-12-27 21:23:41,110: Snapshot:1	Epoch:19	Loss:3.136	translation_Loss:1.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.411                                                   	MRR:26.85	Hits@10:46.59	Best:26.85
2024-12-27 21:23:43,594: Snapshot:1	Epoch:20	Loss:3.006	translation_Loss:1.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.391                                                   	MRR:27.12	Hits@10:46.79	Best:27.12
2024-12-27 21:23:46,092: Snapshot:1	Epoch:21	Loss:2.911	translation_Loss:1.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.372                                                   	MRR:27.28	Hits@10:47.08	Best:27.28
2024-12-27 21:23:48,597: Snapshot:1	Epoch:22	Loss:2.842	translation_Loss:1.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.356                                                   	MRR:27.45	Hits@10:47.22	Best:27.45
2024-12-27 21:23:51,254: Snapshot:1	Epoch:23	Loss:2.734	translation_Loss:1.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.336                                                   	MRR:27.52	Hits@10:47.31	Best:27.52
2024-12-27 21:23:53,776: Snapshot:1	Epoch:24	Loss:2.67	translation_Loss:1.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.319                                                   	MRR:27.63	Hits@10:47.52	Best:27.63
2024-12-27 21:23:56,311: Snapshot:1	Epoch:25	Loss:2.618	translation_Loss:1.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.303                                                   	MRR:27.83	Hits@10:47.74	Best:27.83
2024-12-27 21:23:58,908: Snapshot:1	Epoch:26	Loss:2.544	translation_Loss:1.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.286                                                   	MRR:27.85	Hits@10:47.93	Best:27.85
2024-12-27 21:24:01,389: Snapshot:1	Epoch:27	Loss:2.476	translation_Loss:1.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.271                                                   	MRR:27.96	Hits@10:48.5	Best:27.96
2024-12-27 21:24:03,865: Snapshot:1	Epoch:28	Loss:2.435	translation_Loss:1.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.254                                                   	MRR:28.03	Hits@10:48.56	Best:28.03
2024-12-27 21:24:06,378: Snapshot:1	Epoch:29	Loss:2.407	translation_Loss:1.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.24                                                   	MRR:28.11	Hits@10:48.86	Best:28.11
2024-12-27 21:24:08,885: Snapshot:1	Epoch:30	Loss:2.344	translation_Loss:1.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.227                                                   	MRR:28.35	Hits@10:48.97	Best:28.35
2024-12-27 21:24:11,378: Snapshot:1	Epoch:31	Loss:2.297	translation_Loss:1.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.213                                                   	MRR:28.38	Hits@10:49.14	Best:28.38
2024-12-27 21:24:13,895: Snapshot:1	Epoch:32	Loss:2.262	translation_Loss:1.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.2                                                   	MRR:28.67	Hits@10:49.2	Best:28.67
2024-12-27 21:24:16,344: Snapshot:1	Epoch:33	Loss:2.223	translation_Loss:1.036	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.187                                                   	MRR:28.77	Hits@10:49.43	Best:28.77
2024-12-27 21:24:18,770: Snapshot:1	Epoch:34	Loss:2.196	translation_Loss:1.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.177                                                   	MRR:28.69	Hits@10:49.35	Best:28.77
2024-12-27 21:24:21,238: Snapshot:1	Epoch:35	Loss:2.163	translation_Loss:0.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.169                                                   	MRR:28.75	Hits@10:49.53	Best:28.77
2024-12-27 21:24:23,710: Snapshot:1	Epoch:36	Loss:2.125	translation_Loss:0.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.153                                                   	MRR:28.83	Hits@10:49.64	Best:28.83
2024-12-27 21:24:26,185: Snapshot:1	Epoch:37	Loss:2.106	translation_Loss:0.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.142                                                   	MRR:28.95	Hits@10:49.8	Best:28.95
2024-12-27 21:24:28,684: Snapshot:1	Epoch:38	Loss:2.067	translation_Loss:0.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.133                                                   	MRR:28.97	Hits@10:49.56	Best:28.97
2024-12-27 21:24:31,218: Snapshot:1	Epoch:39	Loss:2.066	translation_Loss:0.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.122                                                   	MRR:29.02	Hits@10:49.94	Best:29.02
2024-12-27 21:24:33,649: Snapshot:1	Epoch:40	Loss:2.016	translation_Loss:0.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.114                                                   	MRR:28.97	Hits@10:49.84	Best:29.02
2024-12-27 21:24:36,099: Snapshot:1	Epoch:41	Loss:2.007	translation_Loss:0.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.103                                                   	MRR:29.05	Hits@10:49.87	Best:29.05
2024-12-27 21:24:38,614: Snapshot:1	Epoch:42	Loss:1.995	translation_Loss:0.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.092                                                   	MRR:29.21	Hits@10:50.17	Best:29.21
2024-12-27 21:24:41,039: Snapshot:1	Epoch:43	Loss:1.96	translation_Loss:0.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.085                                                   	MRR:29.21	Hits@10:50.2	Best:29.21
2024-12-27 21:24:43,503: Snapshot:1	Epoch:44	Loss:1.928	translation_Loss:0.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.078                                                   	MRR:29.21	Hits@10:50.23	Best:29.21
2024-12-27 21:24:45,930: Snapshot:1	Epoch:45	Loss:1.918	translation_Loss:0.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.071                                                   	MRR:29.2	Hits@10:50.19	Best:29.21
2024-12-27 21:24:48,343: Snapshot:1	Epoch:46	Loss:1.902	translation_Loss:0.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.063                                                   	MRR:29.21	Hits@10:50.16	Best:29.21
2024-12-27 21:24:50,802: Early Stopping! Snapshot: 1 Epoch: 47 Best Results: 29.21
2024-12-27 21:24:50,802: Start to training tokens! Snapshot: 1 Epoch: 47 Loss:1.884 MRR:29.0 Best Results: 29.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:24:50,802: Snapshot:1	Epoch:47	Loss:1.884	translation_Loss:0.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.051                                                   	MRR:29.0	Hits@10:50.04	Best:29.21
2024-12-27 21:24:53,176: Snapshot:1	Epoch:48	Loss:24.63	translation_Loss:23.865	multi_layer_Loss:0.765	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.0	Hits@10:50.04	Best:29.21
2024-12-27 21:24:55,538: End of token training: 1 Epoch: 49 Loss:24.236 MRR:29.0 Best Results: 29.21
2024-12-27 21:24:55,539: Snapshot:1	Epoch:49	Loss:24.236	translation_Loss:23.893	multi_layer_Loss:0.343	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.0	Hits@10:50.04	Best:29.21
2024-12-27 21:24:55,796: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_5000/1model_best.tar'
2024-12-27 21:24:59,772: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 | 0.1521 | 0.3121 | 0.377  |   0.45  |
|     1      | 0.2894 | 0.1816 | 0.3398 | 0.4053 |  0.5002 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:25:34,897: Snapshot:2	Epoch:0	Loss:113.68	translation_Loss:112.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.105                                                   	MRR:7.94	Hits@10:17.2	Best:7.94
2024-12-27 21:25:45,674: Snapshot:2	Epoch:1	Loss:75.877	translation_Loss:72.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.096                                                   	MRR:13.03	Hits@10:26.11	Best:13.03
2024-12-27 21:25:56,666: Snapshot:2	Epoch:2	Loss:52.291	translation_Loss:47.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.592                                                   	MRR:16.08	Hits@10:31.0	Best:16.08
2024-12-27 21:26:07,535: Snapshot:2	Epoch:3	Loss:38.808	translation_Loss:33.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.235                                                   	MRR:17.9	Hits@10:33.66	Best:17.9
2024-12-27 21:26:18,308: Snapshot:2	Epoch:4	Loss:31.115	translation_Loss:25.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.411                                                   	MRR:18.75	Hits@10:35.32	Best:18.75
2024-12-27 21:26:29,146: Snapshot:2	Epoch:5	Loss:26.482	translation_Loss:21.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.406                                                   	MRR:19.39	Hits@10:36.36	Best:19.39
2024-12-27 21:26:40,404: Snapshot:2	Epoch:6	Loss:23.493	translation_Loss:18.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.328                                                   	MRR:19.89	Hits@10:37.12	Best:19.89
2024-12-27 21:26:51,272: Snapshot:2	Epoch:7	Loss:21.475	translation_Loss:16.239	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.235                                                   	MRR:20.36	Hits@10:37.67	Best:20.36
2024-12-27 21:27:02,221: Snapshot:2	Epoch:8	Loss:19.995	translation_Loss:14.852	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.143                                                   	MRR:20.56	Hits@10:37.95	Best:20.56
2024-12-27 21:27:13,227: Snapshot:2	Epoch:9	Loss:18.944	translation_Loss:13.879	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.066                                                   	MRR:20.9	Hits@10:38.19	Best:20.9
2024-12-27 21:27:24,073: Snapshot:2	Epoch:10	Loss:18.238	translation_Loss:13.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.979                                                   	MRR:20.93	Hits@10:38.16	Best:20.93
2024-12-27 21:27:34,901: Snapshot:2	Epoch:11	Loss:17.576	translation_Loss:12.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.913                                                   	MRR:21.09	Hits@10:38.2	Best:21.09
2024-12-27 21:27:45,783: Snapshot:2	Epoch:12	Loss:17.219	translation_Loss:12.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.857                                                   	MRR:21.11	Hits@10:38.45	Best:21.11
2024-12-27 21:27:56,592: Snapshot:2	Epoch:13	Loss:16.836	translation_Loss:12.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.806                                                   	MRR:21.2	Hits@10:38.49	Best:21.2
2024-12-27 21:28:07,446: Snapshot:2	Epoch:14	Loss:16.555	translation_Loss:11.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.777                                                   	MRR:21.24	Hits@10:38.28	Best:21.24
2024-12-27 21:28:18,323: Snapshot:2	Epoch:15	Loss:16.291	translation_Loss:11.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.748                                                   	MRR:21.34	Hits@10:38.38	Best:21.34
2024-12-27 21:28:29,098: Snapshot:2	Epoch:16	Loss:16.057	translation_Loss:11.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.708                                                   	MRR:21.28	Hits@10:38.48	Best:21.34
2024-12-27 21:28:40,367: Snapshot:2	Epoch:17	Loss:15.875	translation_Loss:11.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.674                                                   	MRR:21.25	Hits@10:38.47	Best:21.34
2024-12-27 21:28:51,128: Snapshot:2	Epoch:18	Loss:15.779	translation_Loss:11.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.648                                                   	MRR:21.13	Hits@10:38.4	Best:21.34
2024-12-27 21:29:02,045: Snapshot:2	Epoch:19	Loss:15.574	translation_Loss:10.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.625                                                   	MRR:21.28	Hits@10:38.41	Best:21.34
2024-12-27 21:29:12,780: Early Stopping! Snapshot: 2 Epoch: 20 Best Results: 21.34
2024-12-27 21:29:12,781: Start to training tokens! Snapshot: 2 Epoch: 20 Loss:15.466 MRR:21.29 Best Results: 21.34
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:29:12,781: Snapshot:2	Epoch:20	Loss:15.466	translation_Loss:10.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.603                                                   	MRR:21.29	Hits@10:38.64	Best:21.34
2024-12-27 21:29:23,437: Snapshot:2	Epoch:21	Loss:108.299	translation_Loss:107.116	multi_layer_Loss:1.182	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.29	Hits@10:38.64	Best:21.34
2024-12-27 21:29:34,058: End of token training: 2 Epoch: 22 Loss:107.173 MRR:21.29 Best Results: 21.34
2024-12-27 21:29:34,058: Snapshot:2	Epoch:22	Loss:107.173	translation_Loss:107.171	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.29	Hits@10:38.64	Best:21.34
2024-12-27 21:29:34,350: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_5000/2model_best.tar'
2024-12-27 21:29:42,892: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2519 | 0.1434 | 0.3072 | 0.3729 |  0.4501 |
|     1      | 0.2619 | 0.1545 | 0.3093 | 0.378  |  0.4677 |
|     2      | 0.2142 | 0.1265 | 0.2461 | 0.3068 |  0.3871 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:30:24,804: Snapshot:3	Epoch:0	Loss:112.431	translation_Loss:110.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.694                                                   	MRR:9.51	Hits@10:21.18	Best:9.51
2024-12-27 21:30:38,249: Snapshot:3	Epoch:1	Loss:64.681	translation_Loss:59.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.754                                                   	MRR:14.21	Hits@10:29.34	Best:14.21
2024-12-27 21:30:51,462: Snapshot:3	Epoch:2	Loss:43.994	translation_Loss:37.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.641                                                   	MRR:16.79	Hits@10:33.79	Best:16.79
2024-12-27 21:31:04,626: Snapshot:3	Epoch:3	Loss:34.037	translation_Loss:26.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.332                                                   	MRR:18.47	Hits@10:36.05	Best:18.47
2024-12-27 21:31:17,886: Snapshot:3	Epoch:4	Loss:28.474	translation_Loss:21.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.471                                                   	MRR:19.51	Hits@10:37.13	Best:19.51
2024-12-27 21:31:31,220: Snapshot:3	Epoch:5	Loss:25.203	translation_Loss:17.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.361                                                   	MRR:20.05	Hits@10:37.79	Best:20.05
2024-12-27 21:31:44,539: Snapshot:3	Epoch:6	Loss:23.169	translation_Loss:15.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.176                                                   	MRR:20.33	Hits@10:38.15	Best:20.33
2024-12-27 21:31:57,767: Snapshot:3	Epoch:7	Loss:21.711	translation_Loss:14.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.99                                                   	MRR:20.66	Hits@10:38.44	Best:20.66
2024-12-27 21:32:10,948: Snapshot:3	Epoch:8	Loss:20.671	translation_Loss:13.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.809                                                   	MRR:20.91	Hits@10:38.81	Best:20.91
2024-12-27 21:32:24,086: Snapshot:3	Epoch:9	Loss:19.983	translation_Loss:13.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.676                                                   	MRR:20.93	Hits@10:38.82	Best:20.93
2024-12-27 21:32:37,341: Snapshot:3	Epoch:10	Loss:19.427	translation_Loss:12.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.562                                                   	MRR:21.03	Hits@10:38.92	Best:21.03
2024-12-27 21:32:50,735: Snapshot:3	Epoch:11	Loss:18.96	translation_Loss:12.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.459                                                   	MRR:21.12	Hits@10:38.95	Best:21.12
2024-12-27 21:33:04,065: Snapshot:3	Epoch:12	Loss:18.58	translation_Loss:12.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.36                                                   	MRR:21.14	Hits@10:38.91	Best:21.14
2024-12-27 21:33:17,364: Snapshot:3	Epoch:13	Loss:18.319	translation_Loss:12.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.291                                                   	MRR:21.2	Hits@10:39.16	Best:21.2
2024-12-27 21:33:30,637: Snapshot:3	Epoch:14	Loss:18.107	translation_Loss:11.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.238                                                   	MRR:21.25	Hits@10:39.19	Best:21.25
2024-12-27 21:33:43,792: Snapshot:3	Epoch:15	Loss:17.861	translation_Loss:11.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.18                                                   	MRR:21.2	Hits@10:39.1	Best:21.25
2024-12-27 21:33:57,107: Snapshot:3	Epoch:16	Loss:17.731	translation_Loss:11.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.161                                                   	MRR:21.2	Hits@10:39.2	Best:21.25
2024-12-27 21:34:10,276: Snapshot:3	Epoch:17	Loss:17.574	translation_Loss:11.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.123                                                   	MRR:21.21	Hits@10:39.14	Best:21.25
2024-12-27 21:34:23,459: Snapshot:3	Epoch:18	Loss:17.497	translation_Loss:11.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.101                                                   	MRR:21.23	Hits@10:39.18	Best:21.25
2024-12-27 21:34:36,690: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 21.25
2024-12-27 21:34:36,691: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:17.428 MRR:21.23 Best Results: 21.25
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:34:36,691: Snapshot:3	Epoch:19	Loss:17.428	translation_Loss:11.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.058                                                   	MRR:21.23	Hits@10:39.01	Best:21.25
2024-12-27 21:34:49,425: Snapshot:3	Epoch:20	Loss:116.368	translation_Loss:115.117	multi_layer_Loss:1.252	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.23	Hits@10:39.01	Best:21.25
2024-12-27 21:35:02,133: End of token training: 3 Epoch: 21 Loss:115.178 MRR:21.23 Best Results: 21.25
2024-12-27 21:35:02,134: Snapshot:3	Epoch:21	Loss:115.178	translation_Loss:115.176	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.23	Hits@10:39.01	Best:21.25
2024-12-27 21:35:02,421: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_5000/3model_best.tar'
2024-12-27 21:35:16,409: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2598 | 0.1551 | 0.3106 | 0.3741 |  0.4536 |
|     1      | 0.2661 | 0.1634 | 0.3069 | 0.371  |  0.466  |
|     2      | 0.2103 | 0.1234 | 0.2394 | 0.2991 |   0.38  |
|     3      | 0.2119 | 0.1193 | 0.2476 | 0.3096 |  0.389  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:35:35,474: Snapshot:4	Epoch:0	Loss:39.221	translation_Loss:38.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.27                                                   	MRR:7.98	Hits@10:16.45	Best:7.98
2024-12-27 21:35:40,853: Snapshot:4	Epoch:1	Loss:27.448	translation_Loss:26.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.784                                                   	MRR:14.73	Hits@10:29.22	Best:14.73
2024-12-27 21:35:46,316: Snapshot:4	Epoch:2	Loss:20.571	translation_Loss:19.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.396                                                   	MRR:17.57	Hits@10:32.97	Best:17.57
2024-12-27 21:35:51,806: Snapshot:4	Epoch:3	Loss:16.661	translation_Loss:14.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.978                                                   	MRR:18.68	Hits@10:34.37	Best:18.68
2024-12-27 21:35:57,289: Snapshot:4	Epoch:4	Loss:14.161	translation_Loss:11.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.485                                                   	MRR:19.92	Hits@10:35.54	Best:19.92
2024-12-27 21:36:02,787: Snapshot:4	Epoch:5	Loss:12.317	translation_Loss:9.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.896                                                   	MRR:20.43	Hits@10:36.42	Best:20.43
2024-12-27 21:36:08,339: Snapshot:4	Epoch:6	Loss:10.976	translation_Loss:7.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.209                                                   	MRR:21.16	Hits@10:36.65	Best:21.16
2024-12-27 21:36:14,370: Snapshot:4	Epoch:7	Loss:9.973	translation_Loss:6.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.439                                                   	MRR:21.53	Hits@10:37.18	Best:21.53
2024-12-27 21:36:19,788: Snapshot:4	Epoch:8	Loss:9.18	translation_Loss:5.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.603                                                   	MRR:21.77	Hits@10:37.1	Best:21.77
2024-12-27 21:36:25,190: Snapshot:4	Epoch:9	Loss:8.57	translation_Loss:4.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.707                                                   	MRR:22.01	Hits@10:37.36	Best:22.01
2024-12-27 21:36:30,706: Snapshot:4	Epoch:10	Loss:8.081	translation_Loss:4.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.763                                                   	MRR:22.11	Hits@10:37.2	Best:22.11
2024-12-27 21:36:36,223: Snapshot:4	Epoch:11	Loss:7.65	translation_Loss:3.864	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.786                                                   	MRR:22.22	Hits@10:37.5	Best:22.22
2024-12-27 21:36:41,724: Snapshot:4	Epoch:12	Loss:7.354	translation_Loss:3.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.782                                                   	MRR:22.39	Hits@10:37.44	Best:22.39
2024-12-27 21:36:47,036: Snapshot:4	Epoch:13	Loss:7.105	translation_Loss:3.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.77                                                   	MRR:22.27	Hits@10:37.58	Best:22.39
2024-12-27 21:36:52,439: Snapshot:4	Epoch:14	Loss:6.889	translation_Loss:3.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.745                                                   	MRR:22.41	Hits@10:37.58	Best:22.41
2024-12-27 21:36:57,918: Snapshot:4	Epoch:15	Loss:6.755	translation_Loss:3.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.714                                                   	MRR:22.47	Hits@10:37.55	Best:22.47
2024-12-27 21:37:03,357: Snapshot:4	Epoch:16	Loss:6.551	translation_Loss:2.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.684                                                   	MRR:22.63	Hits@10:37.71	Best:22.63
2024-12-27 21:37:08,836: Snapshot:4	Epoch:17	Loss:6.48	translation_Loss:2.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.663                                                   	MRR:22.67	Hits@10:37.55	Best:22.67
2024-12-27 21:37:14,342: Snapshot:4	Epoch:18	Loss:6.383	translation_Loss:2.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.632                                                   	MRR:23.1	Hits@10:37.95	Best:23.1
2024-12-27 21:37:19,799: Snapshot:4	Epoch:19	Loss:6.235	translation_Loss:2.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.596                                                   	MRR:22.98	Hits@10:37.91	Best:23.1
2024-12-27 21:37:25,233: Snapshot:4	Epoch:20	Loss:6.196	translation_Loss:2.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.57                                                   	MRR:23.21	Hits@10:38.23	Best:23.21
2024-12-27 21:37:30,707: Snapshot:4	Epoch:21	Loss:6.159	translation_Loss:2.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.548                                                   	MRR:23.11	Hits@10:37.89	Best:23.21
2024-12-27 21:37:36,140: Snapshot:4	Epoch:22	Loss:6.049	translation_Loss:2.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.52                                                   	MRR:22.84	Hits@10:37.74	Best:23.21
2024-12-27 21:37:41,482: Snapshot:4	Epoch:23	Loss:6.003	translation_Loss:2.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.489                                                   	MRR:22.92	Hits@10:37.83	Best:23.21
2024-12-27 21:37:46,801: Snapshot:4	Epoch:24	Loss:5.969	translation_Loss:2.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.476                                                   	MRR:23.07	Hits@10:37.63	Best:23.21
2024-12-27 21:37:52,204: Early Stopping! Snapshot: 4 Epoch: 25 Best Results: 23.21
2024-12-27 21:37:52,204: Start to training tokens! Snapshot: 4 Epoch: 25 Loss:5.951 MRR:23.01 Best Results: 23.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:37:52,204: Snapshot:4	Epoch:25	Loss:5.951	translation_Loss:2.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.45                                                   	MRR:23.01	Hits@10:38.04	Best:23.21
2024-12-27 21:37:57,365: Snapshot:4	Epoch:26	Loss:52.676	translation_Loss:51.523	multi_layer_Loss:1.153	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.01	Hits@10:38.04	Best:23.21
2024-12-27 21:38:02,529: End of token training: 4 Epoch: 27 Loss:51.719 MRR:23.01 Best Results: 23.21
2024-12-27 21:38:02,530: Snapshot:4	Epoch:27	Loss:51.719	translation_Loss:51.574	multi_layer_Loss:0.145	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.01	Hits@10:38.04	Best:23.21
2024-12-27 21:38:02,823: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_5000/4model_best.tar'
2024-12-27 21:38:19,117: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2194 | 0.1243 | 0.2624 | 0.3216 |  0.3958 |
|     1      | 0.2448 | 0.1484 | 0.2805 | 0.3423 |  0.4349 |
|     2      | 0.1889 | 0.1082 | 0.2141 | 0.2668 |  0.346  |
|     3      | 0.1775 | 0.0898 | 0.2084 | 0.2678 |  0.344  |
|     4      | 0.2325 | 0.1535 | 0.2657 | 0.3161 |  0.3801 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 21:38:19,119: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2585 | 0.1533 | 0.3127 | 0.3778 |  0.4519 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 | 0.1521 | 0.3121 | 0.377  |   0.45  |
|     1      | 0.2894 | 0.1816 | 0.3398 | 0.4053 |  0.5002 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2519 | 0.1434 | 0.3072 | 0.3729 |  0.4501 |
|     1      | 0.2619 | 0.1545 | 0.3093 | 0.378  |  0.4677 |
|     2      | 0.2142 | 0.1265 | 0.2461 | 0.3068 |  0.3871 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2598 | 0.1551 | 0.3106 | 0.3741 |  0.4536 |
|     1      | 0.2661 | 0.1634 | 0.3069 | 0.371  |  0.466  |
|     2      | 0.2103 | 0.1234 | 0.2394 | 0.2991 |   0.38  |
|     3      | 0.2119 | 0.1193 | 0.2476 | 0.3096 |  0.389  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2194 | 0.1243 | 0.2624 | 0.3216 |  0.3958 |
|     1      | 0.2448 | 0.1484 | 0.2805 | 0.3423 |  0.4349 |
|     2      | 0.1889 | 0.1082 | 0.2141 | 0.2668 |  0.346  |
|     3      | 0.1775 | 0.0898 | 0.2084 | 0.2678 |  0.344  |
|     4      | 0.2325 | 0.1535 | 0.2657 | 0.3161 |  0.3801 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 21:38:19,120: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 291.49767231941223 |   0.259   |    0.153     |    0.313     |     0.452     |
|    1     | 132.00544834136963 |   0.266   |     0.16     |    0.319     |     0.463     |
|    2     | 270.0557634830475  |   0.233   |    0.136     |    0.275     |     0.419     |
|    3     | 313.9751033782959  |   0.226   |    0.132     |    0.263     |     0.406     |
|    4     | 163.57800030708313 |    0.2    |    0.114     |    0.232     |     0.365     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 21:38:19,120: Sum_Training_Time:1171.1119878292084
2024-12-27 21:38:19,120: Every_Training_Time:[291.49767231941223, 132.00544834136963, 270.0557634830475, 313.9751033782959, 163.57800030708313]
2024-12-27 21:38:19,120: Forward transfer: 0.044825000000000004 Backward transfer: -0.03585
2024-12-27 21:38:52,789: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227213823/HYBRIDHYBRID_0.0001_512_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_512_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_512_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 21:39:02,579: Snapshot:0	Epoch:0	Loss:94.862	translation_Loss:94.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.14	Hits@10:6.38	Best:3.14
2024-12-27 21:39:08,696: Snapshot:0	Epoch:1	Loss:85.64	translation_Loss:85.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.83	Hits@10:13.12	Best:5.83
2024-12-27 21:39:14,846: Snapshot:0	Epoch:2	Loss:77.346	translation_Loss:77.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.35	Hits@10:17.43	Best:7.35
2024-12-27 21:39:20,976: Snapshot:0	Epoch:3	Loss:69.52	translation_Loss:69.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.73	Hits@10:21.03	Best:8.73
2024-12-27 21:39:27,188: Snapshot:0	Epoch:4	Loss:61.844	translation_Loss:61.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:25.79	Best:10.96
2024-12-27 21:39:33,333: Snapshot:0	Epoch:5	Loss:54.179	translation_Loss:54.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.48	Hits@10:30.59	Best:13.48
2024-12-27 21:39:39,423: Snapshot:0	Epoch:6	Loss:46.401	translation_Loss:46.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.97	Hits@10:34.4	Best:15.97
2024-12-27 21:39:45,551: Snapshot:0	Epoch:7	Loss:39.053	translation_Loss:39.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.23	Hits@10:37.11	Best:18.23
2024-12-27 21:39:52,185: Snapshot:0	Epoch:8	Loss:32.293	translation_Loss:32.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.06	Hits@10:39.36	Best:20.06
2024-12-27 21:39:58,297: Snapshot:0	Epoch:9	Loss:26.754	translation_Loss:26.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.52	Hits@10:41.04	Best:21.52
2024-12-27 21:40:04,513: Snapshot:0	Epoch:10	Loss:22.211	translation_Loss:22.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.6	Hits@10:42.1	Best:22.6
2024-12-27 21:40:10,734: Snapshot:0	Epoch:11	Loss:18.528	translation_Loss:18.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.25	Hits@10:42.91	Best:23.25
2024-12-27 21:40:16,955: Snapshot:0	Epoch:12	Loss:15.467	translation_Loss:15.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.79	Hits@10:43.72	Best:23.79
2024-12-27 21:40:23,166: Snapshot:0	Epoch:13	Loss:13.084	translation_Loss:13.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.25	Hits@10:44.25	Best:24.25
2024-12-27 21:40:29,376: Snapshot:0	Epoch:14	Loss:11.148	translation_Loss:11.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:44.43	Best:24.43
2024-12-27 21:40:35,502: Snapshot:0	Epoch:15	Loss:9.52	translation_Loss:9.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.7	Hits@10:44.85	Best:24.7
2024-12-27 21:40:41,720: Snapshot:0	Epoch:16	Loss:8.253	translation_Loss:8.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.89	Hits@10:45.1	Best:24.89
2024-12-27 21:40:47,837: Snapshot:0	Epoch:17	Loss:7.202	translation_Loss:7.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.08	Hits@10:45.47	Best:25.08
2024-12-27 21:40:53,952: Snapshot:0	Epoch:18	Loss:6.362	translation_Loss:6.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.21	Hits@10:45.59	Best:25.21
2024-12-27 21:41:00,076: Snapshot:0	Epoch:19	Loss:5.69	translation_Loss:5.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.22	Hits@10:45.8	Best:25.22
2024-12-27 21:41:06,214: Snapshot:0	Epoch:20	Loss:5.023	translation_Loss:5.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.3	Hits@10:45.75	Best:25.3
2024-12-27 21:41:12,322: Snapshot:0	Epoch:21	Loss:4.583	translation_Loss:4.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:45.92	Best:25.35
2024-12-27 21:41:18,541: Snapshot:0	Epoch:22	Loss:4.153	translation_Loss:4.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.52	Hits@10:45.82	Best:25.52
2024-12-27 21:41:24,776: Snapshot:0	Epoch:23	Loss:3.789	translation_Loss:3.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.1	Best:25.56
2024-12-27 21:41:30,999: Snapshot:0	Epoch:24	Loss:3.534	translation_Loss:3.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.61	Hits@10:46.09	Best:25.61
2024-12-27 21:41:37,198: Snapshot:0	Epoch:25	Loss:3.196	translation_Loss:3.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.61	Hits@10:46.03	Best:25.61
2024-12-27 21:41:43,747: Snapshot:0	Epoch:26	Loss:2.978	translation_Loss:2.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.09	Best:25.66
2024-12-27 21:41:49,890: Snapshot:0	Epoch:27	Loss:2.783	translation_Loss:2.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.09	Best:25.66
2024-12-27 21:41:56,024: Snapshot:0	Epoch:28	Loss:2.605	translation_Loss:2.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.12	Best:25.74
2024-12-27 21:42:02,227: Snapshot:0	Epoch:29	Loss:2.468	translation_Loss:2.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.88	Hits@10:46.25	Best:25.88
2024-12-27 21:42:08,368: Snapshot:0	Epoch:30	Loss:2.339	translation_Loss:2.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.88	Hits@10:46.18	Best:25.88
2024-12-27 21:42:14,558: Snapshot:0	Epoch:31	Loss:2.191	translation_Loss:2.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.32	Best:25.88
2024-12-27 21:42:20,751: Snapshot:0	Epoch:32	Loss:2.086	translation_Loss:2.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.23	Best:25.88
2024-12-27 21:42:26,830: Snapshot:0	Epoch:33	Loss:1.985	translation_Loss:1.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.22	Best:25.88
2024-12-27 21:42:32,980: Early Stopping! Snapshot: 0 Epoch: 34 Best Results: 25.88
2024-12-27 21:42:32,980: Start to training tokens! Snapshot: 0 Epoch: 34 Loss:1.901 MRR:25.78 Best Results: 25.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:42:32,980: Snapshot:0	Epoch:34	Loss:1.901	translation_Loss:1.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.13	Best:25.88
2024-12-27 21:42:39,888: Snapshot:0	Epoch:35	Loss:71.094	translation_Loss:69.88	multi_layer_Loss:1.214	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.13	Best:25.88
2024-12-27 21:42:46,184: End of token training: 0 Epoch: 36 Loss:69.951 MRR:25.78 Best Results: 25.88
2024-12-27 21:42:46,184: Snapshot:0	Epoch:36	Loss:69.951	translation_Loss:69.913	multi_layer_Loss:0.039	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.78	Hits@10:46.13	Best:25.88
2024-12-27 21:42:46,479: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_10000/0model_best.tar'
2024-12-27 21:42:49,085: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.1527 | 0.3172 | 0.3797 |  0.4518 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:43:00,303: Snapshot:1	Epoch:0	Loss:28.885	translation_Loss:28.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.145                                                   	MRR:6.65	Hits@10:10.97	Best:6.65
2024-12-27 21:43:02,792: Snapshot:1	Epoch:1	Loss:24.605	translation_Loss:24.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.353                                                   	MRR:8.27	Hits@10:14.46	Best:8.27
2024-12-27 21:43:05,258: Snapshot:1	Epoch:2	Loss:20.721	translation_Loss:20.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.636                                                   	MRR:11.08	Hits@10:21.21	Best:11.08
2024-12-27 21:43:07,768: Snapshot:1	Epoch:3	Loss:17.263	translation_Loss:16.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.929                                                   	MRR:14.29	Hits@10:27.04	Best:14.29
2024-12-27 21:43:10,210: Snapshot:1	Epoch:4	Loss:14.44	translation_Loss:13.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.175                                                   	MRR:16.85	Hits@10:30.97	Best:16.85
2024-12-27 21:43:12,684: Snapshot:1	Epoch:5	Loss:12.234	translation_Loss:10.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.359                                                   	MRR:18.68	Hits@10:33.77	Best:18.68
2024-12-27 21:43:15,163: Snapshot:1	Epoch:6	Loss:10.592	translation_Loss:9.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.48                                                   	MRR:20.14	Hits@10:36.2	Best:20.14
2024-12-27 21:43:17,676: Snapshot:1	Epoch:7	Loss:9.328	translation_Loss:7.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.554                                                   	MRR:21.42	Hits@10:38.56	Best:21.42
2024-12-27 21:43:20,203: Snapshot:1	Epoch:8	Loss:8.332	translation_Loss:6.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.595                                                   	MRR:22.55	Hits@10:40.25	Best:22.55
2024-12-27 21:43:22,704: Snapshot:1	Epoch:9	Loss:7.593	translation_Loss:5.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.612                                                   	MRR:23.37	Hits@10:41.64	Best:23.37
2024-12-27 21:43:25,515: Snapshot:1	Epoch:10	Loss:6.963	translation_Loss:5.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.616                                                   	MRR:24.12	Hits@10:43.1	Best:24.12
2024-12-27 21:43:28,009: Snapshot:1	Epoch:11	Loss:6.437	translation_Loss:4.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.609                                                   	MRR:24.63	Hits@10:43.88	Best:24.63
2024-12-27 21:43:30,585: Snapshot:1	Epoch:12	Loss:6.009	translation_Loss:4.415	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.593                                                   	MRR:25.1	Hits@10:44.4	Best:25.1
2024-12-27 21:43:33,060: Snapshot:1	Epoch:13	Loss:5.628	translation_Loss:4.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.575                                                   	MRR:25.45	Hits@10:44.97	Best:25.45
2024-12-27 21:43:35,545: Snapshot:1	Epoch:14	Loss:5.33	translation_Loss:3.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.557                                                   	MRR:26.0	Hits@10:45.9	Best:26.0
2024-12-27 21:43:38,019: Snapshot:1	Epoch:15	Loss:5.039	translation_Loss:3.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.538                                                   	MRR:26.24	Hits@10:46.24	Best:26.24
2024-12-27 21:43:40,486: Snapshot:1	Epoch:16	Loss:4.813	translation_Loss:3.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.516                                                   	MRR:26.65	Hits@10:46.65	Best:26.65
2024-12-27 21:43:42,948: Snapshot:1	Epoch:17	Loss:4.582	translation_Loss:3.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.494                                                   	MRR:26.95	Hits@10:47.35	Best:26.95
2024-12-27 21:43:45,395: Snapshot:1	Epoch:18	Loss:4.434	translation_Loss:2.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.47                                                   	MRR:27.26	Hits@10:47.72	Best:27.26
2024-12-27 21:43:47,866: Snapshot:1	Epoch:19	Loss:4.262	translation_Loss:2.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:27.56	Hits@10:48.14	Best:27.56
2024-12-27 21:43:50,298: Snapshot:1	Epoch:20	Loss:4.134	translation_Loss:2.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.432                                                   	MRR:27.83	Hits@10:48.51	Best:27.83
2024-12-27 21:43:52,759: Snapshot:1	Epoch:21	Loss:4.008	translation_Loss:2.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.411                                                   	MRR:27.99	Hits@10:48.92	Best:27.99
2024-12-27 21:43:55,267: Snapshot:1	Epoch:22	Loss:3.914	translation_Loss:2.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.39                                                   	MRR:28.11	Hits@10:48.84	Best:28.11
2024-12-27 21:43:57,748: Snapshot:1	Epoch:23	Loss:3.792	translation_Loss:2.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.375                                                   	MRR:28.18	Hits@10:49.11	Best:28.18
2024-12-27 21:44:00,173: Snapshot:1	Epoch:24	Loss:3.703	translation_Loss:2.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.358                                                   	MRR:28.19	Hits@10:49.27	Best:28.19
2024-12-27 21:44:02,648: Snapshot:1	Epoch:25	Loss:3.619	translation_Loss:2.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.342                                                   	MRR:28.39	Hits@10:49.41	Best:28.39
2024-12-27 21:44:05,143: Snapshot:1	Epoch:26	Loss:3.587	translation_Loss:2.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.326                                                   	MRR:28.58	Hits@10:49.47	Best:28.58
2024-12-27 21:44:07,606: Snapshot:1	Epoch:27	Loss:3.51	translation_Loss:2.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.312                                                   	MRR:28.62	Hits@10:49.57	Best:28.62
2024-12-27 21:44:10,030: Snapshot:1	Epoch:28	Loss:3.47	translation_Loss:2.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:28.67	Hits@10:50.02	Best:28.67
2024-12-27 21:44:12,491: Snapshot:1	Epoch:29	Loss:3.367	translation_Loss:2.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.284                                                   	MRR:28.93	Hits@10:50.08	Best:28.93
2024-12-27 21:44:14,895: Snapshot:1	Epoch:30	Loss:3.324	translation_Loss:2.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.272                                                   	MRR:28.9	Hits@10:49.98	Best:28.93
2024-12-27 21:44:17,338: Snapshot:1	Epoch:31	Loss:3.287	translation_Loss:2.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.258                                                   	MRR:28.97	Hits@10:50.11	Best:28.97
2024-12-27 21:44:19,768: Snapshot:1	Epoch:32	Loss:3.233	translation_Loss:1.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.248                                                   	MRR:29.02	Hits@10:50.06	Best:29.02
2024-12-27 21:44:22,214: Snapshot:1	Epoch:33	Loss:3.175	translation_Loss:1.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.237                                                   	MRR:29.14	Hits@10:50.41	Best:29.14
2024-12-27 21:44:24,637: Snapshot:1	Epoch:34	Loss:3.136	translation_Loss:1.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.227                                                   	MRR:29.25	Hits@10:50.23	Best:29.25
2024-12-27 21:44:27,063: Snapshot:1	Epoch:35	Loss:3.121	translation_Loss:1.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.218                                                   	MRR:29.22	Hits@10:50.31	Best:29.25
2024-12-27 21:44:29,503: Snapshot:1	Epoch:36	Loss:3.069	translation_Loss:1.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.208                                                   	MRR:29.26	Hits@10:50.41	Best:29.26
2024-12-27 21:44:31,985: Snapshot:1	Epoch:37	Loss:3.027	translation_Loss:1.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.2                                                   	MRR:29.38	Hits@10:50.77	Best:29.38
2024-12-27 21:44:34,425: Snapshot:1	Epoch:38	Loss:3.016	translation_Loss:1.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.191                                                   	MRR:29.32	Hits@10:50.8	Best:29.38
2024-12-27 21:44:36,849: Snapshot:1	Epoch:39	Loss:2.972	translation_Loss:1.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.182                                                   	MRR:29.51	Hits@10:50.84	Best:29.51
2024-12-27 21:44:39,327: Snapshot:1	Epoch:40	Loss:2.937	translation_Loss:1.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.171                                                   	MRR:29.41	Hits@10:50.79	Best:29.51
2024-12-27 21:44:41,753: Snapshot:1	Epoch:41	Loss:2.941	translation_Loss:1.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.166                                                   	MRR:29.44	Hits@10:50.84	Best:29.51
2024-12-27 21:44:44,226: Snapshot:1	Epoch:42	Loss:2.894	translation_Loss:1.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.161                                                   	MRR:29.68	Hits@10:50.91	Best:29.68
2024-12-27 21:44:46,709: Snapshot:1	Epoch:43	Loss:2.875	translation_Loss:1.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.154                                                   	MRR:29.55	Hits@10:51.21	Best:29.68
2024-12-27 21:44:49,167: Snapshot:1	Epoch:44	Loss:2.856	translation_Loss:1.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.144                                                   	MRR:29.67	Hits@10:50.99	Best:29.68
2024-12-27 21:44:51,600: Snapshot:1	Epoch:45	Loss:2.828	translation_Loss:1.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.141                                                   	MRR:29.64	Hits@10:51.16	Best:29.68
2024-12-27 21:44:54,027: Snapshot:1	Epoch:46	Loss:2.824	translation_Loss:1.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.138                                                   	MRR:29.69	Hits@10:51.34	Best:29.69
2024-12-27 21:44:56,479: Snapshot:1	Epoch:47	Loss:2.788	translation_Loss:1.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.132                                                   	MRR:29.89	Hits@10:51.17	Best:29.89
2024-12-27 21:44:58,911: Snapshot:1	Epoch:48	Loss:2.763	translation_Loss:1.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.129                                                   	MRR:29.81	Hits@10:51.23	Best:29.89
2024-12-27 21:45:01,341: Snapshot:1	Epoch:49	Loss:2.752	translation_Loss:1.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.122                                                   	MRR:29.89	Hits@10:51.33	Best:29.89
2024-12-27 21:45:03,771: Snapshot:1	Epoch:50	Loss:2.737	translation_Loss:1.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.118                                                   	MRR:29.78	Hits@10:51.45	Best:29.89
2024-12-27 21:45:06,207: Snapshot:1	Epoch:51	Loss:2.7	translation_Loss:1.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.113                                                   	MRR:29.88	Hits@10:51.34	Best:29.89
2024-12-27 21:45:08,639: Early Stopping! Snapshot: 1 Epoch: 52 Best Results: 29.89
2024-12-27 21:45:08,640: Start to training tokens! Snapshot: 1 Epoch: 52 Loss:2.682 MRR:29.73 Best Results: 29.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:45:08,640: Snapshot:1	Epoch:52	Loss:2.682	translation_Loss:1.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.105                                                   	MRR:29.73	Hits@10:51.38	Best:29.89
2024-12-27 21:45:11,020: Snapshot:1	Epoch:53	Loss:25.456	translation_Loss:24.69	multi_layer_Loss:0.765	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.73	Hits@10:51.38	Best:29.89
2024-12-27 21:45:13,369: End of token training: 1 Epoch: 54 Loss:25.059 MRR:29.73 Best Results: 29.89
2024-12-27 21:45:13,369: Snapshot:1	Epoch:54	Loss:25.059	translation_Loss:24.716	multi_layer_Loss:0.343	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.73	Hits@10:51.38	Best:29.89
2024-12-27 21:45:13,666: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_10000/1model_best.tar'
2024-12-27 21:45:17,657: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.256  | 0.1489 | 0.3144 | 0.3784 |   0.45  |
|     1      | 0.2971 | 0.1868 | 0.3497 | 0.4195 |  0.5133 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:45:52,836: Snapshot:2	Epoch:0	Loss:115.681	translation_Loss:114.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.525                                                   	MRR:7.76	Hits@10:17.02	Best:7.76
2024-12-27 21:46:03,592: Snapshot:2	Epoch:1	Loss:80.981	translation_Loss:77.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.73                                                   	MRR:12.68	Hits@10:25.61	Best:12.68
2024-12-27 21:46:14,295: Snapshot:2	Epoch:2	Loss:58.143	translation_Loss:53.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.943                                                   	MRR:15.62	Hits@10:30.51	Best:15.62
2024-12-27 21:46:24,984: Snapshot:2	Epoch:3	Loss:44.538	translation_Loss:39.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.202                                                   	MRR:17.37	Hits@10:33.26	Best:17.37
2024-12-27 21:46:35,687: Snapshot:2	Epoch:4	Loss:36.67	translation_Loss:31.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.09                                                   	MRR:18.44	Hits@10:35.18	Best:18.44
2024-12-27 21:46:46,529: Snapshot:2	Epoch:5	Loss:31.838	translation_Loss:26.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.896                                                   	MRR:19.17	Hits@10:36.22	Best:19.17
2024-12-27 21:46:57,275: Snapshot:2	Epoch:6	Loss:28.808	translation_Loss:24.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.724                                                   	MRR:19.81	Hits@10:36.89	Best:19.81
2024-12-27 21:47:08,020: Snapshot:2	Epoch:7	Loss:26.659	translation_Loss:22.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.594                                                   	MRR:20.07	Hits@10:37.16	Best:20.07
2024-12-27 21:47:18,848: Snapshot:2	Epoch:8	Loss:25.037	translation_Loss:20.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.48                                                   	MRR:20.5	Hits@10:37.57	Best:20.5
2024-12-27 21:47:29,508: Snapshot:2	Epoch:9	Loss:23.954	translation_Loss:19.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.401                                                   	MRR:20.69	Hits@10:37.64	Best:20.69
2024-12-27 21:47:40,232: Snapshot:2	Epoch:10	Loss:23.188	translation_Loss:18.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.346                                                   	MRR:20.67	Hits@10:37.52	Best:20.69
2024-12-27 21:47:51,064: Snapshot:2	Epoch:11	Loss:22.474	translation_Loss:18.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.289                                                   	MRR:20.73	Hits@10:37.59	Best:20.73
2024-12-27 21:48:01,827: Snapshot:2	Epoch:12	Loss:22.012	translation_Loss:17.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.241                                                   	MRR:20.79	Hits@10:37.76	Best:20.79
2024-12-27 21:48:12,520: Snapshot:2	Epoch:13	Loss:21.582	translation_Loss:17.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.196                                                   	MRR:20.88	Hits@10:37.7	Best:20.88
2024-12-27 21:48:23,193: Snapshot:2	Epoch:14	Loss:21.282	translation_Loss:17.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.169                                                   	MRR:20.93	Hits@10:37.96	Best:20.93
2024-12-27 21:48:33,955: Snapshot:2	Epoch:15	Loss:20.903	translation_Loss:16.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.14                                                   	MRR:21.04	Hits@10:37.99	Best:21.04
2024-12-27 21:48:44,703: Snapshot:2	Epoch:16	Loss:20.786	translation_Loss:16.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.128                                                   	MRR:21.0	Hits@10:37.92	Best:21.04
2024-12-27 21:48:55,397: Snapshot:2	Epoch:17	Loss:20.568	translation_Loss:16.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.106                                                   	MRR:21.1	Hits@10:37.95	Best:21.1
2024-12-27 21:49:06,146: Snapshot:2	Epoch:18	Loss:20.366	translation_Loss:16.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.082                                                   	MRR:21.03	Hits@10:38.08	Best:21.1
2024-12-27 21:49:16,877: Snapshot:2	Epoch:19	Loss:20.162	translation_Loss:16.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.059                                                   	MRR:21.04	Hits@10:37.95	Best:21.1
2024-12-27 21:49:27,561: Snapshot:2	Epoch:20	Loss:20.1	translation_Loss:16.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.054                                                   	MRR:21.05	Hits@10:37.82	Best:21.1
2024-12-27 21:49:38,320: Snapshot:2	Epoch:21	Loss:19.944	translation_Loss:15.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.036                                                   	MRR:20.99	Hits@10:37.78	Best:21.1
2024-12-27 21:49:48,980: Early Stopping! Snapshot: 2 Epoch: 22 Best Results: 21.1
2024-12-27 21:49:48,981: Start to training tokens! Snapshot: 2 Epoch: 22 Loss:19.833 MRR:21.06 Best Results: 21.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:49:48,981: Snapshot:2	Epoch:22	Loss:19.833	translation_Loss:15.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.026                                                   	MRR:21.06	Hits@10:37.82	Best:21.1
2024-12-27 21:49:59,311: Snapshot:2	Epoch:23	Loss:112.295	translation_Loss:111.113	multi_layer_Loss:1.182	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.06	Hits@10:37.82	Best:21.1
2024-12-27 21:50:09,791: End of token training: 2 Epoch: 24 Loss:111.105 MRR:21.06 Best Results: 21.1
2024-12-27 21:50:09,791: Snapshot:2	Epoch:24	Loss:111.105	translation_Loss:111.102	multi_layer_Loss:0.003	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.06	Hits@10:37.82	Best:21.1
2024-12-27 21:50:10,088: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_10000/2model_best.tar'
2024-12-27 21:50:18,800: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2494 | 0.1386 | 0.3078 | 0.3744 |  0.4509 |
|     1      | 0.2724 | 0.1598 | 0.3269 | 0.3919 |  0.4853 |
|     2      | 0.2116 | 0.1248 | 0.2443 | 0.3032 |  0.3829 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:51:00,284: Snapshot:3	Epoch:0	Loss:116.732	translation_Loss:114.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.373                                                   	MRR:9.21	Hits@10:20.79	Best:9.21
2024-12-27 21:51:13,318: Snapshot:3	Epoch:1	Loss:73.233	translation_Loss:67.229	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.005                                                   	MRR:13.78	Hits@10:28.86	Best:13.78
2024-12-27 21:51:26,513: Snapshot:3	Epoch:2	Loss:53.214	translation_Loss:45.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.759                                                   	MRR:16.41	Hits@10:33.15	Best:16.41
2024-12-27 21:51:39,576: Snapshot:3	Epoch:3	Loss:42.909	translation_Loss:34.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.127                                                   	MRR:18.09	Hits@10:35.46	Best:18.09
2024-12-27 21:51:52,714: Snapshot:3	Epoch:4	Loss:36.964	translation_Loss:28.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.033                                                   	MRR:19.0	Hits@10:36.58	Best:19.0
2024-12-27 21:52:05,977: Snapshot:3	Epoch:5	Loss:33.221	translation_Loss:25.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.774                                                   	MRR:19.68	Hits@10:37.18	Best:19.68
2024-12-27 21:52:19,039: Snapshot:3	Epoch:6	Loss:30.914	translation_Loss:23.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.504                                                   	MRR:20.06	Hits@10:37.55	Best:20.06
2024-12-27 21:52:32,203: Snapshot:3	Epoch:7	Loss:29.317	translation_Loss:22.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.287                                                   	MRR:20.41	Hits@10:37.79	Best:20.41
2024-12-27 21:52:45,387: Snapshot:3	Epoch:8	Loss:28.168	translation_Loss:21.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.107                                                   	MRR:20.48	Hits@10:37.95	Best:20.48
2024-12-27 21:52:58,421: Snapshot:3	Epoch:9	Loss:27.346	translation_Loss:20.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.962                                                   	MRR:20.51	Hits@10:38.12	Best:20.51
2024-12-27 21:53:11,463: Snapshot:3	Epoch:10	Loss:26.741	translation_Loss:19.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.847                                                   	MRR:20.64	Hits@10:38.07	Best:20.64
2024-12-27 21:53:24,675: Snapshot:3	Epoch:11	Loss:26.212	translation_Loss:19.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.751                                                   	MRR:20.75	Hits@10:38.1	Best:20.75
2024-12-27 21:53:37,802: Snapshot:3	Epoch:12	Loss:25.742	translation_Loss:19.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.677                                                   	MRR:20.73	Hits@10:38.14	Best:20.75
2024-12-27 21:53:50,961: Snapshot:3	Epoch:13	Loss:25.42	translation_Loss:18.823	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.597                                                   	MRR:20.81	Hits@10:38.01	Best:20.81
2024-12-27 21:54:04,083: Snapshot:3	Epoch:14	Loss:25.154	translation_Loss:18.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.553                                                   	MRR:20.78	Hits@10:38.13	Best:20.81
2024-12-27 21:54:17,072: Snapshot:3	Epoch:15	Loss:24.944	translation_Loss:18.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.508                                                   	MRR:20.68	Hits@10:37.94	Best:20.81
2024-12-27 21:54:30,016: Snapshot:3	Epoch:16	Loss:24.727	translation_Loss:18.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.477                                                   	MRR:20.76	Hits@10:38.08	Best:20.81
2024-12-27 21:54:43,066: Snapshot:3	Epoch:17	Loss:24.583	translation_Loss:18.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.465                                                   	MRR:20.75	Hits@10:38.07	Best:20.81
2024-12-27 21:54:56,130: Early Stopping! Snapshot: 3 Epoch: 18 Best Results: 20.81
2024-12-27 21:54:56,131: Start to training tokens! Snapshot: 3 Epoch: 18 Loss:24.477 MRR:20.81 Best Results: 20.81
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:54:56,131: Snapshot:3	Epoch:18	Loss:24.477	translation_Loss:18.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.447                                                   	MRR:20.81	Hits@10:38.09	Best:20.81
2024-12-27 21:55:08,883: Snapshot:3	Epoch:19	Loss:121.438	translation_Loss:120.186	multi_layer_Loss:1.252	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.81	Hits@10:38.09	Best:20.81
2024-12-27 21:55:21,586: End of token training: 3 Epoch: 20 Loss:120.179 MRR:20.81 Best Results: 20.81
2024-12-27 21:55:21,587: Snapshot:3	Epoch:20	Loss:120.179	translation_Loss:120.178	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.81	Hits@10:38.09	Best:20.81
2024-12-27 21:55:21,877: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_10000/3model_best.tar'
2024-12-27 21:55:35,281: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2552 | 0.1475 | 0.3104 | 0.3734 |  0.4528 |
|     1      | 0.2728 | 0.165  | 0.316  | 0.389  |  0.4822 |
|     2      | 0.2082 | 0.1201 | 0.2404 | 0.3004 |  0.3803 |
|     3      | 0.2083 | 0.1172 | 0.2449 | 0.3035 |  0.3798 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:55:54,678: Snapshot:4	Epoch:0	Loss:40.664	translation_Loss:40.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.416                                                   	MRR:8.1	Hits@10:16.92	Best:8.1
2024-12-27 21:56:00,120: Snapshot:4	Epoch:1	Loss:30.1	translation_Loss:28.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.155                                                   	MRR:13.94	Hits@10:28.34	Best:13.94
2024-12-27 21:56:05,525: Snapshot:4	Epoch:2	Loss:23.934	translation_Loss:21.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.978                                                   	MRR:16.74	Hits@10:31.86	Best:16.74
2024-12-27 21:56:10,949: Snapshot:4	Epoch:3	Loss:20.567	translation_Loss:17.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.676                                                   	MRR:18.29	Hits@10:33.57	Best:18.29
2024-12-27 21:56:16,351: Snapshot:4	Epoch:4	Loss:18.372	translation_Loss:15.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.227                                                   	MRR:19.32	Hits@10:35.14	Best:19.32
2024-12-27 21:56:21,777: Snapshot:4	Epoch:5	Loss:16.709	translation_Loss:13.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.639                                                   	MRR:19.97	Hits@10:35.85	Best:19.97
2024-12-27 21:56:27,155: Snapshot:4	Epoch:6	Loss:15.399	translation_Loss:11.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.934                                                   	MRR:20.51	Hits@10:36.0	Best:20.51
2024-12-27 21:56:32,677: Snapshot:4	Epoch:7	Loss:14.41	translation_Loss:10.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.133                                                   	MRR:21.11	Hits@10:36.17	Best:21.11
2024-12-27 21:56:38,043: Snapshot:4	Epoch:8	Loss:13.536	translation_Loss:9.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.269                                                   	MRR:21.38	Hits@10:35.98	Best:21.38
2024-12-27 21:56:43,370: Snapshot:4	Epoch:9	Loss:12.825	translation_Loss:8.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.345                                                   	MRR:21.57	Hits@10:36.18	Best:21.57
2024-12-27 21:56:48,695: Snapshot:4	Epoch:10	Loss:12.289	translation_Loss:7.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.39                                                   	MRR:21.6	Hits@10:36.57	Best:21.6
2024-12-27 21:56:54,031: Snapshot:4	Epoch:11	Loss:11.795	translation_Loss:7.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.408                                                   	MRR:21.96	Hits@10:36.62	Best:21.96
2024-12-27 21:56:59,469: Snapshot:4	Epoch:12	Loss:11.466	translation_Loss:7.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.403                                                   	MRR:22.09	Hits@10:36.63	Best:22.09
2024-12-27 21:57:04,786: Snapshot:4	Epoch:13	Loss:11.088	translation_Loss:6.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.396                                                   	MRR:22.01	Hits@10:36.71	Best:22.09
2024-12-27 21:57:10,114: Snapshot:4	Epoch:14	Loss:10.912	translation_Loss:6.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.379                                                   	MRR:22.32	Hits@10:37.02	Best:22.32
2024-12-27 21:57:15,597: Snapshot:4	Epoch:15	Loss:10.668	translation_Loss:6.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.359                                                   	MRR:22.1	Hits@10:36.72	Best:22.32
2024-12-27 21:57:20,950: Snapshot:4	Epoch:16	Loss:10.497	translation_Loss:6.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.333                                                   	MRR:22.3	Hits@10:36.88	Best:22.32
2024-12-27 21:57:26,286: Snapshot:4	Epoch:17	Loss:10.343	translation_Loss:6.027	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.316                                                   	MRR:22.55	Hits@10:37.02	Best:22.55
2024-12-27 21:57:31,707: Snapshot:4	Epoch:18	Loss:10.237	translation_Loss:5.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.296                                                   	MRR:22.54	Hits@10:36.95	Best:22.55
2024-12-27 21:57:37,018: Snapshot:4	Epoch:19	Loss:10.137	translation_Loss:5.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.266                                                   	MRR:22.55	Hits@10:36.96	Best:22.55
2024-12-27 21:57:42,363: Snapshot:4	Epoch:20	Loss:10.034	translation_Loss:5.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.248                                                   	MRR:22.71	Hits@10:37.03	Best:22.71
2024-12-27 21:57:47,774: Snapshot:4	Epoch:21	Loss:9.938	translation_Loss:5.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.224                                                   	MRR:22.8	Hits@10:36.99	Best:22.8
2024-12-27 21:57:53,072: Snapshot:4	Epoch:22	Loss:9.867	translation_Loss:5.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.201                                                   	MRR:22.67	Hits@10:36.98	Best:22.8
2024-12-27 21:57:58,420: Snapshot:4	Epoch:23	Loss:9.819	translation_Loss:5.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.185                                                   	MRR:22.76	Hits@10:37.11	Best:22.8
2024-12-27 21:58:03,755: Snapshot:4	Epoch:24	Loss:9.746	translation_Loss:5.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.172                                                   	MRR:22.73	Hits@10:36.8	Best:22.8
2024-12-27 21:58:09,026: Snapshot:4	Epoch:25	Loss:9.666	translation_Loss:5.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.154                                                   	MRR:22.73	Hits@10:36.83	Best:22.8
2024-12-27 21:58:14,306: Early Stopping! Snapshot: 4 Epoch: 26 Best Results: 22.8
2024-12-27 21:58:14,306: Start to training tokens! Snapshot: 4 Epoch: 26 Loss:9.632 MRR:22.61 Best Results: 22.8
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:58:14,307: Snapshot:4	Epoch:26	Loss:9.632	translation_Loss:5.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.137                                                   	MRR:22.61	Hits@10:36.93	Best:22.8
2024-12-27 21:58:19,446: Snapshot:4	Epoch:27	Loss:55.489	translation_Loss:54.335	multi_layer_Loss:1.153	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.61	Hits@10:36.93	Best:22.8
2024-12-27 21:58:24,597: End of token training: 4 Epoch: 28 Loss:54.445 MRR:22.61 Best Results: 22.8
2024-12-27 21:58:24,598: Snapshot:4	Epoch:28	Loss:54.445	translation_Loss:54.3	multi_layer_Loss:0.145	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.61	Hits@10:36.93	Best:22.8
2024-12-27 21:58:24,893: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_512_10000/4model_best.tar'
2024-12-27 21:58:40,958: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2219 | 0.1235 | 0.2684 | 0.3279 |  0.4027 |
|     1      | 0.2559 | 0.1526 | 0.2987 | 0.3653 |  0.4559 |
|     2      | 0.1928 | 0.1105 |  0.22  | 0.2749 |  0.3525 |
|     3      | 0.1791 | 0.0902 | 0.2121 | 0.2708 |  0.3457 |
|     4      | 0.2286 | 0.1523 | 0.2602 | 0.3086 |  0.373  |
+------------+--------+--------+--------+--------+---------+
2024-12-27 21:58:40,961: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.1527 | 0.3172 | 0.3797 |  0.4518 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.256  | 0.1489 | 0.3144 | 0.3784 |   0.45  |
|     1      | 0.2971 | 0.1868 | 0.3497 | 0.4195 |  0.5133 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2494 | 0.1386 | 0.3078 | 0.3744 |  0.4509 |
|     1      | 0.2724 | 0.1598 | 0.3269 | 0.3919 |  0.4853 |
|     2      | 0.2116 | 0.1248 | 0.2443 | 0.3032 |  0.3829 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2552 | 0.1475 | 0.3104 | 0.3734 |  0.4528 |
|     1      | 0.2728 | 0.165  | 0.316  | 0.389  |  0.4822 |
|     2      | 0.2082 | 0.1201 | 0.2404 | 0.3004 |  0.3803 |
|     3      | 0.2083 | 0.1172 | 0.2449 | 0.3035 |  0.3798 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2219 | 0.1235 | 0.2684 | 0.3279 |  0.4027 |
|     1      | 0.2559 | 0.1526 | 0.2987 | 0.3653 |  0.4559 |
|     2      | 0.1928 | 0.1105 |  0.22  | 0.2749 |  0.3525 |
|     3      | 0.1791 | 0.0902 | 0.2121 | 0.2708 |  0.3457 |
|     4      | 0.2286 | 0.1523 | 0.2602 | 0.3086 |  0.373  |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 21:58:40,961: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 233.3949875831604  |   0.259   |    0.153     |    0.317     |     0.452     |
|    1     | 142.78627347946167 |   0.267   |    0.159     |    0.324     |     0.467     |
|    2     | 287.9224443435669  |   0.232   |    0.134     |    0.277     |     0.419     |
|    3     | 297.51864409446716 |   0.223   |    0.128     |    0.263     |     0.403     |
|    4     | 166.5306122303009  |   0.203   |    0.114     |    0.237     |     0.369     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 21:58:40,961: Sum_Training_Time:1128.152961730957
2024-12-27 21:58:40,961: Every_Training_Time:[233.3949875831604, 142.78627347946167, 287.9224443435669, 297.51864409446716, 166.5306122303009]
2024-12-27 21:58:40,961: Forward transfer: 0.045225 Backward transfer: -0.03149999999999999
2024-12-27 21:59:14,735: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227215845/HYBRIDHYBRID_0.0001_1024_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_1024_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_1024_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 21:59:24,428: Snapshot:0	Epoch:0	Loss:48.314	translation_Loss:48.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.3	Hits@10:2.38	Best:1.3
2024-12-27 21:59:30,399: Snapshot:0	Epoch:1	Loss:45.002	translation_Loss:45.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.79	Hits@10:7.92	Best:3.79
2024-12-27 21:59:36,459: Snapshot:0	Epoch:2	Loss:41.936	translation_Loss:41.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:12.67	Best:5.6
2024-12-27 21:59:42,523: Snapshot:0	Epoch:3	Loss:39.022	translation_Loss:39.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.85	Hits@10:16.11	Best:6.85
2024-12-27 21:59:48,563: Snapshot:0	Epoch:4	Loss:36.216	translation_Loss:36.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.82	Hits@10:18.96	Best:7.82
2024-12-27 21:59:54,587: Snapshot:0	Epoch:5	Loss:33.472	translation_Loss:33.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.69	Best:8.89
2024-12-27 22:00:01,077: Snapshot:0	Epoch:6	Loss:30.69	translation_Loss:30.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:25.15	Best:10.29
2024-12-27 22:00:07,154: Snapshot:0	Epoch:7	Loss:27.959	translation_Loss:27.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.01	Hits@10:28.73	Best:12.01
2024-12-27 22:00:13,174: Snapshot:0	Epoch:8	Loss:25.173	translation_Loss:25.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.68	Hits@10:31.86	Best:13.68
2024-12-27 22:00:19,158: Snapshot:0	Epoch:9	Loss:22.449	translation_Loss:22.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.25	Hits@10:34.28	Best:15.25
2024-12-27 22:00:25,142: Snapshot:0	Epoch:10	Loss:19.787	translation_Loss:19.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.95	Hits@10:36.32	Best:16.95
2024-12-27 22:00:31,171: Snapshot:0	Epoch:11	Loss:17.285	translation_Loss:17.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.49	Hits@10:38.16	Best:18.49
2024-12-27 22:00:37,159: Snapshot:0	Epoch:12	Loss:14.981	translation_Loss:14.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.76	Hits@10:39.59	Best:19.76
2024-12-27 22:00:43,167: Snapshot:0	Epoch:13	Loss:13.018	translation_Loss:13.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.88	Hits@10:40.76	Best:20.88
2024-12-27 22:00:49,165: Snapshot:0	Epoch:14	Loss:11.354	translation_Loss:11.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.79	Hits@10:41.76	Best:21.79
2024-12-27 22:00:55,210: Snapshot:0	Epoch:15	Loss:9.881	translation_Loss:9.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.53	Hits@10:42.56	Best:22.53
2024-12-27 22:01:01,253: Snapshot:0	Epoch:16	Loss:8.648	translation_Loss:8.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:43.02	Best:23.1
2024-12-27 22:01:07,778: Snapshot:0	Epoch:17	Loss:7.591	translation_Loss:7.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.47	Hits@10:43.65	Best:23.47
2024-12-27 22:01:13,759: Snapshot:0	Epoch:18	Loss:6.693	translation_Loss:6.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.86	Hits@10:44.11	Best:23.86
2024-12-27 22:01:19,863: Snapshot:0	Epoch:19	Loss:5.931	translation_Loss:5.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:44.31	Best:24.08
2024-12-27 22:01:25,928: Snapshot:0	Epoch:20	Loss:5.225	translation_Loss:5.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:44.65	Best:24.34
2024-12-27 22:01:32,032: Snapshot:0	Epoch:21	Loss:4.673	translation_Loss:4.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:44.92	Best:24.48
2024-12-27 22:01:38,057: Snapshot:0	Epoch:22	Loss:4.193	translation_Loss:4.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:45.04	Best:24.63
2024-12-27 22:01:44,104: Snapshot:0	Epoch:23	Loss:3.782	translation_Loss:3.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:45.31	Best:24.74
2024-12-27 22:01:50,135: Snapshot:0	Epoch:24	Loss:3.458	translation_Loss:3.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:45.41	Best:24.88
2024-12-27 22:01:56,174: Snapshot:0	Epoch:25	Loss:3.107	translation_Loss:3.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:45.54	Best:24.97
2024-12-27 22:02:02,284: Snapshot:0	Epoch:26	Loss:2.85	translation_Loss:2.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.08	Hits@10:45.67	Best:25.08
2024-12-27 22:02:08,322: Snapshot:0	Epoch:27	Loss:2.62	translation_Loss:2.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.73	Best:25.18
2024-12-27 22:02:14,442: Snapshot:0	Epoch:28	Loss:2.413	translation_Loss:2.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.21	Hits@10:45.87	Best:25.21
2024-12-27 22:02:20,931: Snapshot:0	Epoch:29	Loss:2.253	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:45.94	Best:25.31
2024-12-27 22:02:27,004: Snapshot:0	Epoch:30	Loss:2.101	translation_Loss:2.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.36	Hits@10:46.0	Best:25.36
2024-12-27 22:02:33,027: Snapshot:0	Epoch:31	Loss:1.938	translation_Loss:1.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.45	Hits@10:46.14	Best:25.45
2024-12-27 22:02:39,011: Snapshot:0	Epoch:32	Loss:1.819	translation_Loss:1.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:46.13	Best:25.45
2024-12-27 22:02:45,063: Snapshot:0	Epoch:33	Loss:1.721	translation_Loss:1.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.18	Best:25.58
2024-12-27 22:02:51,096: Snapshot:0	Epoch:34	Loss:1.62	translation_Loss:1.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.51	Hits@10:46.09	Best:25.58
2024-12-27 22:02:57,092: Snapshot:0	Epoch:35	Loss:1.527	translation_Loss:1.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.18	Best:25.6
2024-12-27 22:03:03,079: Snapshot:0	Epoch:36	Loss:1.444	translation_Loss:1.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.19	Best:25.6
2024-12-27 22:03:09,102: Snapshot:0	Epoch:37	Loss:1.371	translation_Loss:1.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:46.16	Best:25.68
2024-12-27 22:03:15,114: Snapshot:0	Epoch:38	Loss:1.298	translation_Loss:1.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.21	Best:25.73
2024-12-27 22:03:21,175: Snapshot:0	Epoch:39	Loss:1.248	translation_Loss:1.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.23	Best:25.75
2024-12-27 22:03:27,159: Snapshot:0	Epoch:40	Loss:1.187	translation_Loss:1.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.25	Best:25.84
2024-12-27 22:03:33,223: Snapshot:0	Epoch:41	Loss:1.131	translation_Loss:1.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.2	Best:25.84
2024-12-27 22:03:39,570: Snapshot:0	Epoch:42	Loss:1.083	translation_Loss:1.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.26	Best:25.84
2024-12-27 22:03:45,555: Snapshot:0	Epoch:43	Loss:1.041	translation_Loss:1.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.17	Best:25.84
2024-12-27 22:03:51,525: Snapshot:0	Epoch:44	Loss:1.0	translation_Loss:1.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.12	Best:25.84
2024-12-27 22:03:57,493: Early Stopping! Snapshot: 0 Epoch: 45 Best Results: 25.84
2024-12-27 22:03:57,493: Start to training tokens! Snapshot: 0 Epoch: 45 Loss:0.969 MRR:25.72 Best Results: 25.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:03:57,493: Snapshot:0	Epoch:45	Loss:0.969	translation_Loss:0.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.07	Best:25.84
2024-12-27 22:04:04,065: Snapshot:0	Epoch:46	Loss:35.993	translation_Loss:35.039	multi_layer_Loss:0.954	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.07	Best:25.84
2024-12-27 22:04:10,091: End of token training: 0 Epoch: 47 Loss:35.265 MRR:25.72 Best Results: 25.84
2024-12-27 22:04:10,091: Snapshot:0	Epoch:47	Loss:35.265	translation_Loss:35.003	multi_layer_Loss:0.262	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.72	Hits@10:46.07	Best:25.84
2024-12-27 22:04:10,320: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_1000/0model_best.tar'
2024-12-27 22:04:12,796: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1489 | 0.3161 | 0.3794 |  0.4535 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:04:23,837: Snapshot:1	Epoch:0	Loss:14.974	translation_Loss:14.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.011                                                   	MRR:6.31	Hits@10:10.51	Best:6.31
2024-12-27 22:04:26,197: Snapshot:1	Epoch:1	Loss:13.318	translation_Loss:13.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.035                                                   	MRR:7.15	Hits@10:12.0	Best:7.15
2024-12-27 22:04:28,581: Snapshot:1	Epoch:2	Loss:11.768	translation_Loss:11.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.064                                                   	MRR:8.28	Hits@10:14.41	Best:8.28
2024-12-27 22:04:30,969: Snapshot:1	Epoch:3	Loss:10.305	translation_Loss:10.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.096                                                   	MRR:9.98	Hits@10:18.42	Best:9.98
2024-12-27 22:04:33,375: Snapshot:1	Epoch:4	Loss:8.87	translation_Loss:8.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.133                                                   	MRR:12.13	Hits@10:23.32	Best:12.13
2024-12-27 22:04:35,804: Snapshot:1	Epoch:5	Loss:7.56	translation_Loss:7.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:14.33	Hits@10:27.45	Best:14.33
2024-12-27 22:04:38,199: Snapshot:1	Epoch:6	Loss:6.397	translation_Loss:6.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:16.13	Hits@10:30.34	Best:16.13
2024-12-27 22:04:40,624: Snapshot:1	Epoch:7	Loss:5.423	translation_Loss:5.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:17.73	Hits@10:32.89	Best:17.73
2024-12-27 22:04:42,999: Snapshot:1	Epoch:8	Loss:4.625	translation_Loss:4.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.273                                                   	MRR:18.87	Hits@10:34.61	Best:18.87
2024-12-27 22:04:45,378: Snapshot:1	Epoch:9	Loss:3.962	translation_Loss:3.659	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.303                                                   	MRR:19.87	Hits@10:36.04	Best:19.87
2024-12-27 22:04:47,772: Snapshot:1	Epoch:10	Loss:3.42	translation_Loss:3.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.33                                                   	MRR:20.61	Hits@10:37.22	Best:20.61
2024-12-27 22:04:50,178: Snapshot:1	Epoch:11	Loss:2.996	translation_Loss:2.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:21.4	Hits@10:38.34	Best:21.4
2024-12-27 22:04:52,554: Snapshot:1	Epoch:12	Loss:2.641	translation_Loss:2.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:22.07	Hits@10:39.25	Best:22.07
2024-12-27 22:04:54,922: Snapshot:1	Epoch:13	Loss:2.364	translation_Loss:1.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:22.62	Hits@10:39.96	Best:22.62
2024-12-27 22:04:57,329: Snapshot:1	Epoch:14	Loss:2.105	translation_Loss:1.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.412                                                   	MRR:23.09	Hits@10:40.5	Best:23.09
2024-12-27 22:04:59,694: Snapshot:1	Epoch:15	Loss:1.912	translation_Loss:1.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.425                                                   	MRR:23.35	Hits@10:41.2	Best:23.35
2024-12-27 22:05:02,090: Snapshot:1	Epoch:16	Loss:1.738	translation_Loss:1.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.437                                                   	MRR:23.7	Hits@10:41.6	Best:23.7
2024-12-27 22:05:04,480: Snapshot:1	Epoch:17	Loss:1.588	translation_Loss:1.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.447                                                   	MRR:23.93	Hits@10:42.07	Best:23.93
2024-12-27 22:05:06,888: Snapshot:1	Epoch:18	Loss:1.454	translation_Loss:0.998	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.456                                                   	MRR:24.17	Hits@10:42.61	Best:24.17
2024-12-27 22:05:09,289: Snapshot:1	Epoch:19	Loss:1.346	translation_Loss:0.884	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:24.43	Hits@10:42.92	Best:24.43
2024-12-27 22:05:11,746: Snapshot:1	Epoch:20	Loss:1.269	translation_Loss:0.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.467                                                   	MRR:24.69	Hits@10:43.35	Best:24.69
2024-12-27 22:05:14,533: Snapshot:1	Epoch:21	Loss:1.187	translation_Loss:0.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.47                                                   	MRR:24.92	Hits@10:43.65	Best:24.92
2024-12-27 22:05:16,916: Snapshot:1	Epoch:22	Loss:1.119	translation_Loss:0.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.472                                                   	MRR:25.15	Hits@10:43.69	Best:25.15
2024-12-27 22:05:19,347: Snapshot:1	Epoch:23	Loss:1.063	translation_Loss:0.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.474                                                   	MRR:25.35	Hits@10:44.05	Best:25.35
2024-12-27 22:05:21,695: Snapshot:1	Epoch:24	Loss:1.013	translation_Loss:0.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:25.49	Hits@10:44.44	Best:25.49
2024-12-27 22:05:24,052: Snapshot:1	Epoch:25	Loss:0.95	translation_Loss:0.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:25.64	Hits@10:44.74	Best:25.64
2024-12-27 22:05:26,400: Snapshot:1	Epoch:26	Loss:0.919	translation_Loss:0.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.474                                                   	MRR:25.8	Hits@10:44.98	Best:25.8
2024-12-27 22:05:28,800: Snapshot:1	Epoch:27	Loss:0.892	translation_Loss:0.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.472                                                   	MRR:25.93	Hits@10:45.09	Best:25.93
2024-12-27 22:05:31,143: Snapshot:1	Epoch:28	Loss:0.846	translation_Loss:0.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.47                                                   	MRR:26.02	Hits@10:45.14	Best:26.02
2024-12-27 22:05:33,483: Snapshot:1	Epoch:29	Loss:0.824	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.467                                                   	MRR:26.16	Hits@10:45.43	Best:26.16
2024-12-27 22:05:35,832: Snapshot:1	Epoch:30	Loss:0.806	translation_Loss:0.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.465                                                   	MRR:26.26	Hits@10:45.59	Best:26.26
2024-12-27 22:05:38,241: Snapshot:1	Epoch:31	Loss:0.786	translation_Loss:0.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:26.35	Hits@10:45.75	Best:26.35
2024-12-27 22:05:40,610: Snapshot:1	Epoch:32	Loss:0.766	translation_Loss:0.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.459                                                   	MRR:26.46	Hits@10:45.92	Best:26.46
2024-12-27 22:05:42,955: Snapshot:1	Epoch:33	Loss:0.742	translation_Loss:0.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.456                                                   	MRR:26.43	Hits@10:46.17	Best:26.46
2024-12-27 22:05:45,335: Snapshot:1	Epoch:34	Loss:0.73	translation_Loss:0.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.453                                                   	MRR:26.62	Hits@10:46.3	Best:26.62
2024-12-27 22:05:47,665: Snapshot:1	Epoch:35	Loss:0.713	translation_Loss:0.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.451                                                   	MRR:26.84	Hits@10:46.32	Best:26.84
2024-12-27 22:05:50,058: Snapshot:1	Epoch:36	Loss:0.698	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.447                                                   	MRR:26.86	Hits@10:46.53	Best:26.86
2024-12-27 22:05:52,443: Snapshot:1	Epoch:37	Loss:0.687	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:26.92	Hits@10:46.6	Best:26.92
2024-12-27 22:05:54,799: Snapshot:1	Epoch:38	Loss:0.677	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.44                                                   	MRR:26.86	Hits@10:46.63	Best:26.92
2024-12-27 22:05:57,193: Snapshot:1	Epoch:39	Loss:0.656	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.437                                                   	MRR:26.99	Hits@10:46.84	Best:26.99
2024-12-27 22:05:59,546: Snapshot:1	Epoch:40	Loss:0.652	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.433                                                   	MRR:26.92	Hits@10:46.97	Best:26.99
2024-12-27 22:06:01,931: Snapshot:1	Epoch:41	Loss:0.632	translation_Loss:0.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:27.06	Hits@10:47.13	Best:27.06
2024-12-27 22:06:04,422: Snapshot:1	Epoch:42	Loss:0.628	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.426                                                   	MRR:27.27	Hits@10:47.2	Best:27.27
2024-12-27 22:06:06,862: Snapshot:1	Epoch:43	Loss:0.617	translation_Loss:0.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.423                                                   	MRR:27.29	Hits@10:47.14	Best:27.29
2024-12-27 22:06:09,220: Snapshot:1	Epoch:44	Loss:0.625	translation_Loss:0.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.42                                                   	MRR:27.29	Hits@10:47.52	Best:27.29
2024-12-27 22:06:11,580: Snapshot:1	Epoch:45	Loss:0.601	translation_Loss:0.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.417                                                   	MRR:27.45	Hits@10:47.69	Best:27.45
2024-12-27 22:06:13,947: Snapshot:1	Epoch:46	Loss:0.595	translation_Loss:0.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.414                                                   	MRR:27.46	Hits@10:47.8	Best:27.46
2024-12-27 22:06:16,272: Snapshot:1	Epoch:47	Loss:0.587	translation_Loss:0.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.41                                                   	MRR:27.41	Hits@10:47.77	Best:27.46
2024-12-27 22:06:18,596: Snapshot:1	Epoch:48	Loss:0.586	translation_Loss:0.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.407                                                   	MRR:27.41	Hits@10:47.95	Best:27.46
2024-12-27 22:06:20,954: Snapshot:1	Epoch:49	Loss:0.579	translation_Loss:0.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.404                                                   	MRR:27.49	Hits@10:48.05	Best:27.49
2024-12-27 22:06:23,314: Snapshot:1	Epoch:50	Loss:0.569	translation_Loss:0.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:27.66	Hits@10:48.1	Best:27.66
2024-12-27 22:06:25,670: Snapshot:1	Epoch:51	Loss:0.563	translation_Loss:0.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:27.76	Hits@10:48.2	Best:27.76
2024-12-27 22:06:28,039: Snapshot:1	Epoch:52	Loss:0.552	translation_Loss:0.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.395                                                   	MRR:27.79	Hits@10:48.31	Best:27.79
2024-12-27 22:06:30,409: Snapshot:1	Epoch:53	Loss:0.545	translation_Loss:0.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:27.82	Hits@10:48.51	Best:27.82
2024-12-27 22:06:33,141: Snapshot:1	Epoch:54	Loss:0.541	translation_Loss:0.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.388                                                   	MRR:27.77	Hits@10:48.54	Best:27.82
2024-12-27 22:06:35,487: Snapshot:1	Epoch:55	Loss:0.535	translation_Loss:0.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.385                                                   	MRR:27.72	Hits@10:48.53	Best:27.82
2024-12-27 22:06:37,885: Snapshot:1	Epoch:56	Loss:0.535	translation_Loss:0.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.382                                                   	MRR:27.86	Hits@10:48.53	Best:27.86
2024-12-27 22:06:40,289: Snapshot:1	Epoch:57	Loss:0.527	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.38                                                   	MRR:27.9	Hits@10:48.55	Best:27.9
2024-12-27 22:06:42,735: Snapshot:1	Epoch:58	Loss:0.52	translation_Loss:0.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:27.99	Hits@10:48.61	Best:27.99
2024-12-27 22:06:45,071: Snapshot:1	Epoch:59	Loss:0.518	translation_Loss:0.145	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.374                                                   	MRR:27.99	Hits@10:48.69	Best:27.99
2024-12-27 22:06:47,411: Snapshot:1	Epoch:60	Loss:0.515	translation_Loss:0.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.371                                                   	MRR:27.98	Hits@10:48.53	Best:27.99
2024-12-27 22:06:49,820: Snapshot:1	Epoch:61	Loss:0.515	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.369                                                   	MRR:28.03	Hits@10:48.85	Best:28.03
2024-12-27 22:06:52,248: Snapshot:1	Epoch:62	Loss:0.509	translation_Loss:0.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.367                                                   	MRR:28.19	Hits@10:48.9	Best:28.19
2024-12-27 22:06:54,587: Snapshot:1	Epoch:63	Loss:0.503	translation_Loss:0.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.365                                                   	MRR:28.08	Hits@10:48.8	Best:28.19
2024-12-27 22:06:56,936: Snapshot:1	Epoch:64	Loss:0.501	translation_Loss:0.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.362                                                   	MRR:28.16	Hits@10:48.95	Best:28.19
2024-12-27 22:06:59,312: Snapshot:1	Epoch:65	Loss:0.5	translation_Loss:0.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.36                                                   	MRR:28.3	Hits@10:49.27	Best:28.3
2024-12-27 22:07:01,685: Snapshot:1	Epoch:66	Loss:0.492	translation_Loss:0.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.357                                                   	MRR:28.37	Hits@10:49.46	Best:28.37
2024-12-27 22:07:04,052: Snapshot:1	Epoch:67	Loss:0.488	translation_Loss:0.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:28.32	Hits@10:49.39	Best:28.37
2024-12-27 22:07:06,376: Snapshot:1	Epoch:68	Loss:0.483	translation_Loss:0.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:28.3	Hits@10:49.52	Best:28.37
2024-12-27 22:07:08,706: Snapshot:1	Epoch:69	Loss:0.476	translation_Loss:0.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.35                                                   	MRR:28.3	Hits@10:49.32	Best:28.37
2024-12-27 22:07:11,099: Snapshot:1	Epoch:70	Loss:0.479	translation_Loss:0.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.349                                                   	MRR:28.39	Hits@10:49.45	Best:28.39
2024-12-27 22:07:13,448: Snapshot:1	Epoch:71	Loss:0.474	translation_Loss:0.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.347                                                   	MRR:28.38	Hits@10:49.49	Best:28.39
2024-12-27 22:07:15,770: Snapshot:1	Epoch:72	Loss:0.475	translation_Loss:0.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.345                                                   	MRR:28.33	Hits@10:49.44	Best:28.39
2024-12-27 22:07:18,211: Snapshot:1	Epoch:73	Loss:0.465	translation_Loss:0.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.343                                                   	MRR:28.5	Hits@10:49.51	Best:28.5
2024-12-27 22:07:20,648: Snapshot:1	Epoch:74	Loss:0.462	translation_Loss:0.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.34                                                   	MRR:28.54	Hits@10:49.56	Best:28.54
2024-12-27 22:07:23,056: Snapshot:1	Epoch:75	Loss:0.463	translation_Loss:0.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.338                                                   	MRR:28.57	Hits@10:49.44	Best:28.57
2024-12-27 22:07:25,475: Snapshot:1	Epoch:76	Loss:0.456	translation_Loss:0.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.336                                                   	MRR:28.77	Hits@10:49.46	Best:28.77
2024-12-27 22:07:27,895: Snapshot:1	Epoch:77	Loss:0.458	translation_Loss:0.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.334                                                   	MRR:28.84	Hits@10:49.88	Best:28.84
2024-12-27 22:07:30,292: Snapshot:1	Epoch:78	Loss:0.453	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:28.89	Hits@10:49.9	Best:28.89
2024-12-27 22:07:32,783: Snapshot:1	Epoch:79	Loss:0.444	translation_Loss:0.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.331                                                   	MRR:29.1	Hits@10:50.04	Best:29.1
2024-12-27 22:07:35,110: Snapshot:1	Epoch:80	Loss:0.448	translation_Loss:0.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:29.09	Hits@10:50.05	Best:29.1
2024-12-27 22:07:37,441: Snapshot:1	Epoch:81	Loss:0.443	translation_Loss:0.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.327                                                   	MRR:29.05	Hits@10:50.12	Best:29.1
2024-12-27 22:07:39,766: Snapshot:1	Epoch:82	Loss:0.443	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.325                                                   	MRR:28.92	Hits@10:50.18	Best:29.1
2024-12-27 22:07:42,126: Snapshot:1	Epoch:83	Loss:0.442	translation_Loss:0.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.323                                                   	MRR:29.04	Hits@10:50.25	Best:29.1
2024-12-27 22:07:44,476: Early Stopping! Snapshot: 1 Epoch: 84 Best Results: 29.1
2024-12-27 22:07:44,476: Start to training tokens! Snapshot: 1 Epoch: 84 Loss:0.439 MRR:29.07 Best Results: 29.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:07:44,476: Snapshot:1	Epoch:84	Loss:0.439	translation_Loss:0.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:29.07	Hits@10:50.47	Best:29.1
2024-12-27 22:07:46,758: Snapshot:1	Epoch:85	Loss:11.786	translation_Loss:11.338	multi_layer_Loss:0.448	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.07	Hits@10:50.47	Best:29.1
2024-12-27 22:07:49,064: End of token training: 1 Epoch: 86 Loss:11.674 MRR:29.07 Best Results: 29.1
2024-12-27 22:07:49,064: Snapshot:1	Epoch:86	Loss:11.674	translation_Loss:11.345	multi_layer_Loss:0.329	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.07	Hits@10:50.47	Best:29.1
2024-12-27 22:07:49,308: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_1000/1model_best.tar'
2024-12-27 22:07:53,267: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1352 | 0.2948 | 0.3607 |  0.4407 |
|     1      | 0.2876 | 0.179  | 0.3412 | 0.4072 |  0.4953 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:08:27,621: Snapshot:2	Epoch:0	Loss:60.463	translation_Loss:60.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.126                                                   	MRR:5.68	Hits@10:12.14	Best:5.68
2024-12-27 22:08:37,953: Snapshot:2	Epoch:1	Loss:45.523	translation_Loss:45.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.403                                                   	MRR:9.56	Hits@10:20.38	Best:9.56
2024-12-27 22:08:48,360: Snapshot:2	Epoch:2	Loss:33.447	translation_Loss:32.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.775                                                   	MRR:12.79	Hits@10:26.01	Best:12.79
2024-12-27 22:08:58,671: Snapshot:2	Epoch:3	Loss:24.832	translation_Loss:23.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.157                                                   	MRR:15.15	Hits@10:29.73	Best:15.15
2024-12-27 22:09:08,995: Snapshot:2	Epoch:4	Loss:19.086	translation_Loss:17.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.493                                                   	MRR:16.82	Hits@10:32.34	Best:16.82
2024-12-27 22:09:19,331: Snapshot:2	Epoch:5	Loss:15.167	translation_Loss:13.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.772                                                   	MRR:17.9	Hits@10:33.96	Best:17.9
2024-12-27 22:09:29,614: Snapshot:2	Epoch:6	Loss:12.493	translation_Loss:10.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.992                                                   	MRR:18.76	Hits@10:35.19	Best:18.76
2024-12-27 22:09:39,933: Snapshot:2	Epoch:7	Loss:10.555	translation_Loss:8.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.161                                                   	MRR:19.26	Hits@10:36.11	Best:19.26
2024-12-27 22:09:50,256: Snapshot:2	Epoch:8	Loss:9.222	translation_Loss:6.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.286                                                   	MRR:19.68	Hits@10:36.85	Best:19.68
2024-12-27 22:10:00,539: Snapshot:2	Epoch:9	Loss:8.239	translation_Loss:5.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.375                                                   	MRR:19.92	Hits@10:37.26	Best:19.92
2024-12-27 22:10:10,949: Snapshot:2	Epoch:10	Loss:7.498	translation_Loss:5.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.436                                                   	MRR:20.22	Hits@10:37.62	Best:20.22
2024-12-27 22:10:21,383: Snapshot:2	Epoch:11	Loss:6.904	translation_Loss:4.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.478                                                   	MRR:20.47	Hits@10:38.04	Best:20.47
2024-12-27 22:10:31,648: Snapshot:2	Epoch:12	Loss:6.483	translation_Loss:3.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.502                                                   	MRR:20.65	Hits@10:38.17	Best:20.65
2024-12-27 22:10:41,940: Snapshot:2	Epoch:13	Loss:6.131	translation_Loss:3.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.513                                                   	MRR:20.77	Hits@10:38.5	Best:20.77
2024-12-27 22:10:52,202: Snapshot:2	Epoch:14	Loss:5.836	translation_Loss:3.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.516                                                   	MRR:20.99	Hits@10:38.48	Best:20.99
2024-12-27 22:11:02,468: Snapshot:2	Epoch:15	Loss:5.595	translation_Loss:3.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.511                                                   	MRR:21.02	Hits@10:38.66	Best:21.02
2024-12-27 22:11:12,709: Snapshot:2	Epoch:16	Loss:5.386	translation_Loss:2.885	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.501                                                   	MRR:21.18	Hits@10:38.77	Best:21.18
2024-12-27 22:11:23,046: Snapshot:2	Epoch:17	Loss:5.23	translation_Loss:2.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.488                                                   	MRR:21.3	Hits@10:38.85	Best:21.3
2024-12-27 22:11:33,293: Snapshot:2	Epoch:18	Loss:5.104	translation_Loss:2.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.478                                                   	MRR:21.23	Hits@10:38.78	Best:21.3
2024-12-27 22:11:43,569: Snapshot:2	Epoch:19	Loss:4.965	translation_Loss:2.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.462                                                   	MRR:21.28	Hits@10:38.92	Best:21.3
2024-12-27 22:11:53,865: Snapshot:2	Epoch:20	Loss:4.862	translation_Loss:2.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.447                                                   	MRR:21.33	Hits@10:39.0	Best:21.33
2024-12-27 22:12:04,143: Snapshot:2	Epoch:21	Loss:4.783	translation_Loss:2.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.436                                                   	MRR:21.37	Hits@10:39.02	Best:21.37
2024-12-27 22:12:14,403: Snapshot:2	Epoch:22	Loss:4.701	translation_Loss:2.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.422                                                   	MRR:21.54	Hits@10:39.05	Best:21.54
2024-12-27 22:12:24,687: Snapshot:2	Epoch:23	Loss:4.63	translation_Loss:2.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.404                                                   	MRR:21.52	Hits@10:39.14	Best:21.54
2024-12-27 22:12:34,973: Snapshot:2	Epoch:24	Loss:4.567	translation_Loss:2.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.391                                                   	MRR:21.66	Hits@10:39.12	Best:21.66
2024-12-27 22:12:45,292: Snapshot:2	Epoch:25	Loss:4.507	translation_Loss:2.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.382                                                   	MRR:21.59	Hits@10:39.04	Best:21.66
2024-12-27 22:12:55,540: Snapshot:2	Epoch:26	Loss:4.446	translation_Loss:2.077	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.369                                                   	MRR:21.46	Hits@10:38.89	Best:21.66
2024-12-27 22:13:05,802: Snapshot:2	Epoch:27	Loss:4.38	translation_Loss:2.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.354                                                   	MRR:21.58	Hits@10:39.06	Best:21.66
2024-12-27 22:13:16,400: Snapshot:2	Epoch:28	Loss:4.366	translation_Loss:2.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.345                                                   	MRR:21.64	Hits@10:39.15	Best:21.66
2024-12-27 22:13:26,704: Early Stopping! Snapshot: 2 Epoch: 29 Best Results: 21.66
2024-12-27 22:13:26,705: Start to training tokens! Snapshot: 2 Epoch: 29 Loss:4.323 MRR:21.6 Best Results: 21.66
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:13:26,705: Snapshot:2	Epoch:29	Loss:4.323	translation_Loss:1.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.334                                                   	MRR:21.6	Hits@10:39.02	Best:21.66
2024-12-27 22:13:36,790: Snapshot:2	Epoch:30	Loss:50.2	translation_Loss:49.112	multi_layer_Loss:1.088	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.6	Hits@10:39.02	Best:21.66
2024-12-27 22:13:46,896: End of token training: 2 Epoch: 31 Loss:49.236 MRR:21.6 Best Results: 21.66
2024-12-27 22:13:46,897: Snapshot:2	Epoch:31	Loss:49.236	translation_Loss:49.143	multi_layer_Loss:0.094	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.6	Hits@10:39.02	Best:21.66
2024-12-27 22:13:47,134: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_1000/2model_best.tar'
2024-12-27 22:13:55,501: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.1332 | 0.2937 | 0.3613 |  0.4446 |
|     1      | 0.2499 | 0.1434 | 0.2988 | 0.3653 |  0.4461 |
|     2      | 0.2197 | 0.1287 | 0.2548 | 0.3148 |  0.398  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:14:35,617: Snapshot:3	Epoch:0	Loss:59.908	translation_Loss:59.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.189                                                   	MRR:6.58	Hits@10:15.95	Best:6.58
2024-12-27 22:14:48,136: Snapshot:3	Epoch:1	Loss:37.639	translation_Loss:37.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.611                                                   	MRR:11.41	Hits@10:24.67	Best:11.41
2024-12-27 22:15:00,906: Snapshot:3	Epoch:2	Loss:24.053	translation_Loss:22.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.079                                                   	MRR:14.32	Hits@10:29.59	Best:14.32
2024-12-27 22:15:13,586: Snapshot:3	Epoch:3	Loss:17.006	translation_Loss:15.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.475                                                   	MRR:16.26	Hits@10:33.15	Best:16.26
2024-12-27 22:15:26,244: Snapshot:3	Epoch:4	Loss:12.879	translation_Loss:11.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.777                                                   	MRR:17.9	Hits@10:35.32	Best:17.9
2024-12-27 22:15:38,805: Snapshot:3	Epoch:5	Loss:10.212	translation_Loss:8.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.993                                                   	MRR:18.9	Hits@10:36.5	Best:18.9
2024-12-27 22:15:51,385: Snapshot:3	Epoch:6	Loss:8.599	translation_Loss:6.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.139                                                   	MRR:19.45	Hits@10:37.24	Best:19.45
2024-12-27 22:16:03,960: Snapshot:3	Epoch:7	Loss:7.54	translation_Loss:5.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.238                                                   	MRR:19.93	Hits@10:37.97	Best:19.93
2024-12-27 22:16:16,459: Snapshot:3	Epoch:8	Loss:6.763	translation_Loss:4.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.296                                                   	MRR:20.3	Hits@10:38.43	Best:20.3
2024-12-27 22:16:29,120: Snapshot:3	Epoch:9	Loss:6.219	translation_Loss:3.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.325                                                   	MRR:20.53	Hits@10:38.95	Best:20.53
2024-12-27 22:16:41,668: Snapshot:3	Epoch:10	Loss:5.773	translation_Loss:3.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.337                                                   	MRR:20.75	Hits@10:39.24	Best:20.75
2024-12-27 22:16:54,281: Snapshot:3	Epoch:11	Loss:5.431	translation_Loss:3.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.333                                                   	MRR:20.94	Hits@10:39.33	Best:20.94
2024-12-27 22:17:06,885: Snapshot:3	Epoch:12	Loss:5.178	translation_Loss:2.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.322                                                   	MRR:21.11	Hits@10:39.58	Best:21.11
2024-12-27 22:17:19,627: Snapshot:3	Epoch:13	Loss:4.932	translation_Loss:2.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.306                                                   	MRR:21.25	Hits@10:39.83	Best:21.25
2024-12-27 22:17:32,243: Snapshot:3	Epoch:14	Loss:4.795	translation_Loss:2.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.288                                                   	MRR:21.33	Hits@10:39.94	Best:21.33
2024-12-27 22:17:44,835: Snapshot:3	Epoch:15	Loss:4.61	translation_Loss:2.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.271                                                   	MRR:21.46	Hits@10:39.97	Best:21.46
2024-12-27 22:17:57,347: Snapshot:3	Epoch:16	Loss:4.505	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.252                                                   	MRR:21.51	Hits@10:40.25	Best:21.51
2024-12-27 22:18:09,872: Snapshot:3	Epoch:17	Loss:4.383	translation_Loss:2.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.232                                                   	MRR:21.63	Hits@10:40.34	Best:21.63
2024-12-27 22:18:22,485: Snapshot:3	Epoch:18	Loss:4.304	translation_Loss:2.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.215                                                   	MRR:21.69	Hits@10:40.5	Best:21.69
2024-12-27 22:18:35,036: Snapshot:3	Epoch:19	Loss:4.22	translation_Loss:2.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.194                                                   	MRR:21.82	Hits@10:40.67	Best:21.82
2024-12-27 22:18:47,642: Snapshot:3	Epoch:20	Loss:4.152	translation_Loss:1.972	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.18                                                   	MRR:21.83	Hits@10:40.65	Best:21.83
2024-12-27 22:19:00,375: Snapshot:3	Epoch:21	Loss:4.069	translation_Loss:1.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.161                                                   	MRR:21.86	Hits@10:40.69	Best:21.86
2024-12-27 22:19:12,954: Snapshot:3	Epoch:22	Loss:4.014	translation_Loss:1.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.151                                                   	MRR:21.87	Hits@10:40.72	Best:21.87
2024-12-27 22:19:25,584: Snapshot:3	Epoch:23	Loss:3.967	translation_Loss:1.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.141                                                   	MRR:21.77	Hits@10:40.72	Best:21.87
2024-12-27 22:19:38,189: Snapshot:3	Epoch:24	Loss:3.919	translation_Loss:1.792	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.127                                                   	MRR:21.93	Hits@10:40.83	Best:21.93
2024-12-27 22:19:50,877: Snapshot:3	Epoch:25	Loss:3.903	translation_Loss:1.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.113                                                   	MRR:21.88	Hits@10:40.69	Best:21.93
2024-12-27 22:20:03,404: Snapshot:3	Epoch:26	Loss:3.856	translation_Loss:1.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.101                                                   	MRR:21.86	Hits@10:40.74	Best:21.93
2024-12-27 22:20:16,026: Snapshot:3	Epoch:27	Loss:3.825	translation_Loss:1.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.09                                                   	MRR:21.88	Hits@10:40.8	Best:21.93
2024-12-27 22:20:28,506: Snapshot:3	Epoch:28	Loss:3.81	translation_Loss:1.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.079                                                   	MRR:21.97	Hits@10:40.96	Best:21.97
2024-12-27 22:20:41,013: Snapshot:3	Epoch:29	Loss:3.743	translation_Loss:1.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.069                                                   	MRR:22.05	Hits@10:41.02	Best:22.05
2024-12-27 22:20:53,676: Snapshot:3	Epoch:30	Loss:3.735	translation_Loss:1.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.06                                                   	MRR:21.86	Hits@10:41.07	Best:22.05
2024-12-27 22:21:06,120: Snapshot:3	Epoch:31	Loss:3.701	translation_Loss:1.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.05                                                   	MRR:21.91	Hits@10:41.06	Best:22.05
2024-12-27 22:21:18,698: Snapshot:3	Epoch:32	Loss:3.685	translation_Loss:1.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.04                                                   	MRR:22.07	Hits@10:40.9	Best:22.07
2024-12-27 22:21:31,295: Snapshot:3	Epoch:33	Loss:3.668	translation_Loss:1.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.036                                                   	MRR:22.12	Hits@10:41.17	Best:22.12
2024-12-27 22:21:43,865: Snapshot:3	Epoch:34	Loss:3.654	translation_Loss:1.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.031                                                   	MRR:22.09	Hits@10:41.1	Best:22.12
2024-12-27 22:21:56,460: Snapshot:3	Epoch:35	Loss:3.637	translation_Loss:1.61	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.027                                                   	MRR:22.13	Hits@10:41.14	Best:22.13
2024-12-27 22:22:08,929: Snapshot:3	Epoch:36	Loss:3.601	translation_Loss:1.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.019                                                   	MRR:22.08	Hits@10:41.12	Best:22.13
2024-12-27 22:22:21,597: Snapshot:3	Epoch:37	Loss:3.601	translation_Loss:1.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.01                                                   	MRR:22.14	Hits@10:41.02	Best:22.14
2024-12-27 22:22:34,656: Snapshot:3	Epoch:38	Loss:3.583	translation_Loss:1.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.005                                                   	MRR:22.19	Hits@10:41.23	Best:22.19
2024-12-27 22:22:47,293: Snapshot:3	Epoch:39	Loss:3.578	translation_Loss:1.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.998                                                   	MRR:22.22	Hits@10:41.26	Best:22.22
2024-12-27 22:22:59,758: Snapshot:3	Epoch:40	Loss:3.573	translation_Loss:1.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.997                                                   	MRR:22.12	Hits@10:41.23	Best:22.22
2024-12-27 22:23:12,376: Snapshot:3	Epoch:41	Loss:3.544	translation_Loss:1.551	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.993                                                   	MRR:22.01	Hits@10:41.05	Best:22.22
2024-12-27 22:23:25,034: Snapshot:3	Epoch:42	Loss:3.548	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.987                                                   	MRR:22.06	Hits@10:41.13	Best:22.22
2024-12-27 22:23:37,898: Snapshot:3	Epoch:43	Loss:3.519	translation_Loss:1.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.987                                                   	MRR:22.06	Hits@10:41.14	Best:22.22
2024-12-27 22:23:50,440: Early Stopping! Snapshot: 3 Epoch: 44 Best Results: 22.22
2024-12-27 22:23:50,441: Start to training tokens! Snapshot: 3 Epoch: 44 Loss:3.506 MRR:22.15 Best Results: 22.22
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:23:50,441: Snapshot:3	Epoch:44	Loss:3.506	translation_Loss:1.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.98                                                   	MRR:22.15	Hits@10:41.12	Best:22.22
2024-12-27 22:24:02,748: Snapshot:3	Epoch:45	Loss:53.53	translation_Loss:52.339	multi_layer_Loss:1.191	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.15	Hits@10:41.12	Best:22.22
2024-12-27 22:24:14,975: End of token training: 3 Epoch: 46 Loss:52.363 MRR:22.15 Best Results: 22.22
2024-12-27 22:24:14,976: Snapshot:3	Epoch:46	Loss:52.363	translation_Loss:52.303	multi_layer_Loss:0.06	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.15	Hits@10:41.12	Best:22.22
2024-12-27 22:24:15,257: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_1000/3model_best.tar'
2024-12-27 22:24:28,724: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1413 | 0.289  | 0.3556 |  0.4348 |
|     1      | 0.2456 | 0.1486 | 0.2813 | 0.3394 |  0.4298 |
|     2      | 0.2039 | 0.1131 | 0.2368 | 0.2971 |  0.3775 |
|     3      | 0.2223 | 0.1236 | 0.2623 | 0.3266 |  0.4107 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:24:47,491: Snapshot:4	Epoch:0	Loss:20.03	translation_Loss:20.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.022                                                   	MRR:7.83	Hits@10:16.25	Best:7.83
2024-12-27 22:24:52,715: Snapshot:4	Epoch:1	Loss:15.517	translation_Loss:15.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.071                                                   	MRR:8.84	Hits@10:18.31	Best:8.84
2024-12-27 22:24:57,929: Snapshot:4	Epoch:2	Loss:11.541	translation_Loss:11.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:13.46	Hits@10:29.43	Best:13.46
2024-12-27 22:25:03,094: Snapshot:4	Epoch:3	Loss:8.622	translation_Loss:8.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:16.91	Hits@10:32.54	Best:16.91
2024-12-27 22:25:08,350: Snapshot:4	Epoch:4	Loss:6.727	translation_Loss:6.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.32                                                   	MRR:17.9	Hits@10:34.25	Best:17.9
2024-12-27 22:25:13,549: Snapshot:4	Epoch:5	Loss:5.377	translation_Loss:4.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.41                                                   	MRR:18.77	Hits@10:35.12	Best:18.77
2024-12-27 22:25:18,743: Snapshot:4	Epoch:6	Loss:4.389	translation_Loss:3.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.494                                                   	MRR:19.28	Hits@10:35.99	Best:19.28
2024-12-27 22:25:23,934: Snapshot:4	Epoch:7	Loss:3.62	translation_Loss:3.051	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.569                                                   	MRR:19.29	Hits@10:36.49	Best:19.29
2024-12-27 22:25:29,174: Snapshot:4	Epoch:8	Loss:3.049	translation_Loss:2.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.634                                                   	MRR:19.9	Hits@10:36.64	Best:19.9
2024-12-27 22:25:34,348: Snapshot:4	Epoch:9	Loss:2.597	translation_Loss:1.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.689                                                   	MRR:20.18	Hits@10:36.9	Best:20.18
2024-12-27 22:25:39,659: Snapshot:4	Epoch:10	Loss:2.249	translation_Loss:1.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.731                                                   	MRR:20.63	Hits@10:37.11	Best:20.63
2024-12-27 22:25:44,877: Snapshot:4	Epoch:11	Loss:1.985	translation_Loss:1.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.761                                                   	MRR:20.83	Hits@10:37.11	Best:20.83
2024-12-27 22:25:50,028: Snapshot:4	Epoch:12	Loss:1.79	translation_Loss:1.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.782                                                   	MRR:20.83	Hits@10:36.98	Best:20.83
2024-12-27 22:25:55,139: Snapshot:4	Epoch:13	Loss:1.643	translation_Loss:0.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.796                                                   	MRR:20.84	Hits@10:37.06	Best:20.84
2024-12-27 22:26:00,316: Snapshot:4	Epoch:14	Loss:1.537	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.804                                                   	MRR:20.88	Hits@10:37.02	Best:20.88
2024-12-27 22:26:05,519: Snapshot:4	Epoch:15	Loss:1.443	translation_Loss:0.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.807                                                   	MRR:20.89	Hits@10:37.1	Best:20.89
2024-12-27 22:26:10,675: Snapshot:4	Epoch:16	Loss:1.366	translation_Loss:0.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.807                                                   	MRR:21.05	Hits@10:37.34	Best:21.05
2024-12-27 22:26:15,913: Snapshot:4	Epoch:17	Loss:1.314	translation_Loss:0.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.805                                                   	MRR:21.32	Hits@10:37.25	Best:21.32
2024-12-27 22:26:21,027: Snapshot:4	Epoch:18	Loss:1.263	translation_Loss:0.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.801                                                   	MRR:21.32	Hits@10:37.46	Best:21.32
2024-12-27 22:26:26,227: Snapshot:4	Epoch:19	Loss:1.216	translation_Loss:0.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.795                                                   	MRR:21.62	Hits@10:37.4	Best:21.62
2024-12-27 22:26:31,407: Snapshot:4	Epoch:20	Loss:1.18	translation_Loss:0.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.788                                                   	MRR:21.41	Hits@10:37.5	Best:21.62
2024-12-27 22:26:36,560: Snapshot:4	Epoch:21	Loss:1.149	translation_Loss:0.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.78                                                   	MRR:21.56	Hits@10:37.51	Best:21.62
2024-12-27 22:26:41,624: Snapshot:4	Epoch:22	Loss:1.124	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.773                                                   	MRR:21.6	Hits@10:37.48	Best:21.62
2024-12-27 22:26:46,863: Snapshot:4	Epoch:23	Loss:1.102	translation_Loss:0.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.766                                                   	MRR:21.89	Hits@10:37.65	Best:21.89
2024-12-27 22:26:52,009: Snapshot:4	Epoch:24	Loss:1.077	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.758                                                   	MRR:21.98	Hits@10:37.83	Best:21.98
2024-12-27 22:26:57,193: Snapshot:4	Epoch:25	Loss:1.048	translation_Loss:0.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.751                                                   	MRR:22.11	Hits@10:37.89	Best:22.11
2024-12-27 22:27:02,283: Snapshot:4	Epoch:26	Loss:1.045	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.743                                                   	MRR:22.06	Hits@10:37.84	Best:22.11
2024-12-27 22:27:07,394: Snapshot:4	Epoch:27	Loss:1.015	translation_Loss:0.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.735                                                   	MRR:22.07	Hits@10:37.62	Best:22.11
2024-12-27 22:27:12,523: Snapshot:4	Epoch:28	Loss:1.01	translation_Loss:0.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.728                                                   	MRR:22.48	Hits@10:37.95	Best:22.48
2024-12-27 22:27:17,668: Snapshot:4	Epoch:29	Loss:0.982	translation_Loss:0.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.721                                                   	MRR:22.32	Hits@10:38.12	Best:22.48
2024-12-27 22:27:22,806: Snapshot:4	Epoch:30	Loss:0.981	translation_Loss:0.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.714                                                   	MRR:22.14	Hits@10:38.16	Best:22.48
2024-12-27 22:27:27,953: Snapshot:4	Epoch:31	Loss:0.969	translation_Loss:0.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.708                                                   	MRR:22.4	Hits@10:38.51	Best:22.48
2024-12-27 22:27:33,144: Snapshot:4	Epoch:32	Loss:0.951	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.702                                                   	MRR:22.53	Hits@10:38.53	Best:22.53
2024-12-27 22:27:38,346: Snapshot:4	Epoch:33	Loss:0.947	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.696                                                   	MRR:22.79	Hits@10:38.66	Best:22.79
2024-12-27 22:27:43,515: Snapshot:4	Epoch:34	Loss:0.939	translation_Loss:0.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.69                                                   	MRR:22.63	Hits@10:38.43	Best:22.79
2024-12-27 22:27:48,648: Snapshot:4	Epoch:35	Loss:0.931	translation_Loss:0.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.684                                                   	MRR:22.69	Hits@10:38.28	Best:22.79
2024-12-27 22:27:53,733: Snapshot:4	Epoch:36	Loss:0.917	translation_Loss:0.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.677                                                   	MRR:22.33	Hits@10:38.63	Best:22.79
2024-12-27 22:27:58,881: Snapshot:4	Epoch:37	Loss:0.902	translation_Loss:0.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.671                                                   	MRR:22.56	Hits@10:38.87	Best:22.79
2024-12-27 22:28:04,029: Early Stopping! Snapshot: 4 Epoch: 38 Best Results: 22.79
2024-12-27 22:28:04,029: Start to training tokens! Snapshot: 4 Epoch: 38 Loss:0.898 MRR:22.69 Best Results: 22.79
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:28:04,029: Snapshot:4	Epoch:38	Loss:0.898	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.665                                                   	MRR:22.69	Hits@10:38.75	Best:22.79
2024-12-27 22:28:09,097: Snapshot:4	Epoch:39	Loss:23.399	translation_Loss:22.613	multi_layer_Loss:0.786	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:38.75	Best:22.79
2024-12-27 22:28:14,125: End of token training: 4 Epoch: 40 Loss:22.964 MRR:22.69 Best Results: 22.79
2024-12-27 22:28:14,125: Snapshot:4	Epoch:40	Loss:22.964	translation_Loss:22.593	multi_layer_Loss:0.372	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.69	Hits@10:38.75	Best:22.79
2024-12-27 22:28:14,416: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_1000/4model_best.tar'
2024-12-27 22:28:30,663: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1859 | 0.0964 | 0.2239 | 0.2795 |  0.351  |
|     1      | 0.2128 | 0.1231 | 0.244  | 0.2983 |  0.3917 |
|     2      | 0.1757 | 0.098  | 0.1998 | 0.2505 |  0.3251 |
|     3      | 0.172  | 0.0864 | 0.201  | 0.258  |  0.3337 |
|     4      | 0.225  | 0.1416 | 0.2555 | 0.309  |  0.3819 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 22:28:30,665: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1489 | 0.3161 | 0.3794 |  0.4535 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1352 | 0.2948 | 0.3607 |  0.4407 |
|     1      | 0.2876 | 0.179  | 0.3412 | 0.4072 |  0.4953 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.1332 | 0.2937 | 0.3613 |  0.4446 |
|     1      | 0.2499 | 0.1434 | 0.2988 | 0.3653 |  0.4461 |
|     2      | 0.2197 | 0.1287 | 0.2548 | 0.3148 |  0.398  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1413 | 0.289  | 0.3556 |  0.4348 |
|     1      | 0.2456 | 0.1486 | 0.2813 | 0.3394 |  0.4298 |
|     2      | 0.2039 | 0.1131 | 0.2368 | 0.2971 |  0.3775 |
|     3      | 0.2223 | 0.1236 | 0.2623 | 0.3266 |  0.4107 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1859 | 0.0964 | 0.2239 | 0.2795 |  0.351  |
|     1      | 0.2128 | 0.1231 | 0.244  | 0.2983 |  0.3917 |
|     2      | 0.1757 | 0.098  | 0.1998 | 0.2505 |  0.3251 |
|     3      | 0.172  | 0.0864 | 0.201  | 0.258  |  0.3337 |
|     4      | 0.225  | 0.1416 | 0.2555 | 0.309  |  0.3819 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 22:28:30,666: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 295.3557872772217  |   0.257   |    0.149     |    0.316     |     0.454     |
|    1     | 214.76784682273865 |   0.254   |    0.147     |    0.307     |     0.455     |
|    2     | 349.1587710380554  |   0.231   |    0.132     |    0.274     |      0.42     |
|    3     | 614.2574536800385  |   0.223   |    0.126     |    0.261     |     0.406     |
|    4     | 222.89707732200623 |   0.185   |    0.101     |    0.215     |     0.345     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 22:28:30,666: Sum_Training_Time:1696.4369361400604
2024-12-27 22:28:30,666: Every_Training_Time:[295.3557872772217, 214.76784682273865, 349.1587710380554, 614.2574536800385, 222.89707732200623]
2024-12-27 22:28:30,666: Forward transfer: 0.045524999999999996 Backward transfer: -0.06005000000000001
2024-12-27 22:29:09,164: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227222835/HYBRIDHYBRID_0.0001_1024_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_1024_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_1024_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 22:29:18,896: Snapshot:0	Epoch:0	Loss:48.314	translation_Loss:48.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.3	Hits@10:2.38	Best:1.3
2024-12-27 22:29:24,878: Snapshot:0	Epoch:1	Loss:45.002	translation_Loss:45.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.79	Hits@10:7.92	Best:3.79
2024-12-27 22:29:30,916: Snapshot:0	Epoch:2	Loss:41.936	translation_Loss:41.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:12.67	Best:5.6
2024-12-27 22:29:36,926: Snapshot:0	Epoch:3	Loss:39.022	translation_Loss:39.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.85	Hits@10:16.11	Best:6.85
2024-12-27 22:29:42,926: Snapshot:0	Epoch:4	Loss:36.216	translation_Loss:36.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.83	Hits@10:18.97	Best:7.83
2024-12-27 22:29:48,989: Snapshot:0	Epoch:5	Loss:33.472	translation_Loss:33.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.68	Best:8.89
2024-12-27 22:29:55,487: Snapshot:0	Epoch:6	Loss:30.69	translation_Loss:30.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:25.15	Best:10.29
2024-12-27 22:30:01,585: Snapshot:0	Epoch:7	Loss:27.959	translation_Loss:27.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.01	Hits@10:28.73	Best:12.01
2024-12-27 22:30:07,688: Snapshot:0	Epoch:8	Loss:25.173	translation_Loss:25.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.67	Hits@10:31.87	Best:13.67
2024-12-27 22:30:13,723: Snapshot:0	Epoch:9	Loss:22.449	translation_Loss:22.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.24	Hits@10:34.27	Best:15.24
2024-12-27 22:30:19,699: Snapshot:0	Epoch:10	Loss:19.787	translation_Loss:19.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.96	Hits@10:36.32	Best:16.96
2024-12-27 22:30:25,802: Snapshot:0	Epoch:11	Loss:17.285	translation_Loss:17.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.5	Hits@10:38.15	Best:18.5
2024-12-27 22:30:31,859: Snapshot:0	Epoch:12	Loss:14.981	translation_Loss:14.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:39.63	Best:19.77
2024-12-27 22:30:37,964: Snapshot:0	Epoch:13	Loss:13.018	translation_Loss:13.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.88	Hits@10:40.77	Best:20.88
2024-12-27 22:30:44,063: Snapshot:0	Epoch:14	Loss:11.354	translation_Loss:11.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.8	Hits@10:41.74	Best:21.8
2024-12-27 22:30:50,128: Snapshot:0	Epoch:15	Loss:9.881	translation_Loss:9.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.57	Hits@10:42.53	Best:22.57
2024-12-27 22:30:56,177: Snapshot:0	Epoch:16	Loss:8.648	translation_Loss:8.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.11	Hits@10:43.02	Best:23.11
2024-12-27 22:31:02,665: Snapshot:0	Epoch:17	Loss:7.591	translation_Loss:7.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:43.71	Best:23.51
2024-12-27 22:31:08,680: Snapshot:0	Epoch:18	Loss:6.695	translation_Loss:6.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.85	Hits@10:44.06	Best:23.85
2024-12-27 22:31:14,675: Snapshot:0	Epoch:19	Loss:5.931	translation_Loss:5.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:44.38	Best:24.12
2024-12-27 22:31:20,758: Snapshot:0	Epoch:20	Loss:5.225	translation_Loss:5.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:44.65	Best:24.34
2024-12-27 22:31:26,828: Snapshot:0	Epoch:21	Loss:4.673	translation_Loss:4.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.52	Hits@10:44.9	Best:24.52
2024-12-27 22:31:32,917: Snapshot:0	Epoch:22	Loss:4.194	translation_Loss:4.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.64	Hits@10:45.1	Best:24.64
2024-12-27 22:31:39,028: Snapshot:0	Epoch:23	Loss:3.782	translation_Loss:3.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:45.36	Best:24.72
2024-12-27 22:31:45,140: Snapshot:0	Epoch:24	Loss:3.459	translation_Loss:3.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.51	Best:24.9
2024-12-27 22:31:51,145: Snapshot:0	Epoch:25	Loss:3.107	translation_Loss:3.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:45.59	Best:24.97
2024-12-27 22:31:57,262: Snapshot:0	Epoch:26	Loss:2.851	translation_Loss:2.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.05	Hits@10:45.67	Best:25.05
2024-12-27 22:32:03,366: Snapshot:0	Epoch:27	Loss:2.619	translation_Loss:2.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.17	Hits@10:45.72	Best:25.17
2024-12-27 22:32:09,453: Snapshot:0	Epoch:28	Loss:2.412	translation_Loss:2.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.2	Hits@10:45.9	Best:25.2
2024-12-27 22:32:15,943: Snapshot:0	Epoch:29	Loss:2.253	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.93	Best:25.33
2024-12-27 22:32:21,925: Snapshot:0	Epoch:30	Loss:2.102	translation_Loss:2.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.36	Hits@10:45.96	Best:25.36
2024-12-27 22:32:27,984: Snapshot:0	Epoch:31	Loss:1.939	translation_Loss:1.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.47	Hits@10:46.1	Best:25.47
2024-12-27 22:32:33,995: Snapshot:0	Epoch:32	Loss:1.818	translation_Loss:1.818	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:46.12	Best:25.47
2024-12-27 22:32:40,064: Snapshot:0	Epoch:33	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.26	Best:25.56
2024-12-27 22:32:46,143: Snapshot:0	Epoch:34	Loss:1.62	translation_Loss:1.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.48	Hits@10:46.07	Best:25.56
2024-12-27 22:32:52,113: Snapshot:0	Epoch:35	Loss:1.527	translation_Loss:1.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:46.17	Best:25.56
2024-12-27 22:32:58,097: Snapshot:0	Epoch:36	Loss:1.444	translation_Loss:1.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.23	Best:25.63
2024-12-27 22:33:04,097: Snapshot:0	Epoch:37	Loss:1.371	translation_Loss:1.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.23	Best:25.67
2024-12-27 22:33:10,091: Snapshot:0	Epoch:38	Loss:1.299	translation_Loss:1.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.3	Best:25.71
2024-12-27 22:33:16,087: Snapshot:0	Epoch:39	Loss:1.248	translation_Loss:1.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.3	Best:25.77
2024-12-27 22:33:22,133: Snapshot:0	Epoch:40	Loss:1.187	translation_Loss:1.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.9	Hits@10:46.3	Best:25.9
2024-12-27 22:33:28,119: Snapshot:0	Epoch:41	Loss:1.132	translation_Loss:1.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.23	Best:25.9
2024-12-27 22:33:34,546: Snapshot:0	Epoch:42	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.3	Best:25.9
2024-12-27 22:33:40,585: Snapshot:0	Epoch:43	Loss:1.041	translation_Loss:1.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.2	Best:25.9
2024-12-27 22:33:46,645: Snapshot:0	Epoch:44	Loss:1.0	translation_Loss:1.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.13	Best:25.9
2024-12-27 22:33:52,603: Early Stopping! Snapshot: 0 Epoch: 45 Best Results: 25.9
2024-12-27 22:33:52,604: Start to training tokens! Snapshot: 0 Epoch: 45 Loss:0.97 MRR:25.75 Best Results: 25.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:33:52,604: Snapshot:0	Epoch:45	Loss:0.97	translation_Loss:0.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.05	Best:25.9
2024-12-27 22:33:59,281: Snapshot:0	Epoch:46	Loss:35.991	translation_Loss:35.037	multi_layer_Loss:0.954	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.05	Best:25.9
2024-12-27 22:34:05,432: End of token training: 0 Epoch: 47 Loss:35.262 MRR:25.75 Best Results: 25.9
2024-12-27 22:34:05,432: Snapshot:0	Epoch:47	Loss:35.262	translation_Loss:35.001	multi_layer_Loss:0.262	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.75	Hits@10:46.05	Best:25.9
2024-12-27 22:34:05,661: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_5000/0model_best.tar'
2024-12-27 22:34:08,210: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1512 | 0.3161 | 0.3784 |  0.4519 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:34:19,244: Snapshot:1	Epoch:0	Loss:15.002	translation_Loss:14.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.032                                                   	MRR:6.32	Hits@10:10.47	Best:6.32
2024-12-27 22:34:21,639: Snapshot:1	Epoch:1	Loss:13.418	translation_Loss:13.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.087                                                   	MRR:7.1	Hits@10:11.9	Best:7.1
2024-12-27 22:34:24,056: Snapshot:1	Epoch:2	Loss:11.969	translation_Loss:11.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:8.15	Hits@10:14.22	Best:8.15
2024-12-27 22:34:26,510: Snapshot:1	Epoch:3	Loss:10.636	translation_Loss:10.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.236                                                   	MRR:9.83	Hits@10:18.17	Best:9.83
2024-12-27 22:34:29,000: Snapshot:1	Epoch:4	Loss:9.344	translation_Loss:9.016	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.327                                                   	MRR:11.84	Hits@10:22.93	Best:11.84
2024-12-27 22:34:31,392: Snapshot:1	Epoch:5	Loss:8.166	translation_Loss:7.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.421                                                   	MRR:13.99	Hits@10:27.16	Best:13.99
2024-12-27 22:34:33,785: Snapshot:1	Epoch:6	Loss:7.114	translation_Loss:6.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.508                                                   	MRR:15.85	Hits@10:29.92	Best:15.85
2024-12-27 22:34:36,194: Snapshot:1	Epoch:7	Loss:6.232	translation_Loss:5.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.585                                                   	MRR:17.33	Hits@10:32.16	Best:17.33
2024-12-27 22:34:38,574: Snapshot:1	Epoch:8	Loss:5.512	translation_Loss:4.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.65                                                   	MRR:18.64	Hits@10:33.77	Best:18.64
2024-12-27 22:34:41,060: Snapshot:1	Epoch:9	Loss:4.91	translation_Loss:4.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.703                                                   	MRR:19.62	Hits@10:35.59	Best:19.62
2024-12-27 22:34:43,544: Snapshot:1	Epoch:10	Loss:4.41	translation_Loss:3.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.747                                                   	MRR:20.36	Hits@10:36.77	Best:20.36
2024-12-27 22:34:45,977: Snapshot:1	Epoch:11	Loss:4.016	translation_Loss:3.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.782                                                   	MRR:21.14	Hits@10:37.95	Best:21.14
2024-12-27 22:34:48,348: Snapshot:1	Epoch:12	Loss:3.676	translation_Loss:2.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:21.94	Hits@10:38.84	Best:21.94
2024-12-27 22:34:50,732: Snapshot:1	Epoch:13	Loss:3.405	translation_Loss:2.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.828                                                   	MRR:22.63	Hits@10:39.88	Best:22.63
2024-12-27 22:34:53,101: Snapshot:1	Epoch:14	Loss:3.148	translation_Loss:2.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.841                                                   	MRR:23.19	Hits@10:40.85	Best:23.19
2024-12-27 22:34:55,484: Snapshot:1	Epoch:15	Loss:2.955	translation_Loss:2.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.85                                                   	MRR:23.62	Hits@10:41.58	Best:23.62
2024-12-27 22:34:57,895: Snapshot:1	Epoch:16	Loss:2.773	translation_Loss:1.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.855                                                   	MRR:24.12	Hits@10:42.18	Best:24.12
2024-12-27 22:35:00,260: Snapshot:1	Epoch:17	Loss:2.617	translation_Loss:1.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.857                                                   	MRR:24.47	Hits@10:42.8	Best:24.47
2024-12-27 22:35:02,662: Snapshot:1	Epoch:18	Loss:2.471	translation_Loss:1.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.857                                                   	MRR:24.83	Hits@10:43.4	Best:24.83
2024-12-27 22:35:05,099: Snapshot:1	Epoch:19	Loss:2.348	translation_Loss:1.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.855                                                   	MRR:25.04	Hits@10:43.99	Best:25.04
2024-12-27 22:35:07,485: Snapshot:1	Epoch:20	Loss:2.254	translation_Loss:1.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.851                                                   	MRR:25.28	Hits@10:44.41	Best:25.28
2024-12-27 22:35:10,263: Snapshot:1	Epoch:21	Loss:2.153	translation_Loss:1.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:25.59	Hits@10:44.87	Best:25.59
2024-12-27 22:35:12,696: Snapshot:1	Epoch:22	Loss:2.072	translation_Loss:1.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.841                                                   	MRR:25.86	Hits@10:45.27	Best:25.86
2024-12-27 22:35:15,039: Snapshot:1	Epoch:23	Loss:2.003	translation_Loss:1.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.835                                                   	MRR:26.06	Hits@10:45.6	Best:26.06
2024-12-27 22:35:17,427: Snapshot:1	Epoch:24	Loss:1.929	translation_Loss:1.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.828                                                   	MRR:26.27	Hits@10:45.94	Best:26.27
2024-12-27 22:35:19,803: Snapshot:1	Epoch:25	Loss:1.849	translation_Loss:1.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.821                                                   	MRR:26.41	Hits@10:46.42	Best:26.41
2024-12-27 22:35:22,131: Snapshot:1	Epoch:26	Loss:1.804	translation_Loss:0.991	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.813                                                   	MRR:26.36	Hits@10:46.83	Best:26.41
2024-12-27 22:35:24,455: Snapshot:1	Epoch:27	Loss:1.763	translation_Loss:0.957	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.806                                                   	MRR:26.6	Hits@10:46.99	Best:26.6
2024-12-27 22:35:26,850: Snapshot:1	Epoch:28	Loss:1.698	translation_Loss:0.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.799                                                   	MRR:26.67	Hits@10:47.08	Best:26.67
2024-12-27 22:35:29,218: Snapshot:1	Epoch:29	Loss:1.662	translation_Loss:0.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.791                                                   	MRR:26.79	Hits@10:47.22	Best:26.79
2024-12-27 22:35:31,606: Snapshot:1	Epoch:30	Loss:1.629	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.783                                                   	MRR:27.03	Hits@10:47.62	Best:27.03
2024-12-27 22:35:33,975: Snapshot:1	Epoch:31	Loss:1.595	translation_Loss:0.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.776                                                   	MRR:27.19	Hits@10:47.87	Best:27.19
2024-12-27 22:35:36,313: Snapshot:1	Epoch:32	Loss:1.563	translation_Loss:0.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.769                                                   	MRR:27.22	Hits@10:48.16	Best:27.22
2024-12-27 22:35:38,692: Snapshot:1	Epoch:33	Loss:1.522	translation_Loss:0.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.761                                                   	MRR:27.35	Hits@10:48.28	Best:27.35
2024-12-27 22:35:41,053: Snapshot:1	Epoch:34	Loss:1.507	translation_Loss:0.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.754                                                   	MRR:27.49	Hits@10:48.25	Best:27.49
2024-12-27 22:35:43,436: Snapshot:1	Epoch:35	Loss:1.475	translation_Loss:0.727	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.748                                                   	MRR:27.58	Hits@10:48.62	Best:27.58
2024-12-27 22:35:45,865: Snapshot:1	Epoch:36	Loss:1.444	translation_Loss:0.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.742                                                   	MRR:27.86	Hits@10:48.93	Best:27.86
2024-12-27 22:35:48,242: Snapshot:1	Epoch:37	Loss:1.427	translation_Loss:0.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.736                                                   	MRR:27.88	Hits@10:48.91	Best:27.88
2024-12-27 22:35:50,640: Snapshot:1	Epoch:38	Loss:1.405	translation_Loss:0.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.73                                                   	MRR:28.04	Hits@10:49.02	Best:28.04
2024-12-27 22:35:53,008: Snapshot:1	Epoch:39	Loss:1.375	translation_Loss:0.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.724                                                   	MRR:27.97	Hits@10:49.14	Best:28.04
2024-12-27 22:35:55,331: Snapshot:1	Epoch:40	Loss:1.365	translation_Loss:0.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.717                                                   	MRR:27.97	Hits@10:49.16	Best:28.04
2024-12-27 22:35:57,638: Snapshot:1	Epoch:41	Loss:1.328	translation_Loss:0.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.712                                                   	MRR:28.04	Hits@10:49.16	Best:28.04
2024-12-27 22:35:59,987: Snapshot:1	Epoch:42	Loss:1.318	translation_Loss:0.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.705                                                   	MRR:28.15	Hits@10:49.41	Best:28.15
2024-12-27 22:36:02,369: Snapshot:1	Epoch:43	Loss:1.305	translation_Loss:0.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.699                                                   	MRR:28.37	Hits@10:49.72	Best:28.37
2024-12-27 22:36:04,765: Snapshot:1	Epoch:44	Loss:1.306	translation_Loss:0.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.695                                                   	MRR:28.41	Hits@10:49.68	Best:28.41
2024-12-27 22:36:07,138: Snapshot:1	Epoch:45	Loss:1.272	translation_Loss:0.582	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.69                                                   	MRR:28.43	Hits@10:49.68	Best:28.43
2024-12-27 22:36:09,513: Snapshot:1	Epoch:46	Loss:1.255	translation_Loss:0.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.685                                                   	MRR:28.53	Hits@10:49.83	Best:28.53
2024-12-27 22:36:11,863: Snapshot:1	Epoch:47	Loss:1.248	translation_Loss:0.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.681                                                   	MRR:28.52	Hits@10:49.96	Best:28.53
2024-12-27 22:36:14,240: Snapshot:1	Epoch:48	Loss:1.24	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.675                                                   	MRR:28.58	Hits@10:50.18	Best:28.58
2024-12-27 22:36:16,645: Snapshot:1	Epoch:49	Loss:1.224	translation_Loss:0.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.67                                                   	MRR:28.59	Hits@10:50.42	Best:28.59
2024-12-27 22:36:19,019: Snapshot:1	Epoch:50	Loss:1.211	translation_Loss:0.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.667                                                   	MRR:28.61	Hits@10:50.2	Best:28.61
2024-12-27 22:36:21,436: Snapshot:1	Epoch:51	Loss:1.194	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.662                                                   	MRR:28.74	Hits@10:50.22	Best:28.74
2024-12-27 22:36:23,751: Snapshot:1	Epoch:52	Loss:1.181	translation_Loss:0.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.657                                                   	MRR:28.71	Hits@10:50.26	Best:28.74
2024-12-27 22:36:26,092: Snapshot:1	Epoch:53	Loss:1.166	translation_Loss:0.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.652                                                   	MRR:28.75	Hits@10:50.37	Best:28.75
2024-12-27 22:36:28,892: Snapshot:1	Epoch:54	Loss:1.157	translation_Loss:0.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.648                                                   	MRR:28.95	Hits@10:50.54	Best:28.95
2024-12-27 22:36:31,293: Snapshot:1	Epoch:55	Loss:1.142	translation_Loss:0.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.643                                                   	MRR:28.93	Hits@10:50.34	Best:28.95
2024-12-27 22:36:33,640: Snapshot:1	Epoch:56	Loss:1.144	translation_Loss:0.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.639                                                   	MRR:28.94	Hits@10:50.36	Best:28.95
2024-12-27 22:36:35,979: Snapshot:1	Epoch:57	Loss:1.125	translation_Loss:0.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.635                                                   	MRR:28.93	Hits@10:50.25	Best:28.95
2024-12-27 22:36:38,305: Snapshot:1	Epoch:58	Loss:1.119	translation_Loss:0.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.63                                                   	MRR:29.09	Hits@10:50.5	Best:29.09
2024-12-27 22:36:40,748: Snapshot:1	Epoch:59	Loss:1.114	translation_Loss:0.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.627                                                   	MRR:29.07	Hits@10:50.5	Best:29.09
2024-12-27 22:36:43,147: Snapshot:1	Epoch:60	Loss:1.105	translation_Loss:0.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.625                                                   	MRR:29.16	Hits@10:50.5	Best:29.16
2024-12-27 22:36:45,534: Snapshot:1	Epoch:61	Loss:1.098	translation_Loss:0.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.621                                                   	MRR:29.29	Hits@10:50.71	Best:29.29
2024-12-27 22:36:47,931: Snapshot:1	Epoch:62	Loss:1.097	translation_Loss:0.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.618                                                   	MRR:29.34	Hits@10:50.8	Best:29.34
2024-12-27 22:36:50,277: Snapshot:1	Epoch:63	Loss:1.084	translation_Loss:0.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.614                                                   	MRR:29.21	Hits@10:50.83	Best:29.34
2024-12-27 22:36:52,626: Snapshot:1	Epoch:64	Loss:1.081	translation_Loss:0.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.61                                                   	MRR:29.35	Hits@10:50.85	Best:29.35
2024-12-27 22:36:54,996: Snapshot:1	Epoch:65	Loss:1.078	translation_Loss:0.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:29.5	Hits@10:50.97	Best:29.5
2024-12-27 22:36:57,398: Snapshot:1	Epoch:66	Loss:1.065	translation_Loss:0.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.605                                                   	MRR:29.55	Hits@10:50.89	Best:29.55
2024-12-27 22:36:59,811: Snapshot:1	Epoch:67	Loss:1.061	translation_Loss:0.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.602                                                   	MRR:29.67	Hits@10:51.08	Best:29.67
2024-12-27 22:37:02,158: Snapshot:1	Epoch:68	Loss:1.046	translation_Loss:0.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.599                                                   	MRR:29.63	Hits@10:51.22	Best:29.67
2024-12-27 22:37:04,468: Snapshot:1	Epoch:69	Loss:1.041	translation_Loss:0.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.595                                                   	MRR:29.5	Hits@10:51.25	Best:29.67
2024-12-27 22:37:06,778: Snapshot:1	Epoch:70	Loss:1.042	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.592                                                   	MRR:29.57	Hits@10:51.38	Best:29.67
2024-12-27 22:37:09,081: Snapshot:1	Epoch:71	Loss:1.034	translation_Loss:0.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.591                                                   	MRR:29.62	Hits@10:51.34	Best:29.67
2024-12-27 22:37:11,431: Early Stopping! Snapshot: 1 Epoch: 72 Best Results: 29.67
2024-12-27 22:37:11,431: Start to training tokens! Snapshot: 1 Epoch: 72 Loss:1.034 MRR:29.67 Best Results: 29.67
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:37:11,432: Snapshot:1	Epoch:72	Loss:1.034	translation_Loss:0.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.589                                                   	MRR:29.67	Hits@10:51.37	Best:29.67
2024-12-27 22:37:13,733: Snapshot:1	Epoch:73	Loss:12.497	translation_Loss:12.049	multi_layer_Loss:0.448	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.67	Hits@10:51.37	Best:29.67
2024-12-27 22:37:16,044: End of token training: 1 Epoch: 74 Loss:12.37 MRR:29.67 Best Results: 29.67
2024-12-27 22:37:16,044: Snapshot:1	Epoch:74	Loss:12.37	translation_Loss:12.042	multi_layer_Loss:0.329	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.67	Hits@10:51.37	Best:29.67
2024-12-27 22:37:16,295: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_5000/1model_best.tar'
2024-12-27 22:37:20,634: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2529 | 0.1448 | 0.3122 | 0.3771 |  0.4481 |
|     1      | 0.2928 | 0.1817 | 0.3463 | 0.414  |  0.5101 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:37:54,724: Snapshot:2	Epoch:0	Loss:60.948	translation_Loss:60.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:5.67	Hits@10:12.19	Best:5.67
2024-12-27 22:38:05,016: Snapshot:2	Epoch:1	Loss:47.279	translation_Loss:46.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.033                                                   	MRR:9.35	Hits@10:20.14	Best:9.35
2024-12-27 22:38:15,258: Snapshot:2	Epoch:2	Loss:36.374	translation_Loss:34.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.794                                                   	MRR:12.49	Hits@10:25.51	Best:12.49
2024-12-27 22:38:25,488: Snapshot:2	Epoch:3	Loss:28.627	translation_Loss:26.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.364                                                   	MRR:14.73	Hits@10:29.51	Best:14.73
2024-12-27 22:38:35,700: Snapshot:2	Epoch:4	Loss:23.326	translation_Loss:20.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.721                                                   	MRR:16.34	Hits@10:32.08	Best:16.34
2024-12-27 22:38:45,988: Snapshot:2	Epoch:5	Loss:19.639	translation_Loss:16.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.922                                                   	MRR:17.43	Hits@10:33.86	Best:17.43
2024-12-27 22:38:56,246: Snapshot:2	Epoch:6	Loss:17.03	translation_Loss:14.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.01                                                   	MRR:18.31	Hits@10:35.01	Best:18.31
2024-12-27 22:39:06,512: Snapshot:2	Epoch:7	Loss:15.12	translation_Loss:12.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.028                                                   	MRR:18.9	Hits@10:35.94	Best:18.9
2024-12-27 22:39:16,963: Snapshot:2	Epoch:8	Loss:13.717	translation_Loss:10.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.016                                                   	MRR:19.36	Hits@10:36.57	Best:19.36
2024-12-27 22:39:27,396: Snapshot:2	Epoch:9	Loss:12.661	translation_Loss:9.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.986                                                   	MRR:19.75	Hits@10:37.15	Best:19.75
2024-12-27 22:39:37,617: Snapshot:2	Epoch:10	Loss:11.856	translation_Loss:8.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.943                                                   	MRR:20.12	Hits@10:37.54	Best:20.12
2024-12-27 22:39:48,025: Snapshot:2	Epoch:11	Loss:11.17	translation_Loss:8.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.896                                                   	MRR:20.31	Hits@10:37.66	Best:20.31
2024-12-27 22:39:58,316: Snapshot:2	Epoch:12	Loss:10.68	translation_Loss:7.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.851                                                   	MRR:20.52	Hits@10:37.89	Best:20.52
2024-12-27 22:40:08,700: Snapshot:2	Epoch:13	Loss:10.288	translation_Loss:7.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.809                                                   	MRR:20.73	Hits@10:37.95	Best:20.73
2024-12-27 22:40:19,074: Snapshot:2	Epoch:14	Loss:9.946	translation_Loss:7.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.77                                                   	MRR:20.91	Hits@10:38.08	Best:20.91
2024-12-27 22:40:29,559: Snapshot:2	Epoch:15	Loss:9.686	translation_Loss:6.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.738                                                   	MRR:20.94	Hits@10:38.13	Best:20.94
2024-12-27 22:40:39,949: Snapshot:2	Epoch:16	Loss:9.448	translation_Loss:6.747	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.701                                                   	MRR:21.14	Hits@10:38.33	Best:21.14
2024-12-27 22:40:50,243: Snapshot:2	Epoch:17	Loss:9.239	translation_Loss:6.568	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.671                                                   	MRR:21.13	Hits@10:38.28	Best:21.14
2024-12-27 22:41:00,578: Snapshot:2	Epoch:18	Loss:9.096	translation_Loss:6.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.646                                                   	MRR:21.27	Hits@10:38.32	Best:21.27
2024-12-27 22:41:10,872: Snapshot:2	Epoch:19	Loss:8.926	translation_Loss:6.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.622                                                   	MRR:21.3	Hits@10:38.31	Best:21.3
2024-12-27 22:41:21,142: Snapshot:2	Epoch:20	Loss:8.812	translation_Loss:6.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.602                                                   	MRR:21.24	Hits@10:38.49	Best:21.3
2024-12-27 22:41:31,840: Snapshot:2	Epoch:21	Loss:8.701	translation_Loss:6.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.586                                                   	MRR:21.31	Hits@10:38.43	Best:21.31
2024-12-27 22:41:42,109: Snapshot:2	Epoch:22	Loss:8.635	translation_Loss:6.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.567                                                   	MRR:21.4	Hits@10:38.39	Best:21.4
2024-12-27 22:41:52,487: Snapshot:2	Epoch:23	Loss:8.547	translation_Loss:5.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.554                                                   	MRR:21.46	Hits@10:38.43	Best:21.46
2024-12-27 22:42:02,748: Snapshot:2	Epoch:24	Loss:8.466	translation_Loss:5.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.547                                                   	MRR:21.43	Hits@10:38.47	Best:21.46
2024-12-27 22:42:13,066: Snapshot:2	Epoch:25	Loss:8.379	translation_Loss:5.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.531                                                   	MRR:21.52	Hits@10:38.41	Best:21.52
2024-12-27 22:42:23,319: Snapshot:2	Epoch:26	Loss:8.322	translation_Loss:5.804	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.518                                                   	MRR:21.43	Hits@10:38.51	Best:21.52
2024-12-27 22:42:33,618: Snapshot:2	Epoch:27	Loss:8.309	translation_Loss:5.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.511                                                   	MRR:21.5	Hits@10:38.43	Best:21.52
2024-12-27 22:42:43,938: Snapshot:2	Epoch:28	Loss:8.253	translation_Loss:5.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.501                                                   	MRR:21.45	Hits@10:38.45	Best:21.52
2024-12-27 22:42:54,151: Snapshot:2	Epoch:29	Loss:8.195	translation_Loss:5.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.492                                                   	MRR:21.43	Hits@10:38.51	Best:21.52
2024-12-27 22:43:04,440: Early Stopping! Snapshot: 2 Epoch: 30 Best Results: 21.52
2024-12-27 22:43:04,440: Start to training tokens! Snapshot: 2 Epoch: 30 Loss:8.139 MRR:21.38 Best Results: 21.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:43:04,440: Snapshot:2	Epoch:30	Loss:8.139	translation_Loss:5.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.486                                                   	MRR:21.38	Hits@10:38.5	Best:21.52
2024-12-27 22:43:14,466: Snapshot:2	Epoch:31	Loss:54.673	translation_Loss:53.584	multi_layer_Loss:1.088	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.38	Hits@10:38.5	Best:21.52
2024-12-27 22:43:24,528: End of token training: 2 Epoch: 32 Loss:53.651 MRR:21.38 Best Results: 21.52
2024-12-27 22:43:24,528: Snapshot:2	Epoch:32	Loss:53.651	translation_Loss:53.557	multi_layer_Loss:0.094	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.38	Hits@10:38.5	Best:21.52
2024-12-27 22:43:24,814: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_5000/2model_best.tar'
2024-12-27 22:43:33,470: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2464 | 0.1344 | 0.3052 | 0.3709 |  0.4522 |
|     1      | 0.2615 | 0.1493 | 0.3163 | 0.3849 |  0.4711 |
|     2      | 0.2158 | 0.1267 | 0.2497 | 0.3098 |  0.3885 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:44:13,454: Snapshot:3	Epoch:0	Loss:62.176	translation_Loss:61.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.52                                                   	MRR:6.62	Hits@10:16.2	Best:6.62
2024-12-27 22:44:25,980: Snapshot:3	Epoch:1	Loss:42.522	translation_Loss:40.885	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.637                                                   	MRR:11.04	Hits@10:24.16	Best:11.04
2024-12-27 22:44:38,538: Snapshot:3	Epoch:2	Loss:30.407	translation_Loss:27.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.755                                                   	MRR:13.97	Hits@10:29.13	Best:13.97
2024-12-27 22:44:51,274: Snapshot:3	Epoch:3	Loss:23.788	translation_Loss:20.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.472                                                   	MRR:15.89	Hits@10:32.62	Best:15.89
2024-12-27 22:45:03,867: Snapshot:3	Epoch:4	Loss:19.795	translation_Loss:15.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.849                                                   	MRR:17.37	Hits@10:34.8	Best:17.37
2024-12-27 22:45:16,461: Snapshot:3	Epoch:5	Loss:17.105	translation_Loss:13.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.012                                                   	MRR:18.47	Hits@10:36.19	Best:18.47
2024-12-27 22:45:29,254: Snapshot:3	Epoch:6	Loss:15.218	translation_Loss:11.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.062                                                   	MRR:19.15	Hits@10:37.17	Best:19.15
2024-12-27 22:45:41,824: Snapshot:3	Epoch:7	Loss:13.942	translation_Loss:9.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.035                                                   	MRR:19.68	Hits@10:37.77	Best:19.68
2024-12-27 22:45:54,375: Snapshot:3	Epoch:8	Loss:12.958	translation_Loss:8.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.977                                                   	MRR:19.99	Hits@10:38.16	Best:19.99
2024-12-27 22:46:06,993: Snapshot:3	Epoch:9	Loss:12.217	translation_Loss:8.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.894                                                   	MRR:20.33	Hits@10:38.5	Best:20.33
2024-12-27 22:46:19,554: Snapshot:3	Epoch:10	Loss:11.652	translation_Loss:7.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.815                                                   	MRR:20.57	Hits@10:38.7	Best:20.57
2024-12-27 22:46:32,418: Snapshot:3	Epoch:11	Loss:11.17	translation_Loss:7.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.738                                                   	MRR:20.68	Hits@10:38.89	Best:20.68
2024-12-27 22:46:44,925: Snapshot:3	Epoch:12	Loss:10.795	translation_Loss:7.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.674                                                   	MRR:20.92	Hits@10:39.03	Best:20.92
2024-12-27 22:46:57,465: Snapshot:3	Epoch:13	Loss:10.502	translation_Loss:6.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.605                                                   	MRR:21.05	Hits@10:39.16	Best:21.05
2024-12-27 22:47:10,007: Snapshot:3	Epoch:14	Loss:10.244	translation_Loss:6.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.551                                                   	MRR:21.11	Hits@10:39.35	Best:21.11
2024-12-27 22:47:22,721: Snapshot:3	Epoch:15	Loss:10.009	translation_Loss:6.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.491                                                   	MRR:21.15	Hits@10:39.4	Best:21.15
2024-12-27 22:47:35,232: Snapshot:3	Epoch:16	Loss:9.824	translation_Loss:6.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.438                                                   	MRR:21.15	Hits@10:39.37	Best:21.15
2024-12-27 22:47:47,781: Snapshot:3	Epoch:17	Loss:9.698	translation_Loss:6.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.405                                                   	MRR:21.19	Hits@10:39.32	Best:21.19
2024-12-27 22:48:00,298: Snapshot:3	Epoch:18	Loss:9.537	translation_Loss:6.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.368                                                   	MRR:21.27	Hits@10:39.36	Best:21.27
2024-12-27 22:48:12,942: Snapshot:3	Epoch:19	Loss:9.448	translation_Loss:6.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.335                                                   	MRR:21.28	Hits@10:39.33	Best:21.28
2024-12-27 22:48:25,504: Snapshot:3	Epoch:20	Loss:9.376	translation_Loss:6.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.304                                                   	MRR:21.33	Hits@10:39.38	Best:21.33
2024-12-27 22:48:38,110: Snapshot:3	Epoch:21	Loss:9.247	translation_Loss:5.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.278                                                   	MRR:21.31	Hits@10:39.37	Best:21.33
2024-12-27 22:48:50,564: Snapshot:3	Epoch:22	Loss:9.177	translation_Loss:5.921	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.255                                                   	MRR:21.3	Hits@10:39.37	Best:21.33
2024-12-27 22:49:03,067: Snapshot:3	Epoch:23	Loss:9.133	translation_Loss:5.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.24                                                   	MRR:21.28	Hits@10:39.32	Best:21.33
2024-12-27 22:49:15,506: Snapshot:3	Epoch:24	Loss:9.041	translation_Loss:5.821	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.22                                                   	MRR:21.22	Hits@10:39.27	Best:21.33
2024-12-27 22:49:28,097: Snapshot:3	Epoch:25	Loss:8.991	translation_Loss:5.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.203                                                   	MRR:21.36	Hits@10:39.29	Best:21.36
2024-12-27 22:49:40,703: Snapshot:3	Epoch:26	Loss:8.953	translation_Loss:5.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.188                                                   	MRR:21.34	Hits@10:39.24	Best:21.36
2024-12-27 22:49:53,224: Snapshot:3	Epoch:27	Loss:8.905	translation_Loss:5.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.171                                                   	MRR:21.41	Hits@10:39.52	Best:21.41
2024-12-27 22:50:05,782: Snapshot:3	Epoch:28	Loss:8.864	translation_Loss:5.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.16                                                   	MRR:21.36	Hits@10:39.36	Best:21.41
2024-12-27 22:50:18,358: Snapshot:3	Epoch:29	Loss:8.825	translation_Loss:5.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.151                                                   	MRR:21.34	Hits@10:39.29	Best:21.41
2024-12-27 22:50:30,822: Snapshot:3	Epoch:30	Loss:8.801	translation_Loss:5.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.143                                                   	MRR:21.34	Hits@10:39.44	Best:21.41
2024-12-27 22:50:43,528: Snapshot:3	Epoch:31	Loss:8.777	translation_Loss:5.643	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.134                                                   	MRR:21.46	Hits@10:39.57	Best:21.46
2024-12-27 22:50:56,477: Snapshot:3	Epoch:32	Loss:8.755	translation_Loss:5.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.127                                                   	MRR:21.44	Hits@10:39.49	Best:21.46
2024-12-27 22:51:09,044: Snapshot:3	Epoch:33	Loss:8.712	translation_Loss:5.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.117                                                   	MRR:21.51	Hits@10:39.42	Best:21.51
2024-12-27 22:51:21,772: Snapshot:3	Epoch:34	Loss:8.707	translation_Loss:5.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.117                                                   	MRR:21.42	Hits@10:39.48	Best:21.51
2024-12-27 22:51:34,311: Snapshot:3	Epoch:35	Loss:8.682	translation_Loss:5.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.107                                                   	MRR:21.43	Hits@10:39.5	Best:21.51
2024-12-27 22:51:46,871: Snapshot:3	Epoch:36	Loss:8.67	translation_Loss:5.568	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.101                                                   	MRR:21.41	Hits@10:39.48	Best:21.51
2024-12-27 22:51:59,755: Snapshot:3	Epoch:37	Loss:8.65	translation_Loss:5.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.097                                                   	MRR:21.3	Hits@10:39.39	Best:21.51
2024-12-27 22:52:12,203: Early Stopping! Snapshot: 3 Epoch: 38 Best Results: 21.51
2024-12-27 22:52:12,203: Start to training tokens! Snapshot: 3 Epoch: 38 Loss:8.602 MRR:21.32 Best Results: 21.51
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:52:12,203: Snapshot:3	Epoch:38	Loss:8.602	translation_Loss:5.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.096                                                   	MRR:21.32	Hits@10:39.32	Best:21.51
2024-12-27 22:52:24,478: Snapshot:3	Epoch:39	Loss:58.949	translation_Loss:57.758	multi_layer_Loss:1.191	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.32	Hits@10:39.32	Best:21.51
2024-12-27 22:52:36,749: End of token training: 3 Epoch: 40 Loss:57.826 MRR:21.32 Best Results: 21.51
2024-12-27 22:52:36,750: Snapshot:3	Epoch:40	Loss:57.826	translation_Loss:57.766	multi_layer_Loss:0.06	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.32	Hits@10:39.32	Best:21.51
2024-12-27 22:52:36,983: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_5000/3model_best.tar'
2024-12-27 22:52:50,708: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2556 | 0.1492 | 0.308  | 0.3741 |  0.4534 |
|     1      | 0.2664 | 0.1607 | 0.3122 | 0.3747 |  0.4668 |
|     2      | 0.2098 | 0.1197 | 0.2424 | 0.3054 |  0.3847 |
|     3      | 0.2155 | 0.1212 | 0.2511 | 0.3141 |  0.3949 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:53:09,460: Snapshot:4	Epoch:0	Loss:20.893	translation_Loss:20.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.085                                                   	MRR:7.63	Hits@10:15.32	Best:7.63
2024-12-27 22:53:14,672: Snapshot:4	Epoch:1	Loss:16.895	translation_Loss:16.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:8.65	Hits@10:18.12	Best:8.65
2024-12-27 22:53:19,927: Snapshot:4	Epoch:2	Loss:13.531	translation_Loss:13.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.464                                                   	MRR:12.78	Hits@10:28.6	Best:12.78
2024-12-27 22:53:25,082: Snapshot:4	Epoch:3	Loss:11.017	translation_Loss:10.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.703                                                   	MRR:15.45	Hits@10:31.6	Best:15.45
2024-12-27 22:53:30,354: Snapshot:4	Epoch:4	Loss:9.404	translation_Loss:8.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.929                                                   	MRR:17.17	Hits@10:33.27	Best:17.17
2024-12-27 22:53:35,691: Snapshot:4	Epoch:5	Loss:8.271	translation_Loss:7.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.139                                                   	MRR:18.58	Hits@10:34.42	Best:18.58
2024-12-27 22:53:40,868: Snapshot:4	Epoch:6	Loss:7.43	translation_Loss:6.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.325                                                   	MRR:18.98	Hits@10:34.92	Best:18.98
2024-12-27 22:53:46,039: Snapshot:4	Epoch:7	Loss:6.78	translation_Loss:5.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.486                                                   	MRR:19.39	Hits@10:35.64	Best:19.39
2024-12-27 22:53:51,210: Snapshot:4	Epoch:8	Loss:6.251	translation_Loss:4.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.622                                                   	MRR:19.66	Hits@10:35.9	Best:19.66
2024-12-27 22:53:56,424: Snapshot:4	Epoch:9	Loss:5.792	translation_Loss:4.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.737                                                   	MRR:20.1	Hits@10:35.99	Best:20.1
2024-12-27 22:54:01,731: Snapshot:4	Epoch:10	Loss:5.421	translation_Loss:3.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.828                                                   	MRR:20.31	Hits@10:36.26	Best:20.31
2024-12-27 22:54:07,003: Snapshot:4	Epoch:11	Loss:5.119	translation_Loss:3.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.9                                                   	MRR:20.66	Hits@10:36.78	Best:20.66
2024-12-27 22:54:12,226: Snapshot:4	Epoch:12	Loss:4.855	translation_Loss:2.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.955                                                   	MRR:21.24	Hits@10:36.82	Best:21.24
2024-12-27 22:54:17,507: Snapshot:4	Epoch:13	Loss:4.615	translation_Loss:2.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.994                                                   	MRR:21.26	Hits@10:37.12	Best:21.26
2024-12-27 22:54:22,627: Snapshot:4	Epoch:14	Loss:4.431	translation_Loss:2.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.022                                                   	MRR:21.47	Hits@10:37.03	Best:21.47
2024-12-27 22:54:27,860: Snapshot:4	Epoch:15	Loss:4.254	translation_Loss:2.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.039                                                   	MRR:21.68	Hits@10:37.03	Best:21.68
2024-12-27 22:54:32,935: Snapshot:4	Epoch:16	Loss:4.087	translation_Loss:2.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.047                                                   	MRR:21.61	Hits@10:36.84	Best:21.68
2024-12-27 22:54:38,100: Snapshot:4	Epoch:17	Loss:3.964	translation_Loss:1.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.046                                                   	MRR:21.61	Hits@10:37.01	Best:21.68
2024-12-27 22:54:43,235: Snapshot:4	Epoch:18	Loss:3.843	translation_Loss:1.804	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.04                                                   	MRR:21.63	Hits@10:37.14	Best:21.68
2024-12-27 22:54:48,398: Snapshot:4	Epoch:19	Loss:3.743	translation_Loss:1.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.03                                                   	MRR:21.76	Hits@10:37.11	Best:21.76
2024-12-27 22:54:53,520: Snapshot:4	Epoch:20	Loss:3.646	translation_Loss:1.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.02                                                   	MRR:21.88	Hits@10:37.42	Best:21.88
2024-12-27 22:54:58,598: Snapshot:4	Epoch:21	Loss:3.564	translation_Loss:1.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.008                                                   	MRR:21.52	Hits@10:37.5	Best:21.88
2024-12-27 22:55:03,774: Snapshot:4	Epoch:22	Loss:3.5	translation_Loss:1.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.993                                                   	MRR:22.14	Hits@10:37.64	Best:22.14
2024-12-27 22:55:09,013: Snapshot:4	Epoch:23	Loss:3.408	translation_Loss:1.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.973                                                   	MRR:22.51	Hits@10:37.89	Best:22.51
2024-12-27 22:55:14,173: Snapshot:4	Epoch:24	Loss:3.361	translation_Loss:1.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.956                                                   	MRR:22.27	Hits@10:37.99	Best:22.51
2024-12-27 22:55:19,749: Snapshot:4	Epoch:25	Loss:3.313	translation_Loss:1.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.941                                                   	MRR:22.16	Hits@10:38.13	Best:22.51
2024-12-27 22:55:24,823: Snapshot:4	Epoch:26	Loss:3.242	translation_Loss:1.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.924                                                   	MRR:22.29	Hits@10:38.11	Best:22.51
2024-12-27 22:55:29,938: Snapshot:4	Epoch:27	Loss:3.219	translation_Loss:1.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.906                                                   	MRR:22.61	Hits@10:38.2	Best:22.61
2024-12-27 22:55:35,106: Snapshot:4	Epoch:28	Loss:3.189	translation_Loss:1.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.89                                                   	MRR:22.74	Hits@10:37.95	Best:22.74
2024-12-27 22:55:40,219: Snapshot:4	Epoch:29	Loss:3.119	translation_Loss:1.245	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.874                                                   	MRR:22.5	Hits@10:37.78	Best:22.74
2024-12-27 22:55:45,344: Snapshot:4	Epoch:30	Loss:3.1	translation_Loss:1.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.858                                                   	MRR:22.49	Hits@10:37.71	Best:22.74
2024-12-27 22:55:50,488: Snapshot:4	Epoch:31	Loss:3.083	translation_Loss:1.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.843                                                   	MRR:22.83	Hits@10:37.92	Best:22.83
2024-12-27 22:55:55,650: Snapshot:4	Epoch:32	Loss:3.035	translation_Loss:1.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.831                                                   	MRR:22.96	Hits@10:38.02	Best:22.96
2024-12-27 22:56:00,733: Snapshot:4	Epoch:33	Loss:3.012	translation_Loss:1.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.816                                                   	MRR:22.76	Hits@10:37.9	Best:22.96
2024-12-27 22:56:05,843: Snapshot:4	Epoch:34	Loss:2.992	translation_Loss:1.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.801                                                   	MRR:22.77	Hits@10:37.91	Best:22.96
2024-12-27 22:56:10,909: Snapshot:4	Epoch:35	Loss:2.957	translation_Loss:1.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.788                                                   	MRR:22.85	Hits@10:37.89	Best:22.96
2024-12-27 22:56:16,067: Snapshot:4	Epoch:36	Loss:2.942	translation_Loss:1.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.777                                                   	MRR:23.01	Hits@10:38.18	Best:23.01
2024-12-27 22:56:21,190: Snapshot:4	Epoch:37	Loss:2.933	translation_Loss:1.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.765                                                   	MRR:22.93	Hits@10:38.23	Best:23.01
2024-12-27 22:56:26,400: Snapshot:4	Epoch:38	Loss:2.911	translation_Loss:1.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:23.04	Hits@10:38.22	Best:23.04
2024-12-27 22:56:31,613: Snapshot:4	Epoch:39	Loss:2.904	translation_Loss:1.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.748                                                   	MRR:23.16	Hits@10:38.1	Best:23.16
2024-12-27 22:56:36,819: Snapshot:4	Epoch:40	Loss:2.897	translation_Loss:1.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.739                                                   	MRR:22.99	Hits@10:38.49	Best:23.16
2024-12-27 22:56:41,966: Snapshot:4	Epoch:41	Loss:2.864	translation_Loss:1.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.73                                                   	MRR:23.28	Hits@10:38.67	Best:23.28
2024-12-27 22:56:47,071: Snapshot:4	Epoch:42	Loss:2.855	translation_Loss:1.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.723                                                   	MRR:23.09	Hits@10:37.95	Best:23.28
2024-12-27 22:56:52,136: Snapshot:4	Epoch:43	Loss:2.848	translation_Loss:1.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.715                                                   	MRR:22.96	Hits@10:37.96	Best:23.28
2024-12-27 22:56:57,197: Snapshot:4	Epoch:44	Loss:2.834	translation_Loss:1.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:23.07	Hits@10:38.01	Best:23.28
2024-12-27 22:57:02,285: Snapshot:4	Epoch:45	Loss:2.812	translation_Loss:1.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.702                                                   	MRR:23.07	Hits@10:38.14	Best:23.28
2024-12-27 22:57:07,456: Early Stopping! Snapshot: 4 Epoch: 46 Best Results: 23.28
2024-12-27 22:57:07,456: Start to training tokens! Snapshot: 4 Epoch: 46 Loss:2.818 MRR:22.98 Best Results: 23.28
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:57:07,457: Snapshot:4	Epoch:46	Loss:2.818	translation_Loss:1.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.696                                                   	MRR:22.98	Hits@10:37.91	Best:23.28
2024-12-27 22:57:12,495: Snapshot:4	Epoch:47	Loss:26.907	translation_Loss:26.122	multi_layer_Loss:0.786	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.98	Hits@10:37.91	Best:23.28
2024-12-27 22:57:17,484: End of token training: 4 Epoch: 48 Loss:26.516 MRR:22.98 Best Results: 23.28
2024-12-27 22:57:17,484: Snapshot:4	Epoch:48	Loss:26.516	translation_Loss:26.144	multi_layer_Loss:0.372	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.98	Hits@10:37.91	Best:23.28
2024-12-27 22:57:17,771: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_5000/4model_best.tar'
2024-12-27 22:57:34,361: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2135 | 0.1156 | 0.2589 | 0.3184 |  0.3948 |
|     1      | 0.2468 | 0.1469 | 0.2865 | 0.3508 |  0.4411 |
|     2      | 0.189  | 0.1055 | 0.216  | 0.2731 |  0.3513 |
|     3      | 0.1806 | 0.0905 | 0.2141 | 0.2723 |  0.3493 |
|     4      | 0.2314 | 0.1506 | 0.2652 | 0.3139 |  0.3821 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 22:57:34,363: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2581 | 0.1512 | 0.3161 | 0.3784 |  0.4519 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2529 | 0.1448 | 0.3122 | 0.3771 |  0.4481 |
|     1      | 0.2928 | 0.1817 | 0.3463 | 0.414  |  0.5101 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2464 | 0.1344 | 0.3052 | 0.3709 |  0.4522 |
|     1      | 0.2615 | 0.1493 | 0.3163 | 0.3849 |  0.4711 |
|     2      | 0.2158 | 0.1267 | 0.2497 | 0.3098 |  0.3885 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2556 | 0.1492 | 0.308  | 0.3741 |  0.4534 |
|     1      | 0.2664 | 0.1607 | 0.3122 | 0.3747 |  0.4668 |
|     2      | 0.2098 | 0.1197 | 0.2424 | 0.3054 |  0.3847 |
|     3      | 0.2155 | 0.1212 | 0.2511 | 0.3141 |  0.3949 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2135 | 0.1156 | 0.2589 | 0.3184 |  0.3948 |
|     1      | 0.2468 | 0.1469 | 0.2865 | 0.3508 |  0.4411 |
|     2      | 0.189  | 0.1055 | 0.216  | 0.2731 |  0.3513 |
|     3      | 0.1806 | 0.0905 | 0.2141 | 0.2723 |  0.3493 |
|     4      | 0.2314 | 0.1506 | 0.2652 | 0.3139 |  0.3821 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 22:57:34,364: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 296.26730513572693 |   0.258   |    0.151     |    0.316     |     0.452     |
|    1     | 186.32842016220093 |   0.264   |    0.155     |    0.321     |     0.465     |
|    2     | 359.68266201019287 |   0.232   |    0.132     |    0.277     |     0.421     |
|    3     | 538.0274333953857  |   0.226   |     0.13     |    0.265     |      0.41     |
|    4     | 264.26461458206177 |    0.2    |    0.111     |    0.234     |     0.369     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 22:57:34,364: Sum_Training_Time:1644.5704352855682
2024-12-27 22:57:34,364: Every_Training_Time:[296.26730513572693, 186.32842016220093, 359.68266201019287, 538.0274333953857, 264.26461458206177]
2024-12-27 22:57:34,364: Forward transfer: 0.046024999999999996 Backward transfer: -0.038075
2024-12-27 22:58:07,776: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227225738/HYBRIDHYBRID_0.0001_1024_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_1024_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_1024_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 22:58:17,472: Snapshot:0	Epoch:0	Loss:48.314	translation_Loss:48.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.3	Hits@10:2.38	Best:1.3
2024-12-27 22:58:23,525: Snapshot:0	Epoch:1	Loss:45.002	translation_Loss:45.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.79	Hits@10:7.92	Best:3.79
2024-12-27 22:58:29,548: Snapshot:0	Epoch:2	Loss:41.936	translation_Loss:41.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:12.67	Best:5.6
2024-12-27 22:58:35,639: Snapshot:0	Epoch:3	Loss:39.022	translation_Loss:39.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.85	Hits@10:16.11	Best:6.85
2024-12-27 22:58:41,651: Snapshot:0	Epoch:4	Loss:36.216	translation_Loss:36.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.82	Hits@10:18.97	Best:7.82
2024-12-27 22:58:47,706: Snapshot:0	Epoch:5	Loss:33.472	translation_Loss:33.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.68	Best:8.89
2024-12-27 22:58:54,189: Snapshot:0	Epoch:6	Loss:30.69	translation_Loss:30.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:25.15	Best:10.29
2024-12-27 22:59:00,293: Snapshot:0	Epoch:7	Loss:27.959	translation_Loss:27.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.01	Hits@10:28.75	Best:12.01
2024-12-27 22:59:06,368: Snapshot:0	Epoch:8	Loss:25.173	translation_Loss:25.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.67	Hits@10:31.85	Best:13.67
2024-12-27 22:59:12,498: Snapshot:0	Epoch:9	Loss:22.449	translation_Loss:22.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.24	Hits@10:34.3	Best:15.24
2024-12-27 22:59:18,643: Snapshot:0	Epoch:10	Loss:19.787	translation_Loss:19.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.97	Hits@10:36.33	Best:16.97
2024-12-27 22:59:24,707: Snapshot:0	Epoch:11	Loss:17.285	translation_Loss:17.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.49	Hits@10:38.16	Best:18.49
2024-12-27 22:59:30,757: Snapshot:0	Epoch:12	Loss:14.981	translation_Loss:14.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.76	Hits@10:39.58	Best:19.76
2024-12-27 22:59:36,812: Snapshot:0	Epoch:13	Loss:13.018	translation_Loss:13.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.86	Hits@10:40.72	Best:20.86
2024-12-27 22:59:42,946: Snapshot:0	Epoch:14	Loss:11.354	translation_Loss:11.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.81	Hits@10:41.75	Best:21.81
2024-12-27 22:59:48,981: Snapshot:0	Epoch:15	Loss:9.88	translation_Loss:9.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.55	Hits@10:42.52	Best:22.55
2024-12-27 22:59:55,023: Snapshot:0	Epoch:16	Loss:8.648	translation_Loss:8.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:43.04	Best:23.1
2024-12-27 23:00:01,568: Snapshot:0	Epoch:17	Loss:7.592	translation_Loss:7.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.49	Hits@10:43.64	Best:23.49
2024-12-27 23:00:07,698: Snapshot:0	Epoch:18	Loss:6.693	translation_Loss:6.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:44.12	Best:23.87
2024-12-27 23:00:13,734: Snapshot:0	Epoch:19	Loss:5.93	translation_Loss:5.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.15	Hits@10:44.33	Best:24.15
2024-12-27 23:00:19,769: Snapshot:0	Epoch:20	Loss:5.225	translation_Loss:5.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:44.65	Best:24.33
2024-12-27 23:00:25,816: Snapshot:0	Epoch:21	Loss:4.674	translation_Loss:4.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:44.94	Best:24.43
2024-12-27 23:00:31,884: Snapshot:0	Epoch:22	Loss:4.194	translation_Loss:4.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:45.16	Best:24.63
2024-12-27 23:00:37,938: Snapshot:0	Epoch:23	Loss:3.781	translation_Loss:3.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.7	Hits@10:45.37	Best:24.7
2024-12-27 23:00:43,990: Snapshot:0	Epoch:24	Loss:3.459	translation_Loss:3.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.43	Best:24.9
2024-12-27 23:00:50,035: Snapshot:0	Epoch:25	Loss:3.107	translation_Loss:3.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.96	Hits@10:45.57	Best:24.96
2024-12-27 23:00:56,098: Snapshot:0	Epoch:26	Loss:2.85	translation_Loss:2.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.09	Hits@10:45.7	Best:25.09
2024-12-27 23:01:02,168: Snapshot:0	Epoch:27	Loss:2.619	translation_Loss:2.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.17	Hits@10:45.73	Best:25.17
2024-12-27 23:01:08,260: Snapshot:0	Epoch:28	Loss:2.413	translation_Loss:2.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.22	Hits@10:45.89	Best:25.22
2024-12-27 23:01:14,798: Snapshot:0	Epoch:29	Loss:2.253	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:45.9	Best:25.34
2024-12-27 23:01:20,945: Snapshot:0	Epoch:30	Loss:2.101	translation_Loss:2.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.4	Hits@10:45.92	Best:25.4
2024-12-27 23:01:27,016: Snapshot:0	Epoch:31	Loss:1.938	translation_Loss:1.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.47	Hits@10:46.02	Best:25.47
2024-12-27 23:01:33,102: Snapshot:0	Epoch:32	Loss:1.818	translation_Loss:1.818	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.48	Hits@10:46.12	Best:25.48
2024-12-27 23:01:39,166: Snapshot:0	Epoch:33	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:46.16	Best:25.55
2024-12-27 23:01:45,196: Snapshot:0	Epoch:34	Loss:1.621	translation_Loss:1.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.09	Best:25.58
2024-12-27 23:01:51,240: Snapshot:0	Epoch:35	Loss:1.528	translation_Loss:1.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.57	Hits@10:46.2	Best:25.58
2024-12-27 23:01:57,262: Snapshot:0	Epoch:36	Loss:1.444	translation_Loss:1.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.22	Best:25.62
2024-12-27 23:02:03,281: Snapshot:0	Epoch:37	Loss:1.371	translation_Loss:1.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.2	Best:25.62
2024-12-27 23:02:09,379: Snapshot:0	Epoch:38	Loss:1.299	translation_Loss:1.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.23	Best:25.7
2024-12-27 23:02:15,464: Snapshot:0	Epoch:39	Loss:1.248	translation_Loss:1.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.27	Best:25.79
2024-12-27 23:02:21,529: Snapshot:0	Epoch:40	Loss:1.188	translation_Loss:1.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.28	Best:25.85
2024-12-27 23:02:27,538: Snapshot:0	Epoch:41	Loss:1.131	translation_Loss:1.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.23	Best:25.85
2024-12-27 23:02:33,969: Snapshot:0	Epoch:42	Loss:1.083	translation_Loss:1.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.26	Best:25.85
2024-12-27 23:02:39,945: Snapshot:0	Epoch:43	Loss:1.039	translation_Loss:1.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.26	Best:25.85
2024-12-27 23:02:46,000: Snapshot:0	Epoch:44	Loss:1.0	translation_Loss:1.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.15	Best:25.85
2024-12-27 23:02:52,036: Early Stopping! Snapshot: 0 Epoch: 45 Best Results: 25.85
2024-12-27 23:02:52,036: Start to training tokens! Snapshot: 0 Epoch: 45 Loss:0.97 MRR:25.75 Best Results: 25.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:02:52,036: Snapshot:0	Epoch:45	Loss:0.97	translation_Loss:0.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.18	Best:25.85
2024-12-27 23:02:58,643: Snapshot:0	Epoch:46	Loss:35.965	translation_Loss:35.011	multi_layer_Loss:0.954	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.18	Best:25.85
2024-12-27 23:03:04,698: End of token training: 0 Epoch: 47 Loss:35.236 MRR:25.75 Best Results: 25.85
2024-12-27 23:03:04,698: Snapshot:0	Epoch:47	Loss:35.236	translation_Loss:34.974	multi_layer_Loss:0.262	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.75	Hits@10:46.18	Best:25.85
2024-12-27 23:03:04,926: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_10000/0model_best.tar'
2024-12-27 23:03:07,474: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1492 | 0.3159 | 0.3783 |  0.4523 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:03:18,592: Snapshot:1	Epoch:0	Loss:15.024	translation_Loss:14.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.049                                                   	MRR:6.3	Hits@10:10.42	Best:6.3
2024-12-27 23:03:20,994: Snapshot:1	Epoch:1	Loss:13.489	translation_Loss:13.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.122                                                   	MRR:7.08	Hits@10:11.94	Best:7.08
2024-12-27 23:03:23,377: Snapshot:1	Epoch:2	Loss:12.123	translation_Loss:11.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.217                                                   	MRR:8.09	Hits@10:14.07	Best:8.09
2024-12-27 23:03:25,768: Snapshot:1	Epoch:3	Loss:10.888	translation_Loss:10.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:9.72	Hits@10:17.96	Best:9.72
2024-12-27 23:03:28,142: Snapshot:1	Epoch:4	Loss:9.691	translation_Loss:9.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:11.6	Hits@10:22.41	Best:11.6
2024-12-27 23:03:30,546: Snapshot:1	Epoch:5	Loss:8.595	translation_Loss:8.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.553                                                   	MRR:13.71	Hits@10:26.4	Best:13.71
2024-12-27 23:03:32,994: Snapshot:1	Epoch:6	Loss:7.598	translation_Loss:6.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.647                                                   	MRR:15.54	Hits@10:29.22	Best:15.54
2024-12-27 23:03:35,391: Snapshot:1	Epoch:7	Loss:6.749	translation_Loss:6.027	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.722                                                   	MRR:17.05	Hits@10:31.69	Best:17.05
2024-12-27 23:03:37,826: Snapshot:1	Epoch:8	Loss:6.048	translation_Loss:5.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.778                                                   	MRR:18.24	Hits@10:33.44	Best:18.24
2024-12-27 23:03:40,234: Snapshot:1	Epoch:9	Loss:5.454	translation_Loss:4.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.818                                                   	MRR:19.21	Hits@10:34.77	Best:19.21
2024-12-27 23:03:42,651: Snapshot:1	Epoch:10	Loss:4.955	translation_Loss:4.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:20.09	Hits@10:36.12	Best:20.09
2024-12-27 23:03:45,143: Snapshot:1	Epoch:11	Loss:4.558	translation_Loss:3.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.863                                                   	MRR:20.82	Hits@10:37.5	Best:20.82
2024-12-27 23:03:47,575: Snapshot:1	Epoch:12	Loss:4.209	translation_Loss:3.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.873                                                   	MRR:21.64	Hits@10:38.56	Best:21.64
2024-12-27 23:03:49,930: Snapshot:1	Epoch:13	Loss:3.926	translation_Loss:3.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.876                                                   	MRR:22.26	Hits@10:39.81	Best:22.26
2024-12-27 23:03:52,323: Snapshot:1	Epoch:14	Loss:3.66	translation_Loss:2.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.874                                                   	MRR:22.8	Hits@10:40.79	Best:22.8
2024-12-27 23:03:54,681: Snapshot:1	Epoch:15	Loss:3.456	translation_Loss:2.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.869                                                   	MRR:23.32	Hits@10:41.56	Best:23.32
2024-12-27 23:03:57,061: Snapshot:1	Epoch:16	Loss:3.264	translation_Loss:2.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.864                                                   	MRR:23.85	Hits@10:42.47	Best:23.85
2024-12-27 23:03:59,479: Snapshot:1	Epoch:17	Loss:3.101	translation_Loss:2.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.858                                                   	MRR:24.22	Hits@10:43.08	Best:24.22
2024-12-27 23:04:01,856: Snapshot:1	Epoch:18	Loss:2.941	translation_Loss:2.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.85                                                   	MRR:24.55	Hits@10:43.81	Best:24.55
2024-12-27 23:04:04,238: Snapshot:1	Epoch:19	Loss:2.811	translation_Loss:1.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.84                                                   	MRR:24.74	Hits@10:44.35	Best:24.74
2024-12-27 23:04:06,609: Snapshot:1	Epoch:20	Loss:2.706	translation_Loss:1.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.83                                                   	MRR:24.98	Hits@10:44.97	Best:24.98
2024-12-27 23:04:09,355: Snapshot:1	Epoch:21	Loss:2.598	translation_Loss:1.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.821                                                   	MRR:25.31	Hits@10:45.39	Best:25.31
2024-12-27 23:04:11,746: Snapshot:1	Epoch:22	Loss:2.508	translation_Loss:1.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.811                                                   	MRR:25.67	Hits@10:45.87	Best:25.67
2024-12-27 23:04:14,135: Snapshot:1	Epoch:23	Loss:2.434	translation_Loss:1.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.801                                                   	MRR:25.82	Hits@10:45.99	Best:25.82
2024-12-27 23:04:16,501: Snapshot:1	Epoch:24	Loss:2.352	translation_Loss:1.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.791                                                   	MRR:26.08	Hits@10:46.55	Best:26.08
2024-12-27 23:04:18,946: Snapshot:1	Epoch:25	Loss:2.268	translation_Loss:1.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.782                                                   	MRR:26.27	Hits@10:47.02	Best:26.27
2024-12-27 23:04:21,328: Snapshot:1	Epoch:26	Loss:2.218	translation_Loss:1.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.772                                                   	MRR:26.46	Hits@10:47.35	Best:26.46
2024-12-27 23:04:23,706: Snapshot:1	Epoch:27	Loss:2.171	translation_Loss:1.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.764                                                   	MRR:26.62	Hits@10:47.49	Best:26.62
2024-12-27 23:04:26,149: Snapshot:1	Epoch:28	Loss:2.099	translation_Loss:1.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.755                                                   	MRR:26.82	Hits@10:47.74	Best:26.82
2024-12-27 23:04:28,564: Snapshot:1	Epoch:29	Loss:2.061	translation_Loss:1.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.746                                                   	MRR:26.98	Hits@10:48.02	Best:26.98
2024-12-27 23:04:30,943: Snapshot:1	Epoch:30	Loss:2.019	translation_Loss:1.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.74                                                   	MRR:27.22	Hits@10:48.22	Best:27.22
2024-12-27 23:04:33,316: Snapshot:1	Epoch:31	Loss:1.986	translation_Loss:1.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.731                                                   	MRR:27.31	Hits@10:48.53	Best:27.31
2024-12-27 23:04:35,731: Snapshot:1	Epoch:32	Loss:1.949	translation_Loss:1.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.723                                                   	MRR:27.41	Hits@10:48.9	Best:27.41
2024-12-27 23:04:38,083: Snapshot:1	Epoch:33	Loss:1.903	translation_Loss:1.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.716                                                   	MRR:27.71	Hits@10:48.97	Best:27.71
2024-12-27 23:04:40,449: Snapshot:1	Epoch:34	Loss:1.888	translation_Loss:1.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.71                                                   	MRR:27.74	Hits@10:49.1	Best:27.74
2024-12-27 23:04:42,841: Snapshot:1	Epoch:35	Loss:1.852	translation_Loss:1.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.704                                                   	MRR:27.82	Hits@10:49.09	Best:27.82
2024-12-27 23:04:45,190: Snapshot:1	Epoch:36	Loss:1.818	translation_Loss:1.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.697                                                   	MRR:28.13	Hits@10:49.29	Best:28.13
2024-12-27 23:04:47,601: Snapshot:1	Epoch:37	Loss:1.801	translation_Loss:1.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.692                                                   	MRR:28.17	Hits@10:49.31	Best:28.17
2024-12-27 23:04:49,968: Snapshot:1	Epoch:38	Loss:1.776	translation_Loss:1.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.686                                                   	MRR:28.13	Hits@10:49.49	Best:28.17
2024-12-27 23:04:52,304: Snapshot:1	Epoch:39	Loss:1.743	translation_Loss:1.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.681                                                   	MRR:28.08	Hits@10:49.65	Best:28.17
2024-12-27 23:04:54,617: Snapshot:1	Epoch:40	Loss:1.73	translation_Loss:1.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.675                                                   	MRR:28.11	Hits@10:49.58	Best:28.17
2024-12-27 23:04:57,001: Snapshot:1	Epoch:41	Loss:1.69	translation_Loss:1.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.671                                                   	MRR:28.19	Hits@10:49.65	Best:28.19
2024-12-27 23:04:59,389: Snapshot:1	Epoch:42	Loss:1.677	translation_Loss:1.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.666                                                   	MRR:28.37	Hits@10:49.86	Best:28.37
2024-12-27 23:05:01,883: Snapshot:1	Epoch:43	Loss:1.663	translation_Loss:1.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.661                                                   	MRR:28.43	Hits@10:50.04	Best:28.43
2024-12-27 23:05:04,291: Snapshot:1	Epoch:44	Loss:1.664	translation_Loss:1.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.656                                                   	MRR:28.49	Hits@10:50.2	Best:28.49
2024-12-27 23:05:06,688: Snapshot:1	Epoch:45	Loss:1.629	translation_Loss:0.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.652                                                   	MRR:28.53	Hits@10:50.13	Best:28.53
2024-12-27 23:05:09,102: Snapshot:1	Epoch:46	Loss:1.612	translation_Loss:0.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.647                                                   	MRR:28.61	Hits@10:50.28	Best:28.61
2024-12-27 23:05:11,496: Snapshot:1	Epoch:47	Loss:1.599	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.644                                                   	MRR:28.78	Hits@10:50.3	Best:28.78
2024-12-27 23:05:13,867: Snapshot:1	Epoch:48	Loss:1.589	translation_Loss:0.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.639                                                   	MRR:28.81	Hits@10:50.53	Best:28.81
2024-12-27 23:05:16,240: Snapshot:1	Epoch:49	Loss:1.574	translation_Loss:0.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.635                                                   	MRR:28.87	Hits@10:50.5	Best:28.87
2024-12-27 23:05:18,670: Snapshot:1	Epoch:50	Loss:1.561	translation_Loss:0.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.632                                                   	MRR:28.88	Hits@10:50.46	Best:28.88
2024-12-27 23:05:21,057: Snapshot:1	Epoch:51	Loss:1.539	translation_Loss:0.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.628                                                   	MRR:28.86	Hits@10:50.54	Best:28.88
2024-12-27 23:05:23,472: Snapshot:1	Epoch:52	Loss:1.524	translation_Loss:0.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.624                                                   	MRR:28.93	Hits@10:50.49	Best:28.93
2024-12-27 23:05:25,877: Snapshot:1	Epoch:53	Loss:1.508	translation_Loss:0.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.62                                                   	MRR:29.07	Hits@10:50.75	Best:29.07
2024-12-27 23:05:28,640: Snapshot:1	Epoch:54	Loss:1.498	translation_Loss:0.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.616                                                   	MRR:29.15	Hits@10:50.77	Best:29.15
2024-12-27 23:05:30,972: Snapshot:1	Epoch:55	Loss:1.481	translation_Loss:0.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.612                                                   	MRR:29.07	Hits@10:50.89	Best:29.15
2024-12-27 23:05:33,286: Snapshot:1	Epoch:56	Loss:1.478	translation_Loss:0.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.608                                                   	MRR:29.13	Hits@10:50.81	Best:29.15
2024-12-27 23:05:35,646: Snapshot:1	Epoch:57	Loss:1.458	translation_Loss:0.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.605                                                   	MRR:29.27	Hits@10:50.81	Best:29.27
2024-12-27 23:05:37,964: Snapshot:1	Epoch:58	Loss:1.454	translation_Loss:0.852	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.603                                                   	MRR:29.26	Hits@10:50.77	Best:29.27
2024-12-27 23:05:40,315: Snapshot:1	Epoch:59	Loss:1.447	translation_Loss:0.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.6                                                   	MRR:29.27	Hits@10:50.85	Best:29.27
2024-12-27 23:05:42,640: Snapshot:1	Epoch:60	Loss:1.435	translation_Loss:0.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.599                                                   	MRR:29.22	Hits@10:50.92	Best:29.27
2024-12-27 23:05:45,031: Snapshot:1	Epoch:61	Loss:1.424	translation_Loss:0.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.594                                                   	MRR:29.32	Hits@10:50.98	Best:29.32
2024-12-27 23:05:47,379: Snapshot:1	Epoch:62	Loss:1.427	translation_Loss:0.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.592                                                   	MRR:29.31	Hits@10:51.14	Best:29.32
2024-12-27 23:05:49,756: Snapshot:1	Epoch:63	Loss:1.411	translation_Loss:0.821	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.59                                                   	MRR:29.39	Hits@10:51.16	Best:29.39
2024-12-27 23:05:52,177: Snapshot:1	Epoch:64	Loss:1.407	translation_Loss:0.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.585                                                   	MRR:29.4	Hits@10:51.11	Best:29.4
2024-12-27 23:05:54,583: Snapshot:1	Epoch:65	Loss:1.402	translation_Loss:0.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.583                                                   	MRR:29.54	Hits@10:51.11	Best:29.54
2024-12-27 23:05:56,998: Snapshot:1	Epoch:66	Loss:1.389	translation_Loss:0.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.581                                                   	MRR:29.58	Hits@10:51.37	Best:29.58
2024-12-27 23:05:59,376: Snapshot:1	Epoch:67	Loss:1.386	translation_Loss:0.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.579                                                   	MRR:29.62	Hits@10:51.54	Best:29.62
2024-12-27 23:06:01,742: Snapshot:1	Epoch:68	Loss:1.367	translation_Loss:0.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.577                                                   	MRR:29.52	Hits@10:51.53	Best:29.62
2024-12-27 23:06:04,058: Snapshot:1	Epoch:69	Loss:1.362	translation_Loss:0.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.574                                                   	MRR:29.56	Hits@10:51.49	Best:29.62
2024-12-27 23:06:06,410: Snapshot:1	Epoch:70	Loss:1.361	translation_Loss:0.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.572                                                   	MRR:29.55	Hits@10:51.44	Best:29.62
2024-12-27 23:06:08,778: Snapshot:1	Epoch:71	Loss:1.354	translation_Loss:0.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.57                                                   	MRR:29.54	Hits@10:51.57	Best:29.62
2024-12-27 23:06:11,159: Early Stopping! Snapshot: 1 Epoch: 72 Best Results: 29.62
2024-12-27 23:06:11,159: Start to training tokens! Snapshot: 1 Epoch: 72 Loss:1.352 MRR:29.61 Best Results: 29.62
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:06:11,160: Snapshot:1	Epoch:72	Loss:1.352	translation_Loss:0.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.569                                                   	MRR:29.61	Hits@10:51.49	Best:29.62
2024-12-27 23:06:13,478: Snapshot:1	Epoch:73	Loss:12.883	translation_Loss:12.435	multi_layer_Loss:0.448	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.61	Hits@10:51.49	Best:29.62
2024-12-27 23:06:15,755: End of token training: 1 Epoch: 74 Loss:12.756 MRR:29.61 Best Results: 29.62
2024-12-27 23:06:15,755: Snapshot:1	Epoch:74	Loss:12.756	translation_Loss:12.427	multi_layer_Loss:0.329	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.61	Hits@10:51.49	Best:29.62
2024-12-27 23:06:15,971: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_10000/1model_best.tar'
2024-12-27 23:06:19,956: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1469 | 0.3135 | 0.3794 |  0.4511 |
|     1      | 0.2918 | 0.1805 | 0.3466 | 0.4177 |  0.5096 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:06:54,181: Snapshot:2	Epoch:0	Loss:61.277	translation_Loss:60.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.493                                                   	MRR:5.6	Hits@10:12.05	Best:5.6
2024-12-27 23:07:04,533: Snapshot:2	Epoch:1	Loss:48.474	translation_Loss:47.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.331                                                   	MRR:9.21	Hits@10:19.75	Best:9.21
2024-12-27 23:07:14,823: Snapshot:2	Epoch:2	Loss:38.16	translation_Loss:36.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.076                                                   	MRR:12.27	Hits@10:25.07	Best:12.27
2024-12-27 23:07:25,211: Snapshot:2	Epoch:3	Loss:30.631	translation_Loss:28.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.516                                                   	MRR:14.4	Hits@10:29.06	Best:14.4
2024-12-27 23:07:35,513: Snapshot:2	Epoch:4	Loss:25.359	translation_Loss:22.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.709                                                   	MRR:15.99	Hits@10:31.58	Best:15.99
2024-12-27 23:07:45,904: Snapshot:2	Epoch:5	Loss:21.608	translation_Loss:18.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.757                                                   	MRR:17.21	Hits@10:33.43	Best:17.21
2024-12-27 23:07:56,214: Snapshot:2	Epoch:6	Loss:18.906	translation_Loss:16.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.722                                                   	MRR:18.04	Hits@10:34.62	Best:18.04
2024-12-27 23:08:06,555: Snapshot:2	Epoch:7	Loss:16.92	translation_Loss:14.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.646                                                   	MRR:18.64	Hits@10:35.5	Best:18.64
2024-12-27 23:08:16,848: Snapshot:2	Epoch:8	Loss:15.469	translation_Loss:12.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.565                                                   	MRR:19.13	Hits@10:36.29	Best:19.13
2024-12-27 23:08:27,173: Snapshot:2	Epoch:9	Loss:14.367	translation_Loss:11.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.486                                                   	MRR:19.51	Hits@10:36.8	Best:19.51
2024-12-27 23:08:37,590: Snapshot:2	Epoch:10	Loss:13.526	translation_Loss:11.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.407                                                   	MRR:19.85	Hits@10:37.07	Best:19.85
2024-12-27 23:08:47,994: Snapshot:2	Epoch:11	Loss:12.824	translation_Loss:10.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.346                                                   	MRR:19.98	Hits@10:37.16	Best:19.98
2024-12-27 23:08:58,352: Snapshot:2	Epoch:12	Loss:12.315	translation_Loss:10.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.293                                                   	MRR:20.26	Hits@10:37.29	Best:20.26
2024-12-27 23:09:08,904: Snapshot:2	Epoch:13	Loss:11.92	translation_Loss:9.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.247                                                   	MRR:20.39	Hits@10:37.37	Best:20.39
2024-12-27 23:09:19,264: Snapshot:2	Epoch:14	Loss:11.567	translation_Loss:9.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.211                                                   	MRR:20.52	Hits@10:37.46	Best:20.52
2024-12-27 23:09:29,766: Snapshot:2	Epoch:15	Loss:11.292	translation_Loss:9.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.181                                                   	MRR:20.61	Hits@10:37.56	Best:20.61
2024-12-27 23:09:40,102: Snapshot:2	Epoch:16	Loss:11.045	translation_Loss:8.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.146                                                   	MRR:20.72	Hits@10:37.57	Best:20.72
2024-12-27 23:09:50,365: Snapshot:2	Epoch:17	Loss:10.836	translation_Loss:8.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.121                                                   	MRR:20.74	Hits@10:37.46	Best:20.74
2024-12-27 23:10:00,675: Snapshot:2	Epoch:18	Loss:10.685	translation_Loss:8.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.1                                                   	MRR:20.83	Hits@10:37.78	Best:20.83
2024-12-27 23:10:11,203: Snapshot:2	Epoch:19	Loss:10.511	translation_Loss:8.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.084                                                   	MRR:20.89	Hits@10:37.67	Best:20.89
2024-12-27 23:10:21,525: Snapshot:2	Epoch:20	Loss:10.394	translation_Loss:8.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.069                                                   	MRR:20.94	Hits@10:37.74	Best:20.94
2024-12-27 23:10:32,257: Snapshot:2	Epoch:21	Loss:10.276	translation_Loss:8.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.059                                                   	MRR:20.95	Hits@10:37.8	Best:20.95
2024-12-27 23:10:42,556: Snapshot:2	Epoch:22	Loss:10.215	translation_Loss:8.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.044                                                   	MRR:21.03	Hits@10:37.8	Best:21.03
2024-12-27 23:10:52,897: Snapshot:2	Epoch:23	Loss:10.117	translation_Loss:8.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.035                                                   	MRR:21.08	Hits@10:37.8	Best:21.08
2024-12-27 23:11:03,151: Snapshot:2	Epoch:24	Loss:10.04	translation_Loss:8.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.03                                                   	MRR:21.05	Hits@10:37.78	Best:21.08
2024-12-27 23:11:13,412: Snapshot:2	Epoch:25	Loss:9.943	translation_Loss:7.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.016                                                   	MRR:21.08	Hits@10:37.77	Best:21.08
2024-12-27 23:11:23,783: Snapshot:2	Epoch:26	Loss:9.889	translation_Loss:7.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.008                                                   	MRR:21.03	Hits@10:37.84	Best:21.08
2024-12-27 23:11:34,107: Snapshot:2	Epoch:27	Loss:9.873	translation_Loss:7.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.005                                                   	MRR:21.08	Hits@10:37.91	Best:21.08
2024-12-27 23:11:44,403: Snapshot:2	Epoch:28	Loss:9.812	translation_Loss:7.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.998                                                   	MRR:21.14	Hits@10:37.87	Best:21.14
2024-12-27 23:11:54,743: Snapshot:2	Epoch:29	Loss:9.748	translation_Loss:7.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.99                                                   	MRR:21.06	Hits@10:37.96	Best:21.14
2024-12-27 23:12:05,000: Snapshot:2	Epoch:30	Loss:9.702	translation_Loss:7.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.983                                                   	MRR:21.07	Hits@10:37.95	Best:21.14
2024-12-27 23:12:15,320: Snapshot:2	Epoch:31	Loss:9.667	translation_Loss:7.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.981                                                   	MRR:21.13	Hits@10:38.09	Best:21.14
2024-12-27 23:12:25,616: Snapshot:2	Epoch:32	Loss:9.586	translation_Loss:7.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.97                                                   	MRR:21.06	Hits@10:37.86	Best:21.14
2024-12-27 23:12:36,253: Early Stopping! Snapshot: 2 Epoch: 33 Best Results: 21.14
2024-12-27 23:12:36,254: Start to training tokens! Snapshot: 2 Epoch: 33 Loss:9.586 MRR:20.99 Best Results: 21.14
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:12:36,254: Snapshot:2	Epoch:33	Loss:9.586	translation_Loss:7.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.961                                                   	MRR:20.99	Hits@10:37.98	Best:21.14
2024-12-27 23:12:46,283: Snapshot:2	Epoch:34	Loss:56.391	translation_Loss:55.303	multi_layer_Loss:1.088	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.99	Hits@10:37.98	Best:21.14
2024-12-27 23:12:56,424: End of token training: 2 Epoch: 35 Loss:55.356 MRR:20.99 Best Results: 21.14
2024-12-27 23:12:56,424: Snapshot:2	Epoch:35	Loss:55.356	translation_Loss:55.262	multi_layer_Loss:0.094	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.99	Hits@10:37.98	Best:21.14
2024-12-27 23:12:56,655: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_10000/2model_best.tar'
2024-12-27 23:13:04,830: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2494 | 0.1375 | 0.309  | 0.3752 |  0.4531 |
|     1      | 0.2713 | 0.1601 | 0.322  | 0.3934 |  0.485  |
|     2      | 0.2126 | 0.1259 | 0.2453 | 0.3042 |  0.3828 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:13:44,778: Snapshot:3	Epoch:0	Loss:63.17	translation_Loss:62.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.758                                                   	MRR:6.55	Hits@10:16.05	Best:6.55
2024-12-27 23:13:57,398: Snapshot:3	Epoch:1	Loss:45.148	translation_Loss:42.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.173                                                   	MRR:10.7	Hits@10:23.53	Best:10.7
2024-12-27 23:14:09,988: Snapshot:3	Epoch:2	Loss:34.016	translation_Loss:30.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.348                                                   	MRR:13.52	Hits@10:28.37	Best:13.52
2024-12-27 23:14:22,587: Snapshot:3	Epoch:3	Loss:27.548	translation_Loss:23.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.963                                                   	MRR:15.44	Hits@10:31.83	Best:15.44
2024-12-27 23:14:35,176: Snapshot:3	Epoch:4	Loss:23.516	translation_Loss:19.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.193                                                   	MRR:16.84	Hits@10:34.15	Best:16.84
2024-12-27 23:14:47,794: Snapshot:3	Epoch:5	Loss:20.655	translation_Loss:16.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.221                                                   	MRR:18.12	Hits@10:35.55	Best:18.12
2024-12-27 23:15:00,491: Snapshot:3	Epoch:6	Loss:18.678	translation_Loss:14.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.166                                                   	MRR:18.74	Hits@10:36.43	Best:18.74
2024-12-27 23:15:13,167: Snapshot:3	Epoch:7	Loss:17.227	translation_Loss:13.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.075                                                   	MRR:19.35	Hits@10:37.06	Best:19.35
2024-12-27 23:15:25,846: Snapshot:3	Epoch:8	Loss:16.105	translation_Loss:12.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.958                                                   	MRR:19.82	Hits@10:37.62	Best:19.82
2024-12-27 23:15:38,486: Snapshot:3	Epoch:9	Loss:15.258	translation_Loss:11.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.839                                                   	MRR:20.05	Hits@10:37.83	Best:20.05
2024-12-27 23:15:51,165: Snapshot:3	Epoch:10	Loss:14.64	translation_Loss:10.915	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.726                                                   	MRR:20.3	Hits@10:37.9	Best:20.3
2024-12-27 23:16:03,816: Snapshot:3	Epoch:11	Loss:14.128	translation_Loss:10.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.631                                                   	MRR:20.4	Hits@10:38.02	Best:20.4
2024-12-27 23:16:16,406: Snapshot:3	Epoch:12	Loss:13.649	translation_Loss:10.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.553                                                   	MRR:20.54	Hits@10:38.14	Best:20.54
2024-12-27 23:16:28,971: Snapshot:3	Epoch:13	Loss:13.349	translation_Loss:9.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.484                                                   	MRR:20.64	Hits@10:38.23	Best:20.64
2024-12-27 23:16:41,657: Snapshot:3	Epoch:14	Loss:13.112	translation_Loss:9.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.421                                                   	MRR:20.75	Hits@10:38.34	Best:20.75
2024-12-27 23:16:54,189: Snapshot:3	Epoch:15	Loss:12.871	translation_Loss:9.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.373                                                   	MRR:20.73	Hits@10:38.29	Best:20.75
2024-12-27 23:17:06,845: Snapshot:3	Epoch:16	Loss:12.64	translation_Loss:9.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.324                                                   	MRR:20.77	Hits@10:38.27	Best:20.77
2024-12-27 23:17:19,905: Snapshot:3	Epoch:17	Loss:12.493	translation_Loss:9.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.279                                                   	MRR:20.76	Hits@10:38.23	Best:20.77
2024-12-27 23:17:32,450: Snapshot:3	Epoch:18	Loss:12.311	translation_Loss:9.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.242                                                   	MRR:20.77	Hits@10:38.35	Best:20.77
2024-12-27 23:17:45,035: Snapshot:3	Epoch:19	Loss:12.205	translation_Loss:8.98	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.225                                                   	MRR:20.8	Hits@10:38.28	Best:20.8
2024-12-27 23:17:57,658: Snapshot:3	Epoch:20	Loss:12.122	translation_Loss:8.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.197                                                   	MRR:20.81	Hits@10:38.28	Best:20.81
2024-12-27 23:18:10,317: Snapshot:3	Epoch:21	Loss:11.997	translation_Loss:8.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.172                                                   	MRR:20.83	Hits@10:38.3	Best:20.83
2024-12-27 23:18:23,082: Snapshot:3	Epoch:22	Loss:11.972	translation_Loss:8.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.155                                                   	MRR:20.88	Hits@10:38.28	Best:20.88
2024-12-27 23:18:35,692: Snapshot:3	Epoch:23	Loss:11.822	translation_Loss:8.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.139                                                   	MRR:20.89	Hits@10:38.3	Best:20.89
2024-12-27 23:18:48,239: Snapshot:3	Epoch:24	Loss:11.775	translation_Loss:8.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.118                                                   	MRR:20.96	Hits@10:38.43	Best:20.96
2024-12-27 23:19:00,778: Snapshot:3	Epoch:25	Loss:11.73	translation_Loss:8.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.114                                                   	MRR:20.91	Hits@10:38.33	Best:20.96
2024-12-27 23:19:13,731: Snapshot:3	Epoch:26	Loss:11.673	translation_Loss:8.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.102                                                   	MRR:20.94	Hits@10:38.37	Best:20.96
2024-12-27 23:19:26,408: Snapshot:3	Epoch:27	Loss:11.626	translation_Loss:8.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.086                                                   	MRR:20.89	Hits@10:38.32	Best:20.96
2024-12-27 23:19:39,103: Snapshot:3	Epoch:28	Loss:11.595	translation_Loss:8.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.081                                                   	MRR:20.97	Hits@10:38.4	Best:20.97
2024-12-27 23:19:51,605: Snapshot:3	Epoch:29	Loss:11.592	translation_Loss:8.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.075                                                   	MRR:20.88	Hits@10:38.35	Best:20.97
2024-12-27 23:20:04,156: Snapshot:3	Epoch:30	Loss:11.55	translation_Loss:8.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.067                                                   	MRR:20.84	Hits@10:38.41	Best:20.97
2024-12-27 23:20:16,805: Snapshot:3	Epoch:31	Loss:11.526	translation_Loss:8.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.066                                                   	MRR:20.78	Hits@10:38.19	Best:20.97
2024-12-27 23:20:29,704: Snapshot:3	Epoch:32	Loss:11.437	translation_Loss:8.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.057                                                   	MRR:20.85	Hits@10:38.32	Best:20.97
2024-12-27 23:20:42,267: Snapshot:3	Epoch:33	Loss:11.471	translation_Loss:8.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.053                                                   	MRR:21.0	Hits@10:38.35	Best:21.0
2024-12-27 23:20:54,947: Snapshot:3	Epoch:34	Loss:11.441	translation_Loss:8.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.043                                                   	MRR:21.08	Hits@10:38.47	Best:21.08
2024-12-27 23:21:07,506: Snapshot:3	Epoch:35	Loss:11.429	translation_Loss:8.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.044                                                   	MRR:20.92	Hits@10:38.27	Best:21.08
2024-12-27 23:21:20,038: Snapshot:3	Epoch:36	Loss:11.406	translation_Loss:8.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.034                                                   	MRR:20.91	Hits@10:38.41	Best:21.08
2024-12-27 23:21:32,589: Snapshot:3	Epoch:37	Loss:11.351	translation_Loss:8.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.032                                                   	MRR:20.92	Hits@10:38.21	Best:21.08
2024-12-27 23:21:45,113: Snapshot:3	Epoch:38	Loss:11.333	translation_Loss:8.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.03                                                   	MRR:20.96	Hits@10:38.33	Best:21.08
2024-12-27 23:21:57,728: Early Stopping! Snapshot: 3 Epoch: 39 Best Results: 21.08
2024-12-27 23:21:57,729: Start to training tokens! Snapshot: 3 Epoch: 39 Loss:11.324 MRR:20.89 Best Results: 21.08
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:21:57,729: Snapshot:3	Epoch:39	Loss:11.324	translation_Loss:8.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.028                                                   	MRR:20.89	Hits@10:38.22	Best:21.08
2024-12-27 23:22:10,009: Snapshot:3	Epoch:40	Loss:61.269	translation_Loss:60.077	multi_layer_Loss:1.191	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.89	Hits@10:38.22	Best:21.08
2024-12-27 23:22:22,830: End of token training: 3 Epoch: 41 Loss:60.158 MRR:20.89 Best Results: 21.08
2024-12-27 23:22:22,830: Snapshot:3	Epoch:41	Loss:60.158	translation_Loss:60.097	multi_layer_Loss:0.06	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.89	Hits@10:38.22	Best:21.08
2024-12-27 23:22:23,086: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_10000/3model_best.tar'
2024-12-27 23:22:37,255: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.257  | 0.1487 | 0.3122 | 0.3785 |  0.4565 |
|     1      | 0.2762 | 0.1677 | 0.3212 | 0.3922 |  0.4843 |
|     2      | 0.2096 | 0.1201 | 0.2437 | 0.3034 |  0.3817 |
|     3      | 0.2096 | 0.1185 | 0.2446 | 0.3065 |  0.383  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:22:55,762: Snapshot:4	Epoch:0	Loss:21.438	translation_Loss:21.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.137                                                   	MRR:7.3	Hits@10:14.65	Best:7.3
2024-12-27 23:23:00,899: Snapshot:4	Epoch:1	Loss:17.731	translation_Loss:17.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:8.51	Hits@10:17.62	Best:8.51
2024-12-27 23:23:06,170: Snapshot:4	Epoch:2	Loss:14.777	translation_Loss:14.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.685                                                   	MRR:12.35	Hits@10:27.44	Best:12.35
2024-12-27 23:23:11,420: Snapshot:4	Epoch:3	Loss:12.558	translation_Loss:11.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.994                                                   	MRR:14.86	Hits@10:30.62	Best:14.86
2024-12-27 23:23:16,658: Snapshot:4	Epoch:4	Loss:11.094	translation_Loss:9.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.267                                                   	MRR:16.68	Hits@10:32.35	Best:16.68
2024-12-27 23:23:21,877: Snapshot:4	Epoch:5	Loss:10.115	translation_Loss:8.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.498                                                   	MRR:17.61	Hits@10:33.23	Best:17.61
2024-12-27 23:23:27,069: Snapshot:4	Epoch:6	Loss:9.364	translation_Loss:7.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.691                                                   	MRR:18.54	Hits@10:34.28	Best:18.54
2024-12-27 23:23:32,294: Snapshot:4	Epoch:7	Loss:8.783	translation_Loss:6.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.847                                                   	MRR:18.71	Hits@10:35.0	Best:18.71
2024-12-27 23:23:37,493: Snapshot:4	Epoch:8	Loss:8.287	translation_Loss:6.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.969                                                   	MRR:19.18	Hits@10:35.45	Best:19.18
2024-12-27 23:23:42,696: Snapshot:4	Epoch:9	Loss:7.848	translation_Loss:5.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.067                                                   	MRR:19.54	Hits@10:35.82	Best:19.54
2024-12-27 23:23:47,864: Snapshot:4	Epoch:10	Loss:7.512	translation_Loss:5.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.143                                                   	MRR:20.35	Hits@10:35.77	Best:20.35
2024-12-27 23:23:52,994: Snapshot:4	Epoch:11	Loss:7.172	translation_Loss:4.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.198                                                   	MRR:20.46	Hits@10:36.16	Best:20.46
2024-12-27 23:23:58,108: Snapshot:4	Epoch:12	Loss:6.895	translation_Loss:4.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.239                                                   	MRR:20.42	Hits@10:36.41	Best:20.46
2024-12-27 23:24:03,252: Snapshot:4	Epoch:13	Loss:6.63	translation_Loss:4.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.265                                                   	MRR:20.9	Hits@10:36.46	Best:20.9
2024-12-27 23:24:08,375: Snapshot:4	Epoch:14	Loss:6.399	translation_Loss:4.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.281                                                   	MRR:20.81	Hits@10:36.5	Best:20.9
2024-12-27 23:24:13,513: Snapshot:4	Epoch:15	Loss:6.208	translation_Loss:3.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.292                                                   	MRR:21.32	Hits@10:36.87	Best:21.32
2024-12-27 23:24:18,680: Snapshot:4	Epoch:16	Loss:6.014	translation_Loss:3.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.293                                                   	MRR:21.34	Hits@10:36.98	Best:21.34
2024-12-27 23:24:24,216: Snapshot:4	Epoch:17	Loss:5.852	translation_Loss:3.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.293                                                   	MRR:21.74	Hits@10:36.84	Best:21.74
2024-12-27 23:24:29,387: Snapshot:4	Epoch:18	Loss:5.679	translation_Loss:3.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.286                                                   	MRR:21.64	Hits@10:36.88	Best:21.74
2024-12-27 23:24:34,530: Snapshot:4	Epoch:19	Loss:5.544	translation_Loss:3.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.272                                                   	MRR:22.1	Hits@10:37.03	Best:22.1
2024-12-27 23:24:39,703: Snapshot:4	Epoch:20	Loss:5.45	translation_Loss:3.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.259                                                   	MRR:22.05	Hits@10:37.16	Best:22.1
2024-12-27 23:24:44,772: Snapshot:4	Epoch:21	Loss:5.349	translation_Loss:3.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.249                                                   	MRR:21.68	Hits@10:37.08	Best:22.1
2024-12-27 23:24:49,868: Snapshot:4	Epoch:22	Loss:5.249	translation_Loss:3.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.236                                                   	MRR:21.92	Hits@10:36.95	Best:22.1
2024-12-27 23:24:54,994: Snapshot:4	Epoch:23	Loss:5.145	translation_Loss:2.921	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.223                                                   	MRR:22.12	Hits@10:37.15	Best:22.12
2024-12-27 23:25:00,163: Snapshot:4	Epoch:24	Loss:5.075	translation_Loss:2.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.208                                                   	MRR:22.34	Hits@10:36.98	Best:22.34
2024-12-27 23:25:05,335: Snapshot:4	Epoch:25	Loss:5.019	translation_Loss:2.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.193                                                   	MRR:22.3	Hits@10:37.31	Best:22.34
2024-12-27 23:25:10,424: Snapshot:4	Epoch:26	Loss:4.961	translation_Loss:2.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.18                                                   	MRR:22.21	Hits@10:37.13	Best:22.34
2024-12-27 23:25:15,587: Snapshot:4	Epoch:27	Loss:4.9	translation_Loss:2.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.167                                                   	MRR:22.46	Hits@10:37.17	Best:22.46
2024-12-27 23:25:20,853: Snapshot:4	Epoch:28	Loss:4.854	translation_Loss:2.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.153                                                   	MRR:22.48	Hits@10:37.09	Best:22.48
2024-12-27 23:25:26,027: Snapshot:4	Epoch:29	Loss:4.803	translation_Loss:2.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.143                                                   	MRR:22.45	Hits@10:37.01	Best:22.48
2024-12-27 23:25:31,167: Snapshot:4	Epoch:30	Loss:4.777	translation_Loss:2.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.132                                                   	MRR:22.72	Hits@10:36.99	Best:22.72
2024-12-27 23:25:36,348: Snapshot:4	Epoch:31	Loss:4.749	translation_Loss:2.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.121                                                   	MRR:22.63	Hits@10:37.26	Best:22.72
2024-12-27 23:25:41,423: Snapshot:4	Epoch:32	Loss:4.702	translation_Loss:2.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.11                                                   	MRR:22.58	Hits@10:36.97	Best:22.72
2024-12-27 23:25:46,496: Snapshot:4	Epoch:33	Loss:4.692	translation_Loss:2.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.099                                                   	MRR:22.56	Hits@10:36.85	Best:22.72
2024-12-27 23:25:51,649: Snapshot:4	Epoch:34	Loss:4.651	translation_Loss:2.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.089                                                   	MRR:22.66	Hits@10:37.03	Best:22.72
2024-12-27 23:25:56,849: Snapshot:4	Epoch:35	Loss:4.584	translation_Loss:2.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.08                                                   	MRR:22.75	Hits@10:37.2	Best:22.75
2024-12-27 23:26:02,009: Snapshot:4	Epoch:36	Loss:4.601	translation_Loss:2.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.07                                                   	MRR:22.85	Hits@10:37.27	Best:22.85
2024-12-27 23:26:07,126: Snapshot:4	Epoch:37	Loss:4.568	translation_Loss:2.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.062                                                   	MRR:22.57	Hits@10:36.96	Best:22.85
2024-12-27 23:26:12,323: Snapshot:4	Epoch:38	Loss:4.536	translation_Loss:2.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.055                                                   	MRR:22.74	Hits@10:37.05	Best:22.85
2024-12-27 23:26:17,457: Snapshot:4	Epoch:39	Loss:4.521	translation_Loss:2.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.042                                                   	MRR:22.75	Hits@10:36.97	Best:22.85
2024-12-27 23:26:22,595: Snapshot:4	Epoch:40	Loss:4.528	translation_Loss:2.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.039                                                   	MRR:22.76	Hits@10:37.01	Best:22.85
2024-12-27 23:26:27,708: Early Stopping! Snapshot: 4 Epoch: 41 Best Results: 22.85
2024-12-27 23:26:27,709: Start to training tokens! Snapshot: 4 Epoch: 41 Loss:4.498 MRR:22.79 Best Results: 22.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:26:27,709: Snapshot:4	Epoch:41	Loss:4.498	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.036                                                   	MRR:22.79	Hits@10:37.06	Best:22.85
2024-12-27 23:26:33,090: Snapshot:4	Epoch:42	Loss:28.265	translation_Loss:27.48	multi_layer_Loss:0.786	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.79	Hits@10:37.06	Best:22.85
2024-12-27 23:26:38,059: End of token training: 4 Epoch: 43 Loss:27.823 MRR:22.79 Best Results: 22.85
2024-12-27 23:26:38,059: Snapshot:4	Epoch:43	Loss:27.823	translation_Loss:27.451	multi_layer_Loss:0.372	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.79	Hits@10:37.06	Best:22.85
2024-12-27 23:26:38,289: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_1024_10000/4model_best.tar'
2024-12-27 23:26:54,315: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2219 | 0.1225 | 0.2683 | 0.3299 |  0.4048 |
|     1      | 0.2593 | 0.1563 | 0.3034 | 0.3691 |  0.4602 |
|     2      | 0.1943 | 0.112  | 0.2215 | 0.278  |  0.3545 |
|     3      | 0.1813 | 0.093  | 0.2138 | 0.2734 |  0.3495 |
|     4      | 0.2288 | 0.1526 | 0.2584 | 0.3075 |  0.3737 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 23:26:54,335: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1492 | 0.3159 | 0.3783 |  0.4523 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1469 | 0.3135 | 0.3794 |  0.4511 |
|     1      | 0.2918 | 0.1805 | 0.3466 | 0.4177 |  0.5096 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2494 | 0.1375 | 0.309  | 0.3752 |  0.4531 |
|     1      | 0.2713 | 0.1601 | 0.322  | 0.3934 |  0.485  |
|     2      | 0.2126 | 0.1259 | 0.2453 | 0.3042 |  0.3828 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.257  | 0.1487 | 0.3122 | 0.3785 |  0.4565 |
|     1      | 0.2762 | 0.1677 | 0.3212 | 0.3922 |  0.4843 |
|     2      | 0.2096 | 0.1201 | 0.2437 | 0.3034 |  0.3817 |
|     3      | 0.2096 | 0.1185 | 0.2446 | 0.3065 |  0.383  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2219 | 0.1225 | 0.2683 | 0.3299 |  0.4048 |
|     1      | 0.2593 | 0.1563 | 0.3034 | 0.3691 |  0.4602 |
|     2      | 0.1943 | 0.112  | 0.2215 | 0.278  |  0.3545 |
|     3      | 0.1813 | 0.093  | 0.2138 | 0.2734 |  0.3495 |
|     4      | 0.2288 | 0.1526 | 0.2584 | 0.3075 |  0.3737 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 23:26:54,336: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 296.92210125923157 |   0.257   |    0.149     |    0.316     |     0.452     |
|    1     | 186.77239656448364 |   0.265   |    0.156     |    0.322     |     0.467     |
|    2     | 392.2869794368744  |   0.233   |    0.134     |    0.277     |      0.42     |
|    3     | 552.7395241260529  |   0.225   |    0.129     |    0.265     |     0.406     |
|    4     | 238.27035474777222 |   0.204   |    0.116     |    0.238     |     0.372     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 23:26:54,336: Sum_Training_Time:1666.9913561344147
2024-12-27 23:26:54,336: Every_Training_Time:[296.92210125923157, 186.77239656448364, 392.2869794368744, 552.7395241260529, 238.27035474777222]
2024-12-27 23:26:54,336: Forward transfer: 0.04535 Backward transfer: -0.02855000000000002
2024-12-27 23:27:30,220: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227232659/HYBRIDHYBRID_0.0001_2048_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_2048_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_2048_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 23:27:39,918: Snapshot:0	Epoch:0	Loss:24.333	translation_Loss:24.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.78	Hits@10:0.9	Best:0.78
2024-12-27 23:27:45,786: Snapshot:0	Epoch:1	Loss:23.173	translation_Loss:23.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.53	Hits@10:2.9	Best:1.53
2024-12-27 23:27:52,027: Snapshot:0	Epoch:2	Loss:22.122	translation_Loss:22.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.08	Hits@10:6.68	Best:3.08
2024-12-27 23:27:57,993: Snapshot:0	Epoch:3	Loss:21.1	translation_Loss:21.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:9.98	Best:4.54
2024-12-27 23:28:03,850: Snapshot:0	Epoch:4	Loss:20.098	translation_Loss:20.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:13.19	Best:5.66
2024-12-27 23:28:09,721: Snapshot:0	Epoch:5	Loss:19.123	translation_Loss:19.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.56	Hits@10:15.81	Best:6.56
2024-12-27 23:28:15,618: Snapshot:0	Epoch:6	Loss:18.14	translation_Loss:18.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.34	Hits@10:17.83	Best:7.34
2024-12-27 23:28:21,948: Snapshot:0	Epoch:7	Loss:17.186	translation_Loss:17.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:19.93	Best:8.1
2024-12-27 23:28:27,813: Snapshot:0	Epoch:8	Loss:16.222	translation_Loss:16.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.92	Hits@10:22.11	Best:8.92
2024-12-27 23:28:33,788: Snapshot:0	Epoch:9	Loss:15.277	translation_Loss:15.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.84	Hits@10:24.47	Best:9.84
2024-12-27 23:28:39,758: Snapshot:0	Epoch:10	Loss:14.324	translation_Loss:14.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.82	Hits@10:27.01	Best:10.82
2024-12-27 23:28:46,113: Snapshot:0	Epoch:11	Loss:13.368	translation_Loss:13.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.89	Hits@10:29.48	Best:11.89
2024-12-27 23:28:52,001: Snapshot:0	Epoch:12	Loss:12.388	translation_Loss:12.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.96	Hits@10:31.58	Best:12.96
2024-12-27 23:28:57,879: Snapshot:0	Epoch:13	Loss:11.425	translation_Loss:11.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.1	Hits@10:33.61	Best:14.1
2024-12-27 23:29:03,781: Snapshot:0	Epoch:14	Loss:10.479	translation_Loss:10.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.21	Hits@10:35.19	Best:15.21
2024-12-27 23:29:09,713: Snapshot:0	Epoch:15	Loss:9.541	translation_Loss:9.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.19	Hits@10:36.64	Best:16.19
2024-12-27 23:29:16,110: Snapshot:0	Epoch:16	Loss:8.663	translation_Loss:8.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.22	Hits@10:37.85	Best:17.22
2024-12-27 23:29:22,129: Snapshot:0	Epoch:17	Loss:7.841	translation_Loss:7.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.27	Hits@10:38.93	Best:18.27
2024-12-27 23:29:28,107: Snapshot:0	Epoch:18	Loss:7.082	translation_Loss:7.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.13	Hits@10:39.92	Best:19.13
2024-12-27 23:29:34,092: Snapshot:0	Epoch:19	Loss:6.404	translation_Loss:6.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.92	Hits@10:40.67	Best:19.92
2024-12-27 23:29:40,039: Snapshot:0	Epoch:20	Loss:5.775	translation_Loss:5.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.59	Hits@10:41.31	Best:20.59
2024-12-27 23:29:46,347: Snapshot:0	Epoch:21	Loss:5.224	translation_Loss:5.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.2	Hits@10:41.97	Best:21.2
2024-12-27 23:29:52,262: Snapshot:0	Epoch:22	Loss:4.736	translation_Loss:4.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.74	Hits@10:42.48	Best:21.74
2024-12-27 23:29:58,184: Snapshot:0	Epoch:23	Loss:4.295	translation_Loss:4.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.17	Hits@10:42.86	Best:22.17
2024-12-27 23:30:04,089: Snapshot:0	Epoch:24	Loss:3.914	translation_Loss:3.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.58	Hits@10:43.26	Best:22.58
2024-12-27 23:30:10,094: Snapshot:0	Epoch:25	Loss:3.532	translation_Loss:3.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.94	Hits@10:43.64	Best:22.94
2024-12-27 23:30:16,448: Snapshot:0	Epoch:26	Loss:3.234	translation_Loss:3.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.26	Hits@10:43.94	Best:23.26
2024-12-27 23:30:22,401: Snapshot:0	Epoch:27	Loss:2.949	translation_Loss:2.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.46	Hits@10:44.28	Best:23.46
2024-12-27 23:30:28,382: Snapshot:0	Epoch:28	Loss:2.686	translation_Loss:2.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.7	Hits@10:44.52	Best:23.7
2024-12-27 23:30:34,361: Snapshot:0	Epoch:29	Loss:2.481	translation_Loss:2.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:44.69	Best:23.88
2024-12-27 23:30:40,291: Snapshot:0	Epoch:30	Loss:2.282	translation_Loss:2.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:44.81	Best:24.06
2024-12-27 23:30:46,577: Snapshot:0	Epoch:31	Loss:2.091	translation_Loss:2.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:44.95	Best:24.24
2024-12-27 23:30:52,488: Snapshot:0	Epoch:32	Loss:1.938	translation_Loss:1.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:44.94	Best:24.34
2024-12-27 23:30:58,388: Snapshot:0	Epoch:33	Loss:1.806	translation_Loss:1.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:45.08	Best:24.46
2024-12-27 23:31:04,309: Snapshot:0	Epoch:34	Loss:1.686	translation_Loss:1.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:45.15	Best:24.57
2024-12-27 23:31:10,208: Snapshot:0	Epoch:35	Loss:1.57	translation_Loss:1.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.64	Hits@10:45.27	Best:24.64
2024-12-27 23:31:16,472: Snapshot:0	Epoch:36	Loss:1.464	translation_Loss:1.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:45.36	Best:24.75
2024-12-27 23:31:22,429: Snapshot:0	Epoch:37	Loss:1.372	translation_Loss:1.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.81	Hits@10:45.44	Best:24.81
2024-12-27 23:31:28,314: Snapshot:0	Epoch:38	Loss:1.295	translation_Loss:1.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:45.48	Best:24.91
2024-12-27 23:31:34,225: Snapshot:0	Epoch:39	Loss:1.219	translation_Loss:1.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:45.56	Best:24.99
2024-12-27 23:31:40,112: Snapshot:0	Epoch:40	Loss:1.149	translation_Loss:1.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.06	Hits@10:45.5	Best:25.06
2024-12-27 23:31:46,492: Snapshot:0	Epoch:41	Loss:1.087	translation_Loss:1.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.1	Hits@10:45.63	Best:25.1
2024-12-27 23:31:52,446: Snapshot:0	Epoch:42	Loss:1.031	translation_Loss:1.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.11	Hits@10:45.68	Best:25.11
2024-12-27 23:31:58,352: Snapshot:0	Epoch:43	Loss:0.977	translation_Loss:0.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.14	Hits@10:45.76	Best:25.14
2024-12-27 23:32:04,321: Snapshot:0	Epoch:44	Loss:0.93	translation_Loss:0.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.25	Hits@10:45.85	Best:25.25
2024-12-27 23:32:10,290: Snapshot:0	Epoch:45	Loss:0.892	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.26	Hits@10:45.99	Best:25.26
2024-12-27 23:32:16,520: Snapshot:0	Epoch:46	Loss:0.848	translation_Loss:0.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.32	Hits@10:45.95	Best:25.32
2024-12-27 23:32:22,390: Snapshot:0	Epoch:47	Loss:0.814	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:46.0	Best:25.32
2024-12-27 23:32:28,296: Snapshot:0	Epoch:48	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.32	Hits@10:46.12	Best:25.32
2024-12-27 23:32:34,258: Snapshot:0	Epoch:49	Loss:0.743	translation_Loss:0.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.08	Best:25.35
2024-12-27 23:32:40,598: Snapshot:0	Epoch:50	Loss:0.717	translation_Loss:0.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.36	Hits@10:46.09	Best:25.36
2024-12-27 23:32:46,562: Snapshot:0	Epoch:51	Loss:0.676	translation_Loss:0.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.1	Best:25.36
2024-12-27 23:32:52,509: Snapshot:0	Epoch:52	Loss:0.664	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:46.17	Best:25.41
2024-12-27 23:32:58,395: Snapshot:0	Epoch:53	Loss:0.63	translation_Loss:0.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:46.21	Best:25.44
2024-12-27 23:33:04,327: Snapshot:0	Epoch:54	Loss:0.614	translation_Loss:0.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:46.11	Best:25.44
2024-12-27 23:33:10,607: Snapshot:0	Epoch:55	Loss:0.595	translation_Loss:0.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.38	Hits@10:46.18	Best:25.44
2024-12-27 23:33:16,479: Snapshot:0	Epoch:56	Loss:0.571	translation_Loss:0.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:46.18	Best:25.44
2024-12-27 23:33:22,392: Snapshot:0	Epoch:57	Loss:0.549	translation_Loss:0.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.38	Hits@10:46.18	Best:25.44
2024-12-27 23:33:28,372: Early Stopping! Snapshot: 0 Epoch: 58 Best Results: 25.44
2024-12-27 23:33:28,373: Start to training tokens! Snapshot: 0 Epoch: 58 Loss:0.538 MRR:25.34 Best Results: 25.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:33:28,373: Snapshot:0	Epoch:58	Loss:0.538	translation_Loss:0.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:46.22	Best:25.44
2024-12-27 23:33:34,851: Snapshot:0	Epoch:59	Loss:17.95	translation_Loss:17.363	multi_layer_Loss:0.586	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:46.22	Best:25.44
2024-12-27 23:33:41,073: End of token training: 0 Epoch: 60 Loss:17.726 MRR:25.34 Best Results: 25.44
2024-12-27 23:33:41,073: Snapshot:0	Epoch:60	Loss:17.726	translation_Loss:17.358	multi_layer_Loss:0.368	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.34	Hits@10:46.22	Best:25.44
2024-12-27 23:33:41,290: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_1000/0model_best.tar'
2024-12-27 23:33:43,930: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.255 | 0.1456 | 0.3152 | 0.3777 |  0.4518 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:33:54,952: Snapshot:1	Epoch:0	Loss:7.901	translation_Loss:7.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.003                                                   	MRR:6.0	Hits@10:10.16	Best:6.0
2024-12-27 23:33:57,222: Snapshot:1	Epoch:1	Loss:7.305	translation_Loss:7.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.01                                                   	MRR:6.5	Hits@10:10.87	Best:6.5
2024-12-27 23:33:59,518: Snapshot:1	Epoch:2	Loss:6.788	translation_Loss:6.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.02                                                   	MRR:7.0	Hits@10:11.71	Best:7.0
2024-12-27 23:34:01,834: Snapshot:1	Epoch:3	Loss:6.269	translation_Loss:6.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.03                                                   	MRR:7.64	Hits@10:13.11	Best:7.64
2024-12-27 23:34:04,138: Snapshot:1	Epoch:4	Loss:5.774	translation_Loss:5.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.042                                                   	MRR:8.44	Hits@10:14.62	Best:8.44
2024-12-27 23:34:06,434: Snapshot:1	Epoch:5	Loss:5.28	translation_Loss:5.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.055                                                   	MRR:9.39	Hits@10:17.11	Best:9.39
2024-12-27 23:34:08,789: Snapshot:1	Epoch:6	Loss:4.813	translation_Loss:4.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.069                                                   	MRR:10.59	Hits@10:19.78	Best:10.59
2024-12-27 23:34:11,123: Snapshot:1	Epoch:7	Loss:4.35	translation_Loss:4.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.084                                                   	MRR:11.78	Hits@10:22.69	Best:11.78
2024-12-27 23:34:13,453: Snapshot:1	Epoch:8	Loss:3.917	translation_Loss:3.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.099                                                   	MRR:13.02	Hits@10:25.64	Best:13.02
2024-12-27 23:34:15,793: Snapshot:1	Epoch:9	Loss:3.507	translation_Loss:3.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.115                                                   	MRR:14.33	Hits@10:28.06	Best:14.33
2024-12-27 23:34:18,101: Snapshot:1	Epoch:10	Loss:3.135	translation_Loss:3.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.131                                                   	MRR:15.5	Hits@10:30.0	Best:15.5
2024-12-27 23:34:20,425: Snapshot:1	Epoch:11	Loss:2.81	translation_Loss:2.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.145                                                   	MRR:16.66	Hits@10:31.75	Best:16.66
2024-12-27 23:34:22,729: Snapshot:1	Epoch:12	Loss:2.521	translation_Loss:2.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:17.67	Hits@10:33.15	Best:17.67
2024-12-27 23:34:25,067: Snapshot:1	Epoch:13	Loss:2.265	translation_Loss:2.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:18.48	Hits@10:34.32	Best:18.48
2024-12-27 23:34:27,453: Snapshot:1	Epoch:14	Loss:2.036	translation_Loss:1.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.181                                                   	MRR:19.21	Hits@10:35.45	Best:19.21
2024-12-27 23:34:29,806: Snapshot:1	Epoch:15	Loss:1.844	translation_Loss:1.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.192                                                   	MRR:19.78	Hits@10:36.16	Best:19.78
2024-12-27 23:34:32,123: Snapshot:1	Epoch:16	Loss:1.677	translation_Loss:1.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.201                                                   	MRR:20.35	Hits@10:36.8	Best:20.35
2024-12-27 23:34:34,392: Snapshot:1	Epoch:17	Loss:1.523	translation_Loss:1.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.21                                                   	MRR:20.82	Hits@10:37.63	Best:20.82
2024-12-27 23:34:36,659: Snapshot:1	Epoch:18	Loss:1.389	translation_Loss:1.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.218                                                   	MRR:21.32	Hits@10:38.01	Best:21.32
2024-12-27 23:34:39,286: Snapshot:1	Epoch:19	Loss:1.281	translation_Loss:1.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:21.73	Hits@10:38.38	Best:21.73
2024-12-27 23:34:41,616: Snapshot:1	Epoch:20	Loss:1.184	translation_Loss:0.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:22.04	Hits@10:38.91	Best:22.04
2024-12-27 23:34:43,921: Snapshot:1	Epoch:21	Loss:1.092	translation_Loss:0.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:22.29	Hits@10:39.37	Best:22.29
2024-12-27 23:34:46,246: Snapshot:1	Epoch:22	Loss:1.015	translation_Loss:0.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:22.56	Hits@10:39.68	Best:22.56
2024-12-27 23:34:48,564: Snapshot:1	Epoch:23	Loss:0.951	translation_Loss:0.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:22.79	Hits@10:39.95	Best:22.79
2024-12-27 23:34:50,878: Snapshot:1	Epoch:24	Loss:0.887	translation_Loss:0.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.25                                                   	MRR:23.0	Hits@10:40.3	Best:23.0
2024-12-27 23:34:53,276: Snapshot:1	Epoch:25	Loss:0.84	translation_Loss:0.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:23.21	Hits@10:40.79	Best:23.21
2024-12-27 23:34:55,558: Snapshot:1	Epoch:26	Loss:0.788	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:23.35	Hits@10:41.11	Best:23.35
2024-12-27 23:34:57,856: Snapshot:1	Epoch:27	Loss:0.745	translation_Loss:0.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:23.46	Hits@10:41.35	Best:23.46
2024-12-27 23:35:00,124: Snapshot:1	Epoch:28	Loss:0.711	translation_Loss:0.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.26                                                   	MRR:23.65	Hits@10:41.58	Best:23.65
2024-12-27 23:35:02,434: Snapshot:1	Epoch:29	Loss:0.678	translation_Loss:0.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:23.81	Hits@10:41.78	Best:23.81
2024-12-27 23:35:04,741: Snapshot:1	Epoch:30	Loss:0.649	translation_Loss:0.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:23.87	Hits@10:41.96	Best:23.87
2024-12-27 23:35:07,022: Snapshot:1	Epoch:31	Loss:0.619	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:24.08	Hits@10:42.24	Best:24.08
2024-12-27 23:35:09,718: Snapshot:1	Epoch:32	Loss:0.595	translation_Loss:0.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.263                                                   	MRR:24.26	Hits@10:42.3	Best:24.26
2024-12-27 23:35:12,111: Snapshot:1	Epoch:33	Loss:0.575	translation_Loss:0.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.263                                                   	MRR:24.36	Hits@10:42.48	Best:24.36
2024-12-27 23:35:14,377: Snapshot:1	Epoch:34	Loss:0.554	translation_Loss:0.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.263                                                   	MRR:24.53	Hits@10:42.5	Best:24.53
2024-12-27 23:35:16,688: Snapshot:1	Epoch:35	Loss:0.536	translation_Loss:0.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:24.59	Hits@10:42.67	Best:24.59
2024-12-27 23:35:19,006: Snapshot:1	Epoch:36	Loss:0.519	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:24.69	Hits@10:42.95	Best:24.69
2024-12-27 23:35:21,342: Snapshot:1	Epoch:37	Loss:0.505	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:24.75	Hits@10:43.2	Best:24.75
2024-12-27 23:35:23,637: Snapshot:1	Epoch:38	Loss:0.489	translation_Loss:0.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.26                                                   	MRR:24.85	Hits@10:43.29	Best:24.85
2024-12-27 23:35:25,942: Snapshot:1	Epoch:39	Loss:0.476	translation_Loss:0.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.259                                                   	MRR:24.94	Hits@10:43.52	Best:24.94
2024-12-27 23:35:28,248: Snapshot:1	Epoch:40	Loss:0.464	translation_Loss:0.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:25.01	Hits@10:43.82	Best:25.01
2024-12-27 23:35:30,549: Snapshot:1	Epoch:41	Loss:0.46	translation_Loss:0.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:25.15	Hits@10:44.05	Best:25.15
2024-12-27 23:35:32,843: Snapshot:1	Epoch:42	Loss:0.444	translation_Loss:0.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:25.23	Hits@10:44.17	Best:25.23
2024-12-27 23:35:35,150: Snapshot:1	Epoch:43	Loss:0.438	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.255                                                   	MRR:25.31	Hits@10:44.17	Best:25.31
2024-12-27 23:35:37,450: Snapshot:1	Epoch:44	Loss:0.428	translation_Loss:0.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.254                                                   	MRR:25.33	Hits@10:44.28	Best:25.33
2024-12-27 23:35:39,763: Snapshot:1	Epoch:45	Loss:0.424	translation_Loss:0.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:25.45	Hits@10:44.2	Best:25.45
2024-12-27 23:35:42,412: Snapshot:1	Epoch:46	Loss:0.413	translation_Loss:0.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.251                                                   	MRR:25.5	Hits@10:44.36	Best:25.5
2024-12-27 23:35:44,720: Snapshot:1	Epoch:47	Loss:0.404	translation_Loss:0.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.25                                                   	MRR:25.58	Hits@10:44.6	Best:25.58
2024-12-27 23:35:47,024: Snapshot:1	Epoch:48	Loss:0.398	translation_Loss:0.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.249                                                   	MRR:25.66	Hits@10:44.72	Best:25.66
2024-12-27 23:35:49,344: Snapshot:1	Epoch:49	Loss:0.394	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:25.77	Hits@10:44.8	Best:25.77
2024-12-27 23:35:51,657: Snapshot:1	Epoch:50	Loss:0.392	translation_Loss:0.145	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.246                                                   	MRR:25.78	Hits@10:44.92	Best:25.78
2024-12-27 23:35:54,046: Snapshot:1	Epoch:51	Loss:0.378	translation_Loss:0.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.245                                                   	MRR:25.82	Hits@10:44.97	Best:25.82
2024-12-27 23:35:56,330: Snapshot:1	Epoch:52	Loss:0.377	translation_Loss:0.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.243                                                   	MRR:25.92	Hits@10:45.11	Best:25.92
2024-12-27 23:35:58,619: Snapshot:1	Epoch:53	Loss:0.37	translation_Loss:0.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:26.0	Hits@10:45.26	Best:26.0
2024-12-27 23:36:00,926: Snapshot:1	Epoch:54	Loss:0.366	translation_Loss:0.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:26.09	Hits@10:45.45	Best:26.09
2024-12-27 23:36:03,297: Snapshot:1	Epoch:55	Loss:0.361	translation_Loss:0.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:26.12	Hits@10:45.51	Best:26.12
2024-12-27 23:36:05,607: Snapshot:1	Epoch:56	Loss:0.356	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.238                                                   	MRR:26.19	Hits@10:45.63	Best:26.19
2024-12-27 23:36:07,916: Snapshot:1	Epoch:57	Loss:0.354	translation_Loss:0.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.236                                                   	MRR:26.3	Hits@10:45.74	Best:26.3
2024-12-27 23:36:10,179: Snapshot:1	Epoch:58	Loss:0.347	translation_Loss:0.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.235                                                   	MRR:26.3	Hits@10:45.8	Best:26.3
2024-12-27 23:36:12,840: Snapshot:1	Epoch:59	Loss:0.348	translation_Loss:0.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.233                                                   	MRR:26.38	Hits@10:45.95	Best:26.38
2024-12-27 23:36:15,142: Snapshot:1	Epoch:60	Loss:0.34	translation_Loss:0.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:26.43	Hits@10:46.06	Best:26.43
2024-12-27 23:36:17,447: Snapshot:1	Epoch:61	Loss:0.338	translation_Loss:0.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:26.47	Hits@10:46.11	Best:26.47
2024-12-27 23:36:19,693: Snapshot:1	Epoch:62	Loss:0.329	translation_Loss:0.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:26.46	Hits@10:46.14	Best:26.47
2024-12-27 23:36:21,972: Snapshot:1	Epoch:63	Loss:0.331	translation_Loss:0.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.227                                                   	MRR:26.48	Hits@10:46.04	Best:26.48
2024-12-27 23:36:24,248: Snapshot:1	Epoch:64	Loss:0.328	translation_Loss:0.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.226                                                   	MRR:26.57	Hits@10:46.26	Best:26.57
2024-12-27 23:36:26,590: Snapshot:1	Epoch:65	Loss:0.322	translation_Loss:0.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:26.72	Hits@10:46.46	Best:26.72
2024-12-27 23:36:28,880: Snapshot:1	Epoch:66	Loss:0.321	translation_Loss:0.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.223                                                   	MRR:26.82	Hits@10:46.57	Best:26.82
2024-12-27 23:36:31,262: Snapshot:1	Epoch:67	Loss:0.315	translation_Loss:0.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.222                                                   	MRR:26.94	Hits@10:46.78	Best:26.94
2024-12-27 23:36:33,555: Snapshot:1	Epoch:68	Loss:0.312	translation_Loss:0.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.221                                                   	MRR:26.96	Hits@10:46.91	Best:26.96
2024-12-27 23:36:35,839: Snapshot:1	Epoch:69	Loss:0.312	translation_Loss:0.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.219                                                   	MRR:26.97	Hits@10:46.89	Best:26.97
2024-12-27 23:36:38,143: Snapshot:1	Epoch:70	Loss:0.302	translation_Loss:0.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.218                                                   	MRR:26.99	Hits@10:47.04	Best:26.99
2024-12-27 23:36:40,506: Snapshot:1	Epoch:71	Loss:0.308	translation_Loss:0.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.217                                                   	MRR:26.99	Hits@10:47.01	Best:26.99
2024-12-27 23:36:42,819: Snapshot:1	Epoch:72	Loss:0.304	translation_Loss:0.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.216                                                   	MRR:27.1	Hits@10:47.21	Best:27.1
2024-12-27 23:36:45,522: Snapshot:1	Epoch:73	Loss:0.304	translation_Loss:0.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:27.14	Hits@10:47.27	Best:27.14
2024-12-27 23:36:47,778: Snapshot:1	Epoch:74	Loss:0.295	translation_Loss:0.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:27.02	Hits@10:47.46	Best:27.14
2024-12-27 23:36:50,043: Snapshot:1	Epoch:75	Loss:0.297	translation_Loss:0.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.212                                                   	MRR:27.08	Hits@10:47.41	Best:27.14
2024-12-27 23:36:52,287: Snapshot:1	Epoch:76	Loss:0.293	translation_Loss:0.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:27.14	Hits@10:47.64	Best:27.14
2024-12-27 23:36:54,545: Snapshot:1	Epoch:77	Loss:0.292	translation_Loss:0.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.209                                                   	MRR:27.09	Hits@10:47.66	Best:27.14
2024-12-27 23:36:56,814: Early Stopping! Snapshot: 1 Epoch: 78 Best Results: 27.14
2024-12-27 23:36:56,814: Start to training tokens! Snapshot: 1 Epoch: 78 Loss:0.291 MRR:27.12 Best Results: 27.14
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:36:56,814: Snapshot:1	Epoch:78	Loss:0.291	translation_Loss:0.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:27.12	Hits@10:47.87	Best:27.14
2024-12-27 23:36:59,050: Snapshot:1	Epoch:79	Loss:6.071	translation_Loss:5.824	multi_layer_Loss:0.247	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:27.12	Hits@10:47.87	Best:27.14
2024-12-27 23:37:01,268: End of token training: 1 Epoch: 80 Loss:6.038 MRR:27.12 Best Results: 27.14
2024-12-27 23:37:01,268: Snapshot:1	Epoch:80	Loss:6.038	translation_Loss:5.822	multi_layer_Loss:0.216	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:27.12	Hits@10:47.87	Best:27.14
2024-12-27 23:37:01,502: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_1000/1model_best.tar'
2024-12-27 23:37:05,460: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2397 | 0.1335 | 0.2944 | 0.3604 |  0.4378 |
|     1      | 0.2689 | 0.1628 |  0.32  | 0.3805 |  0.4663 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:37:39,088: Snapshot:2	Epoch:0	Loss:31.418	translation_Loss:31.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.037                                                   	MRR:4.12	Hits@10:8.58	Best:4.12
2024-12-27 23:37:49,161: Snapshot:2	Epoch:1	Loss:26.457	translation_Loss:26.338	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.12                                                   	MRR:6.47	Hits@10:14.23	Best:6.47
2024-12-27 23:37:59,119: Snapshot:2	Epoch:2	Loss:21.838	translation_Loss:21.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:8.91	Hits@10:19.5	Best:8.91
2024-12-27 23:38:09,076: Snapshot:2	Epoch:3	Loss:17.782	translation_Loss:17.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.381                                                   	MRR:11.07	Hits@10:23.23	Best:11.07
2024-12-27 23:38:19,046: Snapshot:2	Epoch:4	Loss:14.475	translation_Loss:13.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.528                                                   	MRR:13.0	Hits@10:26.55	Best:13.0
2024-12-27 23:38:29,148: Snapshot:2	Epoch:5	Loss:11.956	translation_Loss:11.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.668                                                   	MRR:14.53	Hits@10:29.27	Best:14.53
2024-12-27 23:38:39,138: Snapshot:2	Epoch:6	Loss:10.003	translation_Loss:9.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.795                                                   	MRR:15.76	Hits@10:31.28	Best:15.76
2024-12-27 23:38:49,172: Snapshot:2	Epoch:7	Loss:8.53	translation_Loss:7.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.907                                                   	MRR:16.7	Hits@10:32.61	Best:16.7
2024-12-27 23:38:59,209: Snapshot:2	Epoch:8	Loss:7.346	translation_Loss:6.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.003                                                   	MRR:17.43	Hits@10:33.57	Best:17.43
2024-12-27 23:39:09,217: Snapshot:2	Epoch:9	Loss:6.463	translation_Loss:5.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.086                                                   	MRR:18.02	Hits@10:34.41	Best:18.02
2024-12-27 23:39:19,251: Snapshot:2	Epoch:10	Loss:5.759	translation_Loss:4.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.155                                                   	MRR:18.45	Hits@10:35.19	Best:18.45
2024-12-27 23:39:29,592: Snapshot:2	Epoch:11	Loss:5.19	translation_Loss:3.98	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.21                                                   	MRR:18.78	Hits@10:35.97	Best:18.78
2024-12-27 23:39:39,615: Snapshot:2	Epoch:12	Loss:4.741	translation_Loss:3.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.254                                                   	MRR:19.18	Hits@10:36.27	Best:19.18
2024-12-27 23:39:49,663: Snapshot:2	Epoch:13	Loss:4.382	translation_Loss:3.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.287                                                   	MRR:19.51	Hits@10:36.84	Best:19.51
2024-12-27 23:39:59,612: Snapshot:2	Epoch:14	Loss:4.081	translation_Loss:2.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.313                                                   	MRR:19.78	Hits@10:37.13	Best:19.78
2024-12-27 23:40:10,190: Snapshot:2	Epoch:15	Loss:3.838	translation_Loss:2.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.331                                                   	MRR:19.98	Hits@10:37.47	Best:19.98
2024-12-27 23:40:20,209: Snapshot:2	Epoch:16	Loss:3.624	translation_Loss:2.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:20.09	Hits@10:37.8	Best:20.09
2024-12-27 23:40:30,218: Snapshot:2	Epoch:17	Loss:3.443	translation_Loss:2.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.351                                                   	MRR:20.34	Hits@10:37.99	Best:20.34
2024-12-27 23:40:40,632: Snapshot:2	Epoch:18	Loss:3.31	translation_Loss:1.954	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.356                                                   	MRR:20.4	Hits@10:38.12	Best:20.4
2024-12-27 23:40:50,617: Snapshot:2	Epoch:19	Loss:3.181	translation_Loss:1.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.357                                                   	MRR:20.48	Hits@10:38.33	Best:20.48
2024-12-27 23:41:00,663: Snapshot:2	Epoch:20	Loss:3.06	translation_Loss:1.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.356                                                   	MRR:20.62	Hits@10:38.36	Best:20.62
2024-12-27 23:41:10,928: Snapshot:2	Epoch:21	Loss:2.97	translation_Loss:1.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.352                                                   	MRR:20.76	Hits@10:38.41	Best:20.76
2024-12-27 23:41:20,978: Snapshot:2	Epoch:22	Loss:2.885	translation_Loss:1.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.348                                                   	MRR:20.78	Hits@10:38.54	Best:20.78
2024-12-27 23:41:30,961: Snapshot:2	Epoch:23	Loss:2.805	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.342                                                   	MRR:20.93	Hits@10:38.7	Best:20.93
2024-12-27 23:41:41,318: Snapshot:2	Epoch:24	Loss:2.727	translation_Loss:1.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.336                                                   	MRR:20.96	Hits@10:38.75	Best:20.96
2024-12-27 23:41:51,335: Snapshot:2	Epoch:25	Loss:2.668	translation_Loss:1.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.328                                                   	MRR:21.04	Hits@10:38.71	Best:21.04
2024-12-27 23:42:01,396: Snapshot:2	Epoch:26	Loss:2.624	translation_Loss:1.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.32                                                   	MRR:21.06	Hits@10:38.85	Best:21.06
2024-12-27 23:42:11,528: Snapshot:2	Epoch:27	Loss:2.578	translation_Loss:1.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.313                                                   	MRR:21.2	Hits@10:38.96	Best:21.2
2024-12-27 23:42:21,852: Snapshot:2	Epoch:28	Loss:2.521	translation_Loss:1.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.304                                                   	MRR:21.26	Hits@10:38.92	Best:21.26
2024-12-27 23:42:31,834: Snapshot:2	Epoch:29	Loss:2.485	translation_Loss:1.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.295                                                   	MRR:21.22	Hits@10:38.95	Best:21.26
2024-12-27 23:42:41,827: Snapshot:2	Epoch:30	Loss:2.438	translation_Loss:1.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.287                                                   	MRR:21.3	Hits@10:38.94	Best:21.3
2024-12-27 23:42:52,168: Snapshot:2	Epoch:31	Loss:2.407	translation_Loss:1.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.278                                                   	MRR:21.31	Hits@10:38.96	Best:21.31
2024-12-27 23:43:02,187: Snapshot:2	Epoch:32	Loss:2.366	translation_Loss:1.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.272                                                   	MRR:21.42	Hits@10:38.9	Best:21.42
2024-12-27 23:43:12,129: Snapshot:2	Epoch:33	Loss:2.34	translation_Loss:1.077	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.263                                                   	MRR:21.52	Hits@10:38.97	Best:21.52
2024-12-27 23:43:22,556: Snapshot:2	Epoch:34	Loss:2.31	translation_Loss:1.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.254                                                   	MRR:21.52	Hits@10:39.11	Best:21.52
2024-12-27 23:43:32,484: Snapshot:2	Epoch:35	Loss:2.283	translation_Loss:1.034	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.248                                                   	MRR:21.46	Hits@10:39.19	Best:21.52
2024-12-27 23:43:42,371: Snapshot:2	Epoch:36	Loss:2.264	translation_Loss:1.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.242                                                   	MRR:21.45	Hits@10:39.17	Best:21.52
2024-12-27 23:43:52,707: Snapshot:2	Epoch:37	Loss:2.237	translation_Loss:1.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.234                                                   	MRR:21.48	Hits@10:39.11	Best:21.52
2024-12-27 23:44:02,720: Early Stopping! Snapshot: 2 Epoch: 38 Best Results: 21.52
2024-12-27 23:44:02,720: Start to training tokens! Snapshot: 2 Epoch: 38 Loss:2.222 MRR:21.49 Best Results: 21.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:44:02,721: Snapshot:2	Epoch:38	Loss:2.222	translation_Loss:0.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.228                                                   	MRR:21.49	Hits@10:39.22	Best:21.52
2024-12-27 23:44:12,483: Snapshot:2	Epoch:39	Loss:24.786	translation_Loss:24.014	multi_layer_Loss:0.773	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.49	Hits@10:39.22	Best:21.52
2024-12-27 23:44:22,573: End of token training: 2 Epoch: 40 Loss:24.329 MRR:21.49 Best Results: 21.52
2024-12-27 23:44:22,573: Snapshot:2	Epoch:40	Loss:24.329	translation_Loss:24.013	multi_layer_Loss:0.316	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.49	Hits@10:39.22	Best:21.52
2024-12-27 23:44:22,811: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_1000/2model_best.tar'
2024-12-27 23:44:30,703: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2356 | 0.1251 | 0.2905 | 0.3567 |  0.4375 |
|     1      | 0.237  | 0.1298 | 0.2887 | 0.3508 |  0.4292 |
|     2      | 0.2168 | 0.1253 | 0.2501 | 0.3139 |  0.3977 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:45:10,245: Snapshot:3	Epoch:0	Loss:32.01	translation_Loss:31.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.055                                                   	MRR:4.71	Hits@10:11.87	Best:4.71
2024-12-27 23:45:22,525: Snapshot:3	Epoch:1	Loss:24.106	translation_Loss:23.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:7.57	Hits@10:18.22	Best:7.57
2024-12-27 23:45:34,714: Snapshot:3	Epoch:2	Loss:17.432	translation_Loss:17.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.363                                                   	MRR:10.58	Hits@10:23.99	Best:10.58
2024-12-27 23:45:46,857: Snapshot:3	Epoch:3	Loss:12.799	translation_Loss:12.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.54                                                   	MRR:12.85	Hits@10:27.71	Best:12.85
2024-12-27 23:45:59,126: Snapshot:3	Epoch:4	Loss:9.859	translation_Loss:9.163	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.696                                                   	MRR:14.41	Hits@10:30.32	Best:14.41
2024-12-27 23:46:11,242: Snapshot:3	Epoch:5	Loss:7.963	translation_Loss:7.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.827                                                   	MRR:15.67	Hits@10:32.59	Best:15.67
2024-12-27 23:46:23,337: Snapshot:3	Epoch:6	Loss:6.635	translation_Loss:5.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.933                                                   	MRR:16.77	Hits@10:34.01	Best:16.77
2024-12-27 23:46:35,475: Snapshot:3	Epoch:7	Loss:5.668	translation_Loss:4.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.018                                                   	MRR:17.63	Hits@10:35.09	Best:17.63
2024-12-27 23:46:47,619: Snapshot:3	Epoch:8	Loss:4.956	translation_Loss:3.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.085                                                   	MRR:18.31	Hits@10:35.96	Best:18.31
2024-12-27 23:46:59,949: Snapshot:3	Epoch:9	Loss:4.433	translation_Loss:3.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.135                                                   	MRR:18.8	Hits@10:36.81	Best:18.8
2024-12-27 23:47:12,498: Snapshot:3	Epoch:10	Loss:4.05	translation_Loss:2.878	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.172                                                   	MRR:19.16	Hits@10:37.43	Best:19.16
2024-12-27 23:47:24,765: Snapshot:3	Epoch:11	Loss:3.737	translation_Loss:2.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.199                                                   	MRR:19.47	Hits@10:37.66	Best:19.47
2024-12-27 23:47:36,934: Snapshot:3	Epoch:12	Loss:3.467	translation_Loss:2.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.216                                                   	MRR:19.68	Hits@10:38.05	Best:19.68
2024-12-27 23:47:49,464: Snapshot:3	Epoch:13	Loss:3.275	translation_Loss:2.049	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.226                                                   	MRR:19.86	Hits@10:38.31	Best:19.86
2024-12-27 23:48:01,630: Snapshot:3	Epoch:14	Loss:3.092	translation_Loss:1.86	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.232                                                   	MRR:20.05	Hits@10:38.57	Best:20.05
2024-12-27 23:48:13,777: Snapshot:3	Epoch:15	Loss:2.949	translation_Loss:1.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.234                                                   	MRR:20.22	Hits@10:38.77	Best:20.22
2024-12-27 23:48:26,273: Snapshot:3	Epoch:16	Loss:2.817	translation_Loss:1.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.231                                                   	MRR:20.38	Hits@10:39.0	Best:20.38
2024-12-27 23:48:38,519: Snapshot:3	Epoch:17	Loss:2.7	translation_Loss:1.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.225                                                   	MRR:20.46	Hits@10:39.18	Best:20.46
2024-12-27 23:48:50,646: Snapshot:3	Epoch:18	Loss:2.617	translation_Loss:1.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.217                                                   	MRR:20.63	Hits@10:39.35	Best:20.63
2024-12-27 23:49:03,079: Snapshot:3	Epoch:19	Loss:2.537	translation_Loss:1.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.21                                                   	MRR:20.77	Hits@10:39.51	Best:20.77
2024-12-27 23:49:15,278: Snapshot:3	Epoch:20	Loss:2.455	translation_Loss:1.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.2                                                   	MRR:20.91	Hits@10:39.68	Best:20.91
2024-12-27 23:49:27,936: Snapshot:3	Epoch:21	Loss:2.39	translation_Loss:1.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.19                                                   	MRR:21.03	Hits@10:39.75	Best:21.03
2024-12-27 23:49:40,112: Snapshot:3	Epoch:22	Loss:2.329	translation_Loss:1.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.179                                                   	MRR:21.04	Hits@10:39.95	Best:21.04
2024-12-27 23:49:52,217: Snapshot:3	Epoch:23	Loss:2.276	translation_Loss:1.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.169                                                   	MRR:21.14	Hits@10:40.1	Best:21.14
2024-12-27 23:50:04,721: Snapshot:3	Epoch:24	Loss:2.234	translation_Loss:1.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.159                                                   	MRR:21.24	Hits@10:40.18	Best:21.24
2024-12-27 23:50:16,995: Snapshot:3	Epoch:25	Loss:2.182	translation_Loss:1.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.15                                                   	MRR:21.31	Hits@10:40.23	Best:21.31
2024-12-27 23:50:29,144: Snapshot:3	Epoch:26	Loss:2.135	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.14                                                   	MRR:21.41	Hits@10:40.42	Best:21.41
2024-12-27 23:50:41,652: Snapshot:3	Epoch:27	Loss:2.108	translation_Loss:0.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.13                                                   	MRR:21.46	Hits@10:40.49	Best:21.46
2024-12-27 23:50:53,837: Snapshot:3	Epoch:28	Loss:2.077	translation_Loss:0.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.121                                                   	MRR:21.5	Hits@10:40.6	Best:21.5
2024-12-27 23:51:05,980: Snapshot:3	Epoch:29	Loss:2.063	translation_Loss:0.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.113                                                   	MRR:21.55	Hits@10:40.68	Best:21.55
2024-12-27 23:51:18,510: Snapshot:3	Epoch:30	Loss:2.018	translation_Loss:0.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.105                                                   	MRR:21.6	Hits@10:40.66	Best:21.6
2024-12-27 23:51:30,708: Snapshot:3	Epoch:31	Loss:1.99	translation_Loss:0.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:21.67	Hits@10:40.93	Best:21.67
2024-12-27 23:51:43,301: Snapshot:3	Epoch:32	Loss:1.969	translation_Loss:0.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.089                                                   	MRR:21.68	Hits@10:40.83	Best:21.68
2024-12-27 23:51:55,552: Snapshot:3	Epoch:33	Loss:1.947	translation_Loss:0.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.081                                                   	MRR:21.71	Hits@10:40.89	Best:21.71
2024-12-27 23:52:07,759: Snapshot:3	Epoch:34	Loss:1.929	translation_Loss:0.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.074                                                   	MRR:21.79	Hits@10:41.05	Best:21.79
2024-12-27 23:52:20,184: Snapshot:3	Epoch:35	Loss:1.91	translation_Loss:0.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.067                                                   	MRR:21.76	Hits@10:41.01	Best:21.79
2024-12-27 23:52:32,235: Snapshot:3	Epoch:36	Loss:1.899	translation_Loss:0.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.061                                                   	MRR:21.79	Hits@10:40.92	Best:21.79
2024-12-27 23:52:44,441: Snapshot:3	Epoch:37	Loss:1.869	translation_Loss:0.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.054                                                   	MRR:21.82	Hits@10:40.99	Best:21.82
2024-12-27 23:52:57,071: Snapshot:3	Epoch:38	Loss:1.848	translation_Loss:0.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.048                                                   	MRR:21.9	Hits@10:40.98	Best:21.9
2024-12-27 23:53:09,185: Snapshot:3	Epoch:39	Loss:1.846	translation_Loss:0.804	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.042                                                   	MRR:21.87	Hits@10:41.09	Best:21.9
2024-12-27 23:53:21,677: Snapshot:3	Epoch:40	Loss:1.83	translation_Loss:0.792	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.038                                                   	MRR:21.9	Hits@10:41.07	Best:21.9
2024-12-27 23:53:33,828: Snapshot:3	Epoch:41	Loss:1.817	translation_Loss:0.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.032                                                   	MRR:21.99	Hits@10:41.03	Best:21.99
2024-12-27 23:53:46,066: Snapshot:3	Epoch:42	Loss:1.814	translation_Loss:0.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.027                                                   	MRR:22.01	Hits@10:41.1	Best:22.01
2024-12-27 23:53:58,703: Snapshot:3	Epoch:43	Loss:1.793	translation_Loss:0.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.022                                                   	MRR:21.98	Hits@10:41.16	Best:22.01
2024-12-27 23:54:10,781: Snapshot:3	Epoch:44	Loss:1.799	translation_Loss:0.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.018                                                   	MRR:21.91	Hits@10:41.13	Best:22.01
2024-12-27 23:54:22,958: Snapshot:3	Epoch:45	Loss:1.781	translation_Loss:0.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.014                                                   	MRR:21.93	Hits@10:41.12	Best:22.01
2024-12-27 23:54:35,390: Snapshot:3	Epoch:46	Loss:1.768	translation_Loss:0.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.01                                                   	MRR:21.95	Hits@10:41.32	Best:22.01
2024-12-27 23:54:47,452: Early Stopping! Snapshot: 3 Epoch: 47 Best Results: 22.01
2024-12-27 23:54:47,452: Start to training tokens! Snapshot: 3 Epoch: 47 Loss:1.766 MRR:21.91 Best Results: 22.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:54:47,453: Snapshot:3	Epoch:47	Loss:1.766	translation_Loss:0.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.007                                                   	MRR:21.91	Hits@10:41.24	Best:22.01
2024-12-27 23:54:59,338: Snapshot:3	Epoch:48	Loss:26.441	translation_Loss:25.544	multi_layer_Loss:0.897	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.91	Hits@10:41.24	Best:22.01
2024-12-27 23:55:11,721: End of token training: 3 Epoch: 49 Loss:25.857 MRR:21.91 Best Results: 22.01
2024-12-27 23:55:11,721: Snapshot:3	Epoch:49	Loss:25.857	translation_Loss:25.563	multi_layer_Loss:0.294	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.91	Hits@10:41.24	Best:22.01
2024-12-27 23:55:11,957: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_1000/3model_best.tar'
2024-12-27 23:55:25,643: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2352 | 0.1311 | 0.2844 | 0.3472 |  0.4273 |
|     1      | 0.2257 | 0.131  | 0.2659 | 0.3174 |  0.3996 |
|     2      | 0.1958 | 0.1033 | 0.2301 | 0.2902 |  0.3714 |
|     3      | 0.2196 | 0.119  | 0.261  | 0.3257 |  0.4094 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:55:44,121: Snapshot:4	Epoch:0	Loss:10.282	translation_Loss:10.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.006                                                   	MRR:7.96	Hits@10:16.15	Best:7.96
2024-12-27 23:55:49,129: Snapshot:4	Epoch:1	Loss:8.782	translation_Loss:8.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.02                                                   	MRR:8.48	Hits@10:17.29	Best:8.48
2024-12-27 23:55:54,205: Snapshot:4	Epoch:2	Loss:7.438	translation_Loss:7.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.04                                                   	MRR:8.93	Hits@10:18.25	Best:8.93
2024-12-27 23:55:59,240: Snapshot:4	Epoch:3	Loss:6.185	translation_Loss:6.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.067                                                   	MRR:9.77	Hits@10:20.69	Best:9.77
2024-12-27 23:56:04,235: Snapshot:4	Epoch:4	Loss:5.072	translation_Loss:4.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.098                                                   	MRR:12.36	Hits@10:27.71	Best:12.36
2024-12-27 23:56:09,228: Snapshot:4	Epoch:5	Loss:4.174	translation_Loss:4.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.132                                                   	MRR:14.56	Hits@10:31.26	Best:14.56
2024-12-27 23:56:14,233: Snapshot:4	Epoch:6	Loss:3.494	translation_Loss:3.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.166                                                   	MRR:16.06	Hits@10:32.62	Best:16.06
2024-12-27 23:56:19,243: Snapshot:4	Epoch:7	Loss:2.966	translation_Loss:2.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.2                                                   	MRR:16.95	Hits@10:33.78	Best:16.95
2024-12-27 23:56:24,218: Snapshot:4	Epoch:8	Loss:2.543	translation_Loss:2.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:17.7	Hits@10:34.58	Best:17.7
2024-12-27 23:56:29,252: Snapshot:4	Epoch:9	Loss:2.212	translation_Loss:1.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:18.04	Hits@10:35.18	Best:18.04
2024-12-27 23:56:34,255: Snapshot:4	Epoch:10	Loss:1.951	translation_Loss:1.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.291                                                   	MRR:18.38	Hits@10:35.59	Best:18.38
2024-12-27 23:56:39,292: Snapshot:4	Epoch:11	Loss:1.723	translation_Loss:1.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.317                                                   	MRR:18.53	Hits@10:35.8	Best:18.53
2024-12-27 23:56:44,771: Snapshot:4	Epoch:12	Loss:1.53	translation_Loss:1.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.34                                                   	MRR:18.81	Hits@10:36.17	Best:18.81
2024-12-27 23:56:49,809: Snapshot:4	Epoch:13	Loss:1.381	translation_Loss:1.021	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.36                                                   	MRR:19.26	Hits@10:36.26	Best:19.26
2024-12-27 23:56:54,871: Snapshot:4	Epoch:14	Loss:1.259	translation_Loss:0.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.378                                                   	MRR:19.51	Hits@10:36.27	Best:19.51
2024-12-27 23:56:59,923: Snapshot:4	Epoch:15	Loss:1.135	translation_Loss:0.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.392                                                   	MRR:19.57	Hits@10:36.21	Best:19.57
2024-12-27 23:57:04,891: Snapshot:4	Epoch:16	Loss:1.053	translation_Loss:0.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.404                                                   	MRR:19.59	Hits@10:36.41	Best:19.59
2024-12-27 23:57:09,871: Snapshot:4	Epoch:17	Loss:0.986	translation_Loss:0.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.413                                                   	MRR:19.54	Hits@10:36.55	Best:19.59
2024-12-27 23:57:14,842: Snapshot:4	Epoch:18	Loss:0.926	translation_Loss:0.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.42                                                   	MRR:19.65	Hits@10:36.59	Best:19.65
2024-12-27 23:57:20,272: Snapshot:4	Epoch:19	Loss:0.876	translation_Loss:0.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.424                                                   	MRR:19.91	Hits@10:36.57	Best:19.91
2024-12-27 23:57:25,232: Snapshot:4	Epoch:20	Loss:0.827	translation_Loss:0.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.428                                                   	MRR:19.91	Hits@10:36.73	Best:19.91
2024-12-27 23:57:30,238: Snapshot:4	Epoch:21	Loss:0.794	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:20.19	Hits@10:36.87	Best:20.19
2024-12-27 23:57:35,257: Snapshot:4	Epoch:22	Loss:0.758	translation_Loss:0.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.431                                                   	MRR:20.26	Hits@10:36.88	Best:20.26
2024-12-27 23:57:40,270: Snapshot:4	Epoch:23	Loss:0.738	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.431                                                   	MRR:20.27	Hits@10:36.77	Best:20.27
2024-12-27 23:57:45,273: Snapshot:4	Epoch:24	Loss:0.712	translation_Loss:0.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:20.46	Hits@10:37.13	Best:20.46
2024-12-27 23:57:50,291: Snapshot:4	Epoch:25	Loss:0.688	translation_Loss:0.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.429                                                   	MRR:20.64	Hits@10:37.16	Best:20.64
2024-12-27 23:57:55,629: Snapshot:4	Epoch:26	Loss:0.67	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.427                                                   	MRR:20.63	Hits@10:37.02	Best:20.64
2024-12-27 23:58:00,577: Snapshot:4	Epoch:27	Loss:0.657	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.424                                                   	MRR:20.62	Hits@10:36.8	Best:20.64
2024-12-27 23:58:05,567: Snapshot:4	Epoch:28	Loss:0.639	translation_Loss:0.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.422                                                   	MRR:20.71	Hits@10:36.9	Best:20.71
2024-12-27 23:58:10,620: Snapshot:4	Epoch:29	Loss:0.627	translation_Loss:0.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.42                                                   	MRR:20.79	Hits@10:36.88	Best:20.79
2024-12-27 23:58:15,602: Snapshot:4	Epoch:30	Loss:0.614	translation_Loss:0.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.417                                                   	MRR:20.9	Hits@10:37.03	Best:20.9
2024-12-27 23:58:20,577: Snapshot:4	Epoch:31	Loss:0.597	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.414                                                   	MRR:20.9	Hits@10:37.3	Best:20.9
2024-12-27 23:58:25,591: Snapshot:4	Epoch:32	Loss:0.587	translation_Loss:0.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.411                                                   	MRR:20.98	Hits@10:37.5	Best:20.98
2024-12-27 23:58:30,934: Snapshot:4	Epoch:33	Loss:0.575	translation_Loss:0.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.408                                                   	MRR:20.98	Hits@10:37.55	Best:20.98
2024-12-27 23:58:35,920: Snapshot:4	Epoch:34	Loss:0.575	translation_Loss:0.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.405                                                   	MRR:21.05	Hits@10:37.65	Best:21.05
2024-12-27 23:58:40,858: Snapshot:4	Epoch:35	Loss:0.559	translation_Loss:0.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.402                                                   	MRR:20.95	Hits@10:37.43	Best:21.05
2024-12-27 23:58:45,870: Snapshot:4	Epoch:36	Loss:0.551	translation_Loss:0.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:21.19	Hits@10:37.4	Best:21.19
2024-12-27 23:58:50,849: Snapshot:4	Epoch:37	Loss:0.545	translation_Loss:0.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.395                                                   	MRR:21.29	Hits@10:37.6	Best:21.29
2024-12-27 23:58:55,892: Snapshot:4	Epoch:38	Loss:0.535	translation_Loss:0.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.392                                                   	MRR:21.47	Hits@10:37.6	Best:21.47
2024-12-27 23:59:00,887: Snapshot:4	Epoch:39	Loss:0.533	translation_Loss:0.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:21.56	Hits@10:37.79	Best:21.56
2024-12-27 23:59:06,353: Snapshot:4	Epoch:40	Loss:0.524	translation_Loss:0.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.386                                                   	MRR:21.64	Hits@10:37.78	Best:21.64
2024-12-27 23:59:11,358: Snapshot:4	Epoch:41	Loss:0.518	translation_Loss:0.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.383                                                   	MRR:21.66	Hits@10:37.78	Best:21.66
2024-12-27 23:59:16,339: Snapshot:4	Epoch:42	Loss:0.514	translation_Loss:0.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.38                                                   	MRR:21.71	Hits@10:37.92	Best:21.71
2024-12-27 23:59:21,352: Snapshot:4	Epoch:43	Loss:0.51	translation_Loss:0.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:21.84	Hits@10:38.1	Best:21.84
2024-12-27 23:59:26,355: Snapshot:4	Epoch:44	Loss:0.506	translation_Loss:0.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.375                                                   	MRR:21.82	Hits@10:37.99	Best:21.84
2024-12-27 23:59:31,348: Snapshot:4	Epoch:45	Loss:0.499	translation_Loss:0.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.372                                                   	MRR:21.84	Hits@10:37.96	Best:21.84
2024-12-27 23:59:36,398: Snapshot:4	Epoch:46	Loss:0.491	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.369                                                   	MRR:21.93	Hits@10:38.15	Best:21.93
2024-12-27 23:59:41,729: Snapshot:4	Epoch:47	Loss:0.487	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.366                                                   	MRR:21.94	Hits@10:38.2	Best:21.94
2024-12-27 23:59:46,793: Snapshot:4	Epoch:48	Loss:0.489	translation_Loss:0.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.364                                                   	MRR:21.97	Hits@10:38.4	Best:21.97
2024-12-27 23:59:51,843: Snapshot:4	Epoch:49	Loss:0.478	translation_Loss:0.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.361                                                   	MRR:22.04	Hits@10:38.48	Best:22.04
2024-12-27 23:59:56,821: Snapshot:4	Epoch:50	Loss:0.476	translation_Loss:0.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.358                                                   	MRR:22.11	Hits@10:38.31	Best:22.11
2024-12-28 00:00:01,808: Snapshot:4	Epoch:51	Loss:0.47	translation_Loss:0.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.356                                                   	MRR:22.16	Hits@10:38.4	Best:22.16
2024-12-28 00:00:06,845: Snapshot:4	Epoch:52	Loss:0.472	translation_Loss:0.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.353                                                   	MRR:22.21	Hits@10:38.59	Best:22.21
2024-12-28 00:00:11,825: Snapshot:4	Epoch:53	Loss:0.465	translation_Loss:0.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.351                                                   	MRR:22.13	Hits@10:38.61	Best:22.21
2024-12-28 00:00:17,212: Snapshot:4	Epoch:54	Loss:0.456	translation_Loss:0.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.349                                                   	MRR:22.44	Hits@10:38.72	Best:22.44
2024-12-28 00:00:22,233: Snapshot:4	Epoch:55	Loss:0.455	translation_Loss:0.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:22.28	Hits@10:38.69	Best:22.44
2024-12-28 00:00:27,208: Snapshot:4	Epoch:56	Loss:0.451	translation_Loss:0.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.343                                                   	MRR:22.23	Hits@10:38.7	Best:22.44
2024-12-28 00:00:32,127: Snapshot:4	Epoch:57	Loss:0.453	translation_Loss:0.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.341                                                   	MRR:22.24	Hits@10:38.5	Best:22.44
2024-12-28 00:00:37,038: Snapshot:4	Epoch:58	Loss:0.442	translation_Loss:0.104	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.338                                                   	MRR:22.25	Hits@10:38.35	Best:22.44
2024-12-28 00:00:41,935: Early Stopping! Snapshot: 4 Epoch: 59 Best Results: 22.44
2024-12-28 00:00:41,936: Start to training tokens! Snapshot: 4 Epoch: 59 Loss:0.443 MRR:22.4 Best Results: 22.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:00:41,936: Snapshot:4	Epoch:59	Loss:0.443	translation_Loss:0.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.336                                                   	MRR:22.4	Hits@10:38.53	Best:22.44
2024-12-28 00:00:46,772: Snapshot:4	Epoch:60	Loss:11.563	translation_Loss:11.116	multi_layer_Loss:0.447	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.4	Hits@10:38.53	Best:22.44
2024-12-28 00:00:52,015: End of token training: 4 Epoch: 61 Loss:11.46 MRR:22.4 Best Results: 22.44
2024-12-28 00:00:52,015: Snapshot:4	Epoch:61	Loss:11.46	translation_Loss:11.121	multi_layer_Loss:0.339	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.4	Hits@10:38.53	Best:22.44
2024-12-28 00:00:52,264: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_1000/4model_best.tar'
2024-12-28 00:01:08,311: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1784 | 0.0917 | 0.2139 | 0.2672 |  0.3392 |
|     1      | 0.1905 | 0.1054 | 0.2192 | 0.2705 |  0.3538 |
|     2      | 0.1657 | 0.0874 | 0.1906 | 0.2417 |  0.3148 |
|     3      | 0.1651 | 0.081  | 0.1935 | 0.2484 |  0.3247 |
|     4      | 0.2218 | 0.1353 | 0.2533 | 0.3072 |  0.3827 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 00:01:08,313: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.255 | 0.1456 | 0.3152 | 0.3777 |  0.4518 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2397 | 0.1335 | 0.2944 | 0.3604 |  0.4378 |
|     1      | 0.2689 | 0.1628 |  0.32  | 0.3805 |  0.4663 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2356 | 0.1251 | 0.2905 | 0.3567 |  0.4375 |
|     1      | 0.237  | 0.1298 | 0.2887 | 0.3508 |  0.4292 |
|     2      | 0.2168 | 0.1253 | 0.2501 | 0.3139 |  0.3977 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2352 | 0.1311 | 0.2844 | 0.3472 |  0.4273 |
|     1      | 0.2257 | 0.131  | 0.2659 | 0.3174 |  0.3996 |
|     2      | 0.1958 | 0.1033 | 0.2301 | 0.2902 |  0.3714 |
|     3      | 0.2196 | 0.119  | 0.261  | 0.3257 |  0.4094 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1784 | 0.0917 | 0.2139 | 0.2672 |  0.3392 |
|     1      | 0.1905 | 0.1054 | 0.2192 | 0.2705 |  0.3538 |
|     2      | 0.1657 | 0.0874 | 0.1906 | 0.2417 |  0.3148 |
|     3      | 0.1651 | 0.081  | 0.1935 | 0.2484 |  0.3247 |
|     4      | 0.2218 | 0.1353 | 0.2533 | 0.3072 |  0.3827 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 00:01:08,314: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 370.8527057170868  |   0.255   |    0.146     |    0.315     |     0.452     |
|    1     | 196.06876301765442 |   0.247   |    0.141     |    0.301     |     0.445     |
|    2     | 432.9136724472046  |   0.226   |    0.126     |    0.269     |     0.415     |
|    3     | 635.5105495452881  |   0.216   |    0.117     |    0.256     |      0.4      |
|    4     | 323.8502199649811  |   0.177   |    0.094     |    0.206     |     0.334     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 00:01:08,314: Sum_Training_Time:1959.195910692215
2024-12-28 00:01:08,314: Every_Training_Time:[370.8527057170868, 196.06876301765442, 432.9136724472046, 635.5105495452881, 323.8502199649811]
2024-12-28 00:01:08,314: Forward transfer: 0.046425 Backward transfer: -0.06514999999999999
2024-12-28 00:01:47,669: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241228000114/HYBRIDHYBRID_0.0001_2048_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_2048_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_2048_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 00:01:57,168: Snapshot:0	Epoch:0	Loss:24.333	translation_Loss:24.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.78	Hits@10:0.9	Best:0.78
2024-12-28 00:02:03,055: Snapshot:0	Epoch:1	Loss:23.173	translation_Loss:23.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.53	Hits@10:2.9	Best:1.53
2024-12-28 00:02:09,290: Snapshot:0	Epoch:2	Loss:22.122	translation_Loss:22.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.08	Hits@10:6.68	Best:3.08
2024-12-28 00:02:15,153: Snapshot:0	Epoch:3	Loss:21.1	translation_Loss:21.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:9.98	Best:4.54
2024-12-28 00:02:21,068: Snapshot:0	Epoch:4	Loss:20.098	translation_Loss:20.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:13.19	Best:5.66
2024-12-28 00:02:26,962: Snapshot:0	Epoch:5	Loss:19.123	translation_Loss:19.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.56	Hits@10:15.81	Best:6.56
2024-12-28 00:02:32,802: Snapshot:0	Epoch:6	Loss:18.14	translation_Loss:18.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.35	Hits@10:17.83	Best:7.35
2024-12-28 00:02:39,076: Snapshot:0	Epoch:7	Loss:17.186	translation_Loss:17.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:19.92	Best:8.1
2024-12-28 00:02:45,001: Snapshot:0	Epoch:8	Loss:16.222	translation_Loss:16.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.92	Hits@10:22.11	Best:8.92
2024-12-28 00:02:50,884: Snapshot:0	Epoch:9	Loss:15.277	translation_Loss:15.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.84	Hits@10:24.47	Best:9.84
2024-12-28 00:02:56,728: Snapshot:0	Epoch:10	Loss:14.324	translation_Loss:14.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:27.01	Best:10.83
2024-12-28 00:03:02,953: Snapshot:0	Epoch:11	Loss:13.368	translation_Loss:13.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.9	Hits@10:29.48	Best:11.9
2024-12-28 00:03:08,874: Snapshot:0	Epoch:12	Loss:12.388	translation_Loss:12.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.96	Hits@10:31.58	Best:12.96
2024-12-28 00:03:14,796: Snapshot:0	Epoch:13	Loss:11.425	translation_Loss:11.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.1	Hits@10:33.61	Best:14.1
2024-12-28 00:03:20,751: Snapshot:0	Epoch:14	Loss:10.479	translation_Loss:10.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.21	Hits@10:35.19	Best:15.21
2024-12-28 00:03:26,606: Snapshot:0	Epoch:15	Loss:9.541	translation_Loss:9.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.19	Hits@10:36.62	Best:16.19
2024-12-28 00:03:32,894: Snapshot:0	Epoch:16	Loss:8.663	translation_Loss:8.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.23	Hits@10:37.8	Best:17.23
2024-12-28 00:03:38,792: Snapshot:0	Epoch:17	Loss:7.841	translation_Loss:7.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.28	Hits@10:38.9	Best:18.28
2024-12-28 00:03:44,631: Snapshot:0	Epoch:18	Loss:7.081	translation_Loss:7.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.12	Hits@10:39.89	Best:19.12
2024-12-28 00:03:50,538: Snapshot:0	Epoch:19	Loss:6.404	translation_Loss:6.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.92	Hits@10:40.65	Best:19.92
2024-12-28 00:03:56,398: Snapshot:0	Epoch:20	Loss:5.775	translation_Loss:5.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.59	Hits@10:41.32	Best:20.59
2024-12-28 00:04:02,644: Snapshot:0	Epoch:21	Loss:5.224	translation_Loss:5.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.22	Hits@10:41.97	Best:21.22
2024-12-28 00:04:08,465: Snapshot:0	Epoch:22	Loss:4.736	translation_Loss:4.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.73	Hits@10:42.52	Best:21.73
2024-12-28 00:04:14,368: Snapshot:0	Epoch:23	Loss:4.295	translation_Loss:4.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.19	Hits@10:42.9	Best:22.19
2024-12-28 00:04:20,280: Snapshot:0	Epoch:24	Loss:3.914	translation_Loss:3.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.6	Hits@10:43.29	Best:22.6
2024-12-28 00:04:26,180: Snapshot:0	Epoch:25	Loss:3.532	translation_Loss:3.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.95	Hits@10:43.68	Best:22.95
2024-12-28 00:04:32,516: Snapshot:0	Epoch:26	Loss:3.234	translation_Loss:3.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:43.98	Best:23.22
2024-12-28 00:04:38,452: Snapshot:0	Epoch:27	Loss:2.949	translation_Loss:2.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:44.28	Best:23.43
2024-12-28 00:04:44,380: Snapshot:0	Epoch:28	Loss:2.686	translation_Loss:2.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:44.53	Best:23.71
2024-12-28 00:04:50,265: Snapshot:0	Epoch:29	Loss:2.481	translation_Loss:2.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.9	Hits@10:44.72	Best:23.9
2024-12-28 00:04:56,207: Snapshot:0	Epoch:30	Loss:2.283	translation_Loss:2.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.09	Hits@10:44.85	Best:24.09
2024-12-28 00:05:02,510: Snapshot:0	Epoch:31	Loss:2.091	translation_Loss:2.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:44.95	Best:24.22
2024-12-28 00:05:08,438: Snapshot:0	Epoch:32	Loss:1.938	translation_Loss:1.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:44.97	Best:24.34
2024-12-28 00:05:14,362: Snapshot:0	Epoch:33	Loss:1.807	translation_Loss:1.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:45.11	Best:24.41
2024-12-28 00:05:20,321: Snapshot:0	Epoch:34	Loss:1.686	translation_Loss:1.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:45.1	Best:24.59
2024-12-28 00:05:26,179: Snapshot:0	Epoch:35	Loss:1.57	translation_Loss:1.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.6	Hits@10:45.2	Best:24.6
2024-12-28 00:05:32,481: Snapshot:0	Epoch:36	Loss:1.464	translation_Loss:1.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.73	Hits@10:45.33	Best:24.73
2024-12-28 00:05:38,421: Snapshot:0	Epoch:37	Loss:1.373	translation_Loss:1.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:45.37	Best:24.77
2024-12-28 00:05:44,378: Snapshot:0	Epoch:38	Loss:1.295	translation_Loss:1.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:45.48	Best:24.88
2024-12-28 00:05:50,288: Snapshot:0	Epoch:39	Loss:1.219	translation_Loss:1.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:45.51	Best:24.95
2024-12-28 00:05:56,123: Snapshot:0	Epoch:40	Loss:1.149	translation_Loss:1.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:45.57	Best:25.04
2024-12-28 00:06:02,439: Snapshot:0	Epoch:41	Loss:1.087	translation_Loss:1.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.08	Hits@10:45.65	Best:25.08
2024-12-28 00:06:08,300: Snapshot:0	Epoch:42	Loss:1.031	translation_Loss:1.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.12	Hits@10:45.7	Best:25.12
2024-12-28 00:06:14,199: Snapshot:0	Epoch:43	Loss:0.977	translation_Loss:0.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.16	Hits@10:45.76	Best:25.16
2024-12-28 00:06:20,118: Snapshot:0	Epoch:44	Loss:0.93	translation_Loss:0.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.23	Hits@10:45.81	Best:25.23
2024-12-28 00:06:26,054: Snapshot:0	Epoch:45	Loss:0.892	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.23	Hits@10:45.87	Best:25.23
2024-12-28 00:06:32,279: Snapshot:0	Epoch:46	Loss:0.848	translation_Loss:0.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:45.91	Best:25.31
2024-12-28 00:06:38,149: Snapshot:0	Epoch:47	Loss:0.814	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:46.02	Best:25.33
2024-12-28 00:06:43,981: Snapshot:0	Epoch:48	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:46.13	Best:25.33
2024-12-28 00:06:49,810: Snapshot:0	Epoch:49	Loss:0.742	translation_Loss:0.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:46.02	Best:25.34
2024-12-28 00:06:56,077: Snapshot:0	Epoch:50	Loss:0.717	translation_Loss:0.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.06	Best:25.35
2024-12-28 00:07:01,951: Snapshot:0	Epoch:51	Loss:0.676	translation_Loss:0.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.07	Best:25.35
2024-12-28 00:07:07,773: Snapshot:0	Epoch:52	Loss:0.664	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.42	Hits@10:46.14	Best:25.42
2024-12-28 00:07:13,648: Snapshot:0	Epoch:53	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.44	Hits@10:46.15	Best:25.44
2024-12-28 00:07:19,509: Snapshot:0	Epoch:54	Loss:0.615	translation_Loss:0.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.4	Hits@10:46.17	Best:25.44
2024-12-28 00:07:25,717: Snapshot:0	Epoch:55	Loss:0.596	translation_Loss:0.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.37	Hits@10:46.17	Best:25.44
2024-12-28 00:07:31,512: Snapshot:0	Epoch:56	Loss:0.571	translation_Loss:0.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.4	Hits@10:46.16	Best:25.44
2024-12-28 00:07:37,334: Snapshot:0	Epoch:57	Loss:0.549	translation_Loss:0.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.42	Hits@10:46.31	Best:25.44
2024-12-28 00:07:43,203: Early Stopping! Snapshot: 0 Epoch: 58 Best Results: 25.44
2024-12-28 00:07:43,203: Start to training tokens! Snapshot: 0 Epoch: 58 Loss:0.537 MRR:25.38 Best Results: 25.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:07:43,204: Snapshot:0	Epoch:58	Loss:0.537	translation_Loss:0.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.38	Hits@10:46.33	Best:25.44
2024-12-28 00:07:49,578: Snapshot:0	Epoch:59	Loss:17.953	translation_Loss:17.367	multi_layer_Loss:0.586	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.38	Hits@10:46.33	Best:25.44
2024-12-28 00:07:55,934: End of token training: 0 Epoch: 60 Loss:17.729 MRR:25.38 Best Results: 25.44
2024-12-28 00:07:55,934: Snapshot:0	Epoch:60	Loss:17.729	translation_Loss:17.361	multi_layer_Loss:0.368	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.38	Hits@10:46.33	Best:25.44
2024-12-28 00:07:56,210: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_5000/0model_best.tar'
2024-12-28 00:07:58,821: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2552 | 0.1463 | 0.3148 | 0.3785 |  0.4516 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:08:09,885: Snapshot:1	Epoch:0	Loss:7.909	translation_Loss:7.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.009                                                   	MRR:5.97	Hits@10:10.17	Best:5.97
2024-12-28 00:08:12,164: Snapshot:1	Epoch:1	Loss:7.335	translation_Loss:7.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.028                                                   	MRR:6.48	Hits@10:10.85	Best:6.48
2024-12-28 00:08:14,481: Snapshot:1	Epoch:2	Loss:6.849	translation_Loss:6.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.051                                                   	MRR:6.94	Hits@10:11.64	Best:6.94
2024-12-28 00:08:16,899: Snapshot:1	Epoch:3	Loss:6.369	translation_Loss:6.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.078                                                   	MRR:7.57	Hits@10:12.9	Best:7.57
2024-12-28 00:08:19,218: Snapshot:1	Epoch:4	Loss:5.922	translation_Loss:5.813	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.109                                                   	MRR:8.31	Hits@10:14.37	Best:8.31
2024-12-28 00:08:21,567: Snapshot:1	Epoch:5	Loss:5.48	translation_Loss:5.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:9.24	Hits@10:16.9	Best:9.24
2024-12-28 00:08:23,884: Snapshot:1	Epoch:6	Loss:5.069	translation_Loss:4.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:10.37	Hits@10:19.43	Best:10.37
2024-12-28 00:08:26,190: Snapshot:1	Epoch:7	Loss:4.663	translation_Loss:4.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:11.57	Hits@10:21.92	Best:11.57
2024-12-28 00:08:28,587: Snapshot:1	Epoch:8	Loss:4.284	translation_Loss:4.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.25                                                   	MRR:12.79	Hits@10:25.05	Best:12.79
2024-12-28 00:08:30,916: Snapshot:1	Epoch:9	Loss:3.921	translation_Loss:3.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.285                                                   	MRR:14.01	Hits@10:27.46	Best:14.01
2024-12-28 00:08:33,245: Snapshot:1	Epoch:10	Loss:3.587	translation_Loss:3.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.318                                                   	MRR:15.22	Hits@10:29.43	Best:15.22
2024-12-28 00:08:35,520: Snapshot:1	Epoch:11	Loss:3.294	translation_Loss:2.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.347                                                   	MRR:16.26	Hits@10:30.96	Best:16.26
2024-12-28 00:08:37,859: Snapshot:1	Epoch:12	Loss:3.032	translation_Loss:2.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.372                                                   	MRR:17.27	Hits@10:32.35	Best:17.27
2024-12-28 00:08:40,149: Snapshot:1	Epoch:13	Loss:2.799	translation_Loss:2.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.394                                                   	MRR:18.18	Hits@10:33.63	Best:18.18
2024-12-28 00:08:42,447: Snapshot:1	Epoch:14	Loss:2.588	translation_Loss:2.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.413                                                   	MRR:18.89	Hits@10:34.72	Best:18.89
2024-12-28 00:08:44,747: Snapshot:1	Epoch:15	Loss:2.408	translation_Loss:1.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:19.5	Hits@10:35.53	Best:19.5
2024-12-28 00:08:47,048: Snapshot:1	Epoch:16	Loss:2.251	translation_Loss:1.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.443                                                   	MRR:20.08	Hits@10:36.22	Best:20.08
2024-12-28 00:08:49,385: Snapshot:1	Epoch:17	Loss:2.104	translation_Loss:1.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.454                                                   	MRR:20.57	Hits@10:37.04	Best:20.57
2024-12-28 00:08:51,727: Snapshot:1	Epoch:18	Loss:1.973	translation_Loss:1.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.463                                                   	MRR:21.1	Hits@10:37.71	Best:21.1
2024-12-28 00:08:54,345: Snapshot:1	Epoch:19	Loss:1.865	translation_Loss:1.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.469                                                   	MRR:21.55	Hits@10:38.24	Best:21.55
2024-12-28 00:08:56,662: Snapshot:1	Epoch:20	Loss:1.77	translation_Loss:1.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.474                                                   	MRR:21.9	Hits@10:38.75	Best:21.9
2024-12-28 00:08:58,948: Snapshot:1	Epoch:21	Loss:1.674	translation_Loss:1.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.478                                                   	MRR:22.29	Hits@10:39.25	Best:22.29
2024-12-28 00:09:01,228: Snapshot:1	Epoch:22	Loss:1.593	translation_Loss:1.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.48                                                   	MRR:22.71	Hits@10:39.77	Best:22.71
2024-12-28 00:09:03,519: Snapshot:1	Epoch:23	Loss:1.522	translation_Loss:1.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.482                                                   	MRR:23.04	Hits@10:40.28	Best:23.04
2024-12-28 00:09:05,796: Snapshot:1	Epoch:24	Loss:1.453	translation_Loss:0.972	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.481                                                   	MRR:23.27	Hits@10:40.68	Best:23.27
2024-12-28 00:09:08,096: Snapshot:1	Epoch:25	Loss:1.397	translation_Loss:0.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.481                                                   	MRR:23.5	Hits@10:40.96	Best:23.5
2024-12-28 00:09:10,405: Snapshot:1	Epoch:26	Loss:1.34	translation_Loss:0.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.479                                                   	MRR:23.79	Hits@10:41.44	Best:23.79
2024-12-28 00:09:12,691: Snapshot:1	Epoch:27	Loss:1.291	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.477                                                   	MRR:23.94	Hits@10:41.69	Best:23.94
2024-12-28 00:09:14,962: Snapshot:1	Epoch:28	Loss:1.249	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:24.13	Hits@10:42.19	Best:24.13
2024-12-28 00:09:17,269: Snapshot:1	Epoch:29	Loss:1.208	translation_Loss:0.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.472                                                   	MRR:24.34	Hits@10:42.68	Best:24.34
2024-12-28 00:09:19,539: Snapshot:1	Epoch:30	Loss:1.172	translation_Loss:0.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.469                                                   	MRR:24.56	Hits@10:43.04	Best:24.56
2024-12-28 00:09:21,799: Snapshot:1	Epoch:31	Loss:1.132	translation_Loss:0.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.466                                                   	MRR:24.75	Hits@10:43.31	Best:24.75
2024-12-28 00:09:24,515: Snapshot:1	Epoch:32	Loss:1.102	translation_Loss:0.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:24.97	Hits@10:43.66	Best:24.97
2024-12-28 00:09:26,817: Snapshot:1	Epoch:33	Loss:1.074	translation_Loss:0.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.459                                                   	MRR:25.15	Hits@10:44.04	Best:25.15
2024-12-28 00:09:29,101: Snapshot:1	Epoch:34	Loss:1.046	translation_Loss:0.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.456                                                   	MRR:25.36	Hits@10:44.26	Best:25.36
2024-12-28 00:09:31,382: Snapshot:1	Epoch:35	Loss:1.022	translation_Loss:0.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.452                                                   	MRR:25.51	Hits@10:44.44	Best:25.51
2024-12-28 00:09:33,669: Snapshot:1	Epoch:36	Loss:0.998	translation_Loss:0.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.449                                                   	MRR:25.66	Hits@10:44.68	Best:25.66
2024-12-28 00:09:35,931: Snapshot:1	Epoch:37	Loss:0.979	translation_Loss:0.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.445                                                   	MRR:25.8	Hits@10:45.07	Best:25.8
2024-12-28 00:09:38,221: Snapshot:1	Epoch:38	Loss:0.954	translation_Loss:0.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.442                                                   	MRR:25.94	Hits@10:45.29	Best:25.94
2024-12-28 00:09:40,529: Snapshot:1	Epoch:39	Loss:0.932	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.439                                                   	MRR:26.09	Hits@10:45.62	Best:26.09
2024-12-28 00:09:42,952: Snapshot:1	Epoch:40	Loss:0.916	translation_Loss:0.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.435                                                   	MRR:26.25	Hits@10:45.83	Best:26.25
2024-12-28 00:09:45,228: Snapshot:1	Epoch:41	Loss:0.907	translation_Loss:0.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.432                                                   	MRR:26.35	Hits@10:46.04	Best:26.35
2024-12-28 00:09:47,495: Snapshot:1	Epoch:42	Loss:0.887	translation_Loss:0.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.428                                                   	MRR:26.47	Hits@10:46.29	Best:26.47
2024-12-28 00:09:49,792: Snapshot:1	Epoch:43	Loss:0.877	translation_Loss:0.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.425                                                   	MRR:26.61	Hits@10:46.49	Best:26.61
2024-12-28 00:09:52,060: Snapshot:1	Epoch:44	Loss:0.861	translation_Loss:0.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.422                                                   	MRR:26.7	Hits@10:46.57	Best:26.7
2024-12-28 00:09:54,363: Snapshot:1	Epoch:45	Loss:0.852	translation_Loss:0.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.419                                                   	MRR:26.76	Hits@10:46.81	Best:26.76
2024-12-28 00:09:57,048: Snapshot:1	Epoch:46	Loss:0.835	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.416                                                   	MRR:26.87	Hits@10:47.15	Best:26.87
2024-12-28 00:09:59,362: Snapshot:1	Epoch:47	Loss:0.819	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.413                                                   	MRR:26.89	Hits@10:47.4	Best:26.89
2024-12-28 00:10:01,661: Snapshot:1	Epoch:48	Loss:0.81	translation_Loss:0.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.41                                                   	MRR:26.97	Hits@10:47.65	Best:26.97
2024-12-28 00:10:04,033: Snapshot:1	Epoch:49	Loss:0.802	translation_Loss:0.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.407                                                   	MRR:27.0	Hits@10:47.65	Best:27.0
2024-12-28 00:10:06,334: Snapshot:1	Epoch:50	Loss:0.794	translation_Loss:0.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.404                                                   	MRR:27.07	Hits@10:47.69	Best:27.07
2024-12-28 00:10:08,667: Snapshot:1	Epoch:51	Loss:0.778	translation_Loss:0.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:27.21	Hits@10:47.81	Best:27.21
2024-12-28 00:10:11,036: Snapshot:1	Epoch:52	Loss:0.77	translation_Loss:0.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:27.22	Hits@10:48.17	Best:27.22
2024-12-28 00:10:13,348: Snapshot:1	Epoch:53	Loss:0.765	translation_Loss:0.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:27.35	Hits@10:48.36	Best:27.35
2024-12-28 00:10:15,660: Snapshot:1	Epoch:54	Loss:0.753	translation_Loss:0.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.393                                                   	MRR:27.49	Hits@10:48.56	Best:27.49
2024-12-28 00:10:17,972: Snapshot:1	Epoch:55	Loss:0.747	translation_Loss:0.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:27.51	Hits@10:48.66	Best:27.51
2024-12-28 00:10:20,237: Snapshot:1	Epoch:56	Loss:0.735	translation_Loss:0.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.388                                                   	MRR:27.57	Hits@10:48.62	Best:27.57
2024-12-28 00:10:22,538: Snapshot:1	Epoch:57	Loss:0.733	translation_Loss:0.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.386                                                   	MRR:27.68	Hits@10:48.72	Best:27.68
2024-12-28 00:10:24,852: Snapshot:1	Epoch:58	Loss:0.718	translation_Loss:0.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.384                                                   	MRR:27.75	Hits@10:48.97	Best:27.75
2024-12-28 00:10:27,481: Snapshot:1	Epoch:59	Loss:0.718	translation_Loss:0.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.381                                                   	MRR:27.8	Hits@10:49.19	Best:27.8
2024-12-28 00:10:29,751: Snapshot:1	Epoch:60	Loss:0.707	translation_Loss:0.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.379                                                   	MRR:27.94	Hits@10:49.22	Best:27.94
2024-12-28 00:10:32,009: Snapshot:1	Epoch:61	Loss:0.7	translation_Loss:0.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.376                                                   	MRR:27.97	Hits@10:49.47	Best:27.97
2024-12-28 00:10:34,288: Snapshot:1	Epoch:62	Loss:0.689	translation_Loss:0.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.374                                                   	MRR:28.05	Hits@10:49.8	Best:28.05
2024-12-28 00:10:36,606: Snapshot:1	Epoch:63	Loss:0.685	translation_Loss:0.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.371                                                   	MRR:28.07	Hits@10:49.75	Best:28.07
2024-12-28 00:10:38,959: Snapshot:1	Epoch:64	Loss:0.685	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.369                                                   	MRR:28.08	Hits@10:49.9	Best:28.08
2024-12-28 00:10:41,245: Snapshot:1	Epoch:65	Loss:0.671	translation_Loss:0.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.367                                                   	MRR:28.07	Hits@10:49.93	Best:28.08
2024-12-28 00:10:43,539: Snapshot:1	Epoch:66	Loss:0.671	translation_Loss:0.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.365                                                   	MRR:28.2	Hits@10:50.04	Best:28.2
2024-12-28 00:10:45,866: Snapshot:1	Epoch:67	Loss:0.661	translation_Loss:0.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.363                                                   	MRR:28.21	Hits@10:49.98	Best:28.21
2024-12-28 00:10:48,171: Snapshot:1	Epoch:68	Loss:0.656	translation_Loss:0.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.361                                                   	MRR:28.22	Hits@10:50.12	Best:28.22
2024-12-28 00:10:50,463: Snapshot:1	Epoch:69	Loss:0.655	translation_Loss:0.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.359                                                   	MRR:28.38	Hits@10:50.05	Best:28.38
2024-12-28 00:10:52,763: Snapshot:1	Epoch:70	Loss:0.64	translation_Loss:0.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.357                                                   	MRR:28.43	Hits@10:49.99	Best:28.43
2024-12-28 00:10:55,018: Snapshot:1	Epoch:71	Loss:0.644	translation_Loss:0.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:28.42	Hits@10:50.11	Best:28.43
2024-12-28 00:10:57,313: Snapshot:1	Epoch:72	Loss:0.638	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.353                                                   	MRR:28.5	Hits@10:50.13	Best:28.5
2024-12-28 00:10:59,989: Snapshot:1	Epoch:73	Loss:0.636	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:28.44	Hits@10:50.12	Best:28.5
2024-12-28 00:11:02,241: Snapshot:1	Epoch:74	Loss:0.625	translation_Loss:0.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.35                                                   	MRR:28.57	Hits@10:50.23	Best:28.57
2024-12-28 00:11:04,501: Snapshot:1	Epoch:75	Loss:0.625	translation_Loss:0.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.348                                                   	MRR:28.56	Hits@10:50.25	Best:28.57
2024-12-28 00:11:06,759: Snapshot:1	Epoch:76	Loss:0.616	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.347                                                   	MRR:28.64	Hits@10:50.44	Best:28.64
2024-12-28 00:11:09,043: Snapshot:1	Epoch:77	Loss:0.616	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.345                                                   	MRR:28.56	Hits@10:50.43	Best:28.64
2024-12-28 00:11:11,307: Snapshot:1	Epoch:78	Loss:0.613	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.343                                                   	MRR:28.62	Hits@10:50.6	Best:28.64
2024-12-28 00:11:13,543: Snapshot:1	Epoch:79	Loss:0.605	translation_Loss:0.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:28.63	Hits@10:50.63	Best:28.64
2024-12-28 00:11:15,814: Snapshot:1	Epoch:80	Loss:0.604	translation_Loss:0.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.34                                                   	MRR:28.63	Hits@10:50.6	Best:28.64
2024-12-28 00:11:18,128: Snapshot:1	Epoch:81	Loss:0.599	translation_Loss:0.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.338                                                   	MRR:28.75	Hits@10:50.63	Best:28.75
2024-12-28 00:11:20,398: Snapshot:1	Epoch:82	Loss:0.596	translation_Loss:0.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.337                                                   	MRR:28.87	Hits@10:50.63	Best:28.87
2024-12-28 00:11:22,688: Snapshot:1	Epoch:83	Loss:0.593	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.335                                                   	MRR:28.91	Hits@10:50.89	Best:28.91
2024-12-28 00:11:24,994: Snapshot:1	Epoch:84	Loss:0.587	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:28.95	Hits@10:50.78	Best:28.95
2024-12-28 00:11:27,255: Snapshot:1	Epoch:85	Loss:0.581	translation_Loss:0.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.332                                                   	MRR:28.88	Hits@10:50.92	Best:28.95
2024-12-28 00:11:29,525: Snapshot:1	Epoch:86	Loss:0.576	translation_Loss:0.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.33                                                   	MRR:28.89	Hits@10:50.95	Best:28.95
2024-12-28 00:11:32,097: Snapshot:1	Epoch:87	Loss:0.584	translation_Loss:0.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:28.94	Hits@10:50.95	Best:28.95
2024-12-28 00:11:34,367: Snapshot:1	Epoch:88	Loss:0.576	translation_Loss:0.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:29.05	Hits@10:50.97	Best:29.05
2024-12-28 00:11:36,644: Snapshot:1	Epoch:89	Loss:0.571	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.327                                                   	MRR:29.03	Hits@10:51.04	Best:29.05
2024-12-28 00:11:38,923: Snapshot:1	Epoch:90	Loss:0.566	translation_Loss:0.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.325                                                   	MRR:29.07	Hits@10:51.08	Best:29.07
2024-12-28 00:11:41,235: Snapshot:1	Epoch:91	Loss:0.567	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.324                                                   	MRR:29.08	Hits@10:51.03	Best:29.08
2024-12-28 00:11:43,544: Snapshot:1	Epoch:92	Loss:0.564	translation_Loss:0.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.323                                                   	MRR:29.13	Hits@10:50.96	Best:29.13
2024-12-28 00:11:45,806: Snapshot:1	Epoch:93	Loss:0.559	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:29.12	Hits@10:51.02	Best:29.13
2024-12-28 00:11:48,119: Snapshot:1	Epoch:94	Loss:0.56	translation_Loss:0.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.32                                                   	MRR:29.17	Hits@10:50.93	Best:29.17
2024-12-28 00:11:50,443: Snapshot:1	Epoch:95	Loss:0.56	translation_Loss:0.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.319                                                   	MRR:29.29	Hits@10:50.97	Best:29.29
2024-12-28 00:11:52,708: Snapshot:1	Epoch:96	Loss:0.553	translation_Loss:0.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.318                                                   	MRR:29.27	Hits@10:51.09	Best:29.29
2024-12-28 00:11:54,941: Snapshot:1	Epoch:97	Loss:0.551	translation_Loss:0.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.317                                                   	MRR:29.28	Hits@10:51.03	Best:29.29
2024-12-28 00:11:57,217: Snapshot:1	Epoch:98	Loss:0.545	translation_Loss:0.229	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.316                                                   	MRR:29.42	Hits@10:51.26	Best:29.42
2024-12-28 00:11:59,498: Snapshot:1	Epoch:99	Loss:0.545	translation_Loss:0.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:29.45	Hits@10:51.22	Best:29.45
2024-12-28 00:12:02,178: Snapshot:1	Epoch:100	Loss:0.54	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.313                                                   	MRR:29.37	Hits@10:51.2	Best:29.45
2024-12-28 00:12:04,407: Snapshot:1	Epoch:101	Loss:0.538	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.312                                                   	MRR:29.3	Hits@10:51.13	Best:29.45
2024-12-28 00:12:06,676: Snapshot:1	Epoch:102	Loss:0.536	translation_Loss:0.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.31                                                   	MRR:29.33	Hits@10:51.38	Best:29.45
2024-12-28 00:12:08,912: Snapshot:1	Epoch:103	Loss:0.533	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.309                                                   	MRR:29.35	Hits@10:51.4	Best:29.45
2024-12-28 00:12:11,164: Early Stopping! Snapshot: 1 Epoch: 104 Best Results: 29.45
2024-12-28 00:12:11,165: Start to training tokens! Snapshot: 1 Epoch: 104 Loss:0.532 MRR:29.3 Best Results: 29.45
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:12:11,165: Snapshot:1	Epoch:104	Loss:0.532	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.308                                                   	MRR:29.3	Hits@10:51.37	Best:29.45
2024-12-28 00:12:13,366: Snapshot:1	Epoch:105	Loss:6.389	translation_Loss:6.142	multi_layer_Loss:0.247	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.3	Hits@10:51.37	Best:29.45
2024-12-28 00:12:15,586: End of token training: 1 Epoch: 106 Loss:6.371 MRR:29.3 Best Results: 29.45
2024-12-28 00:12:15,587: Snapshot:1	Epoch:106	Loss:6.371	translation_Loss:6.154	multi_layer_Loss:0.216	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.3	Hits@10:51.37	Best:29.45
2024-12-28 00:12:15,845: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_5000/1model_best.tar'
2024-12-28 00:12:19,890: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2512 | 0.1421 | 0.3106 | 0.3747 |  0.4475 |
|     1      | 0.293  |  0.18  | 0.348  | 0.417  |  0.5102 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:12:52,963: Snapshot:2	Epoch:0	Loss:31.562	translation_Loss:31.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.105                                                   	MRR:4.12	Hits@10:8.69	Best:4.12
2024-12-28 00:13:02,993: Snapshot:2	Epoch:1	Loss:26.932	translation_Loss:26.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.324                                                   	MRR:6.45	Hits@10:14.17	Best:6.45
2024-12-28 00:13:13,082: Snapshot:2	Epoch:2	Loss:22.802	translation_Loss:22.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.614                                                   	MRR:8.75	Hits@10:19.3	Best:8.75
2024-12-28 00:13:23,134: Snapshot:2	Epoch:3	Loss:19.199	translation_Loss:18.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.896                                                   	MRR:10.86	Hits@10:22.66	Best:10.86
2024-12-28 00:13:33,611: Snapshot:2	Epoch:4	Loss:16.238	translation_Loss:15.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.133                                                   	MRR:12.67	Hits@10:26.1	Best:12.67
2024-12-28 00:13:43,665: Snapshot:2	Epoch:5	Loss:13.944	translation_Loss:12.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.313                                                   	MRR:14.15	Hits@10:28.93	Best:14.15
2024-12-28 00:13:53,600: Snapshot:2	Epoch:6	Loss:12.192	translation_Loss:10.749	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.443                                                   	MRR:15.32	Hits@10:30.91	Best:15.32
2024-12-28 00:14:03,566: Snapshot:2	Epoch:7	Loss:10.815	translation_Loss:9.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.533                                                   	MRR:16.18	Hits@10:32.47	Best:16.18
2024-12-28 00:14:13,793: Snapshot:2	Epoch:8	Loss:9.729	translation_Loss:8.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.594                                                   	MRR:17.07	Hits@10:33.65	Best:17.07
2024-12-28 00:14:23,718: Snapshot:2	Epoch:9	Loss:8.836	translation_Loss:7.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.63                                                   	MRR:17.75	Hits@10:34.59	Best:17.75
2024-12-28 00:14:33,791: Snapshot:2	Epoch:10	Loss:8.145	translation_Loss:6.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.647                                                   	MRR:18.3	Hits@10:35.4	Best:18.3
2024-12-28 00:14:44,137: Snapshot:2	Epoch:11	Loss:7.576	translation_Loss:5.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.653                                                   	MRR:18.65	Hits@10:35.94	Best:18.65
2024-12-28 00:14:54,045: Snapshot:2	Epoch:12	Loss:7.086	translation_Loss:5.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.647                                                   	MRR:19.06	Hits@10:36.56	Best:19.06
2024-12-28 00:15:04,066: Snapshot:2	Epoch:13	Loss:6.703	translation_Loss:5.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.635                                                   	MRR:19.39	Hits@10:36.93	Best:19.39
2024-12-28 00:15:14,496: Snapshot:2	Epoch:14	Loss:6.341	translation_Loss:4.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.619                                                   	MRR:19.65	Hits@10:37.27	Best:19.65
2024-12-28 00:15:24,471: Snapshot:2	Epoch:15	Loss:6.054	translation_Loss:4.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.596                                                   	MRR:19.82	Hits@10:37.56	Best:19.82
2024-12-28 00:15:34,444: Snapshot:2	Epoch:16	Loss:5.82	translation_Loss:4.245	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.575                                                   	MRR:20.12	Hits@10:37.82	Best:20.12
2024-12-28 00:15:44,975: Snapshot:2	Epoch:17	Loss:5.602	translation_Loss:4.051	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.551                                                   	MRR:20.28	Hits@10:38.03	Best:20.28
2024-12-28 00:15:55,049: Snapshot:2	Epoch:18	Loss:5.411	translation_Loss:3.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.529                                                   	MRR:20.45	Hits@10:38.03	Best:20.45
2024-12-28 00:16:05,029: Snapshot:2	Epoch:19	Loss:5.254	translation_Loss:3.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.508                                                   	MRR:20.54	Hits@10:38.16	Best:20.54
2024-12-28 00:16:15,125: Snapshot:2	Epoch:20	Loss:5.111	translation_Loss:3.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.486                                                   	MRR:20.69	Hits@10:38.2	Best:20.69
2024-12-28 00:16:25,404: Snapshot:2	Epoch:21	Loss:4.979	translation_Loss:3.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.464                                                   	MRR:20.9	Hits@10:38.23	Best:20.9
2024-12-28 00:16:35,350: Snapshot:2	Epoch:22	Loss:4.876	translation_Loss:3.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.446                                                   	MRR:20.94	Hits@10:38.25	Best:20.94
2024-12-28 00:16:45,367: Snapshot:2	Epoch:23	Loss:4.79	translation_Loss:3.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.428                                                   	MRR:21.06	Hits@10:38.36	Best:21.06
2024-12-28 00:16:55,831: Snapshot:2	Epoch:24	Loss:4.699	translation_Loss:3.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.412                                                   	MRR:21.19	Hits@10:38.35	Best:21.19
2024-12-28 00:17:05,811: Snapshot:2	Epoch:25	Loss:4.612	translation_Loss:3.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.398                                                   	MRR:21.21	Hits@10:38.39	Best:21.21
2024-12-28 00:17:15,853: Snapshot:2	Epoch:26	Loss:4.546	translation_Loss:3.163	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.383                                                   	MRR:21.21	Hits@10:38.46	Best:21.21
2024-12-28 00:17:26,256: Snapshot:2	Epoch:27	Loss:4.48	translation_Loss:3.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.369                                                   	MRR:21.29	Hits@10:38.5	Best:21.29
2024-12-28 00:17:36,324: Snapshot:2	Epoch:28	Loss:4.437	translation_Loss:3.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.356                                                   	MRR:21.41	Hits@10:38.38	Best:21.41
2024-12-28 00:17:46,360: Snapshot:2	Epoch:29	Loss:4.394	translation_Loss:3.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.347                                                   	MRR:21.43	Hits@10:38.39	Best:21.43
2024-12-28 00:17:56,851: Snapshot:2	Epoch:30	Loss:4.359	translation_Loss:3.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.336                                                   	MRR:21.5	Hits@10:38.38	Best:21.5
2024-12-28 00:18:06,891: Snapshot:2	Epoch:31	Loss:4.309	translation_Loss:2.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.327                                                   	MRR:21.53	Hits@10:38.44	Best:21.53
2024-12-28 00:18:16,761: Snapshot:2	Epoch:32	Loss:4.278	translation_Loss:2.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.319                                                   	MRR:21.49	Hits@10:38.5	Best:21.53
2024-12-28 00:18:27,063: Snapshot:2	Epoch:33	Loss:4.233	translation_Loss:2.922	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.311                                                   	MRR:21.51	Hits@10:38.43	Best:21.53
2024-12-28 00:18:36,920: Snapshot:2	Epoch:34	Loss:4.2	translation_Loss:2.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.302                                                   	MRR:21.5	Hits@10:38.48	Best:21.53
2024-12-28 00:18:46,787: Snapshot:2	Epoch:35	Loss:4.194	translation_Loss:2.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.295                                                   	MRR:21.5	Hits@10:38.52	Best:21.53
2024-12-28 00:18:56,726: Early Stopping! Snapshot: 2 Epoch: 36 Best Results: 21.53
2024-12-28 00:18:56,726: Start to training tokens! Snapshot: 2 Epoch: 36 Loss:4.144 MRR:21.5 Best Results: 21.53
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:18:56,727: Snapshot:2	Epoch:36	Loss:4.144	translation_Loss:2.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.29                                                   	MRR:21.5	Hits@10:38.52	Best:21.53
2024-12-28 00:19:06,908: Snapshot:2	Epoch:37	Loss:27.198	translation_Loss:26.425	multi_layer_Loss:0.773	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.5	Hits@10:38.52	Best:21.53
2024-12-28 00:19:16,616: End of token training: 2 Epoch: 38 Loss:26.76 MRR:21.5 Best Results: 21.53
2024-12-28 00:19:16,616: Snapshot:2	Epoch:38	Loss:26.76	translation_Loss:26.444	multi_layer_Loss:0.316	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.5	Hits@10:38.52	Best:21.53
2024-12-28 00:19:16,906: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_5000/2model_best.tar'
2024-12-28 00:19:25,609: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1333 | 0.3055 | 0.3712 |  0.4503 |
|     1      | 0.2559 | 0.1449 | 0.3057 | 0.3781 |  0.4662 |
|     2      | 0.2154 | 0.1257 | 0.2489 | 0.3093 |  0.389  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:20:05,170: Snapshot:3	Epoch:0	Loss:32.906	translation_Loss:32.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:4.89	Hits@10:12.31	Best:4.89
2024-12-28 00:20:17,489: Snapshot:3	Epoch:1	Loss:25.937	translation_Loss:25.426	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.511                                                   	MRR:7.56	Hits@10:18.56	Best:7.56
2024-12-28 00:20:29,659: Snapshot:3	Epoch:2	Loss:20.184	translation_Loss:19.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.967                                                   	MRR:10.28	Hits@10:23.57	Best:10.28
2024-12-28 00:20:41,791: Snapshot:3	Epoch:3	Loss:16.071	translation_Loss:14.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.383                                                   	MRR:12.44	Hits@10:27.3	Best:12.44
2024-12-28 00:20:53,952: Snapshot:3	Epoch:4	Loss:13.389	translation_Loss:11.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.693                                                   	MRR:14.15	Hits@10:30.01	Best:14.15
2024-12-28 00:21:06,007: Snapshot:3	Epoch:5	Loss:11.579	translation_Loss:9.678	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.901                                                   	MRR:15.4	Hits@10:32.14	Best:15.4
2024-12-28 00:21:18,194: Snapshot:3	Epoch:6	Loss:10.253	translation_Loss:8.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.031                                                   	MRR:16.49	Hits@10:33.87	Best:16.49
2024-12-28 00:21:30,540: Snapshot:3	Epoch:7	Loss:9.275	translation_Loss:7.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.107                                                   	MRR:17.44	Hits@10:35.17	Best:17.44
2024-12-28 00:21:42,729: Snapshot:3	Epoch:8	Loss:8.512	translation_Loss:6.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.145                                                   	MRR:18.05	Hits@10:36.08	Best:18.05
2024-12-28 00:21:55,434: Snapshot:3	Epoch:9	Loss:7.916	translation_Loss:5.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.156                                                   	MRR:18.53	Hits@10:36.69	Best:18.53
2024-12-28 00:22:07,593: Snapshot:3	Epoch:10	Loss:7.448	translation_Loss:5.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.148                                                   	MRR:18.98	Hits@10:37.11	Best:18.98
2024-12-28 00:22:19,958: Snapshot:3	Epoch:11	Loss:7.045	translation_Loss:4.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.129                                                   	MRR:19.36	Hits@10:37.61	Best:19.36
2024-12-28 00:22:32,572: Snapshot:3	Epoch:12	Loss:6.702	translation_Loss:4.598	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.104                                                   	MRR:19.65	Hits@10:38.04	Best:19.65
2024-12-28 00:22:44,624: Snapshot:3	Epoch:13	Loss:6.42	translation_Loss:4.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.076                                                   	MRR:19.86	Hits@10:38.25	Best:19.86
2024-12-28 00:22:56,807: Snapshot:3	Epoch:14	Loss:6.177	translation_Loss:4.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.047                                                   	MRR:20.11	Hits@10:38.57	Best:20.11
2024-12-28 00:23:09,309: Snapshot:3	Epoch:15	Loss:5.947	translation_Loss:3.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.013                                                   	MRR:20.29	Hits@10:38.67	Best:20.29
2024-12-28 00:23:21,596: Snapshot:3	Epoch:16	Loss:5.75	translation_Loss:3.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.982                                                   	MRR:20.46	Hits@10:38.87	Best:20.46
2024-12-28 00:23:34,230: Snapshot:3	Epoch:17	Loss:5.59	translation_Loss:3.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.952                                                   	MRR:20.55	Hits@10:39.05	Best:20.55
2024-12-28 00:23:46,495: Snapshot:3	Epoch:18	Loss:5.453	translation_Loss:3.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.922                                                   	MRR:20.69	Hits@10:39.07	Best:20.69
2024-12-28 00:23:58,762: Snapshot:3	Epoch:19	Loss:5.324	translation_Loss:3.429	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.895                                                   	MRR:20.78	Hits@10:39.04	Best:20.78
2024-12-28 00:24:11,471: Snapshot:3	Epoch:20	Loss:5.222	translation_Loss:3.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.87                                                   	MRR:20.88	Hits@10:39.29	Best:20.88
2024-12-28 00:24:23,582: Snapshot:3	Epoch:21	Loss:5.124	translation_Loss:3.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.844                                                   	MRR:20.97	Hits@10:39.43	Best:20.97
2024-12-28 00:24:35,875: Snapshot:3	Epoch:22	Loss:5.039	translation_Loss:3.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.82                                                   	MRR:21.03	Hits@10:39.43	Best:21.03
2024-12-28 00:24:48,490: Snapshot:3	Epoch:23	Loss:4.941	translation_Loss:3.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.797                                                   	MRR:21.1	Hits@10:39.52	Best:21.1
2024-12-28 00:25:00,639: Snapshot:3	Epoch:24	Loss:4.885	translation_Loss:3.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.774                                                   	MRR:21.13	Hits@10:39.52	Best:21.13
2024-12-28 00:25:12,827: Snapshot:3	Epoch:25	Loss:4.817	translation_Loss:3.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.758                                                   	MRR:21.13	Hits@10:39.52	Best:21.13
2024-12-28 00:25:25,430: Snapshot:3	Epoch:26	Loss:4.778	translation_Loss:3.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.739                                                   	MRR:21.15	Hits@10:39.61	Best:21.15
2024-12-28 00:25:37,643: Snapshot:3	Epoch:27	Loss:4.705	translation_Loss:2.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.723                                                   	MRR:21.22	Hits@10:39.71	Best:21.22
2024-12-28 00:25:50,169: Snapshot:3	Epoch:28	Loss:4.664	translation_Loss:2.957	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.706                                                   	MRR:21.25	Hits@10:39.6	Best:21.25
2024-12-28 00:26:02,440: Snapshot:3	Epoch:29	Loss:4.622	translation_Loss:2.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.693                                                   	MRR:21.27	Hits@10:39.62	Best:21.27
2024-12-28 00:26:14,562: Snapshot:3	Epoch:30	Loss:4.588	translation_Loss:2.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.68                                                   	MRR:21.31	Hits@10:39.46	Best:21.31
2024-12-28 00:26:27,092: Snapshot:3	Epoch:31	Loss:4.564	translation_Loss:2.896	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.668                                                   	MRR:21.3	Hits@10:39.52	Best:21.31
2024-12-28 00:26:39,267: Snapshot:3	Epoch:32	Loss:4.519	translation_Loss:2.86	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.66                                                   	MRR:21.32	Hits@10:39.64	Best:21.32
2024-12-28 00:26:51,606: Snapshot:3	Epoch:33	Loss:4.489	translation_Loss:2.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.649                                                   	MRR:21.33	Hits@10:39.59	Best:21.33
2024-12-28 00:27:04,224: Snapshot:3	Epoch:34	Loss:4.459	translation_Loss:2.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.639                                                   	MRR:21.34	Hits@10:39.57	Best:21.34
2024-12-28 00:27:16,284: Snapshot:3	Epoch:35	Loss:4.45	translation_Loss:2.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.631                                                   	MRR:21.31	Hits@10:39.68	Best:21.34
2024-12-28 00:27:28,500: Snapshot:3	Epoch:36	Loss:4.412	translation_Loss:2.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.622                                                   	MRR:21.36	Hits@10:39.53	Best:21.36
2024-12-28 00:27:41,034: Snapshot:3	Epoch:37	Loss:4.404	translation_Loss:2.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.615                                                   	MRR:21.36	Hits@10:39.61	Best:21.36
2024-12-28 00:27:53,194: Snapshot:3	Epoch:38	Loss:4.379	translation_Loss:2.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.606                                                   	MRR:21.38	Hits@10:39.61	Best:21.38
2024-12-28 00:28:05,840: Snapshot:3	Epoch:39	Loss:4.363	translation_Loss:2.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.602                                                   	MRR:21.43	Hits@10:39.67	Best:21.43
2024-12-28 00:28:18,086: Snapshot:3	Epoch:40	Loss:4.336	translation_Loss:2.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.594                                                   	MRR:21.38	Hits@10:39.56	Best:21.43
2024-12-28 00:28:30,296: Snapshot:3	Epoch:41	Loss:4.322	translation_Loss:2.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.589                                                   	MRR:21.4	Hits@10:39.61	Best:21.43
2024-12-28 00:28:42,791: Snapshot:3	Epoch:42	Loss:4.303	translation_Loss:2.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.586                                                   	MRR:21.4	Hits@10:39.57	Best:21.43
2024-12-28 00:28:55,072: Snapshot:3	Epoch:43	Loss:4.291	translation_Loss:2.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.581                                                   	MRR:21.36	Hits@10:39.47	Best:21.43
2024-12-28 00:29:07,307: Early Stopping! Snapshot: 3 Epoch: 44 Best Results: 21.43
2024-12-28 00:29:07,307: Start to training tokens! Snapshot: 3 Epoch: 44 Loss:4.291 MRR:21.37 Best Results: 21.43
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:29:07,308: Snapshot:3	Epoch:44	Loss:4.291	translation_Loss:2.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.575                                                   	MRR:21.37	Hits@10:39.58	Best:21.43
2024-12-28 00:29:19,607: Snapshot:3	Epoch:45	Loss:29.474	translation_Loss:28.577	multi_layer_Loss:0.897	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.37	Hits@10:39.58	Best:21.43
2024-12-28 00:29:31,501: End of token training: 3 Epoch: 46 Loss:28.842 MRR:21.37 Best Results: 21.43
2024-12-28 00:29:31,501: Snapshot:3	Epoch:46	Loss:28.842	translation_Loss:28.548	multi_layer_Loss:0.294	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.37	Hits@10:39.58	Best:21.43
2024-12-28 00:29:31,754: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_5000/3model_best.tar'
2024-12-28 00:29:45,414: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2523 | 0.1436 | 0.3073 | 0.3704 |  0.4496 |
|     1      | 0.259  | 0.1521 | 0.3059 | 0.3701 |  0.461  |
|     2      | 0.2062 | 0.1149 | 0.239  | 0.3018 |  0.3832 |
|     3      | 0.2151 | 0.1184 | 0.2545 | 0.3163 |  0.3986 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:30:04,308: Snapshot:4	Epoch:0	Loss:10.693	translation_Loss:10.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.024                                                   	MRR:7.46	Hits@10:14.59	Best:7.46
2024-12-28 00:30:09,356: Snapshot:4	Epoch:1	Loss:9.326	translation_Loss:9.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.076                                                   	MRR:7.97	Hits@10:16.09	Best:7.97
2024-12-28 00:30:14,394: Snapshot:4	Epoch:2	Loss:8.178	translation_Loss:8.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:8.71	Hits@10:17.57	Best:8.71
2024-12-28 00:30:19,413: Snapshot:4	Epoch:3	Loss:7.16	translation_Loss:6.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:9.92	Hits@10:20.69	Best:9.92
2024-12-28 00:30:24,375: Snapshot:4	Epoch:4	Loss:6.253	translation_Loss:5.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.313                                                   	MRR:12.39	Hits@10:27.89	Best:12.39
2024-12-28 00:30:29,433: Snapshot:4	Epoch:5	Loss:5.511	translation_Loss:5.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.402                                                   	MRR:14.02	Hits@10:31.1	Best:14.02
2024-12-28 00:30:34,458: Snapshot:4	Epoch:6	Loss:4.916	translation_Loss:4.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.486                                                   	MRR:15.42	Hits@10:32.37	Best:15.42
2024-12-28 00:30:39,498: Snapshot:4	Epoch:7	Loss:4.454	translation_Loss:3.89	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.565                                                   	MRR:16.35	Hits@10:33.29	Best:16.35
2024-12-28 00:30:44,682: Snapshot:4	Epoch:8	Loss:4.118	translation_Loss:3.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.636                                                   	MRR:17.14	Hits@10:33.86	Best:17.14
2024-12-28 00:30:49,660: Snapshot:4	Epoch:9	Loss:3.83	translation_Loss:3.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.702                                                   	MRR:17.47	Hits@10:34.6	Best:17.47
2024-12-28 00:30:54,681: Snapshot:4	Epoch:10	Loss:3.593	translation_Loss:2.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.761                                                   	MRR:18.28	Hits@10:35.29	Best:18.28
2024-12-28 00:30:59,860: Snapshot:4	Epoch:11	Loss:3.406	translation_Loss:2.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.814                                                   	MRR:18.9	Hits@10:35.59	Best:18.9
2024-12-28 00:31:04,828: Snapshot:4	Epoch:12	Loss:3.23	translation_Loss:2.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.861                                                   	MRR:19.23	Hits@10:35.87	Best:19.23
2024-12-28 00:31:09,795: Snapshot:4	Epoch:13	Loss:3.076	translation_Loss:2.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.903                                                   	MRR:19.41	Hits@10:36.08	Best:19.41
2024-12-28 00:31:14,826: Snapshot:4	Epoch:14	Loss:2.95	translation_Loss:2.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.94                                                   	MRR:19.44	Hits@10:36.15	Best:19.44
2024-12-28 00:31:19,893: Snapshot:4	Epoch:15	Loss:2.825	translation_Loss:1.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.972                                                   	MRR:19.65	Hits@10:36.13	Best:19.65
2024-12-28 00:31:25,391: Snapshot:4	Epoch:16	Loss:2.721	translation_Loss:1.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.999                                                   	MRR:19.73	Hits@10:36.34	Best:19.73
2024-12-28 00:31:30,359: Snapshot:4	Epoch:17	Loss:2.623	translation_Loss:1.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.023                                                   	MRR:20.23	Hits@10:36.36	Best:20.23
2024-12-28 00:31:35,289: Snapshot:4	Epoch:18	Loss:2.53	translation_Loss:1.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.041                                                   	MRR:20.09	Hits@10:36.46	Best:20.23
2024-12-28 00:31:40,329: Snapshot:4	Epoch:19	Loss:2.463	translation_Loss:1.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.057                                                   	MRR:20.32	Hits@10:36.81	Best:20.32
2024-12-28 00:31:45,289: Snapshot:4	Epoch:20	Loss:2.382	translation_Loss:1.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.069                                                   	MRR:20.25	Hits@10:37.08	Best:20.32
2024-12-28 00:31:50,265: Snapshot:4	Epoch:21	Loss:2.322	translation_Loss:1.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.079                                                   	MRR:20.55	Hits@10:37.25	Best:20.55
2024-12-28 00:31:55,296: Snapshot:4	Epoch:22	Loss:2.258	translation_Loss:1.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.086                                                   	MRR:20.94	Hits@10:37.14	Best:20.94
2024-12-28 00:32:00,677: Snapshot:4	Epoch:23	Loss:2.207	translation_Loss:1.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.09                                                   	MRR:21.14	Hits@10:37.32	Best:21.14
2024-12-28 00:32:05,615: Snapshot:4	Epoch:24	Loss:2.152	translation_Loss:1.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.093                                                   	MRR:20.64	Hits@10:37.29	Best:21.14
2024-12-28 00:32:10,577: Snapshot:4	Epoch:25	Loss:2.105	translation_Loss:1.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.094                                                   	MRR:20.78	Hits@10:37.24	Best:21.14
2024-12-28 00:32:15,553: Snapshot:4	Epoch:26	Loss:2.055	translation_Loss:0.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.093                                                   	MRR:21.19	Hits@10:37.35	Best:21.19
2024-12-28 00:32:20,588: Snapshot:4	Epoch:27	Loss:2.012	translation_Loss:0.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.092                                                   	MRR:21.41	Hits@10:37.53	Best:21.41
2024-12-28 00:32:25,514: Snapshot:4	Epoch:28	Loss:1.974	translation_Loss:0.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.088                                                   	MRR:21.3	Hits@10:37.47	Best:21.41
2024-12-28 00:32:30,452: Snapshot:4	Epoch:29	Loss:1.929	translation_Loss:0.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.084                                                   	MRR:21.47	Hits@10:37.3	Best:21.47
2024-12-28 00:32:35,813: Snapshot:4	Epoch:30	Loss:1.895	translation_Loss:0.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.078                                                   	MRR:21.31	Hits@10:37.55	Best:21.47
2024-12-28 00:32:40,743: Snapshot:4	Epoch:31	Loss:1.868	translation_Loss:0.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.073                                                   	MRR:21.27	Hits@10:37.75	Best:21.47
2024-12-28 00:32:45,759: Snapshot:4	Epoch:32	Loss:1.832	translation_Loss:0.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.067                                                   	MRR:21.48	Hits@10:37.78	Best:21.48
2024-12-28 00:32:50,726: Snapshot:4	Epoch:33	Loss:1.803	translation_Loss:0.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.06                                                   	MRR:21.75	Hits@10:37.77	Best:21.75
2024-12-28 00:32:55,689: Snapshot:4	Epoch:34	Loss:1.777	translation_Loss:0.725	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.052                                                   	MRR:21.41	Hits@10:37.89	Best:21.75
2024-12-28 00:33:00,646: Snapshot:4	Epoch:35	Loss:1.754	translation_Loss:0.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.045                                                   	MRR:21.41	Hits@10:37.84	Best:21.75
2024-12-28 00:33:05,589: Snapshot:4	Epoch:36	Loss:1.728	translation_Loss:0.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.038                                                   	MRR:21.17	Hits@10:37.73	Best:21.75
2024-12-28 00:33:10,868: Snapshot:4	Epoch:37	Loss:1.702	translation_Loss:0.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.031                                                   	MRR:21.3	Hits@10:37.8	Best:21.75
2024-12-28 00:33:15,759: Early Stopping! Snapshot: 4 Epoch: 38 Best Results: 21.75
2024-12-28 00:33:15,759: Start to training tokens! Snapshot: 4 Epoch: 38 Loss:1.679 MRR:21.43 Best Results: 21.75
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:33:15,760: Snapshot:4	Epoch:38	Loss:1.679	translation_Loss:0.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.024                                                   	MRR:21.43	Hits@10:37.84	Best:21.75
2024-12-28 00:33:20,684: Snapshot:4	Epoch:39	Loss:12.892	translation_Loss:12.445	multi_layer_Loss:0.447	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.43	Hits@10:37.84	Best:21.75
2024-12-28 00:33:25,571: End of token training: 4 Epoch: 40 Loss:12.787 MRR:21.43 Best Results: 21.75
2024-12-28 00:33:25,571: Snapshot:4	Epoch:40	Loss:12.787	translation_Loss:12.448	multi_layer_Loss:0.339	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.43	Hits@10:37.84	Best:21.75
2024-12-28 00:33:25,882: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_5000/4model_best.tar'
2024-12-28 00:33:42,074: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2049 | 0.1053 | 0.2523 | 0.3106 |  0.3829 |
|     1      | 0.2261 | 0.1277 | 0.2612 | 0.3196 |  0.4196 |
|     2      | 0.1784 | 0.0973 | 0.202  | 0.2583 |  0.3371 |
|     3      | 0.1714 | 0.084  | 0.2014 | 0.2595 |  0.3393 |
|     4      | 0.2172 | 0.1326 | 0.2524 | 0.3054 |  0.3773 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 00:33:42,077: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2552 | 0.1463 | 0.3148 | 0.3785 |  0.4516 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2512 | 0.1421 | 0.3106 | 0.3747 |  0.4475 |
|     1      | 0.293  |  0.18  | 0.348  | 0.417  |  0.5102 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1333 | 0.3055 | 0.3712 |  0.4503 |
|     1      | 0.2559 | 0.1449 | 0.3057 | 0.3781 |  0.4662 |
|     2      | 0.2154 | 0.1257 | 0.2489 | 0.3093 |  0.389  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2523 | 0.1436 | 0.3073 | 0.3704 |  0.4496 |
|     1      | 0.259  | 0.1521 | 0.3059 | 0.3701 |  0.461  |
|     2      | 0.2062 | 0.1149 | 0.239  | 0.3018 |  0.3832 |
|     3      | 0.2151 | 0.1184 | 0.2545 | 0.3163 |  0.3986 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2049 | 0.1053 | 0.2523 | 0.3106 |  0.3829 |
|     1      | 0.2261 | 0.1277 | 0.2612 | 0.3196 |  0.4196 |
|     2      | 0.1784 | 0.0973 | 0.202  | 0.2583 |  0.3371 |
|     3      | 0.1714 | 0.084  | 0.2014 | 0.2595 |  0.3393 |
|     4      | 0.2172 | 0.1326 | 0.2524 | 0.3054 |  0.3773 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 00:33:42,077: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 368.26519417762756 |   0.255   |    0.146     |    0.315     |     0.452     |
|    1     | 255.50095915794373 |   0.262   |    0.152     |    0.321     |     0.464     |
|    2     | 412.49698066711426 |   0.231   |    0.131     |    0.276     |      0.42     |
|    3     | 600.6130380630493  |   0.224   |    0.125     |    0.265     |     0.409     |
|    4     | 217.29312705993652 |   0.189   |    0.101     |    0.222     |     0.357     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 00:33:42,077: Sum_Training_Time:1854.1692991256714
2024-12-28 00:33:42,077: Every_Training_Time:[368.26519417762756, 255.50095915794373, 412.49698066711426, 600.6130380630493, 217.29312705993652]
2024-12-28 00:33:42,077: Forward transfer: 0.04555 Backward transfer: -0.049475
2024-12-28 00:34:17,756: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241228003347/HYBRIDHYBRID_0.0001_2048_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.0001_2048_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.0001_2048_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 00:34:27,414: Snapshot:0	Epoch:0	Loss:24.333	translation_Loss:24.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.78	Hits@10:0.9	Best:0.78
2024-12-28 00:34:33,468: Snapshot:0	Epoch:1	Loss:23.173	translation_Loss:23.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.53	Hits@10:2.9	Best:1.53
2024-12-28 00:34:39,914: Snapshot:0	Epoch:2	Loss:22.122	translation_Loss:22.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.08	Hits@10:6.68	Best:3.08
2024-12-28 00:34:45,971: Snapshot:0	Epoch:3	Loss:21.1	translation_Loss:21.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:9.99	Best:4.54
2024-12-28 00:34:51,932: Snapshot:0	Epoch:4	Loss:20.098	translation_Loss:20.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:13.19	Best:5.66
2024-12-28 00:34:57,904: Snapshot:0	Epoch:5	Loss:19.123	translation_Loss:19.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.56	Hits@10:15.81	Best:6.56
2024-12-28 00:35:03,868: Snapshot:0	Epoch:6	Loss:18.14	translation_Loss:18.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.34	Hits@10:17.83	Best:7.34
2024-12-28 00:35:10,280: Snapshot:0	Epoch:7	Loss:17.186	translation_Loss:17.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:19.91	Best:8.1
2024-12-28 00:35:16,252: Snapshot:0	Epoch:8	Loss:16.222	translation_Loss:16.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.92	Hits@10:22.12	Best:8.92
2024-12-28 00:35:22,285: Snapshot:0	Epoch:9	Loss:15.277	translation_Loss:15.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.84	Hits@10:24.47	Best:9.84
2024-12-28 00:35:28,203: Snapshot:0	Epoch:10	Loss:14.324	translation_Loss:14.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.82	Hits@10:27.01	Best:10.82
2024-12-28 00:35:34,632: Snapshot:0	Epoch:11	Loss:13.368	translation_Loss:13.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.9	Hits@10:29.48	Best:11.9
2024-12-28 00:35:40,678: Snapshot:0	Epoch:12	Loss:12.388	translation_Loss:12.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.97	Hits@10:31.59	Best:12.97
2024-12-28 00:35:46,734: Snapshot:0	Epoch:13	Loss:11.425	translation_Loss:11.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.1	Hits@10:33.62	Best:14.1
2024-12-28 00:35:52,782: Snapshot:0	Epoch:14	Loss:10.479	translation_Loss:10.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.2	Hits@10:35.2	Best:15.2
2024-12-28 00:35:58,839: Snapshot:0	Epoch:15	Loss:9.541	translation_Loss:9.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.19	Hits@10:36.62	Best:16.19
2024-12-28 00:36:05,319: Snapshot:0	Epoch:16	Loss:8.663	translation_Loss:8.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.22	Hits@10:37.83	Best:17.22
2024-12-28 00:36:11,255: Snapshot:0	Epoch:17	Loss:7.841	translation_Loss:7.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.28	Hits@10:38.86	Best:18.28
2024-12-28 00:36:17,314: Snapshot:0	Epoch:18	Loss:7.081	translation_Loss:7.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.15	Hits@10:39.89	Best:19.15
2024-12-28 00:36:23,335: Snapshot:0	Epoch:19	Loss:6.404	translation_Loss:6.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.93	Hits@10:40.65	Best:19.93
2024-12-28 00:36:29,388: Snapshot:0	Epoch:20	Loss:5.775	translation_Loss:5.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.59	Hits@10:41.3	Best:20.59
2024-12-28 00:36:35,837: Snapshot:0	Epoch:21	Loss:5.224	translation_Loss:5.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.2	Hits@10:41.93	Best:21.2
2024-12-28 00:36:41,893: Snapshot:0	Epoch:22	Loss:4.737	translation_Loss:4.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.76	Hits@10:42.49	Best:21.76
2024-12-28 00:36:47,956: Snapshot:0	Epoch:23	Loss:4.295	translation_Loss:4.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.17	Hits@10:42.89	Best:22.17
2024-12-28 00:36:53,943: Snapshot:0	Epoch:24	Loss:3.914	translation_Loss:3.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.6	Hits@10:43.34	Best:22.6
2024-12-28 00:36:59,975: Snapshot:0	Epoch:25	Loss:3.532	translation_Loss:3.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.95	Hits@10:43.66	Best:22.95
2024-12-28 00:37:06,417: Snapshot:0	Epoch:26	Loss:3.234	translation_Loss:3.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.23	Hits@10:43.97	Best:23.23
2024-12-28 00:37:12,493: Snapshot:0	Epoch:27	Loss:2.949	translation_Loss:2.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:44.28	Best:23.45
2024-12-28 00:37:18,586: Snapshot:0	Epoch:28	Loss:2.686	translation_Loss:2.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:44.56	Best:23.69
2024-12-28 00:37:24,680: Snapshot:0	Epoch:29	Loss:2.481	translation_Loss:2.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.84	Hits@10:44.65	Best:23.84
2024-12-28 00:37:30,749: Snapshot:0	Epoch:30	Loss:2.283	translation_Loss:2.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:44.82	Best:24.08
2024-12-28 00:37:37,134: Snapshot:0	Epoch:31	Loss:2.091	translation_Loss:2.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:44.95	Best:24.23
2024-12-28 00:37:43,189: Snapshot:0	Epoch:32	Loss:1.938	translation_Loss:1.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:44.98	Best:24.34
2024-12-28 00:37:49,217: Snapshot:0	Epoch:33	Loss:1.807	translation_Loss:1.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:45.1	Best:24.43
2024-12-28 00:37:55,240: Snapshot:0	Epoch:34	Loss:1.686	translation_Loss:1.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:45.11	Best:24.59
2024-12-28 00:38:01,268: Snapshot:0	Epoch:35	Loss:1.57	translation_Loss:1.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.67	Hits@10:45.17	Best:24.67
2024-12-28 00:38:07,652: Snapshot:0	Epoch:36	Loss:1.464	translation_Loss:1.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.73	Hits@10:45.35	Best:24.73
2024-12-28 00:38:13,718: Snapshot:0	Epoch:37	Loss:1.372	translation_Loss:1.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:45.38	Best:24.78
2024-12-28 00:38:19,732: Snapshot:0	Epoch:38	Loss:1.295	translation_Loss:1.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.89	Hits@10:45.47	Best:24.89
2024-12-28 00:38:25,802: Snapshot:0	Epoch:39	Loss:1.219	translation_Loss:1.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.93	Hits@10:45.48	Best:24.93
2024-12-28 00:38:31,825: Snapshot:0	Epoch:40	Loss:1.149	translation_Loss:1.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:45.54	Best:24.99
2024-12-28 00:38:38,233: Snapshot:0	Epoch:41	Loss:1.087	translation_Loss:1.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:45.66	Best:25.07
2024-12-28 00:38:44,195: Snapshot:0	Epoch:42	Loss:1.031	translation_Loss:1.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:45.67	Best:25.07
2024-12-28 00:38:50,261: Snapshot:0	Epoch:43	Loss:0.977	translation_Loss:0.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.13	Hits@10:45.72	Best:25.13
2024-12-28 00:38:56,277: Snapshot:0	Epoch:44	Loss:0.93	translation_Loss:0.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.2	Hits@10:45.81	Best:25.2
2024-12-28 00:39:02,294: Snapshot:0	Epoch:45	Loss:0.892	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.22	Hits@10:45.91	Best:25.22
2024-12-28 00:39:08,582: Snapshot:0	Epoch:46	Loss:0.848	translation_Loss:0.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.34	Hits@10:45.87	Best:25.34
2024-12-28 00:39:14,607: Snapshot:0	Epoch:47	Loss:0.814	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.29	Hits@10:45.98	Best:25.34
2024-12-28 00:39:20,627: Snapshot:0	Epoch:48	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:46.13	Best:25.34
2024-12-28 00:39:26,709: Snapshot:0	Epoch:49	Loss:0.742	translation_Loss:0.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.37	Hits@10:46.07	Best:25.37
2024-12-28 00:39:33,128: Snapshot:0	Epoch:50	Loss:0.717	translation_Loss:0.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:46.1	Best:25.37
2024-12-28 00:39:39,175: Snapshot:0	Epoch:51	Loss:0.676	translation_Loss:0.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.1	Best:25.37
2024-12-28 00:39:45,159: Snapshot:0	Epoch:52	Loss:0.664	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.46	Hits@10:46.16	Best:25.46
2024-12-28 00:39:51,138: Snapshot:0	Epoch:53	Loss:0.63	translation_Loss:0.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.46	Hits@10:46.16	Best:25.46
2024-12-28 00:39:57,082: Snapshot:0	Epoch:54	Loss:0.615	translation_Loss:0.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.42	Hits@10:46.15	Best:25.46
2024-12-28 00:40:03,390: Snapshot:0	Epoch:55	Loss:0.595	translation_Loss:0.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.35	Hits@10:46.13	Best:25.46
2024-12-28 00:40:09,442: Snapshot:0	Epoch:56	Loss:0.571	translation_Loss:0.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:46.21	Best:25.46
2024-12-28 00:40:15,441: Early Stopping! Snapshot: 0 Epoch: 57 Best Results: 25.46
2024-12-28 00:40:15,442: Start to training tokens! Snapshot: 0 Epoch: 57 Loss:0.549 MRR:25.41 Best Results: 25.46
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:40:15,442: Snapshot:0	Epoch:57	Loss:0.549	translation_Loss:0.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:46.21	Best:25.46
2024-12-28 00:40:22,007: Snapshot:0	Epoch:58	Loss:17.957	translation_Loss:17.371	multi_layer_Loss:0.586	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:46.21	Best:25.46
2024-12-28 00:40:28,088: End of token training: 0 Epoch: 59 Loss:17.736 MRR:25.41 Best Results: 25.46
2024-12-28 00:40:28,088: Snapshot:0	Epoch:59	Loss:17.736	translation_Loss:17.368	multi_layer_Loss:0.368	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.41	Hits@10:46.21	Best:25.46
2024-12-28 00:40:28,381: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_10000/0model_best.tar'
2024-12-28 00:40:31,170: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2546 | 0.1444 | 0.3153 | 0.3796 |  0.4518 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:40:42,304: Snapshot:1	Epoch:0	Loss:7.918	translation_Loss:7.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.015                                                   	MRR:5.99	Hits@10:10.17	Best:5.99
2024-12-28 00:40:44,634: Snapshot:1	Epoch:1	Loss:7.374	translation_Loss:7.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.042                                                   	MRR:6.47	Hits@10:10.81	Best:6.47
2024-12-28 00:40:46,981: Snapshot:1	Epoch:2	Loss:6.898	translation_Loss:6.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.074                                                   	MRR:6.95	Hits@10:11.64	Best:6.95
2024-12-28 00:40:49,262: Snapshot:1	Epoch:3	Loss:6.457	translation_Loss:6.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.112                                                   	MRR:7.52	Hits@10:12.98	Best:7.52
2024-12-28 00:40:51,590: Snapshot:1	Epoch:4	Loss:6.039	translation_Loss:5.884	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:8.27	Hits@10:14.44	Best:8.27
2024-12-28 00:40:53,901: Snapshot:1	Epoch:5	Loss:5.638	translation_Loss:5.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.199                                                   	MRR:9.15	Hits@10:16.59	Best:9.15
2024-12-28 00:40:56,243: Snapshot:1	Epoch:6	Loss:5.265	translation_Loss:5.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:10.2	Hits@10:18.97	Best:10.2
2024-12-28 00:40:58,571: Snapshot:1	Epoch:7	Loss:4.898	translation_Loss:4.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.283                                                   	MRR:11.36	Hits@10:21.56	Best:11.36
2024-12-28 00:41:00,894: Snapshot:1	Epoch:8	Loss:4.547	translation_Loss:4.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:12.51	Hits@10:24.38	Best:12.51
2024-12-28 00:41:03,212: Snapshot:1	Epoch:9	Loss:4.202	translation_Loss:3.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.358                                                   	MRR:13.68	Hits@10:26.71	Best:13.68
2024-12-28 00:41:05,583: Snapshot:1	Epoch:10	Loss:3.888	translation_Loss:3.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:14.91	Hits@10:28.62	Best:14.91
2024-12-28 00:41:07,942: Snapshot:1	Epoch:11	Loss:3.612	translation_Loss:3.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.416                                                   	MRR:15.92	Hits@10:30.31	Best:15.92
2024-12-28 00:41:10,292: Snapshot:1	Epoch:12	Loss:3.354	translation_Loss:2.917	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.437                                                   	MRR:16.89	Hits@10:31.72	Best:16.89
2024-12-28 00:41:12,667: Snapshot:1	Epoch:13	Loss:3.116	translation_Loss:2.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.454                                                   	MRR:17.81	Hits@10:32.99	Best:17.81
2024-12-28 00:41:15,021: Snapshot:1	Epoch:14	Loss:2.906	translation_Loss:2.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.467                                                   	MRR:18.5	Hits@10:33.93	Best:18.5
2024-12-28 00:41:17,746: Snapshot:1	Epoch:15	Loss:2.73	translation_Loss:2.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.476                                                   	MRR:19.13	Hits@10:35.04	Best:19.13
2024-12-28 00:41:20,081: Snapshot:1	Epoch:16	Loss:2.555	translation_Loss:2.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.483                                                   	MRR:19.61	Hits@10:35.77	Best:19.61
2024-12-28 00:41:22,401: Snapshot:1	Epoch:17	Loss:2.417	translation_Loss:1.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.487                                                   	MRR:20.15	Hits@10:36.62	Best:20.15
2024-12-28 00:41:24,747: Snapshot:1	Epoch:18	Loss:2.281	translation_Loss:1.792	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.489                                                   	MRR:20.67	Hits@10:37.25	Best:20.67
2024-12-28 00:41:27,085: Snapshot:1	Epoch:19	Loss:2.161	translation_Loss:1.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.49                                                   	MRR:21.18	Hits@10:37.87	Best:21.18
2024-12-28 00:41:29,444: Snapshot:1	Epoch:20	Loss:2.063	translation_Loss:1.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.489                                                   	MRR:21.6	Hits@10:38.55	Best:21.6
2024-12-28 00:41:31,769: Snapshot:1	Epoch:21	Loss:1.957	translation_Loss:1.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.487                                                   	MRR:22.06	Hits@10:39.15	Best:22.06
2024-12-28 00:41:34,097: Snapshot:1	Epoch:22	Loss:1.877	translation_Loss:1.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.485                                                   	MRR:22.45	Hits@10:39.6	Best:22.45
2024-12-28 00:41:36,438: Snapshot:1	Epoch:23	Loss:1.797	translation_Loss:1.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.482                                                   	MRR:22.7	Hits@10:40.06	Best:22.7
2024-12-28 00:41:38,765: Snapshot:1	Epoch:24	Loss:1.732	translation_Loss:1.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.478                                                   	MRR:22.98	Hits@10:40.8	Best:22.98
2024-12-28 00:41:41,102: Snapshot:1	Epoch:25	Loss:1.665	translation_Loss:1.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.474                                                   	MRR:23.29	Hits@10:41.29	Best:23.29
2024-12-28 00:41:43,491: Snapshot:1	Epoch:26	Loss:1.607	translation_Loss:1.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.47                                                   	MRR:23.63	Hits@10:41.8	Best:23.63
2024-12-28 00:41:45,779: Snapshot:1	Epoch:27	Loss:1.547	translation_Loss:1.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.466                                                   	MRR:23.88	Hits@10:42.5	Best:23.88
2024-12-28 00:41:48,109: Snapshot:1	Epoch:28	Loss:1.505	translation_Loss:1.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:24.13	Hits@10:42.77	Best:24.13
2024-12-28 00:41:50,812: Snapshot:1	Epoch:29	Loss:1.467	translation_Loss:1.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.457                                                   	MRR:24.34	Hits@10:43.27	Best:24.34
2024-12-28 00:41:53,153: Snapshot:1	Epoch:30	Loss:1.413	translation_Loss:0.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.453                                                   	MRR:24.58	Hits@10:43.75	Best:24.58
2024-12-28 00:41:55,480: Snapshot:1	Epoch:31	Loss:1.383	translation_Loss:0.935	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.448                                                   	MRR:24.82	Hits@10:44.23	Best:24.82
2024-12-28 00:41:57,834: Snapshot:1	Epoch:32	Loss:1.337	translation_Loss:0.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:25.01	Hits@10:44.63	Best:25.01
2024-12-28 00:42:00,164: Snapshot:1	Epoch:33	Loss:1.315	translation_Loss:0.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.44                                                   	MRR:25.15	Hits@10:45.14	Best:25.15
2024-12-28 00:42:02,499: Snapshot:1	Epoch:34	Loss:1.282	translation_Loss:0.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.435                                                   	MRR:25.27	Hits@10:45.44	Best:25.27
2024-12-28 00:42:04,846: Snapshot:1	Epoch:35	Loss:1.257	translation_Loss:0.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.431                                                   	MRR:25.45	Hits@10:45.6	Best:25.45
2024-12-28 00:42:07,186: Snapshot:1	Epoch:36	Loss:1.235	translation_Loss:0.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.426                                                   	MRR:25.67	Hits@10:45.99	Best:25.67
2024-12-28 00:42:09,488: Snapshot:1	Epoch:37	Loss:1.203	translation_Loss:0.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.422                                                   	MRR:25.85	Hits@10:46.35	Best:25.85
2024-12-28 00:42:11,841: Snapshot:1	Epoch:38	Loss:1.192	translation_Loss:0.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.418                                                   	MRR:25.96	Hits@10:46.56	Best:25.96
2024-12-28 00:42:14,185: Snapshot:1	Epoch:39	Loss:1.158	translation_Loss:0.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.415                                                   	MRR:26.15	Hits@10:46.75	Best:26.15
2024-12-28 00:42:16,588: Snapshot:1	Epoch:40	Loss:1.137	translation_Loss:0.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.411                                                   	MRR:26.3	Hits@10:46.98	Best:26.3
2024-12-28 00:42:18,971: Snapshot:1	Epoch:41	Loss:1.124	translation_Loss:0.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.407                                                   	MRR:26.4	Hits@10:47.33	Best:26.4
2024-12-28 00:42:21,310: Snapshot:1	Epoch:42	Loss:1.11	translation_Loss:0.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.403                                                   	MRR:26.54	Hits@10:47.44	Best:26.54
2024-12-28 00:42:24,088: Snapshot:1	Epoch:43	Loss:1.089	translation_Loss:0.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:26.67	Hits@10:47.75	Best:26.67
2024-12-28 00:42:26,428: Snapshot:1	Epoch:44	Loss:1.075	translation_Loss:0.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:26.88	Hits@10:47.94	Best:26.88
2024-12-28 00:42:28,829: Snapshot:1	Epoch:45	Loss:1.067	translation_Loss:0.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.395                                                   	MRR:26.95	Hits@10:48.18	Best:26.95
2024-12-28 00:42:31,245: Snapshot:1	Epoch:46	Loss:1.049	translation_Loss:0.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.393                                                   	MRR:27.01	Hits@10:48.42	Best:27.01
2024-12-28 00:42:33,604: Snapshot:1	Epoch:47	Loss:1.038	translation_Loss:0.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.39                                                   	MRR:27.16	Hits@10:48.61	Best:27.16
2024-12-28 00:42:35,959: Snapshot:1	Epoch:48	Loss:1.018	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.387                                                   	MRR:27.22	Hits@10:48.81	Best:27.22
2024-12-28 00:42:38,279: Snapshot:1	Epoch:49	Loss:1.011	translation_Loss:0.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.384                                                   	MRR:27.37	Hits@10:49.03	Best:27.37
2024-12-28 00:42:40,635: Snapshot:1	Epoch:50	Loss:1.0	translation_Loss:0.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.381                                                   	MRR:27.49	Hits@10:49.15	Best:27.49
2024-12-28 00:42:42,982: Snapshot:1	Epoch:51	Loss:0.983	translation_Loss:0.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.378                                                   	MRR:27.58	Hits@10:49.28	Best:27.58
2024-12-28 00:42:45,321: Snapshot:1	Epoch:52	Loss:0.975	translation_Loss:0.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.376                                                   	MRR:27.63	Hits@10:49.28	Best:27.63
2024-12-28 00:42:47,608: Snapshot:1	Epoch:53	Loss:0.965	translation_Loss:0.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.373                                                   	MRR:27.68	Hits@10:49.34	Best:27.68
2024-12-28 00:42:49,943: Snapshot:1	Epoch:54	Loss:0.959	translation_Loss:0.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.371                                                   	MRR:27.85	Hits@10:49.45	Best:27.85
2024-12-28 00:42:52,247: Snapshot:1	Epoch:55	Loss:0.952	translation_Loss:0.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.369                                                   	MRR:27.86	Hits@10:49.51	Best:27.86
2024-12-28 00:42:54,944: Snapshot:1	Epoch:56	Loss:0.936	translation_Loss:0.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.366                                                   	MRR:27.89	Hits@10:49.6	Best:27.89
2024-12-28 00:42:57,283: Snapshot:1	Epoch:57	Loss:0.931	translation_Loss:0.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.364                                                   	MRR:28.0	Hits@10:49.83	Best:28.0
2024-12-28 00:42:59,618: Snapshot:1	Epoch:58	Loss:0.923	translation_Loss:0.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.362                                                   	MRR:28.06	Hits@10:49.83	Best:28.06
2024-12-28 00:43:01,921: Snapshot:1	Epoch:59	Loss:0.909	translation_Loss:0.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.361                                                   	MRR:28.11	Hits@10:49.89	Best:28.11
2024-12-28 00:43:04,213: Snapshot:1	Epoch:60	Loss:0.904	translation_Loss:0.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.359                                                   	MRR:28.27	Hits@10:49.98	Best:28.27
2024-12-28 00:43:06,561: Snapshot:1	Epoch:61	Loss:0.896	translation_Loss:0.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.357                                                   	MRR:28.37	Hits@10:50.05	Best:28.37
2024-12-28 00:43:08,839: Snapshot:1	Epoch:62	Loss:0.887	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:28.37	Hits@10:50.08	Best:28.37
2024-12-28 00:43:11,151: Snapshot:1	Epoch:63	Loss:0.888	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.353                                                   	MRR:28.49	Hits@10:50.24	Best:28.49
2024-12-28 00:43:13,442: Snapshot:1	Epoch:64	Loss:0.877	translation_Loss:0.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.351                                                   	MRR:28.49	Hits@10:50.26	Best:28.49
2024-12-28 00:43:15,778: Snapshot:1	Epoch:65	Loss:0.873	translation_Loss:0.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.349                                                   	MRR:28.56	Hits@10:50.22	Best:28.56
2024-12-28 00:43:18,101: Snapshot:1	Epoch:66	Loss:0.859	translation_Loss:0.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.347                                                   	MRR:28.51	Hits@10:50.18	Best:28.56
2024-12-28 00:43:20,432: Snapshot:1	Epoch:67	Loss:0.856	translation_Loss:0.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:28.57	Hits@10:50.34	Best:28.57
2024-12-28 00:43:22,757: Snapshot:1	Epoch:68	Loss:0.852	translation_Loss:0.508	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.344                                                   	MRR:28.69	Hits@10:50.25	Best:28.69
2024-12-28 00:43:25,087: Snapshot:1	Epoch:69	Loss:0.844	translation_Loss:0.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:28.74	Hits@10:50.1	Best:28.74
2024-12-28 00:43:27,877: Snapshot:1	Epoch:70	Loss:0.844	translation_Loss:0.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.341                                                   	MRR:28.8	Hits@10:50.42	Best:28.8
2024-12-28 00:43:30,183: Snapshot:1	Epoch:71	Loss:0.832	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.339                                                   	MRR:28.8	Hits@10:50.53	Best:28.8
2024-12-28 00:43:32,529: Snapshot:1	Epoch:72	Loss:0.827	translation_Loss:0.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.337                                                   	MRR:28.84	Hits@10:50.51	Best:28.84
2024-12-28 00:43:34,801: Snapshot:1	Epoch:73	Loss:0.825	translation_Loss:0.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.336                                                   	MRR:28.78	Hits@10:50.49	Best:28.84
2024-12-28 00:43:37,086: Snapshot:1	Epoch:74	Loss:0.818	translation_Loss:0.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.335                                                   	MRR:28.84	Hits@10:50.54	Best:28.84
2024-12-28 00:43:39,383: Snapshot:1	Epoch:75	Loss:0.812	translation_Loss:0.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:28.91	Hits@10:50.69	Best:28.91
2024-12-28 00:43:41,724: Snapshot:1	Epoch:76	Loss:0.806	translation_Loss:0.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.332                                                   	MRR:29.02	Hits@10:50.91	Best:29.02
2024-12-28 00:43:44,059: Snapshot:1	Epoch:77	Loss:0.805	translation_Loss:0.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.33                                                   	MRR:29.04	Hits@10:50.91	Best:29.04
2024-12-28 00:43:46,386: Snapshot:1	Epoch:78	Loss:0.803	translation_Loss:0.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.329                                                   	MRR:29.1	Hits@10:50.9	Best:29.1
2024-12-28 00:43:48,696: Snapshot:1	Epoch:79	Loss:0.797	translation_Loss:0.469	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:29.13	Hits@10:51.01	Best:29.13
2024-12-28 00:43:51,056: Snapshot:1	Epoch:80	Loss:0.793	translation_Loss:0.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.327                                                   	MRR:29.06	Hits@10:50.98	Best:29.13
2024-12-28 00:43:53,393: Snapshot:1	Epoch:81	Loss:0.78	translation_Loss:0.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.326                                                   	MRR:29.16	Hits@10:51.22	Best:29.16
2024-12-28 00:43:55,788: Snapshot:1	Epoch:82	Loss:0.782	translation_Loss:0.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.325                                                   	MRR:29.25	Hits@10:51.25	Best:29.25
2024-12-28 00:43:58,139: Snapshot:1	Epoch:83	Loss:0.783	translation_Loss:0.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.323                                                   	MRR:29.29	Hits@10:51.05	Best:29.29
2024-12-28 00:44:00,755: Snapshot:1	Epoch:84	Loss:0.774	translation_Loss:0.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:29.23	Hits@10:51.07	Best:29.29
2024-12-28 00:44:03,055: Snapshot:1	Epoch:85	Loss:0.769	translation_Loss:0.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.321                                                   	MRR:29.18	Hits@10:51.04	Best:29.29
2024-12-28 00:44:05,315: Snapshot:1	Epoch:86	Loss:0.77	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.32                                                   	MRR:29.26	Hits@10:51.13	Best:29.29
2024-12-28 00:44:07,738: Snapshot:1	Epoch:87	Loss:0.765	translation_Loss:0.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.319                                                   	MRR:29.3	Hits@10:51.16	Best:29.3
2024-12-28 00:44:10,109: Snapshot:1	Epoch:88	Loss:0.762	translation_Loss:0.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.318                                                   	MRR:29.35	Hits@10:51.2	Best:29.35
2024-12-28 00:44:12,474: Snapshot:1	Epoch:89	Loss:0.753	translation_Loss:0.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.317                                                   	MRR:29.37	Hits@10:51.16	Best:29.37
2024-12-28 00:44:14,758: Snapshot:1	Epoch:90	Loss:0.759	translation_Loss:0.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.315                                                   	MRR:29.37	Hits@10:51.28	Best:29.37
2024-12-28 00:44:17,095: Snapshot:1	Epoch:91	Loss:0.749	translation_Loss:0.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:29.42	Hits@10:51.31	Best:29.42
2024-12-28 00:44:19,415: Snapshot:1	Epoch:92	Loss:0.747	translation_Loss:0.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:29.37	Hits@10:51.28	Best:29.42
2024-12-28 00:44:21,724: Snapshot:1	Epoch:93	Loss:0.741	translation_Loss:0.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.313                                                   	MRR:29.39	Hits@10:51.25	Best:29.42
2024-12-28 00:44:24,017: Snapshot:1	Epoch:94	Loss:0.74	translation_Loss:0.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.312                                                   	MRR:29.38	Hits@10:51.33	Best:29.42
2024-12-28 00:44:26,314: Snapshot:1	Epoch:95	Loss:0.733	translation_Loss:0.423	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.31                                                   	MRR:29.42	Hits@10:51.37	Best:29.42
2024-12-28 00:44:28,650: Snapshot:1	Epoch:96	Loss:0.735	translation_Loss:0.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.309                                                   	MRR:29.48	Hits@10:51.35	Best:29.48
2024-12-28 00:44:31,477: Snapshot:1	Epoch:97	Loss:0.728	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.308                                                   	MRR:29.56	Hits@10:51.34	Best:29.56
2024-12-28 00:44:33,765: Snapshot:1	Epoch:98	Loss:0.73	translation_Loss:0.423	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.307                                                   	MRR:29.54	Hits@10:51.51	Best:29.56
2024-12-28 00:44:36,032: Snapshot:1	Epoch:99	Loss:0.723	translation_Loss:0.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.306                                                   	MRR:29.52	Hits@10:51.44	Best:29.56
2024-12-28 00:44:38,318: Snapshot:1	Epoch:100	Loss:0.725	translation_Loss:0.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.305                                                   	MRR:29.53	Hits@10:51.38	Best:29.56
2024-12-28 00:44:40,632: Snapshot:1	Epoch:101	Loss:0.717	translation_Loss:0.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.304                                                   	MRR:29.49	Hits@10:51.29	Best:29.56
2024-12-28 00:44:42,932: Early Stopping! Snapshot: 1 Epoch: 102 Best Results: 29.56
2024-12-28 00:44:42,932: Start to training tokens! Snapshot: 1 Epoch: 102 Loss:0.715 MRR:29.51 Best Results: 29.56
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:44:42,933: Snapshot:1	Epoch:102	Loss:0.715	translation_Loss:0.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.303                                                   	MRR:29.51	Hits@10:51.25	Best:29.56
2024-12-28 00:44:45,197: Snapshot:1	Epoch:103	Loss:6.596	translation_Loss:6.349	multi_layer_Loss:0.247	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.51	Hits@10:51.25	Best:29.56
2024-12-28 00:44:47,460: End of token training: 1 Epoch: 104 Loss:6.56 MRR:29.51 Best Results: 29.56
2024-12-28 00:44:47,461: Snapshot:1	Epoch:104	Loss:6.56	translation_Loss:6.343	multi_layer_Loss:0.216	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.51	Hits@10:51.25	Best:29.56
2024-12-28 00:44:47,717: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_10000/1model_best.tar'
2024-12-28 00:44:51,716: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2513 | 0.1418 | 0.3106 | 0.3781 |  0.4498 |
|     1      | 0.2919 | 0.1792 | 0.3479 | 0.4165 |  0.5121 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:45:25,379: Snapshot:2	Epoch:0	Loss:31.683	translation_Loss:31.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:4.11	Hits@10:8.61	Best:4.11
2024-12-28 00:45:35,703: Snapshot:2	Epoch:1	Loss:27.36	translation_Loss:26.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:6.37	Hits@10:13.97	Best:6.37
2024-12-28 00:45:46,185: Snapshot:2	Epoch:2	Loss:23.571	translation_Loss:22.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.763                                                   	MRR:8.55	Hits@10:18.88	Best:8.55
2024-12-28 00:45:56,263: Snapshot:2	Epoch:3	Loss:20.164	translation_Loss:19.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.027                                                   	MRR:10.54	Hits@10:22.26	Best:10.54
2024-12-28 00:46:06,379: Snapshot:2	Epoch:4	Loss:17.351	translation_Loss:16.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.22                                                   	MRR:12.38	Hits@10:25.59	Best:12.38
2024-12-28 00:46:16,844: Snapshot:2	Epoch:5	Loss:15.117	translation_Loss:13.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.345                                                   	MRR:13.84	Hits@10:28.46	Best:13.84
2024-12-28 00:46:26,941: Snapshot:2	Epoch:6	Loss:13.369	translation_Loss:11.952	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.417                                                   	MRR:14.96	Hits@10:30.44	Best:14.96
2024-12-28 00:46:37,213: Snapshot:2	Epoch:7	Loss:11.985	translation_Loss:10.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:15.88	Hits@10:31.95	Best:15.88
2024-12-28 00:46:47,729: Snapshot:2	Epoch:8	Loss:10.888	translation_Loss:9.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:16.71	Hits@10:33.16	Best:16.71
2024-12-28 00:46:57,933: Snapshot:2	Epoch:9	Loss:9.995	translation_Loss:8.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:17.52	Hits@10:34.3	Best:17.52
2024-12-28 00:47:08,016: Snapshot:2	Epoch:10	Loss:9.243	translation_Loss:7.791	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.451                                                   	MRR:18.01	Hits@10:35.06	Best:18.01
2024-12-28 00:47:18,762: Snapshot:2	Epoch:11	Loss:8.618	translation_Loss:7.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.425                                                   	MRR:18.54	Hits@10:35.7	Best:18.54
2024-12-28 00:47:28,995: Snapshot:2	Epoch:12	Loss:8.111	translation_Loss:6.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.394                                                   	MRR:18.96	Hits@10:36.13	Best:18.96
2024-12-28 00:47:39,253: Snapshot:2	Epoch:13	Loss:7.674	translation_Loss:6.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.363                                                   	MRR:19.16	Hits@10:36.46	Best:19.16
2024-12-28 00:47:49,486: Snapshot:2	Epoch:14	Loss:7.314	translation_Loss:5.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.33                                                   	MRR:19.44	Hits@10:36.78	Best:19.44
2024-12-28 00:48:00,063: Snapshot:2	Epoch:15	Loss:7.002	translation_Loss:5.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.298                                                   	MRR:19.65	Hits@10:37.01	Best:19.65
2024-12-28 00:48:10,276: Snapshot:2	Epoch:16	Loss:6.743	translation_Loss:5.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.269                                                   	MRR:19.76	Hits@10:37.17	Best:19.76
2024-12-28 00:48:20,497: Snapshot:2	Epoch:17	Loss:6.486	translation_Loss:5.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.24                                                   	MRR:19.87	Hits@10:37.42	Best:19.87
2024-12-28 00:48:31,055: Snapshot:2	Epoch:18	Loss:6.308	translation_Loss:5.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.216                                                   	MRR:20.01	Hits@10:37.47	Best:20.01
2024-12-28 00:48:41,208: Snapshot:2	Epoch:19	Loss:6.126	translation_Loss:4.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.191                                                   	MRR:20.22	Hits@10:37.47	Best:20.22
2024-12-28 00:48:51,304: Snapshot:2	Epoch:20	Loss:5.973	translation_Loss:4.803	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.17                                                   	MRR:20.31	Hits@10:37.58	Best:20.31
2024-12-28 00:49:01,909: Snapshot:2	Epoch:21	Loss:5.845	translation_Loss:4.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.153                                                   	MRR:20.45	Hits@10:37.68	Best:20.45
2024-12-28 00:49:12,034: Snapshot:2	Epoch:22	Loss:5.737	translation_Loss:4.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.135                                                   	MRR:20.47	Hits@10:37.61	Best:20.47
2024-12-28 00:49:22,182: Snapshot:2	Epoch:23	Loss:5.644	translation_Loss:4.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.118                                                   	MRR:20.6	Hits@10:37.72	Best:20.6
2024-12-28 00:49:32,735: Snapshot:2	Epoch:24	Loss:5.554	translation_Loss:4.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.104                                                   	MRR:20.67	Hits@10:37.82	Best:20.67
2024-12-28 00:49:42,999: Snapshot:2	Epoch:25	Loss:5.467	translation_Loss:4.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.09                                                   	MRR:20.71	Hits@10:37.78	Best:20.71
2024-12-28 00:49:53,195: Snapshot:2	Epoch:26	Loss:5.414	translation_Loss:4.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.078                                                   	MRR:20.81	Hits@10:37.85	Best:20.81
2024-12-28 00:50:03,359: Snapshot:2	Epoch:27	Loss:5.351	translation_Loss:4.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.07                                                   	MRR:20.87	Hits@10:37.76	Best:20.87
2024-12-28 00:50:13,976: Snapshot:2	Epoch:28	Loss:5.303	translation_Loss:4.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.061                                                   	MRR:20.84	Hits@10:37.81	Best:20.87
2024-12-28 00:50:24,266: Snapshot:2	Epoch:29	Loss:5.231	translation_Loss:4.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.053                                                   	MRR:20.96	Hits@10:37.89	Best:20.96
2024-12-28 00:50:34,428: Snapshot:2	Epoch:30	Loss:5.19	translation_Loss:4.145	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.046                                                   	MRR:21.01	Hits@10:37.83	Best:21.01
2024-12-28 00:50:45,105: Snapshot:2	Epoch:31	Loss:5.144	translation_Loss:4.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.038                                                   	MRR:21.02	Hits@10:37.83	Best:21.02
2024-12-28 00:50:55,208: Snapshot:2	Epoch:32	Loss:5.121	translation_Loss:4.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.033                                                   	MRR:21.05	Hits@10:37.84	Best:21.05
2024-12-28 00:51:05,395: Snapshot:2	Epoch:33	Loss:5.067	translation_Loss:4.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.028                                                   	MRR:21.02	Hits@10:37.9	Best:21.05
2024-12-28 00:51:15,905: Snapshot:2	Epoch:34	Loss:5.049	translation_Loss:4.027	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.022                                                   	MRR:21.07	Hits@10:37.92	Best:21.07
2024-12-28 00:51:26,126: Snapshot:2	Epoch:35	Loss:5.007	translation_Loss:3.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.018                                                   	MRR:21.14	Hits@10:37.94	Best:21.14
2024-12-28 00:51:36,195: Snapshot:2	Epoch:36	Loss:5.002	translation_Loss:3.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.013                                                   	MRR:21.12	Hits@10:37.91	Best:21.14
2024-12-28 00:51:46,801: Snapshot:2	Epoch:37	Loss:4.969	translation_Loss:3.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.008                                                   	MRR:21.07	Hits@10:37.93	Best:21.14
2024-12-28 00:51:56,896: Snapshot:2	Epoch:38	Loss:4.935	translation_Loss:3.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.004                                                   	MRR:21.05	Hits@10:38.0	Best:21.14
2024-12-28 00:52:06,939: Snapshot:2	Epoch:39	Loss:4.915	translation_Loss:3.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.001                                                   	MRR:21.02	Hits@10:37.93	Best:21.14
2024-12-28 00:52:17,351: Early Stopping! Snapshot: 2 Epoch: 40 Best Results: 21.14
2024-12-28 00:52:17,351: Start to training tokens! Snapshot: 2 Epoch: 40 Loss:4.9 MRR:21.06 Best Results: 21.14
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:52:17,351: Snapshot:2	Epoch:40	Loss:4.9	translation_Loss:3.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.999                                                   	MRR:21.06	Hits@10:37.92	Best:21.14
2024-12-28 00:52:27,387: Snapshot:2	Epoch:41	Loss:28.159	translation_Loss:27.386	multi_layer_Loss:0.773	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.06	Hits@10:37.92	Best:21.14
2024-12-28 00:52:37,281: End of token training: 2 Epoch: 42 Loss:27.699 MRR:21.06 Best Results: 21.14
2024-12-28 00:52:37,281: Snapshot:2	Epoch:42	Loss:27.699	translation_Loss:27.383	multi_layer_Loss:0.316	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.06	Hits@10:37.92	Best:21.14
2024-12-28 00:52:37,604: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_10000/2model_best.tar'
2024-12-28 00:52:46,073: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2459 | 0.1319 | 0.3067 | 0.3736 |  0.4524 |
|     1      | 0.2619 | 0.149  | 0.3138 | 0.386  |  0.4792 |
|     2      | 0.2117 | 0.1233 | 0.2465 | 0.3054 |  0.3829 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:53:26,236: Snapshot:3	Epoch:0	Loss:33.344	translation_Loss:33.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:4.89	Hits@10:12.38	Best:4.89
2024-12-28 00:53:38,741: Snapshot:3	Epoch:1	Loss:26.973	translation_Loss:26.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.714                                                   	MRR:7.5	Hits@10:18.4	Best:7.5
2024-12-28 00:53:51,217: Snapshot:3	Epoch:2	Loss:21.772	translation_Loss:20.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.249                                                   	MRR:10.0	Hits@10:22.9	Best:10.0
2024-12-28 00:54:03,603: Snapshot:3	Epoch:3	Loss:17.997	translation_Loss:16.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.674                                                   	MRR:12.08	Hits@10:26.32	Best:12.08
2024-12-28 00:54:16,080: Snapshot:3	Epoch:4	Loss:15.451	translation_Loss:13.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.955                                                   	MRR:13.7	Hits@10:28.99	Best:13.7
2024-12-28 00:54:28,474: Snapshot:3	Epoch:5	Loss:13.667	translation_Loss:11.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.12                                                   	MRR:15.0	Hits@10:31.25	Best:15.0
2024-12-28 00:54:40,969: Snapshot:3	Epoch:6	Loss:12.332	translation_Loss:10.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.205                                                   	MRR:16.0	Hits@10:33.03	Best:16.0
2024-12-28 00:54:53,296: Snapshot:3	Epoch:7	Loss:11.293	translation_Loss:9.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.236                                                   	MRR:16.95	Hits@10:34.35	Best:16.95
2024-12-28 00:55:05,623: Snapshot:3	Epoch:8	Loss:10.463	translation_Loss:8.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.236                                                   	MRR:17.62	Hits@10:35.28	Best:17.62
2024-12-28 00:55:17,954: Snapshot:3	Epoch:9	Loss:9.803	translation_Loss:7.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.216                                                   	MRR:18.15	Hits@10:36.03	Best:18.15
2024-12-28 00:55:30,299: Snapshot:3	Epoch:10	Loss:9.283	translation_Loss:7.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.183                                                   	MRR:18.67	Hits@10:36.56	Best:18.67
2024-12-28 00:55:43,088: Snapshot:3	Epoch:11	Loss:8.827	translation_Loss:6.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.144                                                   	MRR:19.03	Hits@10:36.96	Best:19.03
2024-12-28 00:55:55,394: Snapshot:3	Epoch:12	Loss:8.451	translation_Loss:6.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.106                                                   	MRR:19.29	Hits@10:37.32	Best:19.29
2024-12-28 00:56:07,807: Snapshot:3	Epoch:13	Loss:8.109	translation_Loss:6.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.062                                                   	MRR:19.57	Hits@10:37.46	Best:19.57
2024-12-28 00:56:20,702: Snapshot:3	Epoch:14	Loss:7.827	translation_Loss:5.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.017                                                   	MRR:19.78	Hits@10:37.71	Best:19.78
2024-12-28 00:56:32,999: Snapshot:3	Epoch:15	Loss:7.573	translation_Loss:5.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.977                                                   	MRR:19.96	Hits@10:37.9	Best:19.96
2024-12-28 00:56:45,368: Snapshot:3	Epoch:16	Loss:7.349	translation_Loss:5.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.939                                                   	MRR:20.11	Hits@10:38.09	Best:20.11
2024-12-28 00:56:58,275: Snapshot:3	Epoch:17	Loss:7.178	translation_Loss:5.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.903                                                   	MRR:20.26	Hits@10:38.22	Best:20.26
2024-12-28 00:57:10,691: Snapshot:3	Epoch:18	Loss:7.018	translation_Loss:5.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.868                                                   	MRR:20.37	Hits@10:38.16	Best:20.37
2024-12-28 00:57:23,605: Snapshot:3	Epoch:19	Loss:6.887	translation_Loss:5.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.835                                                   	MRR:20.5	Hits@10:38.28	Best:20.5
2024-12-28 00:57:36,049: Snapshot:3	Epoch:20	Loss:6.75	translation_Loss:4.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.807                                                   	MRR:20.51	Hits@10:38.35	Best:20.51
2024-12-28 00:57:48,307: Snapshot:3	Epoch:21	Loss:6.633	translation_Loss:4.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.782                                                   	MRR:20.63	Hits@10:38.4	Best:20.63
2024-12-28 00:58:01,092: Snapshot:3	Epoch:22	Loss:6.556	translation_Loss:4.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:20.64	Hits@10:38.45	Best:20.64
2024-12-28 00:58:13,477: Snapshot:3	Epoch:23	Loss:6.465	translation_Loss:4.73	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.735                                                   	MRR:20.68	Hits@10:38.51	Best:20.68
2024-12-28 00:58:25,785: Snapshot:3	Epoch:24	Loss:6.374	translation_Loss:4.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.715                                                   	MRR:20.72	Hits@10:38.42	Best:20.72
2024-12-28 00:58:38,503: Snapshot:3	Epoch:25	Loss:6.313	translation_Loss:4.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.696                                                   	MRR:20.82	Hits@10:38.45	Best:20.82
2024-12-28 00:58:50,856: Snapshot:3	Epoch:26	Loss:6.262	translation_Loss:4.582	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.68                                                   	MRR:20.83	Hits@10:38.51	Best:20.83
2024-12-28 00:59:03,213: Snapshot:3	Epoch:27	Loss:6.188	translation_Loss:4.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.663                                                   	MRR:20.79	Hits@10:38.34	Best:20.83
2024-12-28 00:59:15,938: Snapshot:3	Epoch:28	Loss:6.144	translation_Loss:4.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.651                                                   	MRR:20.84	Hits@10:38.36	Best:20.84
2024-12-28 00:59:28,256: Snapshot:3	Epoch:29	Loss:6.101	translation_Loss:4.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.638                                                   	MRR:20.8	Hits@10:38.45	Best:20.84
2024-12-28 00:59:40,973: Snapshot:3	Epoch:30	Loss:6.062	translation_Loss:4.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.626                                                   	MRR:20.87	Hits@10:38.52	Best:20.87
2024-12-28 00:59:53,356: Snapshot:3	Epoch:31	Loss:6.018	translation_Loss:4.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.615                                                   	MRR:20.9	Hits@10:38.51	Best:20.9
2024-12-28 01:00:05,878: Snapshot:3	Epoch:32	Loss:5.99	translation_Loss:4.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.606                                                   	MRR:20.93	Hits@10:38.55	Best:20.93
2024-12-28 01:00:18,769: Snapshot:3	Epoch:33	Loss:5.949	translation_Loss:4.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.597                                                   	MRR:20.91	Hits@10:38.46	Best:20.93
2024-12-28 01:00:31,087: Snapshot:3	Epoch:34	Loss:5.92	translation_Loss:4.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.589                                                   	MRR:20.93	Hits@10:38.51	Best:20.93
2024-12-28 01:00:43,348: Snapshot:3	Epoch:35	Loss:5.91	translation_Loss:4.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.583                                                   	MRR:20.92	Hits@10:38.5	Best:20.93
2024-12-28 01:00:55,946: Snapshot:3	Epoch:36	Loss:5.86	translation_Loss:4.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.576                                                   	MRR:20.92	Hits@10:38.44	Best:20.93
2024-12-28 01:01:08,271: Early Stopping! Snapshot: 3 Epoch: 37 Best Results: 20.93
2024-12-28 01:01:08,272: Start to training tokens! Snapshot: 3 Epoch: 37 Loss:5.867 MRR:20.89 Best Results: 20.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 01:01:08,272: Snapshot:3	Epoch:37	Loss:5.867	translation_Loss:4.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.568                                                   	MRR:20.89	Hits@10:38.46	Best:20.93
2024-12-28 01:01:20,385: Snapshot:3	Epoch:38	Loss:30.675	translation_Loss:29.778	multi_layer_Loss:0.897	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.89	Hits@10:38.46	Best:20.93
2024-12-28 01:01:32,934: End of token training: 3 Epoch: 39 Loss:30.108 MRR:20.89 Best Results: 20.93
2024-12-28 01:01:32,934: Snapshot:3	Epoch:39	Loss:30.108	translation_Loss:29.814	multi_layer_Loss:0.294	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.89	Hits@10:38.46	Best:20.93
2024-12-28 01:01:33,194: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_10000/3model_best.tar'
2024-12-28 01:01:46,928: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2509 | 0.1412 | 0.3083 | 0.3737 |  0.4518 |
|     1      | 0.2621 | 0.1533 | 0.3071 | 0.3769 |  0.4717 |
|     2      | 0.2036 | 0.112  | 0.2412 | 0.3009 |  0.3792 |
|     3      | 0.2096 | 0.1162 | 0.2476 | 0.3084 |  0.3865 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 01:02:05,568: Snapshot:4	Epoch:0	Loss:10.949	translation_Loss:10.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.042                                                   	MRR:7.2	Hits@10:14.34	Best:7.2
2024-12-28 01:02:10,678: Snapshot:4	Epoch:1	Loss:9.676	translation_Loss:9.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.121                                                   	MRR:7.72	Hits@10:15.7	Best:7.72
2024-12-28 01:02:15,786: Snapshot:4	Epoch:2	Loss:8.641	translation_Loss:8.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.223                                                   	MRR:8.41	Hits@10:17.2	Best:8.41
2024-12-28 01:02:20,896: Snapshot:4	Epoch:3	Loss:7.74	translation_Loss:7.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.338                                                   	MRR:9.8	Hits@10:20.65	Best:9.8
2024-12-28 01:02:25,997: Snapshot:4	Epoch:4	Loss:6.965	translation_Loss:6.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.452                                                   	MRR:12.34	Hits@10:27.0	Best:12.34
2024-12-28 01:02:31,110: Snapshot:4	Epoch:5	Loss:6.315	translation_Loss:5.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.56                                                   	MRR:13.92	Hits@10:30.18	Best:13.92
2024-12-28 01:02:36,191: Snapshot:4	Epoch:6	Loss:5.794	translation_Loss:5.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.658                                                   	MRR:15.18	Hits@10:31.57	Best:15.18
2024-12-28 01:02:41,356: Snapshot:4	Epoch:7	Loss:5.398	translation_Loss:4.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.744                                                   	MRR:16.08	Hits@10:32.36	Best:16.08
2024-12-28 01:02:46,474: Snapshot:4	Epoch:8	Loss:5.1	translation_Loss:4.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.819                                                   	MRR:17.19	Hits@10:33.02	Best:17.19
2024-12-28 01:02:51,609: Snapshot:4	Epoch:9	Loss:4.843	translation_Loss:3.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.885                                                   	MRR:17.91	Hits@10:33.82	Best:17.91
2024-12-28 01:02:56,749: Snapshot:4	Epoch:10	Loss:4.645	translation_Loss:3.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.942                                                   	MRR:17.94	Hits@10:34.47	Best:17.94
2024-12-28 01:03:02,266: Snapshot:4	Epoch:11	Loss:4.463	translation_Loss:3.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.992                                                   	MRR:18.4	Hits@10:34.72	Best:18.4
2024-12-28 01:03:07,376: Snapshot:4	Epoch:12	Loss:4.311	translation_Loss:3.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.033                                                   	MRR:19.06	Hits@10:35.05	Best:19.06
2024-12-28 01:03:12,481: Snapshot:4	Epoch:13	Loss:4.168	translation_Loss:3.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.07                                                   	MRR:19.18	Hits@10:35.2	Best:19.18
2024-12-28 01:03:17,576: Snapshot:4	Epoch:14	Loss:4.042	translation_Loss:2.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.1                                                   	MRR:19.14	Hits@10:35.62	Best:19.18
2024-12-28 01:03:22,572: Snapshot:4	Epoch:15	Loss:3.935	translation_Loss:2.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.126                                                   	MRR:19.05	Hits@10:35.71	Best:19.18
2024-12-28 01:03:27,607: Snapshot:4	Epoch:16	Loss:3.826	translation_Loss:2.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.149                                                   	MRR:19.6	Hits@10:35.9	Best:19.6
2024-12-28 01:03:32,743: Snapshot:4	Epoch:17	Loss:3.728	translation_Loss:2.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.167                                                   	MRR:20.02	Hits@10:35.98	Best:20.02
2024-12-28 01:03:38,308: Snapshot:4	Epoch:18	Loss:3.633	translation_Loss:2.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.181                                                   	MRR:20.08	Hits@10:36.17	Best:20.08
2024-12-28 01:03:43,477: Snapshot:4	Epoch:19	Loss:3.556	translation_Loss:2.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.192                                                   	MRR:20.52	Hits@10:36.27	Best:20.52
2024-12-28 01:03:48,557: Snapshot:4	Epoch:20	Loss:3.463	translation_Loss:2.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.201                                                   	MRR:20.51	Hits@10:36.51	Best:20.52
2024-12-28 01:03:53,559: Snapshot:4	Epoch:21	Loss:3.392	translation_Loss:2.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.208                                                   	MRR:20.52	Hits@10:36.5	Best:20.52
2024-12-28 01:03:58,610: Snapshot:4	Epoch:22	Loss:3.315	translation_Loss:2.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.213                                                   	MRR:20.58	Hits@10:36.55	Best:20.58
2024-12-28 01:04:03,648: Snapshot:4	Epoch:23	Loss:3.252	translation_Loss:2.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.217                                                   	MRR:20.4	Hits@10:36.62	Best:20.58
2024-12-28 01:04:08,678: Snapshot:4	Epoch:24	Loss:3.197	translation_Loss:1.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.219                                                   	MRR:20.69	Hits@10:36.8	Best:20.69
2024-12-28 01:04:14,151: Snapshot:4	Epoch:25	Loss:3.123	translation_Loss:1.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.218                                                   	MRR:20.96	Hits@10:36.82	Best:20.96
2024-12-28 01:04:19,178: Snapshot:4	Epoch:26	Loss:3.068	translation_Loss:1.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.217                                                   	MRR:20.57	Hits@10:36.95	Best:20.96
2024-12-28 01:04:24,205: Snapshot:4	Epoch:27	Loss:3.018	translation_Loss:1.803	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.215                                                   	MRR:20.75	Hits@10:36.94	Best:20.96
2024-12-28 01:04:29,391: Snapshot:4	Epoch:28	Loss:2.959	translation_Loss:1.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.213                                                   	MRR:21.18	Hits@10:37.1	Best:21.18
2024-12-28 01:04:34,548: Snapshot:4	Epoch:29	Loss:2.906	translation_Loss:1.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.209                                                   	MRR:21.31	Hits@10:37.12	Best:21.31
2024-12-28 01:04:39,613: Snapshot:4	Epoch:30	Loss:2.866	translation_Loss:1.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.205                                                   	MRR:21.27	Hits@10:37.12	Best:21.31
2024-12-28 01:04:44,637: Snapshot:4	Epoch:31	Loss:2.83	translation_Loss:1.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.201                                                   	MRR:21.83	Hits@10:37.22	Best:21.83
2024-12-28 01:04:50,134: Snapshot:4	Epoch:32	Loss:2.775	translation_Loss:1.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.197                                                   	MRR:21.46	Hits@10:37.4	Best:21.83
2024-12-28 01:04:55,139: Snapshot:4	Epoch:33	Loss:2.75	translation_Loss:1.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.192                                                   	MRR:21.5	Hits@10:37.29	Best:21.83
2024-12-28 01:05:00,250: Snapshot:4	Epoch:34	Loss:2.703	translation_Loss:1.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.185                                                   	MRR:21.62	Hits@10:37.22	Best:21.83
2024-12-28 01:05:05,286: Snapshot:4	Epoch:35	Loss:2.677	translation_Loss:1.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.179                                                   	MRR:21.72	Hits@10:37.31	Best:21.83
2024-12-28 01:05:10,297: Early Stopping! Snapshot: 4 Epoch: 36 Best Results: 21.83
2024-12-28 01:05:10,297: Start to training tokens! Snapshot: 4 Epoch: 36 Loss:2.644 MRR:21.7 Best Results: 21.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 01:05:10,297: Snapshot:4	Epoch:36	Loss:2.644	translation_Loss:1.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.174                                                   	MRR:21.7	Hits@10:37.26	Best:21.83
2024-12-28 01:05:15,203: Snapshot:4	Epoch:37	Loss:13.604	translation_Loss:13.157	multi_layer_Loss:0.447	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.7	Hits@10:37.26	Best:21.83
2024-12-28 01:05:20,150: End of token training: 4 Epoch: 38 Loss:13.486 MRR:21.7 Best Results: 21.83
2024-12-28 01:05:20,151: Snapshot:4	Epoch:38	Loss:13.486	translation_Loss:13.147	multi_layer_Loss:0.339	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.7	Hits@10:37.26	Best:21.83
2024-12-28 01:05:20,414: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.0001_2048_10000/4model_best.tar'
2024-12-28 01:05:37,094: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2155 | 0.1157 | 0.2652 | 0.3232 |  0.3942 |
|     1      | 0.2406 | 0.1378 | 0.2816 | 0.3478 |  0.4406 |
|     2      | 0.1844 | 0.1005 | 0.214  | 0.2701 |  0.3467 |
|     3      | 0.1747 | 0.0895 | 0.2041 | 0.2625 |  0.3382 |
|     4      | 0.2168 | 0.1338 | 0.2515 | 0.3037 |  0.3732 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 01:05:37,096: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2546 | 0.1444 | 0.3153 | 0.3796 |  0.4518 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2513 | 0.1418 | 0.3106 | 0.3781 |  0.4498 |
|     1      | 0.2919 | 0.1792 | 0.3479 | 0.4165 |  0.5121 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2459 | 0.1319 | 0.3067 | 0.3736 |  0.4524 |
|     1      | 0.2619 | 0.149  | 0.3138 | 0.386  |  0.4792 |
|     2      | 0.2117 | 0.1233 | 0.2465 | 0.3054 |  0.3829 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2509 | 0.1412 | 0.3083 | 0.3737 |  0.4518 |
|     1      | 0.2621 | 0.1533 | 0.3071 | 0.3769 |  0.4717 |
|     2      | 0.2036 | 0.112  | 0.2412 | 0.3009 |  0.3792 |
|     3      | 0.2096 | 0.1162 | 0.2476 | 0.3084 |  0.3865 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2155 | 0.1157 | 0.2652 | 0.3232 |  0.3942 |
|     1      | 0.2406 | 0.1378 | 0.2816 | 0.3478 |  0.4406 |
|     2      | 0.1844 | 0.1005 | 0.214  | 0.2701 |  0.3467 |
|     3      | 0.1747 | 0.0895 | 0.2041 | 0.2625 |  0.3382 |
|     4      | 0.2168 | 0.1338 | 0.2515 | 0.3037 |  0.3732 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 01:05:37,097: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 370.3316011428833  |   0.255   |    0.144     |    0.315     |     0.452     |
|    1     | 255.02433347702026 |   0.262   |    0.152     |    0.321     |     0.466     |
|    2     | 461.32329535484314 |    0.23   |    0.129     |    0.276     |     0.419     |
|    3     | 521.3278086185455  |   0.221   |    0.123     |    0.263     |     0.405     |
|    4     | 210.70598196983337 |   0.195   |    0.106     |     0.23     |     0.362     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 01:05:37,097: Sum_Training_Time:1818.7130205631256
2024-12-28 01:05:37,097: Every_Training_Time:[370.3316011428833, 255.02433347702026, 461.32329535484314, 521.3278086185455, 210.70598196983337]
2024-12-28 01:05:37,097: Forward transfer: 0.044925 Backward transfer: -0.038149999999999996
2024-12-28 01:06:17,364: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228010542/HYBRIDHYBRID_0.00001_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.00001_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.00001_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 01:06:27,142: Snapshot:0	Epoch:0	Loss:97.396	translation_Loss:97.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.68	Hits@10:0.69	Best:0.68
2024-12-28 01:06:33,255: Snapshot:0	Epoch:1	Loss:96.39	translation_Loss:96.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.7	Hits@10:0.7	Best:0.7
2024-12-28 01:06:39,349: Snapshot:0	Epoch:2	Loss:95.378	translation_Loss:95.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.75	Hits@10:0.83	Best:0.75
2024-12-28 01:06:45,499: Snapshot:0	Epoch:3	Loss:94.399	translation_Loss:94.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.85	Hits@10:1.11	Best:0.85
2024-12-28 01:06:51,666: Snapshot:0	Epoch:4	Loss:93.403	translation_Loss:93.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.05	Hits@10:1.72	Best:1.05
2024-12-28 01:06:57,860: Snapshot:0	Epoch:5	Loss:92.439	translation_Loss:92.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.34	Hits@10:2.52	Best:1.34
2024-12-28 01:07:03,957: Snapshot:0	Epoch:6	Loss:91.443	translation_Loss:91.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.75	Hits@10:3.51	Best:1.75
2024-12-28 01:07:10,180: Snapshot:0	Epoch:7	Loss:90.46	translation_Loss:90.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.22	Hits@10:4.54	Best:2.22
2024-12-28 01:07:16,752: Snapshot:0	Epoch:8	Loss:89.49	translation_Loss:89.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.67	Hits@10:5.46	Best:2.67
2024-12-28 01:07:22,881: Snapshot:0	Epoch:9	Loss:88.558	translation_Loss:88.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.07	Hits@10:6.25	Best:3.07
2024-12-28 01:07:29,040: Snapshot:0	Epoch:10	Loss:87.607	translation_Loss:87.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.41	Hits@10:6.9	Best:3.41
2024-12-28 01:07:35,284: Snapshot:0	Epoch:11	Loss:86.688	translation_Loss:86.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.69	Hits@10:7.46	Best:3.69
2024-12-28 01:07:41,517: Snapshot:0	Epoch:12	Loss:85.744	translation_Loss:85.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.95	Hits@10:8.13	Best:3.95
2024-12-28 01:07:47,624: Snapshot:0	Epoch:13	Loss:84.859	translation_Loss:84.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.22	Hits@10:8.81	Best:4.22
2024-12-28 01:07:53,855: Snapshot:0	Epoch:14	Loss:83.956	translation_Loss:83.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.47	Hits@10:9.53	Best:4.47
2024-12-28 01:08:00,051: Snapshot:0	Epoch:15	Loss:83.083	translation_Loss:83.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.73	Hits@10:10.21	Best:4.73
2024-12-28 01:08:06,172: Snapshot:0	Epoch:16	Loss:82.198	translation_Loss:82.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.99	Hits@10:10.92	Best:4.99
2024-12-28 01:08:12,328: Snapshot:0	Epoch:17	Loss:81.349	translation_Loss:81.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.25	Hits@10:11.63	Best:5.25
2024-12-28 01:08:18,451: Snapshot:0	Epoch:18	Loss:80.501	translation_Loss:80.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.47	Hits@10:12.22	Best:5.47
2024-12-28 01:08:24,544: Snapshot:0	Epoch:19	Loss:79.68	translation_Loss:79.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.69	Hits@10:12.71	Best:5.69
2024-12-28 01:08:30,728: Snapshot:0	Epoch:20	Loss:78.831	translation_Loss:78.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.89	Hits@10:13.28	Best:5.89
2024-12-28 01:08:36,862: Snapshot:0	Epoch:21	Loss:77.989	translation_Loss:77.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.07	Hits@10:13.81	Best:6.07
2024-12-28 01:08:42,988: Snapshot:0	Epoch:22	Loss:77.206	translation_Loss:77.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.25	Hits@10:14.27	Best:6.25
2024-12-28 01:08:49,219: Snapshot:0	Epoch:23	Loss:76.391	translation_Loss:76.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.4	Hits@10:14.69	Best:6.4
2024-12-28 01:08:55,390: Snapshot:0	Epoch:24	Loss:75.63	translation_Loss:75.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.55	Hits@10:15.07	Best:6.55
2024-12-28 01:09:01,544: Snapshot:0	Epoch:25	Loss:74.765	translation_Loss:74.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.69	Hits@10:15.51	Best:6.69
2024-12-28 01:09:08,133: Snapshot:0	Epoch:26	Loss:74.032	translation_Loss:74.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.83	Hits@10:15.91	Best:6.83
2024-12-28 01:09:14,341: Snapshot:0	Epoch:27	Loss:73.241	translation_Loss:73.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.96	Hits@10:16.25	Best:6.96
2024-12-28 01:09:20,504: Snapshot:0	Epoch:28	Loss:72.436	translation_Loss:72.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.1	Hits@10:16.67	Best:7.1
2024-12-28 01:09:26,744: Snapshot:0	Epoch:29	Loss:71.71	translation_Loss:71.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.22	Hits@10:17.01	Best:7.22
2024-12-28 01:09:32,898: Snapshot:0	Epoch:30	Loss:70.938	translation_Loss:70.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.36	Hits@10:17.33	Best:7.36
2024-12-28 01:09:39,121: Snapshot:0	Epoch:31	Loss:70.121	translation_Loss:70.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.49	Hits@10:17.66	Best:7.49
2024-12-28 01:09:45,316: Snapshot:0	Epoch:32	Loss:69.384	translation_Loss:69.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.62	Hits@10:18.05	Best:7.62
2024-12-28 01:09:51,523: Snapshot:0	Epoch:33	Loss:68.612	translation_Loss:68.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.75	Hits@10:18.47	Best:7.75
2024-12-28 01:09:57,719: Snapshot:0	Epoch:34	Loss:67.873	translation_Loss:67.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.87	Hits@10:18.87	Best:7.87
2024-12-28 01:10:03,911: Snapshot:0	Epoch:35	Loss:67.157	translation_Loss:67.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.0	Hits@10:19.24	Best:8.0
2024-12-28 01:10:10,120: Snapshot:0	Epoch:36	Loss:66.306	translation_Loss:66.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.13	Hits@10:19.56	Best:8.13
2024-12-28 01:10:16,287: Snapshot:0	Epoch:37	Loss:65.591	translation_Loss:65.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.26	Hits@10:19.9	Best:8.26
2024-12-28 01:10:22,434: Snapshot:0	Epoch:38	Loss:64.867	translation_Loss:64.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.41	Hits@10:20.22	Best:8.41
2024-12-28 01:10:28,572: Snapshot:0	Epoch:39	Loss:64.089	translation_Loss:64.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.57	Hits@10:20.64	Best:8.57
2024-12-28 01:10:34,766: Snapshot:0	Epoch:40	Loss:63.355	translation_Loss:63.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.73	Hits@10:21.08	Best:8.73
2024-12-28 01:10:41,008: Snapshot:0	Epoch:41	Loss:62.612	translation_Loss:62.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.51	Best:8.89
2024-12-28 01:10:47,181: Snapshot:0	Epoch:42	Loss:61.833	translation_Loss:61.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.08	Hits@10:21.94	Best:9.08
2024-12-28 01:10:53,313: Snapshot:0	Epoch:43	Loss:61.101	translation_Loss:61.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.28	Hits@10:22.36	Best:9.28
2024-12-28 01:10:59,995: Snapshot:0	Epoch:44	Loss:60.338	translation_Loss:60.338	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.46	Hits@10:22.85	Best:9.46
2024-12-28 01:11:06,099: Snapshot:0	Epoch:45	Loss:59.632	translation_Loss:59.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.65	Hits@10:23.33	Best:9.65
2024-12-28 01:11:12,308: Snapshot:0	Epoch:46	Loss:58.866	translation_Loss:58.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.89	Hits@10:23.78	Best:9.89
2024-12-28 01:11:18,507: Snapshot:0	Epoch:47	Loss:58.15	translation_Loss:58.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.14	Hits@10:24.19	Best:10.14
2024-12-28 01:11:24,716: Snapshot:0	Epoch:48	Loss:57.382	translation_Loss:57.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.41	Hits@10:24.69	Best:10.41
2024-12-28 01:11:30,859: Snapshot:0	Epoch:49	Loss:56.704	translation_Loss:56.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.64	Hits@10:25.22	Best:10.64
2024-12-28 01:11:37,011: Snapshot:0	Epoch:50	Loss:55.943	translation_Loss:55.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.86	Hits@10:25.68	Best:10.86
2024-12-28 01:11:43,154: Snapshot:0	Epoch:51	Loss:55.133	translation_Loss:55.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.12	Hits@10:26.16	Best:11.12
2024-12-28 01:11:49,269: Snapshot:0	Epoch:52	Loss:54.44	translation_Loss:54.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.38	Hits@10:26.69	Best:11.38
2024-12-28 01:11:55,407: Snapshot:0	Epoch:53	Loss:53.699	translation_Loss:53.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.65	Hits@10:27.18	Best:11.65
2024-12-28 01:12:01,589: Snapshot:0	Epoch:54	Loss:53.002	translation_Loss:53.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.89	Hits@10:27.72	Best:11.89
2024-12-28 01:12:07,760: Snapshot:0	Epoch:55	Loss:52.243	translation_Loss:52.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.13	Hits@10:28.19	Best:12.13
2024-12-28 01:12:13,888: Snapshot:0	Epoch:56	Loss:51.513	translation_Loss:51.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.39	Hits@10:28.64	Best:12.39
2024-12-28 01:12:20,128: Snapshot:0	Epoch:57	Loss:50.748	translation_Loss:50.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.62	Hits@10:29.07	Best:12.62
2024-12-28 01:12:26,373: Snapshot:0	Epoch:58	Loss:50.103	translation_Loss:50.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.84	Hits@10:29.46	Best:12.84
2024-12-28 01:12:32,603: Snapshot:0	Epoch:59	Loss:49.354	translation_Loss:49.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.07	Hits@10:29.9	Best:13.07
2024-12-28 01:12:38,754: Snapshot:0	Epoch:60	Loss:48.597	translation_Loss:48.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.29	Hits@10:30.3	Best:13.29
2024-12-28 01:12:44,986: Snapshot:0	Epoch:61	Loss:47.842	translation_Loss:47.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.52	Hits@10:30.83	Best:13.52
2024-12-28 01:12:51,573: Snapshot:0	Epoch:62	Loss:47.112	translation_Loss:47.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.78	Hits@10:31.22	Best:13.78
2024-12-28 01:12:57,773: Snapshot:0	Epoch:63	Loss:46.426	translation_Loss:46.426	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.01	Hits@10:31.64	Best:14.01
2024-12-28 01:13:03,891: Snapshot:0	Epoch:64	Loss:45.7	translation_Loss:45.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.24	Hits@10:31.94	Best:14.24
2024-12-28 01:13:10,117: Snapshot:0	Epoch:65	Loss:44.964	translation_Loss:44.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.45	Hits@10:32.26	Best:14.45
2024-12-28 01:13:16,317: Snapshot:0	Epoch:66	Loss:44.296	translation_Loss:44.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.64	Hits@10:32.62	Best:14.64
2024-12-28 01:13:22,469: Snapshot:0	Epoch:67	Loss:43.487	translation_Loss:43.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.84	Hits@10:32.94	Best:14.84
2024-12-28 01:13:28,643: Snapshot:0	Epoch:68	Loss:42.822	translation_Loss:42.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.07	Hits@10:33.38	Best:15.07
2024-12-28 01:13:34,800: Snapshot:0	Epoch:69	Loss:42.072	translation_Loss:42.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.33	Hits@10:33.71	Best:15.33
2024-12-28 01:13:41,261: Snapshot:0	Epoch:70	Loss:41.398	translation_Loss:41.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.55	Hits@10:34.02	Best:15.55
2024-12-28 01:13:47,487: Snapshot:0	Epoch:71	Loss:40.71	translation_Loss:40.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.77	Hits@10:34.3	Best:15.77
2024-12-28 01:13:53,657: Snapshot:0	Epoch:72	Loss:40.012	translation_Loss:40.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.03	Hits@10:34.61	Best:16.03
2024-12-28 01:13:59,866: Snapshot:0	Epoch:73	Loss:39.278	translation_Loss:39.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.25	Hits@10:34.87	Best:16.25
2024-12-28 01:14:06,071: Snapshot:0	Epoch:74	Loss:38.586	translation_Loss:38.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.48	Hits@10:35.21	Best:16.48
2024-12-28 01:14:12,204: Snapshot:0	Epoch:75	Loss:37.976	translation_Loss:37.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.72	Hits@10:35.46	Best:16.72
2024-12-28 01:14:18,395: Snapshot:0	Epoch:76	Loss:37.251	translation_Loss:37.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.92	Hits@10:35.66	Best:16.92
2024-12-28 01:14:24,622: Snapshot:0	Epoch:77	Loss:36.668	translation_Loss:36.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.17	Hits@10:35.92	Best:17.17
2024-12-28 01:14:30,760: Snapshot:0	Epoch:78	Loss:35.833	translation_Loss:35.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.36	Hits@10:36.15	Best:17.36
2024-12-28 01:14:36,950: Snapshot:0	Epoch:79	Loss:35.274	translation_Loss:35.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.55	Hits@10:36.44	Best:17.55
2024-12-28 01:14:43,584: Snapshot:0	Epoch:80	Loss:34.591	translation_Loss:34.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.73	Hits@10:36.7	Best:17.73
2024-12-28 01:14:49,716: Snapshot:0	Epoch:81	Loss:34.025	translation_Loss:34.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.97	Hits@10:36.97	Best:17.97
2024-12-28 01:14:55,965: Snapshot:0	Epoch:82	Loss:33.392	translation_Loss:33.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.18	Hits@10:37.21	Best:18.18
2024-12-28 01:15:02,096: Snapshot:0	Epoch:83	Loss:32.802	translation_Loss:32.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.38	Hits@10:37.45	Best:18.38
2024-12-28 01:15:08,332: Snapshot:0	Epoch:84	Loss:32.109	translation_Loss:32.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.52	Hits@10:37.67	Best:18.52
2024-12-28 01:15:14,496: Snapshot:0	Epoch:85	Loss:31.526	translation_Loss:31.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.72	Hits@10:37.9	Best:18.72
2024-12-28 01:15:20,721: Snapshot:0	Epoch:86	Loss:30.928	translation_Loss:30.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.91	Hits@10:38.0	Best:18.91
2024-12-28 01:15:26,832: Snapshot:0	Epoch:87	Loss:30.42	translation_Loss:30.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.08	Hits@10:38.27	Best:19.08
2024-12-28 01:15:33,069: Snapshot:0	Epoch:88	Loss:29.812	translation_Loss:29.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.22	Hits@10:38.52	Best:19.22
2024-12-28 01:15:39,197: Snapshot:0	Epoch:89	Loss:29.181	translation_Loss:29.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.41	Hits@10:38.65	Best:19.41
2024-12-28 01:15:45,328: Snapshot:0	Epoch:90	Loss:28.65	translation_Loss:28.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.59	Hits@10:38.89	Best:19.59
2024-12-28 01:15:51,495: Snapshot:0	Epoch:91	Loss:28.122	translation_Loss:28.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.78	Hits@10:39.14	Best:19.78
2024-12-28 01:15:57,688: Snapshot:0	Epoch:92	Loss:27.588	translation_Loss:27.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.92	Hits@10:39.34	Best:19.92
2024-12-28 01:16:03,832: Snapshot:0	Epoch:93	Loss:27.085	translation_Loss:27.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.04	Hits@10:39.46	Best:20.04
2024-12-28 01:16:09,963: Snapshot:0	Epoch:94	Loss:26.537	translation_Loss:26.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.18	Hits@10:39.6	Best:20.18
2024-12-28 01:16:16,078: Snapshot:0	Epoch:95	Loss:26.007	translation_Loss:26.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.33	Hits@10:39.79	Best:20.33
2024-12-28 01:16:22,233: Snapshot:0	Epoch:96	Loss:25.467	translation_Loss:25.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.49	Hits@10:40.02	Best:20.49
2024-12-28 01:16:28,467: Snapshot:0	Epoch:97	Loss:25.079	translation_Loss:25.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.63	Hits@10:40.21	Best:20.63
2024-12-28 01:16:34,693: Snapshot:0	Epoch:98	Loss:24.557	translation_Loss:24.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.78	Hits@10:40.38	Best:20.78
2024-12-28 01:16:41,333: Snapshot:0	Epoch:99	Loss:24.04	translation_Loss:24.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.91	Hits@10:40.52	Best:20.91
2024-12-28 01:16:47,510: Snapshot:0	Epoch:100	Loss:23.597	translation_Loss:23.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.07	Hits@10:40.68	Best:21.07
2024-12-28 01:16:53,609: Snapshot:0	Epoch:101	Loss:23.167	translation_Loss:23.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.25	Hits@10:40.83	Best:21.25
2024-12-28 01:16:59,808: Snapshot:0	Epoch:102	Loss:22.76	translation_Loss:22.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.36	Hits@10:40.94	Best:21.36
2024-12-28 01:17:06,026: Snapshot:0	Epoch:103	Loss:22.336	translation_Loss:22.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.48	Hits@10:41.04	Best:21.48
2024-12-28 01:17:12,270: Snapshot:0	Epoch:104	Loss:21.889	translation_Loss:21.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.59	Hits@10:41.08	Best:21.59
2024-12-28 01:17:18,527: Snapshot:0	Epoch:105	Loss:21.534	translation_Loss:21.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.71	Hits@10:41.18	Best:21.71
2024-12-28 01:17:24,762: Snapshot:0	Epoch:106	Loss:21.073	translation_Loss:21.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.87	Hits@10:41.28	Best:21.87
2024-12-28 01:17:30,912: Snapshot:0	Epoch:107	Loss:20.693	translation_Loss:20.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.97	Hits@10:41.4	Best:21.97
2024-12-28 01:17:37,068: Snapshot:0	Epoch:108	Loss:20.329	translation_Loss:20.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.09	Hits@10:41.5	Best:22.09
2024-12-28 01:17:43,224: Snapshot:0	Epoch:109	Loss:19.916	translation_Loss:19.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.18	Hits@10:41.67	Best:22.18
2024-12-28 01:17:49,352: Snapshot:0	Epoch:110	Loss:19.532	translation_Loss:19.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.26	Hits@10:41.8	Best:22.26
2024-12-28 01:17:55,548: Snapshot:0	Epoch:111	Loss:19.111	translation_Loss:19.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.36	Hits@10:41.87	Best:22.36
2024-12-28 01:18:01,687: Snapshot:0	Epoch:112	Loss:18.81	translation_Loss:18.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.45	Hits@10:41.93	Best:22.45
2024-12-28 01:18:07,884: Snapshot:0	Epoch:113	Loss:18.439	translation_Loss:18.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.56	Hits@10:42.05	Best:22.56
2024-12-28 01:18:14,026: Snapshot:0	Epoch:114	Loss:18.173	translation_Loss:18.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.64	Hits@10:42.15	Best:22.64
2024-12-28 01:18:20,224: Snapshot:0	Epoch:115	Loss:17.771	translation_Loss:17.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.68	Hits@10:42.19	Best:22.68
2024-12-28 01:18:26,391: Snapshot:0	Epoch:116	Loss:17.468	translation_Loss:17.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.78	Hits@10:42.29	Best:22.78
2024-12-28 01:18:32,965: Snapshot:0	Epoch:117	Loss:17.109	translation_Loss:17.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.86	Hits@10:42.37	Best:22.86
2024-12-28 01:18:39,179: Snapshot:0	Epoch:118	Loss:16.731	translation_Loss:16.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.94	Hits@10:42.49	Best:22.94
2024-12-28 01:18:45,349: Snapshot:0	Epoch:119	Loss:16.437	translation_Loss:16.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.97	Hits@10:42.61	Best:22.97
2024-12-28 01:18:51,516: Snapshot:0	Epoch:120	Loss:16.103	translation_Loss:16.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.04	Hits@10:42.71	Best:23.04
2024-12-28 01:18:57,718: Snapshot:0	Epoch:121	Loss:15.857	translation_Loss:15.857	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:42.79	Best:23.1
2024-12-28 01:19:03,964: Snapshot:0	Epoch:122	Loss:15.622	translation_Loss:15.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.18	Hits@10:42.85	Best:23.18
2024-12-28 01:19:10,079: Snapshot:0	Epoch:123	Loss:15.205	translation_Loss:15.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:42.93	Best:23.24
2024-12-28 01:19:16,208: Snapshot:0	Epoch:124	Loss:15.006	translation_Loss:15.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.29	Hits@10:43.0	Best:23.29
2024-12-28 01:19:22,448: Snapshot:0	Epoch:125	Loss:14.702	translation_Loss:14.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.33	Hits@10:43.08	Best:23.33
2024-12-28 01:19:28,634: Snapshot:0	Epoch:126	Loss:14.367	translation_Loss:14.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.41	Hits@10:43.16	Best:23.41
2024-12-28 01:19:34,802: Snapshot:0	Epoch:127	Loss:14.197	translation_Loss:14.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:43.25	Best:23.45
2024-12-28 01:19:40,976: Snapshot:0	Epoch:128	Loss:13.875	translation_Loss:13.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.54	Hits@10:43.3	Best:23.54
2024-12-28 01:19:47,209: Snapshot:0	Epoch:129	Loss:13.609	translation_Loss:13.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.61	Hits@10:43.37	Best:23.61
2024-12-28 01:19:53,380: Snapshot:0	Epoch:130	Loss:13.319	translation_Loss:13.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:43.41	Best:23.66
2024-12-28 01:19:59,563: Snapshot:0	Epoch:131	Loss:13.122	translation_Loss:13.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:43.45	Best:23.71
2024-12-28 01:20:05,759: Snapshot:0	Epoch:132	Loss:12.899	translation_Loss:12.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:43.51	Best:23.75
2024-12-28 01:20:11,967: Snapshot:0	Epoch:133	Loss:12.577	translation_Loss:12.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.8	Hits@10:43.57	Best:23.8
2024-12-28 01:20:18,207: Snapshot:0	Epoch:134	Loss:12.419	translation_Loss:12.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.85	Hits@10:43.65	Best:23.85
2024-12-28 01:20:24,813: Snapshot:0	Epoch:135	Loss:12.18	translation_Loss:12.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.91	Hits@10:43.64	Best:23.91
2024-12-28 01:20:31,063: Snapshot:0	Epoch:136	Loss:11.913	translation_Loss:11.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:43.73	Best:23.94
2024-12-28 01:20:37,201: Snapshot:0	Epoch:137	Loss:11.697	translation_Loss:11.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:43.79	Best:24.0
2024-12-28 01:20:43,438: Snapshot:0	Epoch:138	Loss:11.557	translation_Loss:11.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.01	Hits@10:43.88	Best:24.01
2024-12-28 01:20:49,560: Snapshot:0	Epoch:139	Loss:11.267	translation_Loss:11.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:44.0	Best:24.12
2024-12-28 01:20:55,707: Snapshot:0	Epoch:140	Loss:11.1	translation_Loss:11.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:44.06	Best:24.13
2024-12-28 01:21:01,944: Snapshot:0	Epoch:141	Loss:10.888	translation_Loss:10.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.17	Hits@10:44.09	Best:24.17
2024-12-28 01:21:08,108: Snapshot:0	Epoch:142	Loss:10.68	translation_Loss:10.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:44.16	Best:24.2
2024-12-28 01:21:14,338: Snapshot:0	Epoch:143	Loss:10.457	translation_Loss:10.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:44.23	Best:24.21
2024-12-28 01:21:20,557: Snapshot:0	Epoch:144	Loss:10.305	translation_Loss:10.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:44.31	Best:24.24
2024-12-28 01:21:26,763: Snapshot:0	Epoch:145	Loss:10.087	translation_Loss:10.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.29	Hits@10:44.28	Best:24.29
2024-12-28 01:21:32,978: Snapshot:0	Epoch:146	Loss:9.942	translation_Loss:9.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.32	Hits@10:44.37	Best:24.32
2024-12-28 01:21:39,120: Snapshot:0	Epoch:147	Loss:9.763	translation_Loss:9.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.3	Hits@10:44.37	Best:24.32
2024-12-28 01:21:45,236: Snapshot:0	Epoch:148	Loss:9.547	translation_Loss:9.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:44.45	Best:24.32
2024-12-28 01:21:51,337: Snapshot:0	Epoch:149	Loss:9.385	translation_Loss:9.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:44.52	Best:24.34
2024-12-28 01:21:57,542: Snapshot:0	Epoch:150	Loss:9.201	translation_Loss:9.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:44.55	Best:24.38
2024-12-28 01:22:03,662: Snapshot:0	Epoch:151	Loss:9.068	translation_Loss:9.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.4	Hits@10:44.58	Best:24.4
2024-12-28 01:22:09,798: Snapshot:0	Epoch:152	Loss:8.925	translation_Loss:8.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.44	Hits@10:44.64	Best:24.44
2024-12-28 01:22:16,417: Snapshot:0	Epoch:153	Loss:8.759	translation_Loss:8.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:44.68	Best:24.48
2024-12-28 01:22:22,542: Snapshot:0	Epoch:154	Loss:8.635	translation_Loss:8.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:44.7	Best:24.5
2024-12-28 01:22:28,748: Snapshot:0	Epoch:155	Loss:8.445	translation_Loss:8.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:44.74	Best:24.53
2024-12-28 01:22:34,985: Snapshot:0	Epoch:156	Loss:8.349	translation_Loss:8.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.54	Hits@10:44.75	Best:24.54
2024-12-28 01:22:41,221: Snapshot:0	Epoch:157	Loss:8.172	translation_Loss:8.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:44.74	Best:24.59
2024-12-28 01:22:47,393: Snapshot:0	Epoch:158	Loss:8.091	translation_Loss:8.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.62	Hits@10:44.78	Best:24.62
2024-12-28 01:22:53,565: Snapshot:0	Epoch:159	Loss:7.842	translation_Loss:7.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:44.81	Best:24.63
2024-12-28 01:22:59,774: Snapshot:0	Epoch:160	Loss:7.722	translation_Loss:7.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.69	Hits@10:44.84	Best:24.69
2024-12-28 01:23:05,887: Snapshot:0	Epoch:161	Loss:7.637	translation_Loss:7.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.68	Hits@10:44.92	Best:24.69
2024-12-28 01:23:12,042: Snapshot:0	Epoch:162	Loss:7.497	translation_Loss:7.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:44.92	Best:24.74
2024-12-28 01:23:18,256: Snapshot:0	Epoch:163	Loss:7.396	translation_Loss:7.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:44.94	Best:24.74
2024-12-28 01:23:24,464: Snapshot:0	Epoch:164	Loss:7.252	translation_Loss:7.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:44.95	Best:24.75
2024-12-28 01:23:30,575: Snapshot:0	Epoch:165	Loss:7.1	translation_Loss:7.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:44.96	Best:24.78
2024-12-28 01:23:36,796: Snapshot:0	Epoch:166	Loss:7.048	translation_Loss:7.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:44.96	Best:24.78
2024-12-28 01:23:42,921: Snapshot:0	Epoch:167	Loss:6.924	translation_Loss:6.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:45.01	Best:24.8
2024-12-28 01:23:49,064: Snapshot:0	Epoch:168	Loss:6.759	translation_Loss:6.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.83	Hits@10:45.07	Best:24.83
2024-12-28 01:23:55,249: Snapshot:0	Epoch:169	Loss:6.648	translation_Loss:6.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.85	Hits@10:45.13	Best:24.85
2024-12-28 01:24:01,461: Snapshot:0	Epoch:170	Loss:6.547	translation_Loss:6.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.86	Hits@10:45.13	Best:24.86
2024-12-28 01:24:08,059: Snapshot:0	Epoch:171	Loss:6.442	translation_Loss:6.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:45.16	Best:24.88
2024-12-28 01:24:14,268: Snapshot:0	Epoch:172	Loss:6.352	translation_Loss:6.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.89	Hits@10:45.24	Best:24.89
2024-12-28 01:24:20,499: Snapshot:0	Epoch:173	Loss:6.262	translation_Loss:6.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:45.25	Best:24.91
2024-12-28 01:24:26,712: Snapshot:0	Epoch:174	Loss:6.152	translation_Loss:6.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.28	Best:24.91
2024-12-28 01:24:32,877: Snapshot:0	Epoch:175	Loss:6.052	translation_Loss:6.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.27	Best:24.91
2024-12-28 01:24:39,009: Snapshot:0	Epoch:176	Loss:5.981	translation_Loss:5.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.92	Hits@10:45.28	Best:24.92
2024-12-28 01:24:45,220: Snapshot:0	Epoch:177	Loss:5.871	translation_Loss:5.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.92	Hits@10:45.31	Best:24.92
2024-12-28 01:24:51,422: Snapshot:0	Epoch:178	Loss:5.822	translation_Loss:5.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.96	Hits@10:45.34	Best:24.96
2024-12-28 01:24:57,524: Snapshot:0	Epoch:179	Loss:5.714	translation_Loss:5.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:45.3	Best:24.97
2024-12-28 01:25:03,732: Snapshot:0	Epoch:180	Loss:5.641	translation_Loss:5.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:45.34	Best:24.98
2024-12-28 01:25:09,865: Snapshot:0	Epoch:181	Loss:5.548	translation_Loss:5.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:45.34	Best:24.98
2024-12-28 01:25:16,022: Snapshot:0	Epoch:182	Loss:5.437	translation_Loss:5.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.01	Hits@10:45.41	Best:25.01
2024-12-28 01:25:22,239: Snapshot:0	Epoch:183	Loss:5.365	translation_Loss:5.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.05	Hits@10:45.44	Best:25.05
2024-12-28 01:25:28,365: Snapshot:0	Epoch:184	Loss:5.273	translation_Loss:5.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.09	Hits@10:45.46	Best:25.09
2024-12-28 01:25:34,570: Snapshot:0	Epoch:185	Loss:5.208	translation_Loss:5.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.09	Hits@10:45.49	Best:25.09
2024-12-28 01:25:40,749: Snapshot:0	Epoch:186	Loss:5.119	translation_Loss:5.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:45.45	Best:25.09
2024-12-28 01:25:46,854: Snapshot:0	Epoch:187	Loss:5.041	translation_Loss:5.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.09	Hits@10:45.5	Best:25.09
2024-12-28 01:25:53,043: Snapshot:0	Epoch:188	Loss:4.998	translation_Loss:4.998	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.14	Hits@10:45.54	Best:25.14
2024-12-28 01:25:59,628: Snapshot:0	Epoch:189	Loss:4.914	translation_Loss:4.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.11	Hits@10:45.53	Best:25.14
2024-12-28 01:26:05,727: Snapshot:0	Epoch:190	Loss:4.831	translation_Loss:4.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.12	Hits@10:45.51	Best:25.14
2024-12-28 01:26:11,824: Snapshot:0	Epoch:191	Loss:4.761	translation_Loss:4.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.11	Hits@10:45.56	Best:25.14
2024-12-28 01:26:17,980: Snapshot:0	Epoch:192	Loss:4.663	translation_Loss:4.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.16	Hits@10:45.53	Best:25.16
2024-12-28 01:26:24,119: Snapshot:0	Epoch:193	Loss:4.637	translation_Loss:4.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.19	Hits@10:45.6	Best:25.19
2024-12-28 01:26:30,330: Snapshot:0	Epoch:194	Loss:4.572	translation_Loss:4.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.58	Best:25.19
2024-12-28 01:26:36,427: Snapshot:0	Epoch:195	Loss:4.478	translation_Loss:4.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.19	Hits@10:45.6	Best:25.19
2024-12-28 01:26:42,533: Snapshot:0	Epoch:196	Loss:4.399	translation_Loss:4.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.16	Hits@10:45.56	Best:25.19
2024-12-28 01:26:48,621: Snapshot:0	Epoch:197	Loss:4.38	translation_Loss:4.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.55	Best:25.19
2024-12-28 01:26:54,759: Snapshot:0	Epoch:198	Loss:4.365	translation_Loss:4.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.21	Hits@10:45.6	Best:25.21
2024-12-28 01:27:00,921: Snapshot:0	Epoch:199	Loss:4.302	translation_Loss:4.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.2	Hits@10:45.59	Best:25.21
2024-12-28 01:27:01,206: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.00001_512_1000/0model_best.tar'
2024-12-28 01:27:03,721: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2517 | 0.1425 | 0.3128 | 0.3735 |  0.4456 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 01:27:38,546: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228012708/HYBRIDHYBRID_0.00001_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='HYBRID_0.00001_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRIDHYBRID_0.00001_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 01:27:48,252: Snapshot:0	Epoch:0	Loss:97.396	translation_Loss:97.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.68	Hits@10:0.69	Best:0.68
2024-12-28 01:27:54,313: Snapshot:0	Epoch:1	Loss:96.39	translation_Loss:96.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.7	Hits@10:0.7	Best:0.7
2024-12-28 01:28:00,387: Snapshot:0	Epoch:2	Loss:95.378	translation_Loss:95.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.75	Hits@10:0.83	Best:0.75
2024-12-28 01:28:06,504: Snapshot:0	Epoch:3	Loss:94.399	translation_Loss:94.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.85	Hits@10:1.11	Best:0.85
2024-12-28 01:28:12,522: Snapshot:0	Epoch:4	Loss:93.403	translation_Loss:93.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.05	Hits@10:1.72	Best:1.05
2024-12-28 01:28:18,572: Snapshot:0	Epoch:5	Loss:92.439	translation_Loss:92.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.34	Hits@10:2.52	Best:1.34
2024-12-28 01:28:24,702: Snapshot:0	Epoch:6	Loss:91.443	translation_Loss:91.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.75	Hits@10:3.51	Best:1.75
2024-12-28 01:28:30,829: Snapshot:0	Epoch:7	Loss:90.46	translation_Loss:90.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.22	Hits@10:4.54	Best:2.22
2024-12-28 01:28:37,337: Snapshot:0	Epoch:8	Loss:89.49	translation_Loss:89.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.67	Hits@10:5.46	Best:2.67
2024-12-28 01:28:43,467: Snapshot:0	Epoch:9	Loss:88.558	translation_Loss:88.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.07	Hits@10:6.25	Best:3.07
2024-12-28 01:28:49,603: Snapshot:0	Epoch:10	Loss:87.607	translation_Loss:87.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.41	Hits@10:6.9	Best:3.41
2024-12-28 01:28:55,736: Snapshot:0	Epoch:11	Loss:86.688	translation_Loss:86.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.69	Hits@10:7.46	Best:3.69
2024-12-28 01:29:01,774: Snapshot:0	Epoch:12	Loss:85.744	translation_Loss:85.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.95	Hits@10:8.13	Best:3.95
2024-12-28 01:29:07,875: Snapshot:0	Epoch:13	Loss:84.859	translation_Loss:84.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.22	Hits@10:8.81	Best:4.22
2024-12-28 01:29:13,905: Snapshot:0	Epoch:14	Loss:83.956	translation_Loss:83.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.47	Hits@10:9.53	Best:4.47
2024-12-28 01:29:20,067: Snapshot:0	Epoch:15	Loss:83.083	translation_Loss:83.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.73	Hits@10:10.21	Best:4.73
2024-12-28 01:29:26,202: Snapshot:0	Epoch:16	Loss:82.198	translation_Loss:82.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.99	Hits@10:10.92	Best:4.99
2024-12-28 01:29:32,347: Snapshot:0	Epoch:17	Loss:81.349	translation_Loss:81.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.25	Hits@10:11.63	Best:5.25
2024-12-28 01:29:38,492: Snapshot:0	Epoch:18	Loss:80.501	translation_Loss:80.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.47	Hits@10:12.22	Best:5.47
2024-12-28 01:29:44,617: Snapshot:0	Epoch:19	Loss:79.68	translation_Loss:79.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.69	Hits@10:12.71	Best:5.69
2024-12-28 01:29:50,764: Snapshot:0	Epoch:20	Loss:78.831	translation_Loss:78.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.89	Hits@10:13.28	Best:5.89
2024-12-28 01:29:56,910: Snapshot:0	Epoch:21	Loss:77.989	translation_Loss:77.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.07	Hits@10:13.81	Best:6.07
2024-12-28 01:30:03,056: Snapshot:0	Epoch:22	Loss:77.206	translation_Loss:77.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.25	Hits@10:14.27	Best:6.25
2024-12-28 01:30:09,213: Snapshot:0	Epoch:23	Loss:76.391	translation_Loss:76.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.4	Hits@10:14.69	Best:6.4
2024-12-28 01:30:15,269: Snapshot:0	Epoch:24	Loss:75.63	translation_Loss:75.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.55	Hits@10:15.07	Best:6.55
2024-12-28 01:30:21,337: Snapshot:0	Epoch:25	Loss:74.765	translation_Loss:74.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.69	Hits@10:15.51	Best:6.69
2024-12-28 01:30:27,846: Snapshot:0	Epoch:26	Loss:74.032	translation_Loss:74.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.83	Hits@10:15.91	Best:6.83
2024-12-28 01:30:33,882: Snapshot:0	Epoch:27	Loss:73.241	translation_Loss:73.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.96	Hits@10:16.25	Best:6.96
2024-12-28 01:30:39,953: Snapshot:0	Epoch:28	Loss:72.436	translation_Loss:72.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.1	Hits@10:16.67	Best:7.1
2024-12-28 01:30:46,074: Snapshot:0	Epoch:29	Loss:71.71	translation_Loss:71.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.22	Hits@10:17.02	Best:7.22
2024-12-28 01:30:52,144: Snapshot:0	Epoch:30	Loss:70.938	translation_Loss:70.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.36	Hits@10:17.33	Best:7.36
2024-12-28 01:30:58,209: Snapshot:0	Epoch:31	Loss:70.121	translation_Loss:70.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.49	Hits@10:17.66	Best:7.49
2024-12-28 01:31:04,350: Snapshot:0	Epoch:32	Loss:69.384	translation_Loss:69.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.62	Hits@10:18.05	Best:7.62
2024-12-28 01:31:10,410: Snapshot:0	Epoch:33	Loss:68.612	translation_Loss:68.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.75	Hits@10:18.47	Best:7.75
2024-12-28 01:31:16,510: Snapshot:0	Epoch:34	Loss:67.873	translation_Loss:67.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.87	Hits@10:18.87	Best:7.87
2024-12-28 01:31:22,593: Snapshot:0	Epoch:35	Loss:67.157	translation_Loss:67.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.0	Hits@10:19.24	Best:8.0
2024-12-28 01:31:28,718: Snapshot:0	Epoch:36	Loss:66.306	translation_Loss:66.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.13	Hits@10:19.56	Best:8.13
2024-12-28 01:31:34,867: Snapshot:0	Epoch:37	Loss:65.591	translation_Loss:65.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.26	Hits@10:19.9	Best:8.26
2024-12-28 01:31:40,970: Snapshot:0	Epoch:38	Loss:64.867	translation_Loss:64.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.41	Hits@10:20.22	Best:8.41
2024-12-28 01:31:46,992: Snapshot:0	Epoch:39	Loss:64.089	translation_Loss:64.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.57	Hits@10:20.64	Best:8.57
2024-12-28 01:31:53,133: Snapshot:0	Epoch:40	Loss:63.355	translation_Loss:63.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.73	Hits@10:21.08	Best:8.73
2024-12-28 01:31:59,264: Snapshot:0	Epoch:41	Loss:62.612	translation_Loss:62.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.51	Best:8.89
2024-12-28 01:32:05,374: Snapshot:0	Epoch:42	Loss:61.833	translation_Loss:61.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.08	Hits@10:21.93	Best:9.08
2024-12-28 01:32:11,441: Snapshot:0	Epoch:43	Loss:61.101	translation_Loss:61.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.28	Hits@10:22.37	Best:9.28
2024-12-28 01:32:17,940: Snapshot:0	Epoch:44	Loss:60.338	translation_Loss:60.338	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.46	Hits@10:22.85	Best:9.46
2024-12-28 01:32:24,017: Snapshot:0	Epoch:45	Loss:59.632	translation_Loss:59.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.65	Hits@10:23.33	Best:9.65
2024-12-28 01:32:30,162: Snapshot:0	Epoch:46	Loss:58.866	translation_Loss:58.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.89	Hits@10:23.78	Best:9.89
2024-12-28 01:32:36,294: Snapshot:0	Epoch:47	Loss:58.15	translation_Loss:58.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.14	Hits@10:24.19	Best:10.14
2024-12-28 01:32:42,443: Snapshot:0	Epoch:48	Loss:57.382	translation_Loss:57.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.41	Hits@10:24.7	Best:10.41
2024-12-28 01:32:48,565: Snapshot:0	Epoch:49	Loss:56.704	translation_Loss:56.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.63	Hits@10:25.23	Best:10.63
2024-12-28 01:32:54,694: Snapshot:0	Epoch:50	Loss:55.943	translation_Loss:55.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.86	Hits@10:25.68	Best:10.86
2024-12-28 01:33:00,809: Snapshot:0	Epoch:51	Loss:55.133	translation_Loss:55.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.12	Hits@10:26.15	Best:11.12
2024-12-28 01:33:06,951: Snapshot:0	Epoch:52	Loss:54.44	translation_Loss:54.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.38	Hits@10:26.7	Best:11.38
2024-12-28 01:33:12,987: Snapshot:0	Epoch:53	Loss:53.699	translation_Loss:53.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.65	Hits@10:27.17	Best:11.65
2024-12-28 01:33:19,070: Snapshot:0	Epoch:54	Loss:53.002	translation_Loss:53.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.89	Hits@10:27.73	Best:11.89
2024-12-28 01:33:25,189: Snapshot:0	Epoch:55	Loss:52.243	translation_Loss:52.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.13	Hits@10:28.2	Best:12.13
2024-12-28 01:33:31,335: Snapshot:0	Epoch:56	Loss:51.513	translation_Loss:51.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.39	Hits@10:28.65	Best:12.39
2024-12-28 01:33:37,408: Snapshot:0	Epoch:57	Loss:50.748	translation_Loss:50.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.62	Hits@10:29.08	Best:12.62
2024-12-28 01:33:43,564: Snapshot:0	Epoch:58	Loss:50.103	translation_Loss:50.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.85	Hits@10:29.46	Best:12.85
2024-12-28 01:33:49,687: Snapshot:0	Epoch:59	Loss:49.354	translation_Loss:49.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.07	Hits@10:29.9	Best:13.07
2024-12-28 01:33:55,816: Snapshot:0	Epoch:60	Loss:48.597	translation_Loss:48.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.29	Hits@10:30.31	Best:13.29
2024-12-28 01:34:01,907: Snapshot:0	Epoch:61	Loss:47.842	translation_Loss:47.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.52	Hits@10:30.81	Best:13.52
2024-12-28 01:34:08,417: Snapshot:0	Epoch:62	Loss:47.112	translation_Loss:47.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.77	Hits@10:31.22	Best:13.77
2024-12-28 01:34:14,569: Snapshot:0	Epoch:63	Loss:46.425	translation_Loss:46.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.02	Hits@10:31.65	Best:14.02
2024-12-28 01:34:20,647: Snapshot:0	Epoch:64	Loss:45.7	translation_Loss:45.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.24	Hits@10:31.95	Best:14.24
2024-12-28 01:34:26,724: Snapshot:0	Epoch:65	Loss:44.964	translation_Loss:44.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.45	Hits@10:32.27	Best:14.45
2024-12-28 01:34:32,872: Snapshot:0	Epoch:66	Loss:44.296	translation_Loss:44.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.64	Hits@10:32.62	Best:14.64
2024-12-28 01:34:39,026: Snapshot:0	Epoch:67	Loss:43.486	translation_Loss:43.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.85	Hits@10:32.95	Best:14.85
2024-12-28 01:34:45,181: Snapshot:0	Epoch:68	Loss:42.822	translation_Loss:42.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.08	Hits@10:33.39	Best:15.08
2024-12-28 01:34:51,342: Snapshot:0	Epoch:69	Loss:42.072	translation_Loss:42.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.33	Hits@10:33.71	Best:15.33
2024-12-28 01:34:57,465: Snapshot:0	Epoch:70	Loss:41.398	translation_Loss:41.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.55	Hits@10:34.02	Best:15.55
2024-12-28 01:35:03,546: Snapshot:0	Epoch:71	Loss:40.71	translation_Loss:40.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.77	Hits@10:34.3	Best:15.77
2024-12-28 01:35:09,649: Snapshot:0	Epoch:72	Loss:40.012	translation_Loss:40.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.02	Hits@10:34.61	Best:16.02
2024-12-28 01:35:15,799: Snapshot:0	Epoch:73	Loss:39.278	translation_Loss:39.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.25	Hits@10:34.88	Best:16.25
2024-12-28 01:35:21,893: Snapshot:0	Epoch:74	Loss:38.586	translation_Loss:38.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.47	Hits@10:35.2	Best:16.47
2024-12-28 01:35:28,001: Snapshot:0	Epoch:75	Loss:37.976	translation_Loss:37.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.72	Hits@10:35.45	Best:16.72
2024-12-28 01:35:34,134: Snapshot:0	Epoch:76	Loss:37.251	translation_Loss:37.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.93	Hits@10:35.67	Best:16.93
2024-12-28 01:35:40,249: Snapshot:0	Epoch:77	Loss:36.668	translation_Loss:36.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.17	Hits@10:35.92	Best:17.17
2024-12-28 01:35:46,370: Snapshot:0	Epoch:78	Loss:35.833	translation_Loss:35.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.36	Hits@10:36.15	Best:17.36
2024-12-28 01:35:52,522: Snapshot:0	Epoch:79	Loss:35.274	translation_Loss:35.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.55	Hits@10:36.43	Best:17.55
2024-12-28 01:35:59,097: Snapshot:0	Epoch:80	Loss:34.591	translation_Loss:34.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.72	Hits@10:36.68	Best:17.72
2024-12-28 01:36:05,150: Snapshot:0	Epoch:81	Loss:34.025	translation_Loss:34.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.97	Hits@10:36.95	Best:17.97
2024-12-28 01:36:11,269: Snapshot:0	Epoch:82	Loss:33.392	translation_Loss:33.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.17	Hits@10:37.21	Best:18.17
2024-12-28 01:36:17,372: Snapshot:0	Epoch:83	Loss:32.802	translation_Loss:32.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.38	Hits@10:37.44	Best:18.38
2024-12-28 01:36:23,532: Snapshot:0	Epoch:84	Loss:32.109	translation_Loss:32.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.52	Hits@10:37.67	Best:18.52
2024-12-28 01:36:29,617: Snapshot:0	Epoch:85	Loss:31.526	translation_Loss:31.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.72	Hits@10:37.88	Best:18.72
2024-12-28 01:36:35,670: Snapshot:0	Epoch:86	Loss:30.928	translation_Loss:30.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.9	Hits@10:37.99	Best:18.9
2024-12-28 01:36:41,692: Snapshot:0	Epoch:87	Loss:30.42	translation_Loss:30.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.07	Hits@10:38.27	Best:19.07
2024-12-28 01:36:47,772: Snapshot:0	Epoch:88	Loss:29.812	translation_Loss:29.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.24	Hits@10:38.51	Best:19.24
2024-12-28 01:36:53,853: Snapshot:0	Epoch:89	Loss:29.181	translation_Loss:29.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.41	Hits@10:38.66	Best:19.41
2024-12-28 01:36:59,961: Snapshot:0	Epoch:90	Loss:28.65	translation_Loss:28.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.59	Hits@10:38.87	Best:19.59
2024-12-28 01:37:06,060: Snapshot:0	Epoch:91	Loss:28.122	translation_Loss:28.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:39.14	Best:19.77
2024-12-28 01:37:12,128: Snapshot:0	Epoch:92	Loss:27.588	translation_Loss:27.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.91	Hits@10:39.33	Best:19.91
2024-12-28 01:37:18,243: Snapshot:0	Epoch:93	Loss:27.085	translation_Loss:27.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.04	Hits@10:39.47	Best:20.04
2024-12-28 01:37:24,285: Snapshot:0	Epoch:94	Loss:26.537	translation_Loss:26.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.18	Hits@10:39.61	Best:20.18
2024-12-28 01:37:30,454: Snapshot:0	Epoch:95	Loss:26.007	translation_Loss:26.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.33	Hits@10:39.76	Best:20.33
2024-12-28 01:37:36,582: Snapshot:0	Epoch:96	Loss:25.467	translation_Loss:25.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.49	Hits@10:39.99	Best:20.49
2024-12-28 01:37:42,697: Snapshot:0	Epoch:97	Loss:25.078	translation_Loss:25.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.63	Hits@10:40.19	Best:20.63
2024-12-28 01:37:48,764: Snapshot:0	Epoch:98	Loss:24.557	translation_Loss:24.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.76	Hits@10:40.37	Best:20.76
2024-12-28 01:37:55,278: Snapshot:0	Epoch:99	Loss:24.04	translation_Loss:24.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.91	Hits@10:40.52	Best:20.91
2024-12-28 01:38:01,358: Snapshot:0	Epoch:100	Loss:23.597	translation_Loss:23.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.08	Hits@10:40.68	Best:21.08
2024-12-28 01:38:07,477: Snapshot:0	Epoch:101	Loss:23.167	translation_Loss:23.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.24	Hits@10:40.82	Best:21.24
2024-12-28 01:38:13,525: Snapshot:0	Epoch:102	Loss:22.76	translation_Loss:22.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.37	Hits@10:40.94	Best:21.37
2024-12-28 01:38:19,571: Snapshot:0	Epoch:103	Loss:22.336	translation_Loss:22.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.5	Hits@10:41.03	Best:21.5
2024-12-28 01:38:25,693: Snapshot:0	Epoch:104	Loss:21.889	translation_Loss:21.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.58	Hits@10:41.07	Best:21.58
2024-12-28 01:38:31,855: Snapshot:0	Epoch:105	Loss:21.533	translation_Loss:21.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.71	Hits@10:41.17	Best:21.71
2024-12-28 01:38:37,938: Snapshot:0	Epoch:106	Loss:21.073	translation_Loss:21.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.86	Hits@10:41.3	Best:21.86
2024-12-28 01:38:44,039: Snapshot:0	Epoch:107	Loss:20.693	translation_Loss:20.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.97	Hits@10:41.4	Best:21.97
2024-12-28 01:38:50,062: Snapshot:0	Epoch:108	Loss:20.328	translation_Loss:20.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.08	Hits@10:41.5	Best:22.08
2024-12-28 01:38:56,163: Snapshot:0	Epoch:109	Loss:19.916	translation_Loss:19.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.18	Hits@10:41.65	Best:22.18
2024-12-28 01:39:02,247: Snapshot:0	Epoch:110	Loss:19.532	translation_Loss:19.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.26	Hits@10:41.8	Best:22.26
2024-12-28 01:39:08,307: Snapshot:0	Epoch:111	Loss:19.11	translation_Loss:19.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.35	Hits@10:41.86	Best:22.35
2024-12-28 01:39:14,435: Snapshot:0	Epoch:112	Loss:18.81	translation_Loss:18.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.45	Hits@10:41.94	Best:22.45
2024-12-28 01:39:20,575: Snapshot:0	Epoch:113	Loss:18.439	translation_Loss:18.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.59	Hits@10:42.05	Best:22.59
2024-12-28 01:39:26,698: Snapshot:0	Epoch:114	Loss:18.173	translation_Loss:18.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:42.14	Best:22.63
2024-12-28 01:39:32,866: Snapshot:0	Epoch:115	Loss:17.771	translation_Loss:17.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.7	Hits@10:42.17	Best:22.7
2024-12-28 01:39:38,981: Snapshot:0	Epoch:116	Loss:17.468	translation_Loss:17.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.79	Hits@10:42.29	Best:22.79
2024-12-28 01:39:45,580: Snapshot:0	Epoch:117	Loss:17.109	translation_Loss:17.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.85	Hits@10:42.37	Best:22.85
2024-12-28 01:39:51,619: Snapshot:0	Epoch:118	Loss:16.731	translation_Loss:16.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.93	Hits@10:42.49	Best:22.93
2024-12-28 01:39:57,728: Snapshot:0	Epoch:119	Loss:16.436	translation_Loss:16.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.98	Hits@10:42.61	Best:22.98
2024-12-28 01:40:03,821: Snapshot:0	Epoch:120	Loss:16.103	translation_Loss:16.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.05	Hits@10:42.7	Best:23.05
2024-12-28 01:40:09,991: Snapshot:0	Epoch:121	Loss:15.856	translation_Loss:15.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.11	Hits@10:42.78	Best:23.11
2024-12-28 01:40:16,040: Snapshot:0	Epoch:122	Loss:15.622	translation_Loss:15.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.19	Hits@10:42.85	Best:23.19
2024-12-28 01:40:22,116: Snapshot:0	Epoch:123	Loss:15.204	translation_Loss:15.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.25	Hits@10:42.94	Best:23.25
2024-12-28 01:40:28,249: Snapshot:0	Epoch:124	Loss:15.006	translation_Loss:15.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.29	Hits@10:42.99	Best:23.29
2024-12-28 01:40:34,320: Snapshot:0	Epoch:125	Loss:14.701	translation_Loss:14.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.35	Hits@10:43.07	Best:23.35
2024-12-28 01:40:40,396: Snapshot:0	Epoch:126	Loss:14.367	translation_Loss:14.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.4	Hits@10:43.18	Best:23.4
2024-12-28 01:40:46,493: Snapshot:0	Epoch:127	Loss:14.196	translation_Loss:14.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.46	Hits@10:43.24	Best:23.46
2024-12-28 01:40:52,531: Snapshot:0	Epoch:128	Loss:13.875	translation_Loss:13.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.55	Hits@10:43.31	Best:23.55
2024-12-28 01:40:58,682: Snapshot:0	Epoch:129	Loss:13.609	translation_Loss:13.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.61	Hits@10:43.38	Best:23.61
2024-12-28 01:41:04,763: Snapshot:0	Epoch:130	Loss:13.319	translation_Loss:13.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:43.45	Best:23.66
2024-12-28 01:41:10,836: Snapshot:0	Epoch:131	Loss:13.122	translation_Loss:13.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:43.47	Best:23.71
2024-12-28 01:41:16,872: Snapshot:0	Epoch:132	Loss:12.899	translation_Loss:12.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:43.51	Best:23.75
2024-12-28 01:41:23,049: Snapshot:0	Epoch:133	Loss:12.577	translation_Loss:12.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.79	Hits@10:43.56	Best:23.79
2024-12-28 01:41:29,156: Snapshot:0	Epoch:134	Loss:12.419	translation_Loss:12.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.86	Hits@10:43.66	Best:23.86
2024-12-28 01:41:35,784: Snapshot:0	Epoch:135	Loss:12.179	translation_Loss:12.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.91	Hits@10:43.65	Best:23.91
2024-12-28 01:41:41,945: Snapshot:0	Epoch:136	Loss:11.913	translation_Loss:11.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:43.7	Best:23.94
2024-12-28 01:41:48,103: Snapshot:0	Epoch:137	Loss:11.697	translation_Loss:11.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:43.8	Best:24.0
2024-12-28 01:41:54,263: Snapshot:0	Epoch:138	Loss:11.557	translation_Loss:11.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.01	Hits@10:43.88	Best:24.01
2024-12-28 01:42:00,390: Snapshot:0	Epoch:139	Loss:11.267	translation_Loss:11.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:44.01	Best:24.11
2024-12-28 01:42:06,474: Snapshot:0	Epoch:140	Loss:11.1	translation_Loss:11.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:44.06	Best:24.13
2024-12-28 01:42:12,526: Snapshot:0	Epoch:141	Loss:10.888	translation_Loss:10.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.17	Hits@10:44.1	Best:24.17
2024-12-28 01:42:18,656: Snapshot:0	Epoch:142	Loss:10.68	translation_Loss:10.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:44.16	Best:24.2
2024-12-28 01:42:24,799: Snapshot:0	Epoch:143	Loss:10.457	translation_Loss:10.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:44.22	Best:24.21
2024-12-28 01:42:30,885: Snapshot:0	Epoch:144	Loss:10.305	translation_Loss:10.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.25	Hits@10:44.29	Best:24.25
2024-12-28 01:42:37,049: Snapshot:0	Epoch:145	Loss:10.087	translation_Loss:10.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.29	Hits@10:44.3	Best:24.29
2024-12-28 01:42:43,129: Snapshot:0	Epoch:146	Loss:9.942	translation_Loss:9.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:44.36	Best:24.31
2024-12-28 01:42:49,284: Snapshot:0	Epoch:147	Loss:9.762	translation_Loss:9.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:44.39	Best:24.31
2024-12-28 01:42:55,370: Snapshot:0	Epoch:148	Loss:9.547	translation_Loss:9.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:44.44	Best:24.31
2024-12-28 01:43:01,427: Snapshot:0	Epoch:149	Loss:9.385	translation_Loss:9.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:44.49	Best:24.34
2024-12-28 01:43:07,567: Snapshot:0	Epoch:150	Loss:9.201	translation_Loss:9.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:44.54	Best:24.38
2024-12-28 01:43:13,620: Snapshot:0	Epoch:151	Loss:9.068	translation_Loss:9.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:44.61	Best:24.42
2024-12-28 01:43:19,767: Snapshot:0	Epoch:152	Loss:8.925	translation_Loss:8.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.45	Hits@10:44.63	Best:24.45
2024-12-28 01:43:26,376: Snapshot:0	Epoch:153	Loss:8.758	translation_Loss:8.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:44.66	Best:24.49
2024-12-28 01:43:32,439: Snapshot:0	Epoch:154	Loss:8.635	translation_Loss:8.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:44.7	Best:24.51
2024-12-28 01:43:38,546: Snapshot:0	Epoch:155	Loss:8.445	translation_Loss:8.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.52	Hits@10:44.73	Best:24.52
2024-12-28 01:43:44,683: Snapshot:0	Epoch:156	Loss:8.349	translation_Loss:8.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.56	Hits@10:44.72	Best:24.56
2024-12-28 01:43:50,719: Snapshot:0	Epoch:157	Loss:8.172	translation_Loss:8.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.6	Hits@10:44.73	Best:24.6
2024-12-28 01:43:56,876: Snapshot:0	Epoch:158	Loss:8.091	translation_Loss:8.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.62	Hits@10:44.75	Best:24.62
2024-12-28 01:44:02,948: Snapshot:0	Epoch:159	Loss:7.842	translation_Loss:7.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:44.79	Best:24.63
2024-12-28 01:44:09,049: Snapshot:0	Epoch:160	Loss:7.722	translation_Loss:7.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.68	Hits@10:44.87	Best:24.68
2024-12-28 01:44:15,211: Snapshot:0	Epoch:161	Loss:7.638	translation_Loss:7.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.69	Hits@10:44.91	Best:24.69
2024-12-28 01:44:21,270: Snapshot:0	Epoch:162	Loss:7.497	translation_Loss:7.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.73	Hits@10:44.91	Best:24.73
2024-12-28 01:44:27,432: Snapshot:0	Epoch:163	Loss:7.396	translation_Loss:7.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:44.91	Best:24.74
2024-12-28 01:44:33,466: Snapshot:0	Epoch:164	Loss:7.252	translation_Loss:7.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:44.94	Best:24.74
2024-12-28 01:44:39,507: Snapshot:0	Epoch:165	Loss:7.099	translation_Loss:7.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:44.99	Best:24.77
2024-12-28 01:44:45,630: Snapshot:0	Epoch:166	Loss:7.048	translation_Loss:7.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:45.0	Best:24.77
2024-12-28 01:44:51,661: Snapshot:0	Epoch:167	Loss:6.923	translation_Loss:6.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.79	Hits@10:45.0	Best:24.79
2024-12-28 01:44:57,814: Snapshot:0	Epoch:168	Loss:6.759	translation_Loss:6.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.83	Hits@10:45.05	Best:24.83
2024-12-28 01:45:03,905: Snapshot:0	Epoch:169	Loss:6.647	translation_Loss:6.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.86	Hits@10:45.1	Best:24.86
2024-12-28 01:45:09,991: Snapshot:0	Epoch:170	Loss:6.546	translation_Loss:6.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.84	Hits@10:45.11	Best:24.86
2024-12-28 01:45:16,540: Snapshot:0	Epoch:171	Loss:6.441	translation_Loss:6.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.15	Best:24.9
2024-12-28 01:45:22,592: Snapshot:0	Epoch:172	Loss:6.352	translation_Loss:6.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:45.24	Best:24.9
2024-12-28 01:45:28,695: Snapshot:0	Epoch:173	Loss:6.261	translation_Loss:6.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:45.26	Best:24.91
2024-12-28 01:45:34,763: Snapshot:0	Epoch:174	Loss:6.152	translation_Loss:6.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.92	Hits@10:45.27	Best:24.92
2024-12-28 01:45:40,803: Snapshot:0	Epoch:175	Loss:6.052	translation_Loss:6.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:45.27	Best:24.92
2024-12-28 01:45:46,896: Snapshot:0	Epoch:176	Loss:5.981	translation_Loss:5.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:45.3	Best:24.92
2024-12-28 01:45:52,928: Snapshot:0	Epoch:177	Loss:5.871	translation_Loss:5.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.93	Hits@10:45.27	Best:24.93
2024-12-28 01:45:59,016: Snapshot:0	Epoch:178	Loss:5.822	translation_Loss:5.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:45.32	Best:24.97
2024-12-28 01:46:05,112: Snapshot:0	Epoch:179	Loss:5.714	translation_Loss:5.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.96	Hits@10:45.33	Best:24.97
2024-12-28 01:46:11,138: Snapshot:0	Epoch:180	Loss:5.641	translation_Loss:5.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:45.34	Best:24.97
2024-12-28 01:46:17,194: Snapshot:0	Epoch:181	Loss:5.548	translation_Loss:5.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:45.3	Best:24.97
2024-12-28 01:46:23,232: Snapshot:0	Epoch:182	Loss:5.437	translation_Loss:5.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.02	Hits@10:45.39	Best:25.02
2024-12-28 01:46:29,376: Snapshot:0	Epoch:183	Loss:5.365	translation_Loss:5.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.05	Hits@10:45.42	Best:25.05
2024-12-28 01:46:35,538: Snapshot:0	Epoch:184	Loss:5.273	translation_Loss:5.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.06	Hits@10:45.43	Best:25.06
2024-12-28 01:46:41,620: Snapshot:0	Epoch:185	Loss:5.207	translation_Loss:5.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:45.46	Best:25.07
2024-12-28 01:46:47,726: Snapshot:0	Epoch:186	Loss:5.119	translation_Loss:5.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.06	Hits@10:45.48	Best:25.07
2024-12-28 01:46:53,771: Snapshot:0	Epoch:187	Loss:5.04	translation_Loss:5.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.09	Hits@10:45.49	Best:25.09
2024-12-28 01:46:59,915: Snapshot:0	Epoch:188	Loss:4.998	translation_Loss:4.998	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.12	Hits@10:45.54	Best:25.12
2024-12-28 01:47:06,420: Snapshot:0	Epoch:189	Loss:4.914	translation_Loss:4.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.12	Hits@10:45.57	Best:25.12
2024-12-28 01:47:12,474: Snapshot:0	Epoch:190	Loss:4.832	translation_Loss:4.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.1	Hits@10:45.55	Best:25.12
2024-12-28 01:47:18,623: Snapshot:0	Epoch:191	Loss:4.761	translation_Loss:4.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.1	Hits@10:45.59	Best:25.12
2024-12-28 01:47:24,667: Snapshot:0	Epoch:192	Loss:4.663	translation_Loss:4.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.13	Hits@10:45.55	Best:25.13
2024-12-28 01:47:30,846: Snapshot:0	Epoch:193	Loss:4.637	translation_Loss:4.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.57	Best:25.18
2024-12-28 01:47:36,908: Snapshot:0	Epoch:194	Loss:4.572	translation_Loss:4.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.19	Hits@10:45.56	Best:25.19
2024-12-28 01:47:42,981: Snapshot:0	Epoch:195	Loss:4.478	translation_Loss:4.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.56	Best:25.19
2024-12-28 01:47:49,068: Snapshot:0	Epoch:196	Loss:4.4	translation_Loss:4.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.6	Best:25.19
2024-12-28 01:47:55,195: Snapshot:0	Epoch:197	Loss:4.38	translation_Loss:4.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.19	Hits@10:45.54	Best:25.19
2024-12-28 01:48:01,246: Snapshot:0	Epoch:198	Loss:4.365	translation_Loss:4.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.59	Best:25.19
2024-12-28 01:48:07,308: Early Stopping! Snapshot: 0 Epoch: 199 Best Results: 25.19
2024-12-28 01:48:07,309: Start to training tokens! Snapshot: 0 Epoch: 199 Loss:4.302 MRR:25.19 Best Results: 25.19
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 01:48:07,309: Snapshot:0	Epoch:199	Loss:4.302	translation_Loss:4.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.19	Hits@10:45.61	Best:25.19
2024-12-28 01:48:07,634: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.00001_512_5000/0model_best.tar'
2024-12-28 01:48:10,180: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2514 | 0.1426 | 0.3122 | 0.3731 |  0.4451 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 01:48:22,158: Snapshot:1	Epoch:0	Loss:30.347	translation_Loss:30.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.002                                                   	MRR:5.88	Hits@10:9.68	Best:5.88
2024-12-28 01:48:24,633: Snapshot:1	Epoch:1	Loss:29.895	translation_Loss:29.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.008                                                   	MRR:5.99	Hits@10:9.88	Best:5.99
2024-12-28 01:48:27,159: Snapshot:1	Epoch:2	Loss:29.461	translation_Loss:29.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.015                                                   	MRR:6.08	Hits@10:9.98	Best:6.08
2024-12-28 01:48:29,660: Snapshot:1	Epoch:3	Loss:28.969	translation_Loss:28.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.023                                                   	MRR:6.17	Hits@10:10.14	Best:6.17
2024-12-28 01:48:32,091: Snapshot:1	Epoch:4	Loss:28.547	translation_Loss:28.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.033                                                   	MRR:6.29	Hits@10:10.25	Best:6.29
2024-12-28 01:48:34,539: Snapshot:1	Epoch:5	Loss:28.113	translation_Loss:28.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.044                                                   	MRR:6.39	Hits@10:10.41	Best:6.39
2024-12-28 01:48:37,029: Snapshot:1	Epoch:6	Loss:27.665	translation_Loss:27.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.056                                                   	MRR:6.49	Hits@10:10.6	Best:6.49
2024-12-28 01:48:39,462: Snapshot:1	Epoch:7	Loss:27.198	translation_Loss:27.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.069                                                   	MRR:6.58	Hits@10:10.8	Best:6.58
2024-12-28 01:48:41,906: Snapshot:1	Epoch:8	Loss:26.802	translation_Loss:26.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.084                                                   	MRR:6.69	Hits@10:10.95	Best:6.69
2024-12-28 01:48:44,376: Snapshot:1	Epoch:9	Loss:26.34	translation_Loss:26.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.099                                                   	MRR:6.8	Hits@10:11.13	Best:6.8
2024-12-28 01:48:47,264: Snapshot:1	Epoch:10	Loss:25.939	translation_Loss:25.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.115                                                   	MRR:6.93	Hits@10:11.39	Best:6.93
2024-12-28 01:48:49,743: Snapshot:1	Epoch:11	Loss:25.491	translation_Loss:25.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.132                                                   	MRR:7.06	Hits@10:11.6	Best:7.06
2024-12-28 01:48:52,212: Snapshot:1	Epoch:12	Loss:25.079	translation_Loss:24.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.15                                                   	MRR:7.21	Hits@10:11.88	Best:7.21
2024-12-28 01:48:54,690: Snapshot:1	Epoch:13	Loss:24.689	translation_Loss:24.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:7.37	Hits@10:12.08	Best:7.37
2024-12-28 01:48:57,186: Snapshot:1	Epoch:14	Loss:24.202	translation_Loss:24.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:7.57	Hits@10:12.48	Best:7.57
2024-12-28 01:48:59,730: Snapshot:1	Epoch:15	Loss:23.832	translation_Loss:23.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:7.75	Hits@10:12.95	Best:7.75
2024-12-28 01:49:02,214: Snapshot:1	Epoch:16	Loss:23.42	translation_Loss:23.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.228                                                   	MRR:7.95	Hits@10:13.47	Best:7.95
2024-12-28 01:49:04,657: Snapshot:1	Epoch:17	Loss:23.038	translation_Loss:22.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.25                                                   	MRR:8.17	Hits@10:13.88	Best:8.17
2024-12-28 01:49:07,105: Snapshot:1	Epoch:18	Loss:22.631	translation_Loss:22.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.271                                                   	MRR:8.39	Hits@10:14.4	Best:8.39
2024-12-28 01:49:09,534: Snapshot:1	Epoch:19	Loss:22.216	translation_Loss:21.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.294                                                   	MRR:8.64	Hits@10:14.85	Best:8.64
2024-12-28 01:49:11,986: Snapshot:1	Epoch:20	Loss:21.807	translation_Loss:21.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.316                                                   	MRR:8.89	Hits@10:15.38	Best:8.89
2024-12-28 01:49:14,464: Snapshot:1	Epoch:21	Loss:21.412	translation_Loss:21.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.339                                                   	MRR:9.16	Hits@10:16.11	Best:9.16
2024-12-28 01:49:16,981: Snapshot:1	Epoch:22	Loss:21.065	translation_Loss:20.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.363                                                   	MRR:9.45	Hits@10:17.01	Best:9.45
2024-12-28 01:49:19,485: Snapshot:1	Epoch:23	Loss:20.644	translation_Loss:20.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.386                                                   	MRR:9.71	Hits@10:17.69	Best:9.71
2024-12-28 01:49:21,974: Snapshot:1	Epoch:24	Loss:20.244	translation_Loss:19.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.41                                                   	MRR:10.02	Hits@10:18.48	Best:10.02
2024-12-28 01:49:24,455: Snapshot:1	Epoch:25	Loss:19.901	translation_Loss:19.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.435                                                   	MRR:10.3	Hits@10:19.35	Best:10.3
2024-12-28 01:49:26,952: Snapshot:1	Epoch:26	Loss:19.501	translation_Loss:19.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.459                                                   	MRR:10.6	Hits@10:20.04	Best:10.6
2024-12-28 01:49:29,427: Snapshot:1	Epoch:27	Loss:19.134	translation_Loss:18.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.484                                                   	MRR:10.91	Hits@10:20.81	Best:10.91
2024-12-28 01:49:31,880: Snapshot:1	Epoch:28	Loss:18.795	translation_Loss:18.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.508                                                   	MRR:11.24	Hits@10:21.46	Best:11.24
2024-12-28 01:49:34,361: Snapshot:1	Epoch:29	Loss:18.44	translation_Loss:17.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.533                                                   	MRR:11.52	Hits@10:22.34	Best:11.52
2024-12-28 01:49:36,873: Snapshot:1	Epoch:30	Loss:18.075	translation_Loss:17.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.558                                                   	MRR:11.86	Hits@10:23.03	Best:11.86
2024-12-28 01:49:39,364: Snapshot:1	Epoch:31	Loss:17.766	translation_Loss:17.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.582                                                   	MRR:12.15	Hits@10:23.66	Best:12.15
2024-12-28 01:49:41,836: Snapshot:1	Epoch:32	Loss:17.424	translation_Loss:16.818	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:12.45	Hits@10:24.23	Best:12.45
2024-12-28 01:49:44,277: Snapshot:1	Epoch:33	Loss:17.121	translation_Loss:16.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.631                                                   	MRR:12.73	Hits@10:24.8	Best:12.73
2024-12-28 01:49:46,758: Snapshot:1	Epoch:34	Loss:16.79	translation_Loss:16.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.655                                                   	MRR:13.02	Hits@10:25.38	Best:13.02
2024-12-28 01:49:49,305: Snapshot:1	Epoch:35	Loss:16.445	translation_Loss:15.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.679                                                   	MRR:13.32	Hits@10:25.96	Best:13.32
2024-12-28 01:49:51,776: Snapshot:1	Epoch:36	Loss:16.176	translation_Loss:15.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.703                                                   	MRR:13.63	Hits@10:26.56	Best:13.63
2024-12-28 01:49:54,243: Snapshot:1	Epoch:37	Loss:15.86	translation_Loss:15.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.727                                                   	MRR:13.89	Hits@10:27.1	Best:13.89
2024-12-28 01:49:56,708: Snapshot:1	Epoch:38	Loss:15.595	translation_Loss:14.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.75                                                   	MRR:14.17	Hits@10:27.6	Best:14.17
2024-12-28 01:49:59,182: Snapshot:1	Epoch:39	Loss:15.276	translation_Loss:14.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.774                                                   	MRR:14.45	Hits@10:28.12	Best:14.45
2024-12-28 01:50:01,680: Snapshot:1	Epoch:40	Loss:15.007	translation_Loss:14.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.797                                                   	MRR:14.73	Hits@10:28.67	Best:14.73
2024-12-28 01:50:04,136: Snapshot:1	Epoch:41	Loss:14.769	translation_Loss:13.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.82                                                   	MRR:14.95	Hits@10:29.01	Best:14.95
2024-12-28 01:50:06,623: Snapshot:1	Epoch:42	Loss:14.505	translation_Loss:13.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.843                                                   	MRR:15.19	Hits@10:29.46	Best:15.19
2024-12-28 01:50:09,144: Snapshot:1	Epoch:43	Loss:14.275	translation_Loss:13.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.866                                                   	MRR:15.42	Hits@10:29.86	Best:15.42
2024-12-28 01:50:11,617: Snapshot:1	Epoch:44	Loss:13.99	translation_Loss:13.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.888                                                   	MRR:15.66	Hits@10:30.32	Best:15.66
2024-12-28 01:50:14,070: Snapshot:1	Epoch:45	Loss:13.789	translation_Loss:12.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.91                                                   	MRR:15.93	Hits@10:30.59	Best:15.93
2024-12-28 01:50:16,497: Snapshot:1	Epoch:46	Loss:13.561	translation_Loss:12.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.931                                                   	MRR:16.16	Hits@10:30.96	Best:16.16
2024-12-28 01:50:19,044: Snapshot:1	Epoch:47	Loss:13.301	translation_Loss:12.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.953                                                   	MRR:16.36	Hits@10:31.17	Best:16.36
2024-12-28 01:50:21,574: Snapshot:1	Epoch:48	Loss:13.074	translation_Loss:12.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.974                                                   	MRR:16.58	Hits@10:31.43	Best:16.58
2024-12-28 01:50:24,026: Snapshot:1	Epoch:49	Loss:12.876	translation_Loss:11.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.994                                                   	MRR:16.81	Hits@10:31.77	Best:16.81
2024-12-28 01:50:26,479: Snapshot:1	Epoch:50	Loss:12.616	translation_Loss:11.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.015                                                   	MRR:16.95	Hits@10:32.04	Best:16.95
2024-12-28 01:50:28,956: Snapshot:1	Epoch:51	Loss:12.41	translation_Loss:11.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.035                                                   	MRR:17.16	Hits@10:32.36	Best:17.16
2024-12-28 01:50:31,431: Snapshot:1	Epoch:52	Loss:12.266	translation_Loss:11.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.054                                                   	MRR:17.32	Hits@10:32.59	Best:17.32
2024-12-28 01:50:33,917: Snapshot:1	Epoch:53	Loss:12.033	translation_Loss:10.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.074                                                   	MRR:17.54	Hits@10:32.93	Best:17.54
2024-12-28 01:50:36,372: Snapshot:1	Epoch:54	Loss:11.856	translation_Loss:10.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.092                                                   	MRR:17.74	Hits@10:33.21	Best:17.74
2024-12-28 01:50:38,843: Snapshot:1	Epoch:55	Loss:11.675	translation_Loss:10.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.111                                                   	MRR:17.97	Hits@10:33.54	Best:17.97
2024-12-28 01:50:41,358: Snapshot:1	Epoch:56	Loss:11.472	translation_Loss:10.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.129                                                   	MRR:18.16	Hits@10:33.81	Best:18.16
2024-12-28 01:50:43,865: Snapshot:1	Epoch:57	Loss:11.331	translation_Loss:10.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.147                                                   	MRR:18.32	Hits@10:33.89	Best:18.32
2024-12-28 01:50:46,362: Snapshot:1	Epoch:58	Loss:11.124	translation_Loss:9.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.165                                                   	MRR:18.45	Hits@10:34.22	Best:18.45
2024-12-28 01:50:49,378: Snapshot:1	Epoch:59	Loss:10.982	translation_Loss:9.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.183                                                   	MRR:18.63	Hits@10:34.56	Best:18.63
2024-12-28 01:50:51,833: Snapshot:1	Epoch:60	Loss:10.812	translation_Loss:9.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.2                                                   	MRR:18.81	Hits@10:34.77	Best:18.81
2024-12-28 01:50:54,345: Snapshot:1	Epoch:61	Loss:10.672	translation_Loss:9.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.216                                                   	MRR:18.96	Hits@10:34.99	Best:18.96
2024-12-28 01:50:56,836: Snapshot:1	Epoch:62	Loss:10.545	translation_Loss:9.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.233                                                   	MRR:19.12	Hits@10:35.29	Best:19.12
2024-12-28 01:50:59,303: Snapshot:1	Epoch:63	Loss:10.356	translation_Loss:9.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.249                                                   	MRR:19.23	Hits@10:35.53	Best:19.23
2024-12-28 01:51:01,762: Snapshot:1	Epoch:64	Loss:10.203	translation_Loss:8.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.265                                                   	MRR:19.36	Hits@10:35.93	Best:19.36
2024-12-28 01:51:04,212: Snapshot:1	Epoch:65	Loss:10.103	translation_Loss:8.823	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.28                                                   	MRR:19.54	Hits@10:36.13	Best:19.54
2024-12-28 01:51:06,638: Snapshot:1	Epoch:66	Loss:9.946	translation_Loss:8.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.295                                                   	MRR:19.65	Hits@10:36.3	Best:19.65
2024-12-28 01:51:09,085: Snapshot:1	Epoch:67	Loss:9.834	translation_Loss:8.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.31                                                   	MRR:19.78	Hits@10:36.51	Best:19.78
2024-12-28 01:51:11,571: Snapshot:1	Epoch:68	Loss:9.714	translation_Loss:8.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.324                                                   	MRR:19.92	Hits@10:36.8	Best:19.92
2024-12-28 01:51:14,105: Snapshot:1	Epoch:69	Loss:9.575	translation_Loss:8.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.339                                                   	MRR:20.03	Hits@10:36.99	Best:20.03
2024-12-28 01:51:16,571: Snapshot:1	Epoch:70	Loss:9.416	translation_Loss:8.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.353                                                   	MRR:20.08	Hits@10:37.26	Best:20.08
2024-12-28 01:51:19,066: Snapshot:1	Epoch:71	Loss:9.319	translation_Loss:7.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.366                                                   	MRR:20.23	Hits@10:37.44	Best:20.23
2024-12-28 01:51:21,509: Snapshot:1	Epoch:72	Loss:9.221	translation_Loss:7.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.379                                                   	MRR:20.38	Hits@10:37.59	Best:20.38
2024-12-28 01:51:23,991: Snapshot:1	Epoch:73	Loss:9.105	translation_Loss:7.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.392                                                   	MRR:20.51	Hits@10:37.71	Best:20.51
2024-12-28 01:51:26,497: Snapshot:1	Epoch:74	Loss:8.983	translation_Loss:7.578	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.405                                                   	MRR:20.63	Hits@10:37.89	Best:20.63
2024-12-28 01:51:29,030: Snapshot:1	Epoch:75	Loss:8.865	translation_Loss:7.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.417                                                   	MRR:20.74	Hits@10:38.11	Best:20.74
2024-12-28 01:51:31,493: Snapshot:1	Epoch:76	Loss:8.752	translation_Loss:7.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.429                                                   	MRR:20.84	Hits@10:38.24	Best:20.84
2024-12-28 01:51:33,952: Snapshot:1	Epoch:77	Loss:8.625	translation_Loss:7.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.441                                                   	MRR:20.96	Hits@10:38.41	Best:20.96
2024-12-28 01:51:36,396: Snapshot:1	Epoch:78	Loss:8.536	translation_Loss:7.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:21.06	Hits@10:38.55	Best:21.06
2024-12-28 01:51:38,880: Snapshot:1	Epoch:79	Loss:8.448	translation_Loss:6.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.464                                                   	MRR:21.19	Hits@10:38.75	Best:21.19
2024-12-28 01:51:41,374: Snapshot:1	Epoch:80	Loss:8.365	translation_Loss:6.89	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.475                                                   	MRR:21.34	Hits@10:38.99	Best:21.34
2024-12-28 01:51:43,849: Snapshot:1	Epoch:81	Loss:8.281	translation_Loss:6.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.485                                                   	MRR:21.46	Hits@10:39.1	Best:21.46
2024-12-28 01:51:46,290: Snapshot:1	Epoch:82	Loss:8.215	translation_Loss:6.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.495                                                   	MRR:21.58	Hits@10:39.22	Best:21.58
2024-12-28 01:51:48,718: Snapshot:1	Epoch:83	Loss:8.076	translation_Loss:6.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.506                                                   	MRR:21.73	Hits@10:39.43	Best:21.73
2024-12-28 01:51:51,188: Snapshot:1	Epoch:84	Loss:7.974	translation_Loss:6.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.515                                                   	MRR:21.83	Hits@10:39.62	Best:21.83
2024-12-28 01:51:53,678: Snapshot:1	Epoch:85	Loss:7.917	translation_Loss:6.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.525                                                   	MRR:21.94	Hits@10:39.78	Best:21.94
2024-12-28 01:51:56,139: Snapshot:1	Epoch:86	Loss:7.826	translation_Loss:6.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.534                                                   	MRR:22.06	Hits@10:40.01	Best:22.06
2024-12-28 01:51:58,582: Snapshot:1	Epoch:87	Loss:7.743	translation_Loss:6.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.543                                                   	MRR:22.17	Hits@10:40.19	Best:22.17
2024-12-28 01:52:01,065: Snapshot:1	Epoch:88	Loss:7.669	translation_Loss:6.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.552                                                   	MRR:22.32	Hits@10:40.35	Best:22.32
2024-12-28 01:52:03,516: Snapshot:1	Epoch:89	Loss:7.581	translation_Loss:6.021	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.56                                                   	MRR:22.41	Hits@10:40.49	Best:22.41
2024-12-28 01:52:05,986: Snapshot:1	Epoch:90	Loss:7.499	translation_Loss:5.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.569                                                   	MRR:22.55	Hits@10:40.72	Best:22.55
2024-12-28 01:52:08,440: Snapshot:1	Epoch:91	Loss:7.443	translation_Loss:5.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.577                                                   	MRR:22.7	Hits@10:40.96	Best:22.7
2024-12-28 01:52:10,929: Snapshot:1	Epoch:92	Loss:7.358	translation_Loss:5.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.584                                                   	MRR:22.79	Hits@10:41.13	Best:22.79
2024-12-28 01:52:13,423: Snapshot:1	Epoch:93	Loss:7.313	translation_Loss:5.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.592                                                   	MRR:22.89	Hits@10:41.22	Best:22.89
2024-12-28 01:52:15,896: Snapshot:1	Epoch:94	Loss:7.229	translation_Loss:5.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.599                                                   	MRR:22.98	Hits@10:41.37	Best:22.98
2024-12-28 01:52:18,376: Snapshot:1	Epoch:95	Loss:7.123	translation_Loss:5.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.606                                                   	MRR:23.09	Hits@10:41.57	Best:23.09
2024-12-28 01:52:20,836: Snapshot:1	Epoch:96	Loss:7.097	translation_Loss:5.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.613                                                   	MRR:23.17	Hits@10:41.8	Best:23.17
2024-12-28 01:52:23,313: Snapshot:1	Epoch:97	Loss:7.042	translation_Loss:5.422	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.62                                                   	MRR:23.24	Hits@10:41.87	Best:23.24
2024-12-28 01:52:25,804: Snapshot:1	Epoch:98	Loss:6.935	translation_Loss:5.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.626                                                   	MRR:23.31	Hits@10:42.06	Best:23.31
2024-12-28 01:52:28,290: Snapshot:1	Epoch:99	Loss:6.886	translation_Loss:5.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.632                                                   	MRR:23.39	Hits@10:42.22	Best:23.39
2024-12-28 01:52:30,775: Snapshot:1	Epoch:100	Loss:6.831	translation_Loss:5.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.638                                                   	MRR:23.46	Hits@10:42.3	Best:23.46
2024-12-28 01:52:33,253: Snapshot:1	Epoch:101	Loss:6.726	translation_Loss:5.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.644                                                   	MRR:23.52	Hits@10:42.38	Best:23.52
2024-12-28 01:52:35,733: Snapshot:1	Epoch:102	Loss:6.692	translation_Loss:5.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.65                                                   	MRR:23.61	Hits@10:42.56	Best:23.61
2024-12-28 01:52:38,163: Snapshot:1	Epoch:103	Loss:6.624	translation_Loss:4.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.656                                                   	MRR:23.7	Hits@10:42.65	Best:23.7
2024-12-28 01:52:40,637: Snapshot:1	Epoch:104	Loss:6.564	translation_Loss:4.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.661                                                   	MRR:23.79	Hits@10:42.87	Best:23.79
2024-12-28 01:52:43,068: Snapshot:1	Epoch:105	Loss:6.499	translation_Loss:4.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.665                                                   	MRR:23.88	Hits@10:43.07	Best:23.88
2024-12-28 01:52:45,540: Snapshot:1	Epoch:106	Loss:6.463	translation_Loss:4.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.67                                                   	MRR:23.92	Hits@10:43.2	Best:23.92
2024-12-28 01:52:47,983: Snapshot:1	Epoch:107	Loss:6.419	translation_Loss:4.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.675                                                   	MRR:24.03	Hits@10:43.32	Best:24.03
2024-12-28 01:52:50,430: Snapshot:1	Epoch:108	Loss:6.331	translation_Loss:4.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.679                                                   	MRR:24.09	Hits@10:43.41	Best:24.09
2024-12-28 01:52:53,322: Snapshot:1	Epoch:109	Loss:6.327	translation_Loss:4.643	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.684                                                   	MRR:24.22	Hits@10:43.56	Best:24.22
2024-12-28 01:52:55,760: Snapshot:1	Epoch:110	Loss:6.242	translation_Loss:4.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.688                                                   	MRR:24.32	Hits@10:43.6	Best:24.32
2024-12-28 01:52:58,215: Snapshot:1	Epoch:111	Loss:6.219	translation_Loss:4.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.692                                                   	MRR:24.37	Hits@10:43.75	Best:24.37
2024-12-28 01:53:00,688: Snapshot:1	Epoch:112	Loss:6.166	translation_Loss:4.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.696                                                   	MRR:24.49	Hits@10:43.8	Best:24.49
2024-12-28 01:53:03,157: Snapshot:1	Epoch:113	Loss:6.137	translation_Loss:4.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.7                                                   	MRR:24.55	Hits@10:43.89	Best:24.55
2024-12-28 01:53:05,639: Snapshot:1	Epoch:114	Loss:6.053	translation_Loss:4.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.703                                                   	MRR:24.62	Hits@10:43.93	Best:24.62
2024-12-28 01:53:08,116: Snapshot:1	Epoch:115	Loss:6.008	translation_Loss:4.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.706                                                   	MRR:24.69	Hits@10:44.02	Best:24.69
2024-12-28 01:53:10,608: Snapshot:1	Epoch:116	Loss:5.948	translation_Loss:4.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.71                                                   	MRR:24.71	Hits@10:44.17	Best:24.71
2024-12-28 01:53:13,089: Snapshot:1	Epoch:117	Loss:5.929	translation_Loss:4.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.713                                                   	MRR:24.78	Hits@10:44.25	Best:24.78
2024-12-28 01:53:15,576: Snapshot:1	Epoch:118	Loss:5.888	translation_Loss:4.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.716                                                   	MRR:24.87	Hits@10:44.37	Best:24.87
2024-12-28 01:53:18,109: Snapshot:1	Epoch:119	Loss:5.813	translation_Loss:4.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.719                                                   	MRR:24.9	Hits@10:44.41	Best:24.9
2024-12-28 01:53:20,566: Snapshot:1	Epoch:120	Loss:5.795	translation_Loss:4.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.721                                                   	MRR:24.95	Hits@10:44.48	Best:24.95
2024-12-28 01:53:23,033: Snapshot:1	Epoch:121	Loss:5.75	translation_Loss:4.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.724                                                   	MRR:25.02	Hits@10:44.53	Best:25.02
2024-12-28 01:53:25,517: Snapshot:1	Epoch:122	Loss:5.687	translation_Loss:3.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.727                                                   	MRR:25.09	Hits@10:44.57	Best:25.09
2024-12-28 01:53:27,960: Snapshot:1	Epoch:123	Loss:5.639	translation_Loss:3.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.729                                                   	MRR:25.09	Hits@10:44.62	Best:25.09
2024-12-28 01:53:30,408: Snapshot:1	Epoch:124	Loss:5.614	translation_Loss:3.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.731                                                   	MRR:25.16	Hits@10:44.72	Best:25.16
2024-12-28 01:53:32,909: Snapshot:1	Epoch:125	Loss:5.563	translation_Loss:3.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.733                                                   	MRR:25.19	Hits@10:44.73	Best:25.19
2024-12-28 01:53:35,359: Snapshot:1	Epoch:126	Loss:5.539	translation_Loss:3.804	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.735                                                   	MRR:25.26	Hits@10:44.81	Best:25.26
2024-12-28 01:53:37,867: Snapshot:1	Epoch:127	Loss:5.488	translation_Loss:3.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.737                                                   	MRR:25.3	Hits@10:44.95	Best:25.3
2024-12-28 01:53:40,376: Snapshot:1	Epoch:128	Loss:5.459	translation_Loss:3.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.739                                                   	MRR:25.4	Hits@10:45.09	Best:25.4
2024-12-28 01:53:42,870: Snapshot:1	Epoch:129	Loss:5.42	translation_Loss:3.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.741                                                   	MRR:25.45	Hits@10:45.11	Best:25.45
2024-12-28 01:53:45,339: Snapshot:1	Epoch:130	Loss:5.394	translation_Loss:3.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.742                                                   	MRR:25.52	Hits@10:45.1	Best:25.52
2024-12-28 01:53:47,848: Snapshot:1	Epoch:131	Loss:5.343	translation_Loss:3.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.744                                                   	MRR:25.56	Hits@10:45.21	Best:25.56
2024-12-28 01:53:50,299: Snapshot:1	Epoch:132	Loss:5.307	translation_Loss:3.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.745                                                   	MRR:25.58	Hits@10:45.32	Best:25.58
2024-12-28 01:53:52,764: Snapshot:1	Epoch:133	Loss:5.271	translation_Loss:3.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.746                                                   	MRR:25.63	Hits@10:45.4	Best:25.63
2024-12-28 01:53:55,202: Snapshot:1	Epoch:134	Loss:5.256	translation_Loss:3.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.747                                                   	MRR:25.61	Hits@10:45.39	Best:25.63
2024-12-28 01:53:57,684: Snapshot:1	Epoch:135	Loss:5.2	translation_Loss:3.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.749                                                   	MRR:25.66	Hits@10:45.39	Best:25.66
2024-12-28 01:54:00,165: Snapshot:1	Epoch:136	Loss:5.17	translation_Loss:3.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.75                                                   	MRR:25.71	Hits@10:45.49	Best:25.71
2024-12-28 01:54:02,648: Snapshot:1	Epoch:137	Loss:5.144	translation_Loss:3.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.751                                                   	MRR:25.78	Hits@10:45.51	Best:25.78
2024-12-28 01:54:05,145: Snapshot:1	Epoch:138	Loss:5.118	translation_Loss:3.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.751                                                   	MRR:25.82	Hits@10:45.59	Best:25.82
2024-12-28 01:54:07,673: Snapshot:1	Epoch:139	Loss:5.079	translation_Loss:3.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.753                                                   	MRR:25.85	Hits@10:45.62	Best:25.85
2024-12-28 01:54:10,153: Snapshot:1	Epoch:140	Loss:5.025	translation_Loss:3.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.753                                                   	MRR:25.87	Hits@10:45.65	Best:25.87
2024-12-28 01:54:12,627: Snapshot:1	Epoch:141	Loss:5.023	translation_Loss:3.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.754                                                   	MRR:25.94	Hits@10:45.76	Best:25.94
2024-12-28 01:54:15,112: Snapshot:1	Epoch:142	Loss:4.995	translation_Loss:3.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.754                                                   	MRR:25.97	Hits@10:45.84	Best:25.97
2024-12-28 01:54:17,617: Snapshot:1	Epoch:143	Loss:4.967	translation_Loss:3.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.754                                                   	MRR:26.02	Hits@10:45.9	Best:26.02
2024-12-28 01:54:20,066: Snapshot:1	Epoch:144	Loss:4.949	translation_Loss:3.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.05	Hits@10:45.9	Best:26.05
2024-12-28 01:54:22,552: Snapshot:1	Epoch:145	Loss:4.907	translation_Loss:3.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.11	Hits@10:45.94	Best:26.11
2024-12-28 01:54:25,038: Snapshot:1	Epoch:146	Loss:4.877	translation_Loss:3.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.17	Hits@10:45.96	Best:26.17
2024-12-28 01:54:27,465: Snapshot:1	Epoch:147	Loss:4.844	translation_Loss:3.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:26.13	Hits@10:46.02	Best:26.17
2024-12-28 01:54:29,944: Snapshot:1	Epoch:148	Loss:4.797	translation_Loss:3.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:26.19	Hits@10:46.16	Best:26.19
2024-12-28 01:54:32,410: Snapshot:1	Epoch:149	Loss:4.785	translation_Loss:3.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:26.21	Hits@10:46.2	Best:26.21
2024-12-28 01:54:34,839: Snapshot:1	Epoch:150	Loss:4.739	translation_Loss:2.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:26.26	Hits@10:46.36	Best:26.26
2024-12-28 01:54:37,278: Snapshot:1	Epoch:151	Loss:4.71	translation_Loss:2.954	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:26.3	Hits@10:46.34	Best:26.3
2024-12-28 01:54:39,770: Snapshot:1	Epoch:152	Loss:4.706	translation_Loss:2.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:26.36	Hits@10:46.34	Best:26.36
2024-12-28 01:54:42,245: Snapshot:1	Epoch:153	Loss:4.685	translation_Loss:2.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.38	Hits@10:46.37	Best:26.38
2024-12-28 01:54:44,772: Snapshot:1	Epoch:154	Loss:4.647	translation_Loss:2.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.43	Hits@10:46.47	Best:26.43
2024-12-28 01:54:47,248: Snapshot:1	Epoch:155	Loss:4.621	translation_Loss:2.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.48	Hits@10:46.49	Best:26.48
2024-12-28 01:54:49,760: Snapshot:1	Epoch:156	Loss:4.61	translation_Loss:2.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.55	Hits@10:46.47	Best:26.55
2024-12-28 01:54:52,207: Snapshot:1	Epoch:157	Loss:4.592	translation_Loss:2.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.56	Hits@10:46.54	Best:26.56
2024-12-28 01:54:54,635: Snapshot:1	Epoch:158	Loss:4.556	translation_Loss:2.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:26.58	Hits@10:46.56	Best:26.58
2024-12-28 01:54:57,512: Snapshot:1	Epoch:159	Loss:4.532	translation_Loss:2.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.754                                                   	MRR:26.55	Hits@10:46.67	Best:26.58
2024-12-28 01:54:59,910: Snapshot:1	Epoch:160	Loss:4.494	translation_Loss:2.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.754                                                   	MRR:26.58	Hits@10:46.78	Best:26.58
2024-12-28 01:55:02,348: Snapshot:1	Epoch:161	Loss:4.474	translation_Loss:2.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.753                                                   	MRR:26.64	Hits@10:46.74	Best:26.64
2024-12-28 01:55:04,823: Snapshot:1	Epoch:162	Loss:4.463	translation_Loss:2.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.753                                                   	MRR:26.65	Hits@10:46.79	Best:26.65
2024-12-28 01:55:07,303: Snapshot:1	Epoch:163	Loss:4.453	translation_Loss:2.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.752                                                   	MRR:26.66	Hits@10:46.81	Best:26.66
2024-12-28 01:55:09,756: Snapshot:1	Epoch:164	Loss:4.419	translation_Loss:2.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.752                                                   	MRR:26.71	Hits@10:46.9	Best:26.71
2024-12-28 01:55:12,273: Snapshot:1	Epoch:165	Loss:4.402	translation_Loss:2.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.752                                                   	MRR:26.78	Hits@10:47.03	Best:26.78
2024-12-28 01:55:14,765: Snapshot:1	Epoch:166	Loss:4.344	translation_Loss:2.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.751                                                   	MRR:26.76	Hits@10:47.11	Best:26.78
2024-12-28 01:55:17,258: Snapshot:1	Epoch:167	Loss:4.345	translation_Loss:2.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.75                                                   	MRR:26.84	Hits@10:47.1	Best:26.84
2024-12-28 01:55:19,752: Snapshot:1	Epoch:168	Loss:4.329	translation_Loss:2.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.749                                                   	MRR:26.88	Hits@10:47.17	Best:26.88
2024-12-28 01:55:22,228: Snapshot:1	Epoch:169	Loss:4.312	translation_Loss:2.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.749                                                   	MRR:26.89	Hits@10:47.2	Best:26.89
2024-12-28 01:55:24,729: Snapshot:1	Epoch:170	Loss:4.277	translation_Loss:2.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.748                                                   	MRR:26.9	Hits@10:47.2	Best:26.9
2024-12-28 01:55:27,207: Snapshot:1	Epoch:171	Loss:4.268	translation_Loss:2.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.747                                                   	MRR:26.91	Hits@10:47.33	Best:26.91
2024-12-28 01:55:29,768: Snapshot:1	Epoch:172	Loss:4.233	translation_Loss:2.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.746                                                   	MRR:26.94	Hits@10:47.37	Best:26.94
2024-12-28 01:55:32,229: Snapshot:1	Epoch:173	Loss:4.226	translation_Loss:2.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.745                                                   	MRR:26.95	Hits@10:47.46	Best:26.95
2024-12-28 01:55:34,684: Snapshot:1	Epoch:174	Loss:4.199	translation_Loss:2.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.744                                                   	MRR:26.95	Hits@10:47.51	Best:26.95
2024-12-28 01:55:37,095: Snapshot:1	Epoch:175	Loss:4.186	translation_Loss:2.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.743                                                   	MRR:26.92	Hits@10:47.51	Best:26.95
2024-12-28 01:55:39,559: Snapshot:1	Epoch:176	Loss:4.184	translation_Loss:2.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.742                                                   	MRR:26.98	Hits@10:47.49	Best:26.98
2024-12-28 01:55:42,020: Snapshot:1	Epoch:177	Loss:4.145	translation_Loss:2.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.742                                                   	MRR:27.03	Hits@10:47.53	Best:27.03
2024-12-28 01:55:44,423: Snapshot:1	Epoch:178	Loss:4.142	translation_Loss:2.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.74                                                   	MRR:27.02	Hits@10:47.58	Best:27.03
2024-12-28 01:55:46,838: Snapshot:1	Epoch:179	Loss:4.125	translation_Loss:2.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.739                                                   	MRR:26.98	Hits@10:47.62	Best:27.03
2024-12-28 01:55:49,258: Snapshot:1	Epoch:180	Loss:4.116	translation_Loss:2.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.738                                                   	MRR:27.0	Hits@10:47.69	Best:27.03
2024-12-28 01:55:51,699: Snapshot:1	Epoch:181	Loss:4.092	translation_Loss:2.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.737                                                   	MRR:27.04	Hits@10:47.74	Best:27.04
2024-12-28 01:55:54,112: Snapshot:1	Epoch:182	Loss:4.082	translation_Loss:2.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.736                                                   	MRR:27.03	Hits@10:47.72	Best:27.04
2024-12-28 01:55:56,603: Snapshot:1	Epoch:183	Loss:4.067	translation_Loss:2.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.735                                                   	MRR:27.06	Hits@10:47.81	Best:27.06
2024-12-28 01:55:59,041: Snapshot:1	Epoch:184	Loss:4.036	translation_Loss:2.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.734                                                   	MRR:27.01	Hits@10:47.71	Best:27.06
2024-12-28 01:56:01,433: Snapshot:1	Epoch:185	Loss:4.004	translation_Loss:2.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.733                                                   	MRR:27.05	Hits@10:47.8	Best:27.06
2024-12-28 01:56:03,893: Snapshot:1	Epoch:186	Loss:3.997	translation_Loss:2.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.731                                                   	MRR:27.08	Hits@10:47.89	Best:27.08
2024-12-28 01:56:06,372: Snapshot:1	Epoch:187	Loss:3.96	translation_Loss:2.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.73                                                   	MRR:27.12	Hits@10:47.89	Best:27.12
2024-12-28 01:56:08,809: Snapshot:1	Epoch:188	Loss:3.971	translation_Loss:2.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.729                                                   	MRR:27.13	Hits@10:47.94	Best:27.13
2024-12-28 01:56:11,217: Snapshot:1	Epoch:189	Loss:3.984	translation_Loss:2.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.727                                                   	MRR:27.12	Hits@10:48.0	Best:27.13
2024-12-28 01:56:13,665: Snapshot:1	Epoch:190	Loss:3.934	translation_Loss:2.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.726                                                   	MRR:27.2	Hits@10:48.11	Best:27.2
2024-12-28 01:56:16,106: Snapshot:1	Epoch:191	Loss:3.933	translation_Loss:2.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.724                                                   	MRR:27.24	Hits@10:48.12	Best:27.24
2024-12-28 01:56:18,587: Snapshot:1	Epoch:192	Loss:3.907	translation_Loss:2.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.723                                                   	MRR:27.27	Hits@10:48.2	Best:27.27
2024-12-28 01:56:21,024: Snapshot:1	Epoch:193	Loss:3.884	translation_Loss:2.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.722                                                   	MRR:27.32	Hits@10:48.19	Best:27.32
2024-12-28 01:56:23,454: Snapshot:1	Epoch:194	Loss:3.892	translation_Loss:2.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.721                                                   	MRR:27.31	Hits@10:48.19	Best:27.32
2024-12-28 01:56:25,920: Snapshot:1	Epoch:195	Loss:3.86	translation_Loss:2.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.72                                                   	MRR:27.33	Hits@10:48.29	Best:27.33
2024-12-28 01:56:28,372: Snapshot:1	Epoch:196	Loss:3.838	translation_Loss:2.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.718                                                   	MRR:27.35	Hits@10:48.28	Best:27.35
2024-12-28 01:56:30,828: Snapshot:1	Epoch:197	Loss:3.829	translation_Loss:2.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.717                                                   	MRR:27.36	Hits@10:48.35	Best:27.36
2024-12-28 01:56:33,308: Snapshot:1	Epoch:198	Loss:3.828	translation_Loss:2.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.716                                                   	MRR:27.42	Hits@10:48.37	Best:27.42
2024-12-28 01:56:35,724: Snapshot:1	Epoch:199	Loss:3.808	translation_Loss:2.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.715                                                   	MRR:27.37	Hits@10:48.36	Best:27.42
2024-12-28 01:56:36,060: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.00001_512_5000/1model_best.tar'
2024-12-28 01:56:40,204: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2442 | 0.1356 | 0.3029 | 0.368  |  0.4412 |
|     1      | 0.2703 | 0.1602 | 0.3265 | 0.3915 |  0.4788 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 01:57:15,779: Snapshot:2	Epoch:0	Loss:132.54	translation_Loss:132.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.028                                                   	MRR:3.13	Hits@10:5.75	Best:3.13
2024-12-28 01:57:26,566: Snapshot:2	Epoch:1	Loss:128.258	translation_Loss:128.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.096                                                   	MRR:3.3	Hits@10:6.14	Best:3.3
2024-12-28 01:57:37,277: Snapshot:2	Epoch:2	Loss:124.008	translation_Loss:123.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:3.54	Hits@10:6.81	Best:3.54
2024-12-28 01:57:47,951: Snapshot:2	Epoch:3	Loss:119.782	translation_Loss:119.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:3.92	Hits@10:7.88	Best:3.92
2024-12-28 01:57:58,725: Snapshot:2	Epoch:4	Loss:115.513	translation_Loss:115.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.51                                                   	MRR:4.51	Hits@10:9.49	Best:4.51
2024-12-28 01:58:09,406: Snapshot:2	Epoch:5	Loss:111.46	translation_Loss:110.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.699                                                   	MRR:5.23	Hits@10:11.36	Best:5.23
2024-12-28 01:58:20,090: Snapshot:2	Epoch:6	Loss:107.47	translation_Loss:106.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.904                                                   	MRR:5.92	Hits@10:12.98	Best:5.92
2024-12-28 01:58:30,800: Snapshot:2	Epoch:7	Loss:103.615	translation_Loss:102.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.12                                                   	MRR:6.52	Hits@10:14.5	Best:6.52
2024-12-28 01:58:41,463: Snapshot:2	Epoch:8	Loss:99.889	translation_Loss:98.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:7.03	Hits@10:15.67	Best:7.03
2024-12-28 01:58:52,220: Snapshot:2	Epoch:9	Loss:96.311	translation_Loss:94.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.572                                                   	MRR:7.5	Hits@10:16.91	Best:7.5
2024-12-28 01:59:03,034: Snapshot:2	Epoch:10	Loss:92.883	translation_Loss:91.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.802                                                   	MRR:7.96	Hits@10:17.87	Best:7.96
2024-12-28 01:59:13,759: Snapshot:2	Epoch:11	Loss:89.559	translation_Loss:87.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.033                                                   	MRR:8.4	Hits@10:18.77	Best:8.4
2024-12-28 01:59:24,576: Snapshot:2	Epoch:12	Loss:86.48	translation_Loss:84.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.262                                                   	MRR:8.84	Hits@10:19.62	Best:8.84
2024-12-28 01:59:35,301: Snapshot:2	Epoch:13	Loss:83.436	translation_Loss:80.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.488                                                   	MRR:9.29	Hits@10:20.51	Best:9.29
2024-12-28 01:59:46,026: Snapshot:2	Epoch:14	Loss:80.474	translation_Loss:77.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.708                                                   	MRR:9.72	Hits@10:21.44	Best:9.72
2024-12-28 01:59:56,689: Snapshot:2	Epoch:15	Loss:77.728	translation_Loss:74.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.922                                                   	MRR:10.23	Hits@10:22.32	Best:10.23
2024-12-28 02:00:07,487: Snapshot:2	Epoch:16	Loss:75.132	translation_Loss:72.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.129                                                   	MRR:10.71	Hits@10:23.18	Best:10.71
2024-12-28 02:00:18,215: Snapshot:2	Epoch:17	Loss:72.682	translation_Loss:69.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.329                                                   	MRR:11.16	Hits@10:23.93	Best:11.16
2024-12-28 02:00:28,912: Snapshot:2	Epoch:18	Loss:70.22	translation_Loss:66.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.519                                                   	MRR:11.61	Hits@10:24.66	Best:11.61
2024-12-28 02:00:39,607: Snapshot:2	Epoch:19	Loss:68.028	translation_Loss:64.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.702                                                   	MRR:12.05	Hits@10:25.36	Best:12.05
2024-12-28 02:00:50,243: Snapshot:2	Epoch:20	Loss:65.876	translation_Loss:62.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.876                                                   	MRR:12.49	Hits@10:25.95	Best:12.49
2024-12-28 02:01:01,185: Snapshot:2	Epoch:21	Loss:63.864	translation_Loss:59.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.04                                                   	MRR:12.89	Hits@10:26.56	Best:12.89
2024-12-28 02:01:11,970: Snapshot:2	Epoch:22	Loss:61.95	translation_Loss:57.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.196                                                   	MRR:13.26	Hits@10:27.13	Best:13.26
2024-12-28 02:01:22,735: Snapshot:2	Epoch:23	Loss:60.189	translation_Loss:55.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.342                                                   	MRR:13.64	Hits@10:27.66	Best:13.64
2024-12-28 02:01:33,550: Snapshot:2	Epoch:24	Loss:58.517	translation_Loss:54.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.48                                                   	MRR:13.94	Hits@10:28.2	Best:13.94
2024-12-28 02:01:44,306: Snapshot:2	Epoch:25	Loss:56.817	translation_Loss:52.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.609                                                   	MRR:14.26	Hits@10:28.66	Best:14.26
2024-12-28 02:01:54,964: Snapshot:2	Epoch:26	Loss:55.392	translation_Loss:50.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.73                                                   	MRR:14.56	Hits@10:29.19	Best:14.56
2024-12-28 02:02:05,660: Snapshot:2	Epoch:27	Loss:53.838	translation_Loss:48.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.844                                                   	MRR:14.84	Hits@10:29.59	Best:14.84
2024-12-28 02:02:16,368: Snapshot:2	Epoch:28	Loss:52.472	translation_Loss:47.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.95                                                   	MRR:15.06	Hits@10:30.0	Best:15.06
2024-12-28 02:02:27,060: Snapshot:2	Epoch:29	Loss:51.184	translation_Loss:46.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.049                                                   	MRR:15.31	Hits@10:30.29	Best:15.31
2024-12-28 02:02:37,878: Snapshot:2	Epoch:30	Loss:49.934	translation_Loss:44.791	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.142                                                   	MRR:15.54	Hits@10:30.69	Best:15.54
2024-12-28 02:02:48,724: Snapshot:2	Epoch:31	Loss:48.795	translation_Loss:43.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.229                                                   	MRR:15.75	Hits@10:31.13	Best:15.75
2024-12-28 02:02:59,442: Snapshot:2	Epoch:32	Loss:47.606	translation_Loss:42.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.311                                                   	MRR:15.96	Hits@10:31.49	Best:15.96
2024-12-28 02:03:10,133: Snapshot:2	Epoch:33	Loss:46.624	translation_Loss:41.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.386                                                   	MRR:16.16	Hits@10:31.84	Best:16.16
2024-12-28 02:03:20,940: Snapshot:2	Epoch:34	Loss:45.583	translation_Loss:40.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.456                                                   	MRR:16.33	Hits@10:32.14	Best:16.33
2024-12-28 02:03:31,650: Snapshot:2	Epoch:35	Loss:44.577	translation_Loss:39.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.521                                                   	MRR:16.5	Hits@10:32.45	Best:16.5
2024-12-28 02:03:42,429: Snapshot:2	Epoch:36	Loss:43.711	translation_Loss:38.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.581                                                   	MRR:16.65	Hits@10:32.77	Best:16.65
2024-12-28 02:03:53,076: Snapshot:2	Epoch:37	Loss:42.846	translation_Loss:37.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.638                                                   	MRR:16.8	Hits@10:33.05	Best:16.8
2024-12-28 02:04:04,311: Snapshot:2	Epoch:38	Loss:41.923	translation_Loss:36.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.69                                                   	MRR:16.94	Hits@10:33.35	Best:16.94
2024-12-28 02:04:15,103: Snapshot:2	Epoch:39	Loss:41.19	translation_Loss:35.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.737                                                   	MRR:17.09	Hits@10:33.53	Best:17.09
2024-12-28 02:04:25,786: Snapshot:2	Epoch:40	Loss:40.481	translation_Loss:34.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.781                                                   	MRR:17.25	Hits@10:33.76	Best:17.25
2024-12-28 02:04:36,498: Snapshot:2	Epoch:41	Loss:39.646	translation_Loss:33.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.822                                                   	MRR:17.38	Hits@10:33.96	Best:17.38
2024-12-28 02:04:47,160: Snapshot:2	Epoch:42	Loss:38.965	translation_Loss:33.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.86                                                   	MRR:17.49	Hits@10:34.19	Best:17.49
2024-12-28 02:04:57,916: Snapshot:2	Epoch:43	Loss:38.262	translation_Loss:32.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.895                                                   	MRR:17.63	Hits@10:34.44	Best:17.63
2024-12-28 02:05:08,769: Snapshot:2	Epoch:44	Loss:37.658	translation_Loss:31.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.926                                                   	MRR:17.74	Hits@10:34.63	Best:17.74
2024-12-28 02:05:19,640: Snapshot:2	Epoch:45	Loss:36.99	translation_Loss:31.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.953                                                   	MRR:17.83	Hits@10:34.88	Best:17.83
2024-12-28 02:05:30,564: Snapshot:2	Epoch:46	Loss:36.456	translation_Loss:30.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.98                                                   	MRR:17.99	Hits@10:34.99	Best:17.99
2024-12-28 02:05:41,465: Snapshot:2	Epoch:47	Loss:35.9	translation_Loss:29.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.002                                                   	MRR:18.1	Hits@10:35.15	Best:18.1
2024-12-28 02:05:52,247: Snapshot:2	Epoch:48	Loss:35.317	translation_Loss:29.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.022                                                   	MRR:18.17	Hits@10:35.34	Best:18.17
2024-12-28 02:06:03,052: Snapshot:2	Epoch:49	Loss:34.84	translation_Loss:28.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.041                                                   	MRR:18.31	Hits@10:35.53	Best:18.31
2024-12-28 02:06:14,220: Snapshot:2	Epoch:50	Loss:34.333	translation_Loss:28.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.057                                                   	MRR:18.39	Hits@10:35.67	Best:18.39
2024-12-28 02:06:25,056: Snapshot:2	Epoch:51	Loss:33.852	translation_Loss:27.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.073                                                   	MRR:18.48	Hits@10:35.78	Best:18.48
2024-12-28 02:06:35,714: Snapshot:2	Epoch:52	Loss:33.328	translation_Loss:27.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.087                                                   	MRR:18.58	Hits@10:35.83	Best:18.58
2024-12-28 02:06:46,422: Snapshot:2	Epoch:53	Loss:32.923	translation_Loss:26.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.099                                                   	MRR:18.66	Hits@10:35.96	Best:18.66
2024-12-28 02:06:57,183: Snapshot:2	Epoch:54	Loss:32.475	translation_Loss:26.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.107                                                   	MRR:18.73	Hits@10:36.14	Best:18.73
2024-12-28 02:07:07,901: Snapshot:2	Epoch:55	Loss:32.086	translation_Loss:25.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.116                                                   	MRR:18.86	Hits@10:36.27	Best:18.86
2024-12-28 02:07:18,634: Snapshot:2	Epoch:56	Loss:31.743	translation_Loss:25.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.121                                                   	MRR:18.96	Hits@10:36.37	Best:18.96
2024-12-28 02:07:29,510: Snapshot:2	Epoch:57	Loss:31.311	translation_Loss:25.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.127                                                   	MRR:19.04	Hits@10:36.46	Best:19.04
2024-12-28 02:07:40,220: Snapshot:2	Epoch:58	Loss:30.976	translation_Loss:24.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.132                                                   	MRR:19.1	Hits@10:36.57	Best:19.1
2024-12-28 02:07:50,973: Snapshot:2	Epoch:59	Loss:30.616	translation_Loss:24.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.137                                                   	MRR:19.2	Hits@10:36.61	Best:19.2
2024-12-28 02:08:01,757: Snapshot:2	Epoch:60	Loss:30.225	translation_Loss:24.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.139                                                   	MRR:19.25	Hits@10:36.69	Best:19.25
2024-12-28 02:08:12,566: Snapshot:2	Epoch:61	Loss:29.943	translation_Loss:23.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.143                                                   	MRR:19.34	Hits@10:36.84	Best:19.34
2024-12-28 02:08:23,855: Snapshot:2	Epoch:62	Loss:29.686	translation_Loss:23.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.145                                                   	MRR:19.43	Hits@10:36.94	Best:19.43
2024-12-28 02:08:34,699: Snapshot:2	Epoch:63	Loss:29.315	translation_Loss:23.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.147                                                   	MRR:19.5	Hits@10:37.05	Best:19.5
2024-12-28 02:08:45,443: Snapshot:2	Epoch:64	Loss:29.041	translation_Loss:22.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.147                                                   	MRR:19.54	Hits@10:37.11	Best:19.54
2024-12-28 02:08:56,138: Snapshot:2	Epoch:65	Loss:28.821	translation_Loss:22.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.147                                                   	MRR:19.59	Hits@10:37.1	Best:19.59
2024-12-28 02:09:06,993: Snapshot:2	Epoch:66	Loss:28.512	translation_Loss:22.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.146                                                   	MRR:19.65	Hits@10:37.19	Best:19.65
2024-12-28 02:09:17,909: Snapshot:2	Epoch:67	Loss:28.182	translation_Loss:22.036	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.146                                                   	MRR:19.71	Hits@10:37.24	Best:19.71
2024-12-28 02:09:28,779: Snapshot:2	Epoch:68	Loss:28.027	translation_Loss:21.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.144                                                   	MRR:19.78	Hits@10:37.3	Best:19.78
2024-12-28 02:09:39,464: Snapshot:2	Epoch:69	Loss:27.775	translation_Loss:21.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.143                                                   	MRR:19.83	Hits@10:37.42	Best:19.83
2024-12-28 02:09:50,149: Snapshot:2	Epoch:70	Loss:27.407	translation_Loss:21.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.14                                                   	MRR:19.87	Hits@10:37.49	Best:19.87
2024-12-28 02:10:00,885: Snapshot:2	Epoch:71	Loss:27.277	translation_Loss:21.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.14                                                   	MRR:19.93	Hits@10:37.56	Best:19.93
2024-12-28 02:10:11,718: Snapshot:2	Epoch:72	Loss:27.009	translation_Loss:20.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.136                                                   	MRR:19.96	Hits@10:37.59	Best:19.96
2024-12-28 02:10:22,910: Snapshot:2	Epoch:73	Loss:26.755	translation_Loss:20.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.134                                                   	MRR:19.97	Hits@10:37.65	Best:19.97
2024-12-28 02:10:33,759: Snapshot:2	Epoch:74	Loss:26.54	translation_Loss:20.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.132                                                   	MRR:20.02	Hits@10:37.65	Best:20.02
2024-12-28 02:10:44,483: Snapshot:2	Epoch:75	Loss:26.324	translation_Loss:20.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.126                                                   	MRR:20.08	Hits@10:37.74	Best:20.08
2024-12-28 02:10:55,158: Snapshot:2	Epoch:76	Loss:26.201	translation_Loss:20.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.122                                                   	MRR:20.13	Hits@10:37.81	Best:20.13
2024-12-28 02:11:05,971: Snapshot:2	Epoch:77	Loss:25.956	translation_Loss:19.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.118                                                   	MRR:20.18	Hits@10:37.81	Best:20.18
2024-12-28 02:11:16,670: Snapshot:2	Epoch:78	Loss:25.75	translation_Loss:19.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.116                                                   	MRR:20.2	Hits@10:37.86	Best:20.2
2024-12-28 02:11:27,441: Snapshot:2	Epoch:79	Loss:25.569	translation_Loss:19.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.114                                                   	MRR:20.27	Hits@10:37.93	Best:20.27
2024-12-28 02:11:38,250: Snapshot:2	Epoch:80	Loss:25.442	translation_Loss:19.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.109                                                   	MRR:20.28	Hits@10:38.04	Best:20.28
2024-12-28 02:11:49,041: Snapshot:2	Epoch:81	Loss:25.227	translation_Loss:19.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.104                                                   	MRR:20.3	Hits@10:38.05	Best:20.3
2024-12-28 02:11:59,766: Snapshot:2	Epoch:82	Loss:25.059	translation_Loss:18.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.098                                                   	MRR:20.34	Hits@10:38.08	Best:20.34
2024-12-28 02:12:10,466: Snapshot:2	Epoch:83	Loss:24.92	translation_Loss:18.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.092                                                   	MRR:20.38	Hits@10:38.1	Best:20.38
2024-12-28 02:12:21,311: Snapshot:2	Epoch:84	Loss:24.826	translation_Loss:18.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.088                                                   	MRR:20.44	Hits@10:38.09	Best:20.44
2024-12-28 02:12:32,497: Snapshot:2	Epoch:85	Loss:24.626	translation_Loss:18.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.082                                                   	MRR:20.48	Hits@10:38.05	Best:20.48
2024-12-28 02:12:43,222: Snapshot:2	Epoch:86	Loss:24.504	translation_Loss:18.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.077                                                   	MRR:20.5	Hits@10:38.06	Best:20.5
2024-12-28 02:12:54,014: Snapshot:2	Epoch:87	Loss:24.333	translation_Loss:18.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.072                                                   	MRR:20.52	Hits@10:38.09	Best:20.52
2024-12-28 02:13:04,820: Snapshot:2	Epoch:88	Loss:24.178	translation_Loss:18.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.066                                                   	MRR:20.53	Hits@10:38.15	Best:20.53
2024-12-28 02:13:15,427: Snapshot:2	Epoch:89	Loss:24.122	translation_Loss:18.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.062                                                   	MRR:20.53	Hits@10:38.18	Best:20.53
2024-12-28 02:13:26,243: Snapshot:2	Epoch:90	Loss:23.925	translation_Loss:17.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.056                                                   	MRR:20.56	Hits@10:38.23	Best:20.56
2024-12-28 02:13:36,947: Snapshot:2	Epoch:91	Loss:23.757	translation_Loss:17.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.053                                                   	MRR:20.58	Hits@10:38.26	Best:20.58
2024-12-28 02:13:47,605: Snapshot:2	Epoch:92	Loss:23.672	translation_Loss:17.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.048                                                   	MRR:20.61	Hits@10:38.3	Best:20.61
2024-12-28 02:13:58,269: Snapshot:2	Epoch:93	Loss:23.57	translation_Loss:17.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.042                                                   	MRR:20.64	Hits@10:38.36	Best:20.64
2024-12-28 02:14:09,012: Snapshot:2	Epoch:94	Loss:23.394	translation_Loss:17.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.035                                                   	MRR:20.66	Hits@10:38.4	Best:20.66
2024-12-28 02:14:19,852: Snapshot:2	Epoch:95	Loss:23.301	translation_Loss:17.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.029                                                   	MRR:20.68	Hits@10:38.38	Best:20.68
2024-12-28 02:14:30,544: Snapshot:2	Epoch:96	Loss:23.273	translation_Loss:17.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.021                                                   	MRR:20.69	Hits@10:38.37	Best:20.69
2024-12-28 02:14:41,599: Snapshot:2	Epoch:97	Loss:23.095	translation_Loss:17.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.014                                                   	MRR:20.69	Hits@10:38.35	Best:20.69
2024-12-28 02:14:52,326: Snapshot:2	Epoch:98	Loss:22.899	translation_Loss:16.89	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.009                                                   	MRR:20.69	Hits@10:38.35	Best:20.69
2024-12-28 02:15:03,107: Snapshot:2	Epoch:99	Loss:22.893	translation_Loss:16.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.005                                                   	MRR:20.7	Hits@10:38.34	Best:20.7
2024-12-28 02:15:13,899: Snapshot:2	Epoch:100	Loss:22.807	translation_Loss:16.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.998                                                   	MRR:20.75	Hits@10:38.36	Best:20.75
2024-12-28 02:15:24,677: Snapshot:2	Epoch:101	Loss:22.715	translation_Loss:16.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.993                                                   	MRR:20.74	Hits@10:38.39	Best:20.75
2024-12-28 02:15:35,491: Snapshot:2	Epoch:102	Loss:22.579	translation_Loss:16.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.987                                                   	MRR:20.78	Hits@10:38.38	Best:20.78
2024-12-28 02:15:46,166: Snapshot:2	Epoch:103	Loss:22.456	translation_Loss:16.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.982                                                   	MRR:20.78	Hits@10:38.38	Best:20.78
2024-12-28 02:15:56,892: Snapshot:2	Epoch:104	Loss:22.367	translation_Loss:16.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.976                                                   	MRR:20.8	Hits@10:38.37	Best:20.8
2024-12-28 02:16:07,631: Snapshot:2	Epoch:105	Loss:22.265	translation_Loss:16.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.971                                                   	MRR:20.82	Hits@10:38.43	Best:20.82
2024-12-28 02:16:18,312: Snapshot:2	Epoch:106	Loss:22.197	translation_Loss:16.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.964                                                   	MRR:20.86	Hits@10:38.39	Best:20.86
2024-12-28 02:16:29,116: Snapshot:2	Epoch:107	Loss:22.082	translation_Loss:16.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.959                                                   	MRR:20.82	Hits@10:38.47	Best:20.86
2024-12-28 02:16:39,934: Snapshot:2	Epoch:108	Loss:22.015	translation_Loss:16.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.954                                                   	MRR:20.84	Hits@10:38.45	Best:20.86
2024-12-28 02:16:50,999: Snapshot:2	Epoch:109	Loss:21.952	translation_Loss:16.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.949                                                   	MRR:20.84	Hits@10:38.47	Best:20.86
2024-12-28 02:17:01,741: Snapshot:2	Epoch:110	Loss:21.82	translation_Loss:15.878	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.942                                                   	MRR:20.85	Hits@10:38.49	Best:20.86
2024-12-28 02:17:12,329: Early Stopping! Snapshot: 2 Epoch: 111 Best Results: 20.86
2024-12-28 02:17:12,329: Start to training tokens! Snapshot: 2 Epoch: 111 Loss:21.781 MRR:20.84 Best Results: 20.86
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:17:12,329: Snapshot:2	Epoch:111	Loss:21.781	translation_Loss:15.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.934                                                   	MRR:20.84	Hits@10:38.52	Best:20.86
2024-12-28 02:17:22,760: Snapshot:2	Epoch:112	Loss:111.211	translation_Loss:107.531	multi_layer_Loss:3.681	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.84	Hits@10:38.52	Best:20.86
2024-12-28 02:17:33,088: End of token training: 2 Epoch: 113 Loss:110.441 MRR:20.84 Best Results: 20.86
2024-12-28 02:17:33,089: Snapshot:2	Epoch:113	Loss:110.441	translation_Loss:107.618	multi_layer_Loss:2.823	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.84	Hits@10:38.52	Best:20.86
2024-12-28 02:17:33,362: => loading checkpoint './checkpoint/HYBRIDHYBRID_0.00001_512_5000/2model_best.tar'
2024-12-28 02:17:41,682: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1299 | 0.3009 | 0.3664 |  0.4422 |
|     1      | 0.2474 | 0.1351 | 0.3062 | 0.3708 |  0.4528 |
|     2      | 0.2092 | 0.117  | 0.2461 | 0.3085 |  0.3875 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 02:18:23,065: Snapshot:3	Epoch:0	Loss:142.145	translation_Loss:142.104	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.042                                                   	MRR:3.78	Hits@10:9.87	Best:3.78
2024-12-28 02:18:36,220: Snapshot:3	Epoch:1	Loss:135.002	translation_Loss:134.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.15                                                   	MRR:4.35	Hits@10:11.24	Best:4.35
2024-12-28 02:18:49,452: Snapshot:3	Epoch:2	Loss:127.904	translation_Loss:127.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.319                                                   	MRR:5.03	Hits@10:12.73	Best:5.03
2024-12-28 02:19:02,490: Snapshot:3	Epoch:3	Loss:121.042	translation_Loss:120.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.545                                                   	MRR:5.61	Hits@10:13.96	Best:5.61
2024-12-28 02:19:15,700: Snapshot:3	Epoch:4	Loss:114.298	translation_Loss:113.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.818                                                   	MRR:6.07	Hits@10:14.95	Best:6.07
2024-12-28 02:19:28,835: Snapshot:3	Epoch:5	Loss:107.823	translation_Loss:106.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.128                                                   	MRR:6.59	Hits@10:16.33	Best:6.59
2024-12-28 02:19:42,005: Snapshot:3	Epoch:6	Loss:101.644	translation_Loss:100.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.465                                                   	MRR:7.31	Hits@10:18.23	Best:7.31
2024-12-28 02:19:55,132: Snapshot:3	Epoch:7	Loss:95.897	translation_Loss:94.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.819                                                   	MRR:8.0	Hits@10:19.62	Best:8.0
2024-12-28 02:20:08,390: Snapshot:3	Epoch:8	Loss:90.634	translation_Loss:88.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.182                                                   	MRR:8.58	Hits@10:20.59	Best:8.58
2024-12-28 02:20:21,514: Snapshot:3	Epoch:9	Loss:85.734	translation_Loss:83.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.55                                                   	MRR:9.12	Hits@10:21.48	Best:9.12
2024-12-28 02:20:34,573: Snapshot:3	Epoch:10	Loss:81.312	translation_Loss:78.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.915                                                   	MRR:9.69	Hits@10:22.52	Best:9.69
2024-12-28 02:20:47,656: Snapshot:3	Epoch:11	Loss:77.263	translation_Loss:73.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.274                                                   	MRR:10.23	Hits@10:23.35	Best:10.23
2024-12-28 02:21:01,305: Snapshot:3	Epoch:12	Loss:73.644	translation_Loss:70.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.622                                                   	MRR:10.7	Hits@10:24.17	Best:10.7
2024-12-28 02:21:14,349: Snapshot:3	Epoch:13	Loss:70.447	translation_Loss:66.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.957                                                   	MRR:11.14	Hits@10:25.0	Best:11.14
2024-12-28 02:21:27,638: Snapshot:3	Epoch:14	Loss:67.409	translation_Loss:63.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.275                                                   	MRR:11.58	Hits@10:25.78	Best:11.58
2024-12-28 02:21:40,737: Snapshot:3	Epoch:15	Loss:64.621	translation_Loss:60.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.578                                                   	MRR:12.01	Hits@10:26.49	Best:12.01
2024-12-28 02:21:54,104: Snapshot:3	Epoch:16	Loss:62.052	translation_Loss:57.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.863                                                   	MRR:12.43	Hits@10:27.22	Best:12.43
2024-12-28 02:22:07,466: Snapshot:3	Epoch:17	Loss:59.772	translation_Loss:54.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.131                                                   	MRR:12.86	Hits@10:27.96	Best:12.86
2024-12-28 02:22:20,555: Snapshot:3	Epoch:18	Loss:57.639	translation_Loss:52.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.383                                                   	MRR:13.23	Hits@10:28.58	Best:13.23
2024-12-28 02:22:33,728: Snapshot:3	Epoch:19	Loss:55.742	translation_Loss:50.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.617                                                   	MRR:13.6	Hits@10:29.24	Best:13.6
2024-12-28 02:22:46,829: Snapshot:3	Epoch:20	Loss:53.897	translation_Loss:48.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.837                                                   	MRR:13.93	Hits@10:29.86	Best:13.93
2024-12-28 02:23:00,061: Snapshot:3	Epoch:21	Loss:52.245	translation_Loss:46.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.042                                                   	MRR:14.28	Hits@10:30.48	Best:14.28
2024-12-28 02:23:13,637: Snapshot:3	Epoch:22	Loss:50.668	translation_Loss:44.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.23                                                   	MRR:14.64	Hits@10:31.03	Best:14.64
2024-12-28 02:23:26,769: Snapshot:3	Epoch:23	Loss:49.251	translation_Loss:42.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.406                                                   	MRR:14.94	Hits@10:31.59	Best:14.94
2024-12-28 02:23:40,051: Snapshot:3	Epoch:24	Loss:47.884	translation_Loss:41.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.568                                                   	MRR:15.24	Hits@10:32.04	Best:15.24
2024-12-28 02:23:53,219: Snapshot:3	Epoch:25	Loss:46.691	translation_Loss:39.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.718                                                   	MRR:15.52	Hits@10:32.49	Best:15.52
2024-12-28 02:24:06,413: Snapshot:3	Epoch:26	Loss:45.528	translation_Loss:38.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.857                                                   	MRR:15.73	Hits@10:32.89	Best:15.73
2024-12-28 02:24:19,723: Snapshot:3	Epoch:27	Loss:44.391	translation_Loss:37.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.985                                                   	MRR:15.99	Hits@10:33.37	Best:15.99
2024-12-28 02:24:32,897: Snapshot:3	Epoch:28	Loss:43.378	translation_Loss:36.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.104                                                   	MRR:16.23	Hits@10:33.75	Best:16.23
2024-12-28 02:24:46,158: Snapshot:3	Epoch:29	Loss:42.424	translation_Loss:35.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.212                                                   	MRR:16.44	Hits@10:34.11	Best:16.44
2024-12-28 02:24:59,335: Snapshot:3	Epoch:30	Loss:41.407	translation_Loss:34.094	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.312                                                   	MRR:16.65	Hits@10:34.49	Best:16.65
2024-12-28 02:25:12,576: Snapshot:3	Epoch:31	Loss:40.579	translation_Loss:33.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.406                                                   	MRR:16.84	Hits@10:34.8	Best:16.84
2024-12-28 02:25:26,242: Snapshot:3	Epoch:32	Loss:39.744	translation_Loss:32.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.491                                                   	MRR:17.0	Hits@10:35.07	Best:17.0
2024-12-28 02:25:39,345: Snapshot:3	Epoch:33	Loss:38.963	translation_Loss:31.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.567                                                   	MRR:17.19	Hits@10:35.3	Best:17.19
2024-12-28 02:25:52,690: Snapshot:3	Epoch:34	Loss:38.194	translation_Loss:30.555	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.639                                                   	MRR:17.38	Hits@10:35.61	Best:17.38
2024-12-28 02:26:05,943: Snapshot:3	Epoch:35	Loss:37.453	translation_Loss:29.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.704                                                   	MRR:17.56	Hits@10:35.88	Best:17.56
2024-12-28 02:26:19,092: Snapshot:3	Epoch:36	Loss:36.824	translation_Loss:29.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.765                                                   	MRR:17.76	Hits@10:36.03	Best:17.76
2024-12-28 02:26:32,237: Snapshot:3	Epoch:37	Loss:36.152	translation_Loss:28.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.818                                                   	MRR:17.89	Hits@10:36.23	Best:17.89
2024-12-28 02:26:45,358: Snapshot:3	Epoch:38	Loss:35.509	translation_Loss:27.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.866                                                   	MRR:18.04	Hits@10:36.42	Best:18.04
2024-12-28 02:26:58,633: Snapshot:3	Epoch:39	Loss:34.93	translation_Loss:27.021	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.909                                                   	MRR:18.23	Hits@10:36.58	Best:18.23
2024-12-28 02:27:11,794: Snapshot:3	Epoch:40	Loss:34.354	translation_Loss:26.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.949                                                   	MRR:18.39	Hits@10:36.74	Best:18.39
2024-12-28 02:27:24,955: Snapshot:3	Epoch:41	Loss:33.834	translation_Loss:25.849	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.985                                                   	MRR:18.57	Hits@10:36.94	Best:18.57
2024-12-28 02:27:38,572: Snapshot:3	Epoch:42	Loss:33.304	translation_Loss:25.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.016                                                   	MRR:18.69	Hits@10:37.12	Best:18.69
2024-12-28 02:27:51,887: Snapshot:3	Epoch:43	Loss:32.852	translation_Loss:24.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.04                                                   	MRR:18.79	Hits@10:37.22	Best:18.79
2024-12-28 02:28:05,041: Snapshot:3	Epoch:44	Loss:32.392	translation_Loss:24.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.063                                                   	MRR:18.89	Hits@10:37.4	Best:18.89
2024-12-28 02:28:18,326: Snapshot:3	Epoch:45	Loss:31.942	translation_Loss:23.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.084                                                   	MRR:18.99	Hits@10:37.51	Best:18.99
2024-12-28 02:28:31,687: Snapshot:3	Epoch:46	Loss:31.549	translation_Loss:23.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.103                                                   	MRR:19.08	Hits@10:37.56	Best:19.08
2024-12-28 02:28:44,820: Snapshot:3	Epoch:47	Loss:31.204	translation_Loss:23.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.119                                                   	MRR:19.17	Hits@10:37.72	Best:19.17
2024-12-28 02:28:58,117: Snapshot:3	Epoch:48	Loss:30.784	translation_Loss:22.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.13                                                   	MRR:19.24	Hits@10:37.83	Best:19.24
2024-12-28 02:29:11,227: Snapshot:3	Epoch:49	Loss:30.457	translation_Loss:22.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.14                                                   	MRR:19.32	Hits@10:37.84	Best:19.32
2024-12-28 02:29:24,421: Snapshot:3	Epoch:50	Loss:30.075	translation_Loss:21.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.149                                                   	MRR:19.39	Hits@10:37.95	Best:19.39
2024-12-28 02:29:37,676: Snapshot:3	Epoch:51	Loss:29.811	translation_Loss:21.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.155                                                   	MRR:19.44	Hits@10:38.02	Best:19.44
2024-12-28 02:29:51,214: Snapshot:3	Epoch:52	Loss:29.473	translation_Loss:21.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.16                                                   	MRR:19.51	Hits@10:38.09	Best:19.51
2024-12-28 02:30:04,439: Snapshot:3	Epoch:53	Loss:29.103	translation_Loss:20.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.163                                                   	MRR:19.6	Hits@10:38.24	Best:19.6
2024-12-28 02:30:17,677: Snapshot:3	Epoch:54	Loss:28.896	translation_Loss:20.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.16                                                   	MRR:19.64	Hits@10:38.36	Best:19.64
2024-12-28 02:30:30,827: Snapshot:3	Epoch:55	Loss:28.65	translation_Loss:20.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.159                                                   	MRR:19.73	Hits@10:38.46	Best:19.73
2024-12-28 02:30:44,005: Snapshot:3	Epoch:56	Loss:28.3	translation_Loss:20.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.158                                                   	MRR:19.77	Hits@10:38.56	Best:19.77
2024-12-28 02:30:57,116: Snapshot:3	Epoch:57	Loss:28.017	translation_Loss:19.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.155                                                   	MRR:19.84	Hits@10:38.65	Best:19.84
2024-12-28 02:31:10,422: Snapshot:3	Epoch:58	Loss:27.839	translation_Loss:19.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.15                                                   	MRR:19.88	Hits@10:38.69	Best:19.88
2024-12-28 02:31:23,659: Snapshot:3	Epoch:59	Loss:27.669	translation_Loss:19.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.146                                                   	MRR:19.93	Hits@10:38.77	Best:19.93
2024-12-28 02:31:36,899: Snapshot:3	Epoch:60	Loss:27.387	translation_Loss:19.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.139                                                   	MRR:19.97	Hits@10:38.88	Best:19.97
2024-12-28 02:31:49,955: Snapshot:3	Epoch:61	Loss:27.151	translation_Loss:19.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.132                                                   	MRR:20.03	Hits@10:38.97	Best:20.03
2024-12-28 02:32:03,043: Snapshot:3	Epoch:62	Loss:26.913	translation_Loss:18.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.125                                                   	MRR:20.07	Hits@10:39.02	Best:20.07
2024-12-28 02:32:16,802: Snapshot:3	Epoch:63	Loss:26.723	translation_Loss:18.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.118                                                   	MRR:20.13	Hits@10:39.05	Best:20.13
2024-12-28 02:32:29,994: Snapshot:3	Epoch:64	Loss:26.561	translation_Loss:18.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.109                                                   	MRR:20.18	Hits@10:39.04	Best:20.18
2024-12-28 02:32:43,126: Snapshot:3	Epoch:65	Loss:26.334	translation_Loss:18.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.1                                                   	MRR:20.21	Hits@10:39.15	Best:20.21
2024-12-28 02:32:56,285: Snapshot:3	Epoch:66	Loss:26.126	translation_Loss:18.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.088                                                   	MRR:20.26	Hits@10:39.19	Best:20.26
2024-12-28 02:33:09,501: Snapshot:3	Epoch:67	Loss:25.961	translation_Loss:17.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.074                                                   	MRR:20.3	Hits@10:39.19	Best:20.3
2024-12-28 02:33:22,664: Snapshot:3	Epoch:68	Loss:25.829	translation_Loss:17.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.062                                                   	MRR:20.37	Hits@10:39.2	Best:20.37
2024-12-28 02:33:35,890: Snapshot:3	Epoch:69	Loss:25.642	translation_Loss:17.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.05                                                   	MRR:20.39	Hits@10:39.23	Best:20.39
2024-12-28 02:33:48,934: Snapshot:3	Epoch:70	Loss:25.412	translation_Loss:17.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.039                                                   	MRR:20.43	Hits@10:39.34	Best:20.43
2024-12-28 02:34:02,147: Snapshot:3	Epoch:71	Loss:25.336	translation_Loss:17.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.027                                                   	MRR:20.47	Hits@10:39.35	Best:20.47
2024-12-28 02:34:15,221: Snapshot:3	Epoch:72	Loss:25.172	translation_Loss:17.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.015                                                   	MRR:20.51	Hits@10:39.41	Best:20.51
2024-12-28 02:34:28,766: Snapshot:3	Epoch:73	Loss:25.05	translation_Loss:17.049	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.002                                                   	MRR:20.53	Hits@10:39.37	Best:20.53
2024-12-28 02:34:42,123: Snapshot:3	Epoch:74	Loss:24.849	translation_Loss:16.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.99                                                   	MRR:20.57	Hits@10:39.38	Best:20.57
2024-12-28 02:34:55,158: Snapshot:3	Epoch:75	Loss:24.756	translation_Loss:16.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.979                                                   	MRR:20.56	Hits@10:39.38	Best:20.57
2024-12-28 02:35:08,449: Snapshot:3	Epoch:76	Loss:24.569	translation_Loss:16.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.968                                                   	MRR:20.6	Hits@10:39.42	Best:20.6
2024-12-28 02:35:21,558: Snapshot:3	Epoch:77	Loss:24.532	translation_Loss:16.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.956                                                   	MRR:20.63	Hits@10:39.48	Best:20.63
2024-12-28 02:35:34,809: Snapshot:3	Epoch:78	Loss:24.362	translation_Loss:16.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.943                                                   	MRR:20.63	Hits@10:39.56	Best:20.63
2024-12-28 02:35:48,040: Snapshot:3	Epoch:79	Loss:24.242	translation_Loss:16.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.931                                                   	MRR:20.64	Hits@10:39.59	Best:20.64
2024-12-28 02:36:01,269: Snapshot:3	Epoch:80	Loss:24.118	translation_Loss:16.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.923                                                   	MRR:20.66	Hits@10:39.64	Best:20.66
2024-12-28 02:36:14,461: Snapshot:3	Epoch:81	Loss:24.014	translation_Loss:16.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.912                                                   	MRR:20.69	Hits@10:39.61	Best:20.69
2024-12-28 02:36:27,563: Snapshot:3	Epoch:82	Loss:23.818	translation_Loss:15.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.899                                                   	MRR:20.71	Hits@10:39.65	Best:20.71
2024-12-28 02:36:41,138: Snapshot:3	Epoch:83	Loss:23.749	translation_Loss:15.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.885                                                   	MRR:20.74	Hits@10:39.66	Best:20.74
2024-12-28 02:36:54,238: Snapshot:3	Epoch:84	Loss:23.62	translation_Loss:15.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.874                                                   	MRR:20.78	Hits@10:39.64	Best:20.78
2024-12-28 02:37:07,465: Snapshot:3	Epoch:85	Loss:23.525	translation_Loss:15.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.863                                                   	MRR:20.79	Hits@10:39.68	Best:20.79
2024-12-28 02:37:20,589: Snapshot:3	Epoch:86	Loss:23.417	translation_Loss:15.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.852                                                   	MRR:20.81	Hits@10:39.67	Best:20.81
2024-12-28 02:37:33,704: Snapshot:3	Epoch:87	Loss:23.387	translation_Loss:15.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.841                                                   	MRR:20.83	Hits@10:39.68	Best:20.83
2024-12-28 02:37:46,805: Snapshot:3	Epoch:88	Loss:23.176	translation_Loss:15.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.83                                                   	MRR:20.83	Hits@10:39.71	Best:20.83
2024-12-28 02:37:59,916: Snapshot:3	Epoch:89	Loss:23.179	translation_Loss:15.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.818                                                   	MRR:20.84	Hits@10:39.72	Best:20.84
2024-12-28 02:38:13,050: Snapshot:3	Epoch:90	Loss:23.084	translation_Loss:15.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.808                                                   	MRR:20.85	Hits@10:39.75	Best:20.85
2024-12-28 02:38:26,307: Snapshot:3	Epoch:91	Loss:22.975	translation_Loss:15.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.797                                                   	MRR:20.9	Hits@10:39.78	Best:20.9
2024-12-28 02:38:39,552: Snapshot:3	Epoch:92	Loss:22.9	translation_Loss:15.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.788                                                   	MRR:20.9	Hits@10:39.8	Best:20.9
2024-12-28 02:38:53,188: Snapshot:3	Epoch:93	Loss:22.785	translation_Loss:15.009	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.776                                                   	MRR:20.93	Hits@10:39.81	Best:20.93
2024-12-28 02:39:06,302: Snapshot:3	Epoch:94	Loss:22.747	translation_Loss:14.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.763                                                   	MRR:20.92	Hits@10:39.81	Best:20.93
2024-12-28 02:39:19,432: Snapshot:3	Epoch:95	Loss:22.638	translation_Loss:14.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.752                                                   	MRR:20.95	Hits@10:39.75	Best:20.95
2024-12-28 02:39:32,587: Snapshot:3	Epoch:96	Loss:22.572	translation_Loss:14.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.74                                                   	MRR:20.97	Hits@10:39.79	Best:20.97
2024-12-28 02:39:45,612: Snapshot:3	Epoch:97	Loss:22.499	translation_Loss:14.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.729                                                   	MRR:20.97	Hits@10:39.86	Best:20.97
2024-12-28 02:39:58,705: Snapshot:3	Epoch:98	Loss:22.385	translation_Loss:14.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.718                                                   	MRR:20.99	Hits@10:39.8	Best:20.99
2024-12-28 02:40:12,013: Snapshot:3	Epoch:99	Loss:22.336	translation_Loss:14.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.709                                                   	MRR:21.01	Hits@10:39.75	Best:21.01
2024-12-28 02:40:25,061: Snapshot:3	Epoch:100	Loss:22.28	translation_Loss:14.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.699                                                   	MRR:21.01	Hits@10:39.75	Best:21.01
2024-12-28 02:40:38,218: Snapshot:3	Epoch:101	Loss:22.23	translation_Loss:14.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.689                                                   	MRR:21.01	Hits@10:39.86	Best:21.01
2024-12-28 02:40:51,378: Snapshot:3	Epoch:102	Loss:22.124	translation_Loss:14.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.677                                                   	MRR:21.07	Hits@10:39.8	Best:21.07
2024-12-28 02:41:05,085: Snapshot:3	Epoch:103	Loss:22.034	translation_Loss:14.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.664                                                   	MRR:21.06	Hits@10:39.82	Best:21.07
2024-12-28 02:41:18,176: Snapshot:3	Epoch:104	Loss:22.044	translation_Loss:14.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.655                                                   	MRR:21.06	Hits@10:39.89	Best:21.07
2024-12-28 02:41:31,313: Snapshot:3	Epoch:105	Loss:22.027	translation_Loss:14.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.647                                                   	MRR:21.06	Hits@10:39.85	Best:21.07
2024-12-28 02:41:44,468: Snapshot:3	Epoch:106	Loss:21.888	translation_Loss:14.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.636                                                   	MRR:21.09	Hits@10:39.83	Best:21.09
2024-12-28 02:41:57,637: Snapshot:3	Epoch:107	Loss:21.754	translation_Loss:14.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.626                                                   	MRR:21.07	Hits@10:39.79	Best:21.09
2024-12-28 02:42:10,748: Snapshot:3	Epoch:108	Loss:21.73	translation_Loss:14.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.617                                                   	MRR:21.08	Hits@10:39.77	Best:21.09
2024-12-28 02:42:23,854: Snapshot:3	Epoch:109	Loss:21.696	translation_Loss:14.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.61                                                   	MRR:21.08	Hits@10:39.85	Best:21.09
2024-12-28 02:42:37,049: Snapshot:3	Epoch:110	Loss:21.59	translation_Loss:13.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.601                                                   	MRR:21.11	Hits@10:39.84	Best:21.11
2024-12-28 02:42:50,174: Snapshot:3	Epoch:111	Loss:21.659	translation_Loss:14.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.593                                                   	MRR:21.15	Hits@10:39.84	Best:21.15
2024-12-28 02:43:03,394: Snapshot:3	Epoch:112	Loss:21.552	translation_Loss:13.967	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.585                                                   	MRR:21.14	Hits@10:39.87	Best:21.15
2024-12-28 02:43:16,910: Snapshot:3	Epoch:113	Loss:21.506	translation_Loss:13.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.575                                                   	MRR:21.13	Hits@10:39.88	Best:21.15
2024-12-28 02:43:29,972: Snapshot:3	Epoch:114	Loss:21.429	translation_Loss:13.864	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.565                                                   	MRR:21.12	Hits@10:39.9	Best:21.15
2024-12-28 02:43:43,049: Snapshot:3	Epoch:115	Loss:21.398	translation_Loss:13.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.553                                                   	MRR:21.14	Hits@10:39.88	Best:21.15
2024-12-28 02:43:56,281: Early Stopping! Snapshot: 3 Epoch: 116 Best Results: 21.15
2024-12-28 02:43:56,281: Start to training tokens! Snapshot: 3 Epoch: 116 Loss:21.305 MRR:21.13 Best Results: 21.15
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 02:43:56,281: Snapshot:3	Epoch:116	Loss:21.305	translation_Loss:13.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.543                                                   	MRR:21.13	Hits@10:39.94	Best:21.15
2024-12-28 02:44:08,958: Snapshot:3	Epoch:117	Loss:118.405	translation_Loss:114.226	multi_layer_Loss:4.179	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.13	Hits@10:39.94	Best:21.15
