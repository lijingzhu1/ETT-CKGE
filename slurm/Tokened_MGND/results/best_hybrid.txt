2024-12-29 03:50:28,743: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229034958/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 03:50:38,915: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.0	Best:7.3
2024-12-29 03:50:44,625: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.05	Best:12.4
2024-12-29 03:50:51,072: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.69	Best:18.93
2024-12-29 03:50:57,073: Snapshot:0	Epoch:3	Loss:4.124	translation_Loss:4.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:43.52	Best:22.67
2024-12-29 03:51:03,528: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:45.39	Best:24.24
2024-12-29 03:51:09,498: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:46.01	Best:24.95
2024-12-29 03:51:15,967: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.32	Hits@10:46.18	Best:25.32
2024-12-29 03:51:22,155: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.4	Best:25.65
2024-12-29 03:51:28,648: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.42	Best:25.67
2024-12-29 03:51:34,676: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:46.36	Best:25.67
2024-12-29 03:51:41,009: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.39	Best:25.67
2024-12-29 03:51:46,958: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.31	Best:25.74
2024-12-29 03:51:53,433: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.33	Best:25.74
2024-12-29 03:51:59,049: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.14	Best:25.78
2024-12-29 03:52:06,025: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.24	Best:25.78
2024-12-29 03:52:11,791: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:45.84	Best:25.78
2024-12-29 03:52:18,561: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.13	Best:25.78
2024-12-29 03:52:24,290: Snapshot:0	Epoch:17	Loss:0.238	translation_Loss:0.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.08	Best:25.78
2024-12-29 03:52:31,092: Early Stopping! Snapshot: 0 Epoch: 18 Best Results: 25.78
2024-12-29 03:52:31,092: Start to training tokens! Snapshot: 0 Epoch: 18 Loss:0.222 MRR:25.74 Best Results: 25.78
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:52:31,093: Snapshot:0	Epoch:18	Loss:0.222	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.97	Best:25.78
2024-12-29 03:52:37,200: Snapshot:0	Epoch:19	Loss:26.527	translation_Loss:11.529	multi_layer_Loss:14.999	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.97	Best:25.78
2024-12-29 03:52:43,991: End of token training: 0 Epoch: 20 Loss:11.895 MRR:25.74 Best Results: 25.78
2024-12-29 03:52:43,992: Snapshot:0	Epoch:20	Loss:11.895	translation_Loss:11.522	multi_layer_Loss:0.373	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.74	Hits@10:45.97	Best:25.78
2024-12-29 03:52:44,224: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-29 03:52:46,713: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1499 | 0.3141 | 0.3778 |  0.4536 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:52:57,496: Snapshot:1	Epoch:0	Loss:4.914	translation_Loss:4.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.162                                                   	MRR:9.78	Hits@10:17.13	Best:9.78
2024-12-29 03:52:59,576: Snapshot:1	Epoch:1	Loss:2.934	translation_Loss:2.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.344                                                   	MRR:16.84	Hits@10:30.9	Best:16.84
2024-12-29 03:53:01,644: Snapshot:1	Epoch:2	Loss:1.852	translation_Loss:1.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.46                                                   	MRR:21.04	Hits@10:37.11	Best:21.04
2024-12-29 03:53:04,137: Snapshot:1	Epoch:3	Loss:1.299	translation_Loss:0.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.458                                                   	MRR:23.91	Hits@10:40.77	Best:23.91
2024-12-29 03:53:06,359: Snapshot:1	Epoch:4	Loss:0.989	translation_Loss:0.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:25.04	Hits@10:43.15	Best:25.04
2024-12-29 03:53:08,427: Snapshot:1	Epoch:5	Loss:0.809	translation_Loss:0.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.337                                                   	MRR:25.77	Hits@10:45.05	Best:25.77
2024-12-29 03:53:10,531: Snapshot:1	Epoch:6	Loss:0.701	translation_Loss:0.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.294                                                   	MRR:26.65	Hits@10:46.83	Best:26.65
2024-12-29 03:53:13,011: Snapshot:1	Epoch:7	Loss:0.63	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.268                                                   	MRR:27.31	Hits@10:48.04	Best:27.31
2024-12-29 03:53:15,263: Snapshot:1	Epoch:8	Loss:0.587	translation_Loss:0.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:27.97	Hits@10:48.61	Best:27.97
2024-12-29 03:53:17,734: Snapshot:1	Epoch:9	Loss:0.556	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:28.22	Hits@10:49.28	Best:28.22
2024-12-29 03:53:19,802: Snapshot:1	Epoch:10	Loss:0.522	translation_Loss:0.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:28.71	Hits@10:49.56	Best:28.71
2024-12-29 03:53:21,928: Snapshot:1	Epoch:11	Loss:0.495	translation_Loss:0.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.223                                                   	MRR:28.86	Hits@10:49.78	Best:28.86
2024-12-29 03:53:24,406: Snapshot:1	Epoch:12	Loss:0.48	translation_Loss:0.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.216                                                   	MRR:29.14	Hits@10:50.01	Best:29.14
2024-12-29 03:53:26,898: Snapshot:1	Epoch:13	Loss:0.464	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:29.43	Hits@10:50.18	Best:29.43
2024-12-29 03:53:29,164: Snapshot:1	Epoch:14	Loss:0.452	translation_Loss:0.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.206                                                   	MRR:29.57	Hits@10:50.53	Best:29.57
2024-12-29 03:53:31,598: Snapshot:1	Epoch:15	Loss:0.434	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:29.79	Hits@10:50.93	Best:29.79
2024-12-29 03:53:34,088: Snapshot:1	Epoch:16	Loss:0.429	translation_Loss:0.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:29.89	Hits@10:50.68	Best:29.89
2024-12-29 03:53:36,417: Snapshot:1	Epoch:17	Loss:0.417	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:30.1	Hits@10:51.03	Best:30.1
2024-12-29 03:53:38,521: Snapshot:1	Epoch:18	Loss:0.416	translation_Loss:0.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:29.99	Hits@10:51.22	Best:30.1
2024-12-29 03:53:41,002: Snapshot:1	Epoch:19	Loss:0.402	translation_Loss:0.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:30.09	Hits@10:51.13	Best:30.1
2024-12-29 03:53:43,439: Snapshot:1	Epoch:20	Loss:0.399	translation_Loss:0.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:30.15	Hits@10:50.89	Best:30.15
2024-12-29 03:53:45,612: Snapshot:1	Epoch:21	Loss:0.393	translation_Loss:0.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.185                                                   	MRR:30.08	Hits@10:50.86	Best:30.15
2024-12-29 03:53:48,367: Snapshot:1	Epoch:22	Loss:0.386	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:29.86	Hits@10:50.99	Best:30.15
2024-12-29 03:53:50,523: Snapshot:1	Epoch:23	Loss:0.381	translation_Loss:0.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.181                                                   	MRR:29.9	Hits@10:50.97	Best:30.15
2024-12-29 03:53:52,963: Snapshot:1	Epoch:24	Loss:0.377	translation_Loss:0.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:30.0	Hits@10:50.99	Best:30.15
2024-12-29 03:53:55,187: Early Stopping! Snapshot: 1 Epoch: 25 Best Results: 30.15
2024-12-29 03:53:55,187: Start to training tokens! Snapshot: 1 Epoch: 25 Loss:0.374 MRR:30.04 Best Results: 30.15
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:53:55,187: Snapshot:1	Epoch:25	Loss:0.374	translation_Loss:0.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:30.04	Hits@10:51.19	Best:30.15
2024-12-29 03:53:57,277: Snapshot:1	Epoch:26	Loss:16.152	translation_Loss:4.363	multi_layer_Loss:11.789	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.04	Hits@10:51.19	Best:30.15
2024-12-29 03:53:59,331: End of token training: 1 Epoch: 27 Loss:6.999 MRR:30.04 Best Results: 30.15
2024-12-29 03:53:59,331: Snapshot:1	Epoch:27	Loss:6.999	translation_Loss:4.361	multi_layer_Loss:2.638	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.04	Hits@10:51.19	Best:30.15
2024-12-29 03:53:59,534: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-29 03:54:03,219: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1496 | 0.3123 |  0.38  |  0.4556 |
|     1      | 0.2971 | 0.1907 | 0.3442 | 0.4149 |  0.5061 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:54:33,545: Snapshot:2	Epoch:0	Loss:15.993	translation_Loss:14.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.211                                                   	MRR:14.06	Hits@10:28.69	Best:14.06
2024-12-29 03:54:42,585: Snapshot:2	Epoch:1	Loss:7.411	translation_Loss:5.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.445                                                   	MRR:19.0	Hits@10:35.94	Best:19.0
2024-12-29 03:54:51,910: Snapshot:2	Epoch:2	Loss:4.895	translation_Loss:3.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.164                                                   	MRR:20.38	Hits@10:37.28	Best:20.38
2024-12-29 03:55:01,316: Snapshot:2	Epoch:3	Loss:4.05	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.013                                                   	MRR:21.0	Hits@10:37.7	Best:21.0
2024-12-29 03:55:11,859: Snapshot:2	Epoch:4	Loss:3.692	translation_Loss:2.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.949                                                   	MRR:21.19	Hits@10:38.01	Best:21.19
2024-12-29 03:55:21,381: Snapshot:2	Epoch:5	Loss:3.534	translation_Loss:2.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.921                                                   	MRR:21.28	Hits@10:38.03	Best:21.28
2024-12-29 03:55:32,202: Snapshot:2	Epoch:6	Loss:3.433	translation_Loss:2.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.909                                                   	MRR:21.33	Hits@10:38.13	Best:21.33
2024-12-29 03:55:42,633: Snapshot:2	Epoch:7	Loss:3.383	translation_Loss:2.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.9                                                   	MRR:21.39	Hits@10:38.15	Best:21.39
2024-12-29 03:55:53,339: Snapshot:2	Epoch:8	Loss:3.344	translation_Loss:2.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.896                                                   	MRR:21.33	Hits@10:38.12	Best:21.39
2024-12-29 03:56:02,752: Snapshot:2	Epoch:9	Loss:3.314	translation_Loss:2.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.895                                                   	MRR:21.44	Hits@10:38.1	Best:21.44
2024-12-29 03:56:11,713: Snapshot:2	Epoch:10	Loss:3.285	translation_Loss:2.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.52	Hits@10:38.04	Best:21.52
2024-12-29 03:56:21,005: Snapshot:2	Epoch:11	Loss:3.269	translation_Loss:2.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.53	Hits@10:38.24	Best:21.53
2024-12-29 03:56:31,875: Snapshot:2	Epoch:12	Loss:3.252	translation_Loss:2.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.895                                                   	MRR:21.32	Hits@10:38.08	Best:21.53
2024-12-29 03:56:41,022: Snapshot:2	Epoch:13	Loss:3.242	translation_Loss:2.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.896                                                   	MRR:21.37	Hits@10:38.03	Best:21.53
2024-12-29 03:56:50,284: Snapshot:2	Epoch:14	Loss:3.225	translation_Loss:2.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.89                                                   	MRR:21.21	Hits@10:37.87	Best:21.53
2024-12-29 03:57:01,087: Snapshot:2	Epoch:15	Loss:3.226	translation_Loss:2.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.37	Hits@10:38.13	Best:21.53
2024-12-29 03:57:10,446: Snapshot:2	Epoch:16	Loss:3.209	translation_Loss:2.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.55	Hits@10:38.18	Best:21.55
2024-12-29 03:57:19,324: Snapshot:2	Epoch:17	Loss:3.208	translation_Loss:2.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.29	Hits@10:37.88	Best:21.55
2024-12-29 03:57:28,584: Snapshot:2	Epoch:18	Loss:3.194	translation_Loss:2.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.892                                                   	MRR:21.24	Hits@10:38.1	Best:21.55
2024-12-29 03:57:37,888: Snapshot:2	Epoch:19	Loss:3.186	translation_Loss:2.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.2	Hits@10:38.08	Best:21.55
2024-12-29 03:57:46,969: Snapshot:2	Epoch:20	Loss:3.187	translation_Loss:2.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.38	Hits@10:38.18	Best:21.55
2024-12-29 03:57:56,197: Early Stopping! Snapshot: 2 Epoch: 21 Best Results: 21.55
2024-12-29 03:57:56,197: Start to training tokens! Snapshot: 2 Epoch: 21 Loss:3.175 MRR:21.18 Best Results: 21.55
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 03:57:56,198: Snapshot:2	Epoch:21	Loss:3.175	translation_Loss:2.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:21.18	Hits@10:38.1	Best:21.55
2024-12-29 03:58:05,438: Snapshot:2	Epoch:22	Loss:32.632	translation_Loss:18.194	multi_layer_Loss:14.438	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.18	Hits@10:38.1	Best:21.55
2024-12-29 03:58:14,368: End of token training: 2 Epoch: 23 Loss:18.328 MRR:21.18 Best Results: 21.55
2024-12-29 03:58:14,368: Snapshot:2	Epoch:23	Loss:18.328	translation_Loss:18.201	multi_layer_Loss:0.127	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.18	Hits@10:38.1	Best:21.55
2024-12-29 03:58:14,600: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-29 03:58:22,208: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1365 | 0.3073 | 0.378  |  0.4591 |
|     1      | 0.2868 | 0.1823 | 0.3321 | 0.398  |  0.4883 |
|     2      | 0.2148 | 0.1287 | 0.2468 | 0.3045 |  0.3817 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 03:58:57,780: Snapshot:3	Epoch:0	Loss:15.432	translation_Loss:13.724	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:15.39	Hits@10:31.18	Best:15.39
2024-12-29 03:59:10,396: Snapshot:3	Epoch:1	Loss:7.309	translation_Loss:5.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.969                                                   	MRR:19.51	Hits@10:36.75	Best:19.51
2024-12-29 03:59:22,935: Snapshot:3	Epoch:2	Loss:5.385	translation_Loss:3.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.62                                                   	MRR:20.45	Hits@10:37.4	Best:20.45
2024-12-29 03:59:35,877: Snapshot:3	Epoch:3	Loss:4.735	translation_Loss:3.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.457                                                   	MRR:20.71	Hits@10:37.54	Best:20.71
2024-12-29 03:59:48,742: Snapshot:3	Epoch:4	Loss:4.457	translation_Loss:3.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.376                                                   	MRR:20.9	Hits@10:37.69	Best:20.9
2024-12-29 04:00:00,191: Snapshot:3	Epoch:5	Loss:4.328	translation_Loss:2.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.345                                                   	MRR:20.91	Hits@10:37.57	Best:20.91
2024-12-29 04:00:11,659: Snapshot:3	Epoch:6	Loss:4.261	translation_Loss:2.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.332                                                   	MRR:20.95	Hits@10:37.71	Best:20.95
2024-12-29 04:00:22,876: Snapshot:3	Epoch:7	Loss:4.191	translation_Loss:2.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.319                                                   	MRR:20.98	Hits@10:37.65	Best:20.98
2024-12-29 04:00:33,747: Snapshot:3	Epoch:8	Loss:4.171	translation_Loss:2.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.315                                                   	MRR:20.86	Hits@10:37.52	Best:20.98
2024-12-29 04:00:46,625: Snapshot:3	Epoch:9	Loss:4.154	translation_Loss:2.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.32                                                   	MRR:20.91	Hits@10:37.74	Best:20.98
2024-12-29 04:00:57,863: Snapshot:3	Epoch:10	Loss:4.121	translation_Loss:2.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.309                                                   	MRR:20.84	Hits@10:37.79	Best:20.98
2024-12-29 04:01:10,820: Snapshot:3	Epoch:11	Loss:4.117	translation_Loss:2.803	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.313                                                   	MRR:20.82	Hits@10:37.65	Best:20.98
2024-12-29 04:01:23,779: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 20.98
2024-12-29 04:01:23,780: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:4.112 MRR:20.73 Best Results: 20.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:01:23,780: Snapshot:3	Epoch:12	Loss:4.112	translation_Loss:2.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.317                                                   	MRR:20.73	Hits@10:37.57	Best:20.98
2024-12-29 04:01:35,050: Snapshot:3	Epoch:13	Loss:35.013	translation_Loss:19.665	multi_layer_Loss:15.348	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.73	Hits@10:37.57	Best:20.98
2024-12-29 04:01:45,795: End of token training: 3 Epoch: 14 Loss:19.758 MRR:20.73 Best Results: 20.98
2024-12-29 04:01:45,796: Snapshot:3	Epoch:14	Loss:19.758	translation_Loss:19.687	multi_layer_Loss:0.071	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.73	Hits@10:37.57	Best:20.98
2024-12-29 04:01:46,028: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2024-12-29 04:01:58,410: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1528 | 0.3185 | 0.383  |  0.4622 |
|     1      | 0.2816 | 0.176  | 0.3243 | 0.3904 |  0.4856 |
|     2      | 0.2152 | 0.1293 | 0.2454 | 0.3042 |  0.3835 |
|     3      | 0.2098 | 0.1216 | 0.2428 | 0.3014 |  0.3787 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 04:02:15,888: Snapshot:4	Epoch:0	Loss:6.638	translation_Loss:6.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.408                                                   	MRR:9.39	Hits@10:20.1	Best:9.39
2024-12-29 04:02:20,613: Snapshot:4	Epoch:1	Loss:4.131	translation_Loss:3.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:15.73	Hits@10:33.19	Best:15.73
2024-12-29 04:02:25,209: Snapshot:4	Epoch:2	Loss:3.045	translation_Loss:2.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.954                                                   	MRR:18.96	Hits@10:34.74	Best:18.96
2024-12-29 04:02:30,468: Snapshot:4	Epoch:3	Loss:2.508	translation_Loss:1.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.954                                                   	MRR:20.65	Hits@10:36.01	Best:20.65
2024-12-29 04:02:35,523: Snapshot:4	Epoch:4	Loss:2.191	translation_Loss:1.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.916                                                   	MRR:21.4	Hits@10:36.72	Best:21.4
2024-12-29 04:02:40,859: Snapshot:4	Epoch:5	Loss:1.985	translation_Loss:1.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.874                                                   	MRR:22.05	Hits@10:36.72	Best:22.05
2024-12-29 04:02:45,457: Snapshot:4	Epoch:6	Loss:1.842	translation_Loss:1.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.837                                                   	MRR:22.19	Hits@10:37.1	Best:22.19
2024-12-29 04:02:50,921: Snapshot:4	Epoch:7	Loss:1.746	translation_Loss:0.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.806                                                   	MRR:22.11	Hits@10:36.98	Best:22.19
2024-12-29 04:02:55,498: Snapshot:4	Epoch:8	Loss:1.688	translation_Loss:0.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.782                                                   	MRR:22.08	Hits@10:36.95	Best:22.19
2024-12-29 04:03:00,003: Snapshot:4	Epoch:9	Loss:1.644	translation_Loss:0.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.768                                                   	MRR:22.43	Hits@10:37.09	Best:22.43
2024-12-29 04:03:04,765: Snapshot:4	Epoch:10	Loss:1.628	translation_Loss:0.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.757                                                   	MRR:22.23	Hits@10:36.98	Best:22.43
2024-12-29 04:03:09,973: Snapshot:4	Epoch:11	Loss:1.6	translation_Loss:0.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.745                                                   	MRR:22.58	Hits@10:37.16	Best:22.58
2024-12-29 04:03:14,550: Snapshot:4	Epoch:12	Loss:1.582	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.736                                                   	MRR:22.14	Hits@10:37.27	Best:22.58
2024-12-29 04:03:18,926: Snapshot:4	Epoch:13	Loss:1.583	translation_Loss:0.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.734                                                   	MRR:21.95	Hits@10:36.31	Best:22.58
2024-12-29 04:03:23,645: Snapshot:4	Epoch:14	Loss:1.57	translation_Loss:0.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.73                                                   	MRR:22.1	Hits@10:36.93	Best:22.58
2024-12-29 04:03:28,037: Snapshot:4	Epoch:15	Loss:1.565	translation_Loss:0.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.726                                                   	MRR:22.27	Hits@10:37.0	Best:22.58
2024-12-29 04:03:33,151: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 22.58
2024-12-29 04:03:33,151: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:1.553 MRR:22.07 Best Results: 22.58
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 04:03:33,151: Snapshot:4	Epoch:16	Loss:1.553	translation_Loss:0.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.723                                                   	MRR:22.07	Hits@10:36.67	Best:22.58
2024-12-29 04:03:38,116: Snapshot:4	Epoch:17	Loss:24.146	translation_Loss:9.083	multi_layer_Loss:15.063	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.07	Hits@10:36.67	Best:22.58
2024-12-29 04:03:42,506: End of token training: 4 Epoch: 18 Loss:9.997 MRR:22.07 Best Results: 22.58
2024-12-29 04:03:42,507: Snapshot:4	Epoch:18	Loss:9.997	translation_Loss:9.081	multi_layer_Loss:0.916	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.07	Hits@10:36.67	Best:22.58
2024-12-29 04:03:42,741: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2024-12-29 04:03:57,275: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2272 | 0.1246 | 0.2766 | 0.3397 |  0.4135 |
|     1      | 0.2693 | 0.1679 | 0.3073 | 0.3737 |  0.4666 |
|     2      | 0.2036 | 0.1207 | 0.2297 | 0.2872 |  0.3664 |
|     3      | 0.1862 | 0.1005 | 0.2146 | 0.2732 |  0.3507 |
|     4      | 0.227  | 0.1509 | 0.2587 | 0.3076 |  0.3702 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 04:03:57,277: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1499 | 0.3141 | 0.3778 |  0.4536 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1496 | 0.3123 |  0.38  |  0.4556 |
|     1      | 0.2971 | 0.1907 | 0.3442 | 0.4149 |  0.5061 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.1365 | 0.3073 | 0.378  |  0.4591 |
|     1      | 0.2868 | 0.1823 | 0.3321 | 0.398  |  0.4883 |
|     2      | 0.2148 | 0.1287 | 0.2468 | 0.3045 |  0.3817 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1528 | 0.3185 | 0.383  |  0.4622 |
|     1      | 0.2816 | 0.176  | 0.3243 | 0.3904 |  0.4856 |
|     2      | 0.2152 | 0.1293 | 0.2454 | 0.3042 |  0.3835 |
|     3      | 0.2098 | 0.1216 | 0.2428 | 0.3014 |  0.3787 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2272 | 0.1246 | 0.2766 | 0.3397 |  0.4135 |
|     1      | 0.2693 | 0.1679 | 0.3073 | 0.3737 |  0.4666 |
|     2      | 0.2036 | 0.1207 | 0.2297 | 0.2872 |  0.3664 |
|     3      | 0.1862 | 0.1005 | 0.2146 | 0.2732 |  0.3507 |
|     4      | 0.227  | 0.1509 | 0.2587 | 0.3076 |  0.3702 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 04:03:57,278: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 135.24804186820984 |   0.257   |     0.15     |    0.314     |     0.454     |
|    1     | 71.39151120185852  |   0.268   |    0.161     |    0.321     |     0.469     |
|    2     | 247.16854310035706 |   0.236   |    0.138     |    0.278     |     0.422     |
|    3     | 198.90277004241943 |   0.228   |    0.135     |    0.266     |     0.406     |
|    4     | 101.80217432975769 |    0.21   |    0.122     |    0.242     |     0.377     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 04:03:57,278: Sum_Training_Time:754.5130405426025
2024-12-29 04:03:57,278: Every_Training_Time:[135.24804186820984, 71.39151120185852, 247.16854310035706, 198.90277004241943, 101.80217432975769]
2024-12-29 04:03:57,278: Forward transfer: 0.042825 Backward transfer: -0.02319999999999999
[lijing@p0313 IncDE]$ python main.py -dataset HYBRID -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 5 -multi_layer_weight 1 -token_distillation_weight 5000 5000 1000 10000 -token_num 5
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2025-01-02 15:18:44,815: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250102151817/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[5000.0, 5000.0, 1000.0, 10000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-02 15:18:53,974: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-02 15:18:59,493: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-02 15:19:04,993: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.67	Best:18.94
2025-01-02 15:19:10,819: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.68	Hits@10:43.55	Best:22.68
2025-01-02 15:19:16,265: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:45.3	Best:24.22
2025-01-02 15:19:22,016: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:46.0	Best:25.0
2025-01-02 15:19:27,432: Snapshot:0	Epoch:6	Loss:1.068	translation_Loss:1.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.29	Hits@10:46.21	Best:25.29
2025-01-02 15:19:33,266: Snapshot:0	Epoch:7	Loss:0.8	translation_Loss:0.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:46.46	Best:25.55
2025-01-02 15:19:38,712: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.51	Best:25.7
2025-01-02 15:19:44,493: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.48	Best:25.75
2025-01-02 15:19:49,937: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.32	Best:25.75
2025-01-02 15:19:55,724: Snapshot:0	Epoch:11	Loss:0.403	translation_Loss:0.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.9	Hits@10:46.28	Best:25.9
2025-01-02 15:20:01,181: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.28	Best:25.9
2025-01-02 15:20:06,635: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.23	Best:25.9
2025-01-02 15:20:12,422: Snapshot:0	Epoch:14	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.09	Best:25.9
2025-01-02 15:20:17,785: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:45.85	Best:25.9
2025-01-02 15:20:23,493: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 25.9
2025-01-02 15:20:23,493: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.252 MRR:25.77 Best Results: 25.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 15:20:23,494: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:45.96	Best:25.9
2025-01-02 15:20:29,386: Snapshot:0	Epoch:17	Loss:26.573	translation_Loss:11.575	multi_layer_Loss:14.999	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:45.96	Best:25.9
2025-01-02 15:20:35,148: End of token training: 0 Epoch: 18 Loss:11.929 MRR:25.77 Best Results: 25.9
2025-01-02 15:20:35,148: Snapshot:0	Epoch:18	Loss:11.929	translation_Loss:11.556	multi_layer_Loss:0.373	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.77	Hits@10:45.96	Best:25.9
2025-01-02 15:20:35,387: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-02 15:20:37,770: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2569 | 0.1482 | 0.3165 | 0.3789 |  0.4525 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 15:20:48,005: Snapshot:1	Epoch:0	Loss:4.928	translation_Loss:4.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:9.92	Hits@10:17.54	Best:9.92
2025-01-02 15:20:50,119: Snapshot:1	Epoch:1	Loss:2.97	translation_Loss:2.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.345                                                   	MRR:16.9	Hits@10:31.56	Best:16.9
2025-01-02 15:20:52,177: Snapshot:1	Epoch:2	Loss:1.885	translation_Loss:1.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.464                                                   	MRR:21.09	Hits@10:37.11	Best:21.09
2025-01-02 15:20:54,249: Snapshot:1	Epoch:3	Loss:1.339	translation_Loss:0.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.466                                                   	MRR:23.67	Hits@10:40.87	Best:23.67
2025-01-02 15:20:56,351: Snapshot:1	Epoch:4	Loss:1.03	translation_Loss:0.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.407                                                   	MRR:24.99	Hits@10:43.33	Best:24.99
2025-01-02 15:20:58,435: Snapshot:1	Epoch:5	Loss:0.836	translation_Loss:0.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:26.04	Hits@10:45.71	Best:26.04
2025-01-02 15:21:00,555: Snapshot:1	Epoch:6	Loss:0.73	translation_Loss:0.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.305                                                   	MRR:27.13	Hits@10:47.7	Best:27.13
2025-01-02 15:21:02,681: Snapshot:1	Epoch:7	Loss:0.662	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.279                                                   	MRR:27.72	Hits@10:48.55	Best:27.72
2025-01-02 15:21:05,115: Snapshot:1	Epoch:8	Loss:0.618	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:28.23	Hits@10:49.26	Best:28.23
2025-01-02 15:21:07,248: Snapshot:1	Epoch:9	Loss:0.573	translation_Loss:0.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.25                                                   	MRR:28.47	Hits@10:49.63	Best:28.47
2025-01-02 15:21:09,335: Snapshot:1	Epoch:10	Loss:0.544	translation_Loss:0.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.24                                                   	MRR:28.94	Hits@10:50.22	Best:28.94
2025-01-02 15:21:11,374: Snapshot:1	Epoch:11	Loss:0.518	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:29.08	Hits@10:50.36	Best:29.08
2025-01-02 15:21:13,466: Snapshot:1	Epoch:12	Loss:0.505	translation_Loss:0.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:29.53	Hits@10:50.6	Best:29.53
2025-01-02 15:21:15,571: Snapshot:1	Epoch:13	Loss:0.484	translation_Loss:0.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.221                                                   	MRR:29.81	Hits@10:50.68	Best:29.81
2025-01-02 15:21:18,035: Snapshot:1	Epoch:14	Loss:0.472	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:29.98	Hits@10:50.95	Best:29.98
2025-01-02 15:21:20,126: Snapshot:1	Epoch:15	Loss:0.46	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:30.05	Hits@10:51.1	Best:30.05
2025-01-02 15:21:22,166: Snapshot:1	Epoch:16	Loss:0.452	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:29.82	Hits@10:50.97	Best:30.05
2025-01-02 15:21:24,233: Snapshot:1	Epoch:17	Loss:0.441	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.205                                                   	MRR:30.12	Hits@10:50.98	Best:30.12
2025-01-02 15:21:26,316: Snapshot:1	Epoch:18	Loss:0.431	translation_Loss:0.229	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:30.34	Hits@10:51.21	Best:30.34
2025-01-02 15:21:28,408: Snapshot:1	Epoch:19	Loss:0.426	translation_Loss:0.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.199                                                   	MRR:30.28	Hits@10:51.26	Best:30.34
2025-01-02 15:21:30,798: Snapshot:1	Epoch:20	Loss:0.422	translation_Loss:0.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:30.17	Hits@10:51.4	Best:30.34
2025-01-02 15:21:32,832: Snapshot:1	Epoch:21	Loss:0.41	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.195                                                   	MRR:30.16	Hits@10:51.01	Best:30.34
2025-01-02 15:21:34,916: Snapshot:1	Epoch:22	Loss:0.411	translation_Loss:0.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:30.24	Hits@10:51.04	Best:30.34
2025-01-02 15:21:37,028: Snapshot:1	Epoch:23	Loss:0.401	translation_Loss:0.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.192                                                   	MRR:30.52	Hits@10:51.02	Best:30.52
2025-01-02 15:21:39,109: Snapshot:1	Epoch:24	Loss:0.396	translation_Loss:0.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:30.28	Hits@10:51.22	Best:30.52
2025-01-02 15:21:41,138: Snapshot:1	Epoch:25	Loss:0.393	translation_Loss:0.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:30.5	Hits@10:51.43	Best:30.52
2025-01-02 15:21:43,491: Snapshot:1	Epoch:26	Loss:0.388	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:30.43	Hits@10:51.51	Best:30.52
2025-01-02 15:21:45,556: Snapshot:1	Epoch:27	Loss:0.389	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.185                                                   	MRR:30.52	Hits@10:51.45	Best:30.52
2025-01-02 15:21:47,609: Early Stopping! Snapshot: 1 Epoch: 28 Best Results: 30.52
2025-01-02 15:21:47,609: Start to training tokens! Snapshot: 1 Epoch: 28 Loss:0.386 MRR:30.44 Best Results: 30.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 15:21:47,610: Snapshot:1	Epoch:28	Loss:0.386	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.184                                                   	MRR:30.44	Hits@10:51.39	Best:30.52
2025-01-02 15:21:49,625: Snapshot:1	Epoch:29	Loss:16.174	translation_Loss:4.386	multi_layer_Loss:11.789	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.44	Hits@10:51.39	Best:30.52
2025-01-02 15:21:51,650: End of token training: 1 Epoch: 30 Loss:7.018 MRR:30.44 Best Results: 30.52
2025-01-02 15:21:51,650: Snapshot:1	Epoch:30	Loss:7.018	translation_Loss:4.379	multi_layer_Loss:2.638	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.44	Hits@10:51.39	Best:30.52
2025-01-02 15:21:51,894: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-02 15:21:55,514: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 |  0.15  | 0.3154 | 0.3785 |  0.4534 |
|     1      | 0.2979 | 0.1907 | 0.3463 | 0.4218 |  0.512  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 15:22:25,914: Snapshot:2	Epoch:0	Loss:16.099	translation_Loss:14.884	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.216                                                   	MRR:14.06	Hits@10:28.62	Best:14.06
2025-01-02 15:22:34,796: Snapshot:2	Epoch:1	Loss:7.592	translation_Loss:6.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.465                                                   	MRR:18.89	Hits@10:35.67	Best:18.89
2025-01-02 15:22:43,696: Snapshot:2	Epoch:2	Loss:5.082	translation_Loss:3.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.196                                                   	MRR:20.43	Hits@10:37.3	Best:20.43
2025-01-02 15:22:53,146: Snapshot:2	Epoch:3	Loss:4.207	translation_Loss:3.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.051                                                   	MRR:20.98	Hits@10:37.61	Best:20.98
2025-01-02 15:23:02,538: Snapshot:2	Epoch:4	Loss:3.855	translation_Loss:2.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.986                                                   	MRR:21.11	Hits@10:37.69	Best:21.11
2025-01-02 15:23:11,660: Snapshot:2	Epoch:5	Loss:3.671	translation_Loss:2.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.959                                                   	MRR:21.4	Hits@10:37.91	Best:21.4
2025-01-02 15:23:21,066: Snapshot:2	Epoch:6	Loss:3.581	translation_Loss:2.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.944                                                   	MRR:21.35	Hits@10:37.85	Best:21.4
2025-01-02 15:23:30,327: Snapshot:2	Epoch:7	Loss:3.523	translation_Loss:2.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.935                                                   	MRR:21.49	Hits@10:38.05	Best:21.49
2025-01-02 15:23:39,257: Snapshot:2	Epoch:8	Loss:3.478	translation_Loss:2.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.934                                                   	MRR:21.6	Hits@10:37.98	Best:21.6
2025-01-02 15:23:48,471: Snapshot:2	Epoch:9	Loss:3.454	translation_Loss:2.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.93                                                   	MRR:21.51	Hits@10:38.06	Best:21.6
2025-01-02 15:23:57,735: Snapshot:2	Epoch:10	Loss:3.427	translation_Loss:2.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.93                                                   	MRR:21.42	Hits@10:38.1	Best:21.6
2025-01-02 15:24:06,751: Snapshot:2	Epoch:11	Loss:3.409	translation_Loss:2.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.93                                                   	MRR:21.46	Hits@10:37.96	Best:21.6
2025-01-02 15:24:16,009: Snapshot:2	Epoch:12	Loss:3.389	translation_Loss:2.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.931                                                   	MRR:21.56	Hits@10:38.12	Best:21.6
2025-01-02 15:24:25,196: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.6
2025-01-02 15:24:25,196: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:3.378 MRR:21.34 Best Results: 21.6
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 15:24:25,197: Snapshot:2	Epoch:13	Loss:3.378	translation_Loss:2.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.931                                                   	MRR:21.34	Hits@10:38.04	Best:21.6
2025-01-02 15:24:34,488: Snapshot:2	Epoch:14	Loss:32.616	translation_Loss:18.178	multi_layer_Loss:14.438	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.34	Hits@10:38.04	Best:21.6
2025-01-02 15:24:43,306: End of token training: 2 Epoch: 15 Loss:18.313 MRR:21.34 Best Results: 21.6
2025-01-02 15:24:43,306: Snapshot:2	Epoch:15	Loss:18.313	translation_Loss:18.185	multi_layer_Loss:0.127	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.34	Hits@10:38.04	Best:21.6
2025-01-02 15:24:43,542: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-02 15:24:50,936: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2537 | 0.1417 | 0.3144 |  0.38  |  0.4595 |
|     1      | 0.2856 | 0.1786 | 0.3302 | 0.4014 |  0.4974 |
|     2      | 0.2165 | 0.1318 | 0.2464 | 0.3046 |  0.3825 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 15:25:26,985: Snapshot:3	Epoch:0	Loss:14.516	translation_Loss:13.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.885                                                   	MRR:16.02	Hits@10:32.3	Best:16.02
2025-01-02 15:25:38,139: Snapshot:3	Epoch:1	Loss:5.469	translation_Loss:3.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.514                                                   	MRR:20.28	Hits@10:38.08	Best:20.28
2025-01-02 15:25:49,097: Snapshot:3	Epoch:2	Loss:3.696	translation_Loss:2.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.475                                                   	MRR:20.99	Hits@10:39.03	Best:20.99
2025-01-02 15:25:59,945: Snapshot:3	Epoch:3	Loss:3.156	translation_Loss:1.747	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.408                                                   	MRR:21.53	Hits@10:39.6	Best:21.53
2025-01-02 15:26:11,406: Snapshot:3	Epoch:4	Loss:2.919	translation_Loss:1.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.359                                                   	MRR:21.68	Hits@10:39.96	Best:21.68
2025-01-02 15:26:22,784: Snapshot:3	Epoch:5	Loss:2.803	translation_Loss:1.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.341                                                   	MRR:21.82	Hits@10:40.01	Best:21.82
2025-01-02 15:26:33,754: Snapshot:3	Epoch:6	Loss:2.734	translation_Loss:1.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.326                                                   	MRR:21.76	Hits@10:39.86	Best:21.82
2025-01-02 15:26:45,025: Snapshot:3	Epoch:7	Loss:2.682	translation_Loss:1.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.312                                                   	MRR:21.9	Hits@10:40.16	Best:21.9
2025-01-02 15:26:56,465: Snapshot:3	Epoch:8	Loss:2.658	translation_Loss:1.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.308                                                   	MRR:21.93	Hits@10:39.85	Best:21.93
2025-01-02 15:27:07,690: Snapshot:3	Epoch:9	Loss:2.632	translation_Loss:1.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.301                                                   	MRR:21.84	Hits@10:39.79	Best:21.93
2025-01-02 15:27:18,820: Snapshot:3	Epoch:10	Loss:2.617	translation_Loss:1.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:21.73	Hits@10:39.81	Best:21.93
2025-01-02 15:27:29,773: Snapshot:3	Epoch:11	Loss:2.623	translation_Loss:1.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.301                                                   	MRR:21.94	Hits@10:39.9	Best:21.94
2025-01-02 15:27:41,189: Snapshot:3	Epoch:12	Loss:2.597	translation_Loss:1.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.3                                                   	MRR:21.89	Hits@10:39.97	Best:21.94
2025-01-02 15:27:52,426: Snapshot:3	Epoch:13	Loss:2.589	translation_Loss:1.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.298                                                   	MRR:21.87	Hits@10:39.94	Best:21.94
2025-01-02 15:28:03,569: Snapshot:3	Epoch:14	Loss:2.584	translation_Loss:1.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.296                                                   	MRR:21.89	Hits@10:39.85	Best:21.94
2025-01-02 15:28:14,917: Snapshot:3	Epoch:15	Loss:2.584	translation_Loss:1.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.305                                                   	MRR:21.77	Hits@10:40.04	Best:21.94
2025-01-02 15:28:26,092: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 21.94
2025-01-02 15:28:26,092: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:2.568 MRR:21.86 Best Results: 21.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 15:28:26,092: Snapshot:3	Epoch:16	Loss:2.568	translation_Loss:1.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.296                                                   	MRR:21.86	Hits@10:39.83	Best:21.94
2025-01-02 15:28:36,858: Snapshot:3	Epoch:17	Loss:33.703	translation_Loss:18.355	multi_layer_Loss:15.348	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.86	Hits@10:39.83	Best:21.94
2025-01-02 15:28:48,283: End of token training: 3 Epoch: 18 Loss:18.427 MRR:21.86 Best Results: 21.94
2025-01-02 15:28:48,284: Snapshot:3	Epoch:18	Loss:18.427	translation_Loss:18.357	multi_layer_Loss:0.071	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.86	Hits@10:39.83	Best:21.94
2025-01-02 15:28:48,577: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-02 15:29:01,042: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1489 | 0.3066 | 0.3721 |  0.4533 |
|     1      | 0.2753 | 0.1729 | 0.3104 | 0.3793 |  0.4816 |
|     2      | 0.2083 | 0.123  | 0.2363 | 0.2962 |  0.3759 |
|     3      | 0.2175 | 0.1239 | 0.2533 | 0.3173 |  0.3968 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 15:29:18,047: Snapshot:4	Epoch:0	Loss:6.62	translation_Loss:6.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.536                                                   	MRR:9.4	Hits@10:20.71	Best:9.4
2025-01-02 15:29:22,574: Snapshot:4	Epoch:1	Loss:4.345	translation_Loss:3.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.878                                                   	MRR:15.34	Hits@10:33.03	Best:15.34
2025-01-02 15:29:27,077: Snapshot:4	Epoch:2	Loss:3.313	translation_Loss:2.415	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.898                                                   	MRR:19.17	Hits@10:35.41	Best:19.17
2025-01-02 15:29:31,691: Snapshot:4	Epoch:3	Loss:2.747	translation_Loss:1.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.851                                                   	MRR:20.95	Hits@10:36.26	Best:20.95
2025-01-02 15:29:36,286: Snapshot:4	Epoch:4	Loss:2.394	translation_Loss:1.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.794                                                   	MRR:21.91	Hits@10:36.9	Best:21.91
2025-01-02 15:29:40,745: Snapshot:4	Epoch:5	Loss:2.149	translation_Loss:1.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.752                                                   	MRR:22.28	Hits@10:37.13	Best:22.28
2025-01-02 15:29:45,536: Snapshot:4	Epoch:6	Loss:2.0	translation_Loss:1.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.713                                                   	MRR:22.35	Hits@10:37.43	Best:22.35
2025-01-02 15:29:50,112: Snapshot:4	Epoch:7	Loss:1.917	translation_Loss:1.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.689                                                   	MRR:22.4	Hits@10:37.03	Best:22.4
2025-01-02 15:29:54,560: Snapshot:4	Epoch:8	Loss:1.849	translation_Loss:1.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.671                                                   	MRR:22.35	Hits@10:37.24	Best:22.4
2025-01-02 15:29:59,378: Snapshot:4	Epoch:9	Loss:1.808	translation_Loss:1.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.657                                                   	MRR:21.97	Hits@10:37.11	Best:22.4
2025-01-02 15:30:03,890: Snapshot:4	Epoch:10	Loss:1.77	translation_Loss:1.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.645                                                   	MRR:22.27	Hits@10:36.92	Best:22.4
2025-01-02 15:30:08,454: Snapshot:4	Epoch:11	Loss:1.76	translation_Loss:1.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.639                                                   	MRR:22.76	Hits@10:37.25	Best:22.76
2025-01-02 15:30:13,192: Snapshot:4	Epoch:12	Loss:1.746	translation_Loss:1.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.63                                                   	MRR:22.41	Hits@10:37.11	Best:22.76
2025-01-02 15:30:17,647: Snapshot:4	Epoch:13	Loss:1.735	translation_Loss:1.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.626                                                   	MRR:22.43	Hits@10:37.22	Best:22.76
2025-01-02 15:30:22,035: Snapshot:4	Epoch:14	Loss:1.729	translation_Loss:1.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.622                                                   	MRR:22.31	Hits@10:36.84	Best:22.76
2025-01-02 15:30:26,781: Snapshot:4	Epoch:15	Loss:1.722	translation_Loss:1.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.62                                                   	MRR:22.19	Hits@10:37.18	Best:22.76
2025-01-02 15:30:31,239: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 22.76
2025-01-02 15:30:31,239: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:1.725 MRR:22.74 Best Results: 22.76
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 15:30:31,240: Snapshot:4	Epoch:16	Loss:1.725	translation_Loss:1.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.62                                                   	MRR:22.74	Hits@10:37.16	Best:22.76
2025-01-02 15:30:35,681: Snapshot:4	Epoch:17	Loss:24.244	translation_Loss:9.181	multi_layer_Loss:15.063	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.74	Hits@10:37.16	Best:22.76
2025-01-02 15:30:40,412: End of token training: 4 Epoch: 18 Loss:10.104 MRR:22.74 Best Results: 22.76
2025-01-02 15:30:40,412: Snapshot:4	Epoch:18	Loss:10.104	translation_Loss:9.189	multi_layer_Loss:0.916	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.74	Hits@10:37.16	Best:22.76
2025-01-02 15:30:40,650: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-02 15:30:55,317: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2251 | 0.1243 | 0.2715 | 0.3331 |  0.4128 |
|     1      | 0.2703 | 0.1716 | 0.303  | 0.3714 |  0.4673 |
|     2      | 0.2001 | 0.1167 | 0.2251 | 0.285  |  0.3635 |
|     3      | 0.2032 | 0.1115 | 0.2362 | 0.2987 |  0.3812 |
|     4      | 0.2266 | 0.1491 | 0.2571 | 0.3085 |  0.3735 |
+------------+--------+--------+--------+--------+---------+
2025-01-02 15:30:55,319: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2569 | 0.1482 | 0.3165 | 0.3789 |  0.4525 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 |  0.15  | 0.3154 | 0.3785 |  0.4534 |
|     1      | 0.2979 | 0.1907 | 0.3463 | 0.4218 |  0.512  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2537 | 0.1417 | 0.3144 |  0.38  |  0.4595 |
|     1      | 0.2856 | 0.1786 | 0.3302 | 0.4014 |  0.4974 |
|     2      | 0.2165 | 0.1318 | 0.2464 | 0.3046 |  0.3825 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1489 | 0.3066 | 0.3721 |  0.4533 |
|     1      | 0.2753 | 0.1729 | 0.3104 | 0.3793 |  0.4816 |
|     2      | 0.2083 | 0.123  | 0.2363 | 0.2962 |  0.3759 |
|     3      | 0.2175 | 0.1239 | 0.2533 | 0.3173 |  0.3968 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2251 | 0.1243 | 0.2715 | 0.3331 |  0.4128 |
|     1      | 0.2703 | 0.1716 | 0.303  | 0.3714 |  0.4673 |
|     2      | 0.2001 | 0.1167 | 0.2251 | 0.285  |  0.3635 |
|     3      | 0.2032 | 0.1115 | 0.2362 | 0.2987 |  0.3812 |
|     4      | 0.2266 | 0.1491 | 0.2571 | 0.3085 |  0.3735 |
+------------+--------+--------+--------+--------+---------+]
2025-01-02 15:30:55,321: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 110.33337116241455 |   0.257   |    0.148     |    0.317     |     0.452     |
|    1     |  72.7098708152771  |   0.268   |    0.161     |    0.324     |     0.469     |
|    2     | 164.03002977371216 |   0.238   |    0.141     |     0.28     |     0.424     |
|    3     | 232.4361023902893  |   0.227   |    0.133     |    0.264     |     0.409     |
|    4     | 96.83075547218323  |   0.214   |    0.124     |    0.247     |     0.387     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-02 15:30:55,321: Sum_Training_Time:676.3401296138763
2025-01-02 15:30:55,321: Every_Training_Time:[110.33337116241455, 72.7098708152771, 164.03002977371216, 232.4361023902893, 96.83075547218323]
2025-01-02 15:30:55,321: Forward transfer: 0.044075 Backward transfer: -0.02252500000000001
[lijing@p0316 IncDE]$ python main.py -dataset HYBRID -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 5 -multi_layer_weight 1 -token_distillation_weight 10000 5000 1000 15000 -token_num 5
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2025-01-02 16:13:53,040: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250102161326/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-02 16:14:02,292: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-02 16:14:07,696: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-02 16:14:13,122: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.68	Best:18.93
2025-01-02 16:14:18,998: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.54	Best:22.69
2025-01-02 16:14:24,416: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:45.29	Best:24.2
2025-01-02 16:14:30,129: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:45.94	Best:24.97
2025-01-02 16:14:35,521: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.29	Hits@10:46.2	Best:25.29
2025-01-02 16:14:41,391: Snapshot:0	Epoch:7	Loss:0.798	translation_Loss:0.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.38	Best:25.62
2025-01-02 16:14:46,782: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.47	Best:25.64
2025-01-02 16:14:52,591: Snapshot:0	Epoch:9	Loss:0.536	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.45	Best:25.7
2025-01-02 16:14:57,966: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.17	Best:25.71
2025-01-02 16:15:03,823: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.31	Best:25.77
2025-01-02 16:15:09,296: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.22	Best:25.77
2025-01-02 16:15:14,686: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:46.11	Best:25.77
2025-01-02 16:15:20,360: Snapshot:0	Epoch:14	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:45.88	Best:25.77
2025-01-02 16:15:25,747: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:45.9	Best:25.77
2025-01-02 16:15:31,465: Snapshot:0	Epoch:16	Loss:0.253	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.18	Best:25.85
2025-01-02 16:15:36,823: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.0	Best:25.85
2025-01-02 16:15:42,510: Snapshot:0	Epoch:18	Loss:0.223	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:45.9	Best:25.85
2025-01-02 16:15:47,863: Snapshot:0	Epoch:19	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.05	Best:25.85
2025-01-02 16:15:53,603: Snapshot:0	Epoch:20	Loss:0.197	translation_Loss:0.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.46	Hits@10:45.71	Best:25.85
2025-01-02 16:15:59,036: Early Stopping! Snapshot: 0 Epoch: 21 Best Results: 25.85
2025-01-02 16:15:59,036: Start to training tokens! Snapshot: 0 Epoch: 21 Loss:0.195 MRR:25.32 Best Results: 25.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 16:15:59,036: Snapshot:0	Epoch:21	Loss:0.195	translation_Loss:0.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.32	Hits@10:45.6	Best:25.85
2025-01-02 16:16:04,868: Snapshot:0	Epoch:22	Loss:26.492	translation_Loss:11.494	multi_layer_Loss:14.999	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.32	Hits@10:45.6	Best:25.85
2025-01-02 16:16:10,582: End of token training: 0 Epoch: 23 Loss:11.884 MRR:25.32 Best Results: 25.85
2025-01-02 16:16:10,583: Snapshot:0	Epoch:23	Loss:11.884	translation_Loss:11.511	multi_layer_Loss:0.373	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.32	Hits@10:45.6	Best:25.85
2025-01-02 16:16:10,842: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-02 16:16:13,143: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1497 | 0.3136 | 0.3785 |  0.4527 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 16:16:23,122: Snapshot:1	Epoch:0	Loss:4.958	translation_Loss:4.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.217                                                   	MRR:9.63	Hits@10:16.95	Best:9.63
2025-01-02 16:16:25,172: Snapshot:1	Epoch:1	Loss:3.031	translation_Loss:2.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:16.34	Hits@10:29.78	Best:16.34
2025-01-02 16:16:27,545: Snapshot:1	Epoch:2	Loss:1.95	translation_Loss:1.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.445                                                   	MRR:20.51	Hits@10:35.94	Best:20.51
2025-01-02 16:16:29,676: Snapshot:1	Epoch:3	Loss:1.382	translation_Loss:0.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.405                                                   	MRR:23.0	Hits@10:39.58	Best:23.0
2025-01-02 16:16:31,736: Snapshot:1	Epoch:4	Loss:1.07	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.338                                                   	MRR:24.32	Hits@10:42.65	Best:24.32
2025-01-02 16:16:33,847: Snapshot:1	Epoch:5	Loss:0.891	translation_Loss:0.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.285                                                   	MRR:25.28	Hits@10:44.97	Best:25.28
2025-01-02 16:16:35,927: Snapshot:1	Epoch:6	Loss:0.782	translation_Loss:0.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:26.22	Hits@10:46.49	Best:26.22
2025-01-02 16:16:38,032: Snapshot:1	Epoch:7	Loss:0.703	translation_Loss:0.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:27.1	Hits@10:47.35	Best:27.1
2025-01-02 16:16:40,381: Snapshot:1	Epoch:8	Loss:0.658	translation_Loss:0.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.222                                                   	MRR:27.43	Hits@10:48.01	Best:27.43
2025-01-02 16:16:42,441: Snapshot:1	Epoch:9	Loss:0.62	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:27.86	Hits@10:48.41	Best:27.86
2025-01-02 16:16:44,532: Snapshot:1	Epoch:10	Loss:0.583	translation_Loss:0.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.206                                                   	MRR:28.11	Hits@10:48.81	Best:28.11
2025-01-02 16:16:46,575: Snapshot:1	Epoch:11	Loss:0.559	translation_Loss:0.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.196                                                   	MRR:28.48	Hits@10:49.22	Best:28.48
2025-01-02 16:16:48,710: Snapshot:1	Epoch:12	Loss:0.542	translation_Loss:0.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:28.6	Hits@10:49.51	Best:28.6
2025-01-02 16:16:50,800: Snapshot:1	Epoch:13	Loss:0.528	translation_Loss:0.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:28.92	Hits@10:49.88	Best:28.92
2025-01-02 16:16:53,217: Snapshot:1	Epoch:14	Loss:0.513	translation_Loss:0.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:29.07	Hits@10:49.98	Best:29.07
2025-01-02 16:16:55,289: Snapshot:1	Epoch:15	Loss:0.492	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:29.29	Hits@10:50.04	Best:29.29
2025-01-02 16:16:57,420: Snapshot:1	Epoch:16	Loss:0.479	translation_Loss:0.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:29.62	Hits@10:49.92	Best:29.62
2025-01-02 16:16:59,546: Snapshot:1	Epoch:17	Loss:0.472	translation_Loss:0.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:29.67	Hits@10:49.7	Best:29.67
2025-01-02 16:17:01,620: Snapshot:1	Epoch:18	Loss:0.461	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.63	Hits@10:49.95	Best:29.67
2025-01-02 16:17:03,623: Snapshot:1	Epoch:19	Loss:0.454	translation_Loss:0.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.56	Hits@10:50.01	Best:29.67
2025-01-02 16:17:05,622: Snapshot:1	Epoch:20	Loss:0.447	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:29.57	Hits@10:50.2	Best:29.67
2025-01-02 16:17:07,941: Snapshot:1	Epoch:21	Loss:0.443	translation_Loss:0.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.58	Hits@10:50.26	Best:29.67
2025-01-02 16:17:09,958: Early Stopping! Snapshot: 1 Epoch: 22 Best Results: 29.67
2025-01-02 16:17:09,958: Start to training tokens! Snapshot: 1 Epoch: 22 Loss:0.434 MRR:29.38 Best Results: 29.67
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 16:17:09,959: Snapshot:1	Epoch:22	Loss:0.434	translation_Loss:0.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.159                                                   	MRR:29.38	Hits@10:50.41	Best:29.67
2025-01-02 16:17:12,004: Snapshot:1	Epoch:23	Loss:16.241	translation_Loss:4.452	multi_layer_Loss:11.789	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.38	Hits@10:50.41	Best:29.67
2025-01-02 16:17:14,031: End of token training: 1 Epoch: 24 Loss:7.093 MRR:29.38 Best Results: 29.67
2025-01-02 16:17:14,031: Snapshot:1	Epoch:24	Loss:7.093	translation_Loss:4.454	multi_layer_Loss:2.638	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.38	Hits@10:50.41	Best:29.67
2025-01-02 16:17:14,217: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-02 16:17:17,853: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1529 | 0.315  | 0.3793 |  0.4508 |
|     1      | 0.291  | 0.1869 | 0.3356 | 0.406  |  0.4989 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 16:17:48,008: Snapshot:2	Epoch:0	Loss:15.885	translation_Loss:14.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.202                                                   	MRR:13.91	Hits@10:28.27	Best:13.91
2025-01-02 16:17:56,896: Snapshot:2	Epoch:1	Loss:7.312	translation_Loss:5.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.425                                                   	MRR:18.97	Hits@10:35.7	Best:18.97
2025-01-02 16:18:06,145: Snapshot:2	Epoch:2	Loss:4.821	translation_Loss:3.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.137                                                   	MRR:20.24	Hits@10:37.27	Best:20.24
2025-01-02 16:18:15,079: Snapshot:2	Epoch:3	Loss:3.958	translation_Loss:2.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.991                                                   	MRR:20.87	Hits@10:37.61	Best:20.87
2025-01-02 16:18:24,261: Snapshot:2	Epoch:4	Loss:3.623	translation_Loss:2.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.919                                                   	MRR:21.11	Hits@10:37.85	Best:21.11
2025-01-02 16:18:33,465: Snapshot:2	Epoch:5	Loss:3.452	translation_Loss:2.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.893                                                   	MRR:21.22	Hits@10:37.92	Best:21.22
2025-01-02 16:18:42,315: Snapshot:2	Epoch:6	Loss:3.361	translation_Loss:2.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.878                                                   	MRR:21.31	Hits@10:37.9	Best:21.31
2025-01-02 16:18:51,584: Snapshot:2	Epoch:7	Loss:3.309	translation_Loss:2.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.874                                                   	MRR:21.45	Hits@10:38.03	Best:21.45
2025-01-02 16:19:00,784: Snapshot:2	Epoch:8	Loss:3.264	translation_Loss:2.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.873                                                   	MRR:21.3	Hits@10:37.86	Best:21.45
2025-01-02 16:19:09,612: Snapshot:2	Epoch:9	Loss:3.226	translation_Loss:2.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.867                                                   	MRR:21.12	Hits@10:37.94	Best:21.45
2025-01-02 16:19:18,804: Snapshot:2	Epoch:10	Loss:3.215	translation_Loss:2.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.867                                                   	MRR:21.45	Hits@10:37.86	Best:21.45
2025-01-02 16:19:27,981: Snapshot:2	Epoch:11	Loss:3.189	translation_Loss:2.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.866                                                   	MRR:21.27	Hits@10:38.08	Best:21.45
2025-01-02 16:19:37,132: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 21.45
2025-01-02 16:19:37,132: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:3.185 MRR:21.29 Best Results: 21.45
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 16:19:37,133: Snapshot:2	Epoch:12	Loss:3.185	translation_Loss:2.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.865                                                   	MRR:21.29	Hits@10:37.8	Best:21.45
2025-01-02 16:19:45,979: Snapshot:2	Epoch:13	Loss:32.606	translation_Loss:18.168	multi_layer_Loss:14.438	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.29	Hits@10:37.8	Best:21.45
2025-01-02 16:19:55,163: End of token training: 2 Epoch: 14 Loss:18.329 MRR:21.29 Best Results: 21.45
2025-01-02 16:19:55,163: Snapshot:2	Epoch:14	Loss:18.329	translation_Loss:18.202	multi_layer_Loss:0.127	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.29	Hits@10:37.8	Best:21.45
2025-01-02 16:19:55,396: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-02 16:20:02,789: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2526 | 0.1429 | 0.3078 | 0.3782 |  0.4563 |
|     1      | 0.2798 | 0.1752 | 0.3247 | 0.3901 |  0.4795 |
|     2      | 0.2134 | 0.1294 | 0.2415 | 0.2986 |  0.3799 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 16:20:39,055: Snapshot:3	Epoch:0	Loss:14.303	translation_Loss:13.422	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.881                                                   	MRR:15.99	Hits@10:32.03	Best:15.99
2025-01-02 16:20:49,889: Snapshot:3	Epoch:1	Loss:5.313	translation_Loss:3.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.488                                                   	MRR:20.12	Hits@10:37.67	Best:20.12
2025-01-02 16:21:00,754: Snapshot:3	Epoch:2	Loss:3.538	translation_Loss:2.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.422                                                   	MRR:20.98	Hits@10:38.6	Best:20.98
2025-01-02 16:21:11,638: Snapshot:3	Epoch:3	Loss:3.007	translation_Loss:1.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.34                                                   	MRR:21.43	Hits@10:39.4	Best:21.43
2025-01-02 16:21:22,551: Snapshot:3	Epoch:4	Loss:2.784	translation_Loss:1.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:21.63	Hits@10:39.34	Best:21.63
2025-01-02 16:21:33,658: Snapshot:3	Epoch:5	Loss:2.669	translation_Loss:1.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.272                                                   	MRR:21.73	Hits@10:39.75	Best:21.73
2025-01-02 16:21:44,656: Snapshot:3	Epoch:6	Loss:2.61	translation_Loss:1.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.258                                                   	MRR:21.7	Hits@10:39.69	Best:21.73
2025-01-02 16:21:55,496: Snapshot:3	Epoch:7	Loss:2.558	translation_Loss:1.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.244                                                   	MRR:21.9	Hits@10:39.73	Best:21.9
2025-01-02 16:22:06,681: Snapshot:3	Epoch:8	Loss:2.518	translation_Loss:1.282	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.236                                                   	MRR:21.75	Hits@10:39.84	Best:21.9
2025-01-02 16:22:17,738: Snapshot:3	Epoch:9	Loss:2.514	translation_Loss:1.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.233                                                   	MRR:21.71	Hits@10:39.68	Best:21.9
2025-01-02 16:22:28,762: Snapshot:3	Epoch:10	Loss:2.499	translation_Loss:1.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.233                                                   	MRR:21.79	Hits@10:39.28	Best:21.9
2025-01-02 16:22:39,834: Snapshot:3	Epoch:11	Loss:2.502	translation_Loss:1.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.234                                                   	MRR:21.62	Hits@10:39.48	Best:21.9
2025-01-02 16:22:50,893: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 21.9
2025-01-02 16:22:50,893: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:2.481 MRR:21.68 Best Results: 21.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 16:22:50,894: Snapshot:3	Epoch:12	Loss:2.481	translation_Loss:1.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.231                                                   	MRR:21.68	Hits@10:39.29	Best:21.9
2025-01-02 16:23:01,647: Snapshot:3	Epoch:13	Loss:33.722	translation_Loss:18.375	multi_layer_Loss:15.348	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.68	Hits@10:39.29	Best:21.9
2025-01-02 16:23:12,695: End of token training: 3 Epoch: 14 Loss:18.47 MRR:21.68 Best Results: 21.9
2025-01-02 16:23:12,695: Snapshot:3	Epoch:14	Loss:18.47	translation_Loss:18.399	multi_layer_Loss:0.071	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.68	Hits@10:39.29	Best:21.9
2025-01-02 16:23:12,922: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-02 16:23:25,026: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2595 | 0.1552 | 0.3087 | 0.3733 |  0.4556 |
|     1      | 0.2681 | 0.1709 | 0.2995 | 0.3624 |  0.4648 |
|     2      | 0.2087 | 0.1243 | 0.2364 | 0.294  |  0.3745 |
|     3      | 0.2177 | 0.124  | 0.2526 | 0.3175 |  0.3968 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 16:23:41,759: Snapshot:4	Epoch:0	Loss:6.659	translation_Loss:6.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.611                                                   	MRR:9.62	Hits@10:20.03	Best:9.62
2025-01-02 16:23:46,237: Snapshot:4	Epoch:1	Loss:4.495	translation_Loss:3.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.869                                                   	MRR:15.82	Hits@10:32.5	Best:15.82
2025-01-02 16:23:51,046: Snapshot:4	Epoch:2	Loss:3.483	translation_Loss:2.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.834                                                   	MRR:19.57	Hits@10:35.45	Best:19.57
2025-01-02 16:23:55,592: Snapshot:4	Epoch:3	Loss:2.903	translation_Loss:2.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.772                                                   	MRR:21.36	Hits@10:36.17	Best:21.36
2025-01-02 16:24:00,075: Snapshot:4	Epoch:4	Loss:2.543	translation_Loss:1.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.716                                                   	MRR:22.44	Hits@10:36.83	Best:22.44
2025-01-02 16:24:04,964: Snapshot:4	Epoch:5	Loss:2.288	translation_Loss:1.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.68                                                   	MRR:22.67	Hits@10:36.68	Best:22.67
2025-01-02 16:24:09,397: Snapshot:4	Epoch:6	Loss:2.134	translation_Loss:1.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.645                                                   	MRR:23.03	Hits@10:37.11	Best:23.03
2025-01-02 16:24:13,800: Snapshot:4	Epoch:7	Loss:2.033	translation_Loss:1.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.621                                                   	MRR:22.71	Hits@10:36.91	Best:23.03
2025-01-02 16:24:18,551: Snapshot:4	Epoch:8	Loss:1.978	translation_Loss:1.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.603                                                   	MRR:22.71	Hits@10:36.93	Best:23.03
2025-01-02 16:24:22,911: Snapshot:4	Epoch:9	Loss:1.937	translation_Loss:1.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.592                                                   	MRR:22.89	Hits@10:36.93	Best:23.03
2025-01-02 16:24:27,368: Snapshot:4	Epoch:10	Loss:1.909	translation_Loss:1.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.583                                                   	MRR:22.73	Hits@10:36.82	Best:23.03
2025-01-02 16:24:31,801: Snapshot:4	Epoch:11	Loss:1.888	translation_Loss:1.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.575                                                   	MRR:23.09	Hits@10:36.64	Best:23.09
2025-01-02 16:24:36,556: Snapshot:4	Epoch:12	Loss:1.874	translation_Loss:1.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.571                                                   	MRR:22.71	Hits@10:36.73	Best:23.09
2025-01-02 16:24:40,925: Snapshot:4	Epoch:13	Loss:1.858	translation_Loss:1.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.567                                                   	MRR:22.73	Hits@10:36.42	Best:23.09
2025-01-02 16:24:45,282: Snapshot:4	Epoch:14	Loss:1.855	translation_Loss:1.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.566                                                   	MRR:22.91	Hits@10:36.83	Best:23.09
2025-01-02 16:24:50,013: Snapshot:4	Epoch:15	Loss:1.858	translation_Loss:1.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.566                                                   	MRR:22.87	Hits@10:36.55	Best:23.09
2025-01-02 16:24:54,451: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 23.09
2025-01-02 16:24:54,451: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:1.843 MRR:23.07 Best Results: 23.09
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2025-01-02 16:24:54,452: Snapshot:4	Epoch:16	Loss:1.843	translation_Loss:1.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.563                                                   	MRR:23.07	Hits@10:36.83	Best:23.09
2025-01-02 16:24:58,868: Snapshot:4	Epoch:17	Loss:24.34	translation_Loss:9.277	multi_layer_Loss:15.063	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.07	Hits@10:36.83	Best:23.09
2025-01-02 16:25:03,695: End of token training: 4 Epoch: 18 Loss:10.194 MRR:23.07 Best Results: 23.09
2025-01-02 16:25:03,696: Snapshot:4	Epoch:18	Loss:10.194	translation_Loss:9.278	multi_layer_Loss:0.916	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.07	Hits@10:36.83	Best:23.09
2025-01-02 16:25:03,957: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-02 16:25:18,451: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2306 | 0.1285 | 0.2789 | 0.3412 |  0.4186 |
|     1      | 0.2621 | 0.1642 | 0.2986 | 0.3565 |  0.4565 |
|     2      | 0.2015 | 0.1172 | 0.2304 | 0.286  |  0.3656 |
|     3      | 0.2045 | 0.1122 | 0.2365 | 0.3005 |  0.3827 |
|     4      | 0.2303 | 0.1575 | 0.2574 | 0.3078 |  0.3677 |
+------------+--------+--------+--------+--------+---------+
2025-01-02 16:25:18,453: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.257 | 0.1497 | 0.3136 | 0.3785 |  0.4527 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1529 | 0.315  | 0.3793 |  0.4508 |
|     1      | 0.291  | 0.1869 | 0.3356 | 0.406  |  0.4989 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2526 | 0.1429 | 0.3078 | 0.3782 |  0.4563 |
|     1      | 0.2798 | 0.1752 | 0.3247 | 0.3901 |  0.4795 |
|     2      | 0.2134 | 0.1294 | 0.2415 | 0.2986 |  0.3799 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2595 | 0.1552 | 0.3087 | 0.3733 |  0.4556 |
|     1      | 0.2681 | 0.1709 | 0.2995 | 0.3624 |  0.4648 |
|     2      | 0.2087 | 0.1243 | 0.2364 | 0.294  |  0.3745 |
|     3      | 0.2177 | 0.124  | 0.2526 | 0.3175 |  0.3968 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2306 | 0.1285 | 0.2789 | 0.3412 |  0.4186 |
|     1      | 0.2621 | 0.1642 | 0.2986 | 0.3565 |  0.4565 |
|     2      | 0.2015 | 0.1172 | 0.2304 | 0.286  |  0.3656 |
|     3      | 0.2045 | 0.1122 | 0.2365 | 0.3005 |  0.3827 |
|     4      | 0.2303 | 0.1575 | 0.2574 | 0.3078 |  0.3677 |
+------------+--------+--------+--------+--------+---------+]
2025-01-02 16:25:18,455: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 137.5421380996704  |   0.257   |     0.15     |    0.314     |     0.453     |
|    1     | 59.74391222000122  |   0.267   |    0.162     |     0.32     |     0.464     |
|    2     | 153.5704414844513  |   0.235   |     0.14     |    0.275     |     0.419     |
|    3     | 184.77248072624207 |   0.228   |    0.134     |    0.263     |     0.407     |
|    4     | 96.17906737327576  |   0.216   |    0.126     |     0.25     |     0.388     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-02 16:25:18,455: Sum_Training_Time:631.8080399036407
2025-01-02 16:25:18,455: Every_Training_Time:[137.5421380996704, 59.74391222000122, 153.5704414844513, 184.77248072624207, 96.17906737327576]
2025-01-02 16:25:18,455: Forward transfer: 0.04345 Backward transfer: -0.0201
2025-01-05 20:59:15,821: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250105205902/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 3000.0, 800.0, 200000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-05 20:59:24,686: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-05 20:59:30,539: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-05 20:59:35,996: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.94	Hits@10:39.68	Best:18.94
2025-01-05 20:59:41,427: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:22.69	Hits@10:43.57	Best:22.69
2025-01-05 20:59:47,283: Snapshot:0	Epoch:4	Loss:2.463	translation_Loss:2.463	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.23	Hits@10:45.22	Best:24.23
2025-01-05 20:59:52,754: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.02	Hits@10:46.04	Best:25.02
2025-01-05 20:59:58,544: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.26	Hits@10:46.2	Best:25.26
2025-01-05 21:00:04,195: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.63	Hits@10:46.55	Best:25.63
2025-01-05 21:00:10,296: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.68	Hits@10:46.68	Best:25.68
2025-01-05 21:00:15,811: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.66	Hits@10:46.49	Best:25.68
2025-01-05 21:00:21,527: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.66	Hits@10:46.4	Best:25.68
2025-01-05 21:00:27,049: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.79	Hits@10:46.48	Best:25.79
2025-01-05 21:00:32,814: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.76	Hits@10:46.28	Best:25.79
2025-01-05 21:00:38,260: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.85	Hits@10:46.1	Best:25.85
2025-01-05 21:00:43,780: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.77	Hits@10:46.12	Best:25.85
2025-01-05 21:00:49,626: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.76	Hits@10:46.0	Best:25.85
2025-01-05 21:00:55,120: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.68	Hits@10:45.9	Best:25.85
2025-01-05 21:01:00,949: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.49	Hits@10:45.85	Best:25.85
2025-01-05 21:01:06,574: Early Stopping! Snapshot: 0 Epoch: 18 Best Results: 25.85
2025-01-05 21:01:06,574: Start to training tokens! Snapshot: 0 Epoch: 18 Loss:0.223 MRR:25.48 Best Results: 25.85
Token added to optimizer, embeddings excluded successfully.
2025-01-05 21:01:06,575: Snapshot:0	Epoch:18	Loss:0.223	translation_Loss:0.223	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.48	Hits@10:45.63	Best:25.85
2025-01-05 21:01:12,886: Snapshot:0	Epoch:19	Loss:26.507	translation_Loss:11.508	token_training_loss:14.999	distillation_Loss:0.0                                                   	MRR:25.48	Hits@10:45.63	Best:25.85
2025-01-05 21:01:18,372: End of token training: 0 Epoch: 20 Loss:11.874 MRR:25.48 Best Results: 25.85
2025-01-05 21:01:18,372: Snapshot:0	Epoch:20	Loss:11.874	translation_Loss:11.501	token_training_loss:0.373	distillation_Loss:0.0                                                           	MRR:25.48	Hits@10:45.63	Best:25.85
2025-01-05 21:01:18,612: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-05 21:01:20,916: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 | 0.1502 | 0.314  | 0.3778 |  0.4518 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 21:01:27,819: Snapshot:1	Epoch:0	Loss:4.975	translation_Loss:4.756	token_training_loss:0.0	distillation_Loss:0.218                                                   	MRR:9.63	Hits@10:16.84	Best:9.63
2025-01-05 21:01:30,231: Snapshot:1	Epoch:1	Loss:3.085	translation_Loss:2.685	token_training_loss:0.0	distillation_Loss:0.4                                                   	MRR:16.45	Hits@10:30.2	Best:16.45
2025-01-05 21:01:32,409: Snapshot:1	Epoch:2	Loss:2.005	translation_Loss:1.554	token_training_loss:0.0	distillation_Loss:0.451                                                   	MRR:20.73	Hits@10:35.9	Best:20.73
2025-01-05 21:01:34,506: Snapshot:1	Epoch:3	Loss:1.434	translation_Loss:1.02	token_training_loss:0.0	distillation_Loss:0.414                                                   	MRR:23.37	Hits@10:39.94	Best:23.37
2025-01-05 21:01:36,577: Snapshot:1	Epoch:4	Loss:1.11	translation_Loss:0.76	token_training_loss:0.0	distillation_Loss:0.35                                                   	MRR:24.56	Hits@10:43.47	Best:24.56
2025-01-05 21:01:38,744: Snapshot:1	Epoch:5	Loss:0.926	translation_Loss:0.627	token_training_loss:0.0	distillation_Loss:0.298                                                   	MRR:25.5	Hits@10:45.65	Best:25.5
2025-01-05 21:01:40,872: Snapshot:1	Epoch:6	Loss:0.813	translation_Loss:0.547	token_training_loss:0.0	distillation_Loss:0.266                                                   	MRR:26.4	Hits@10:47.29	Best:26.4
2025-01-05 21:01:43,281: Snapshot:1	Epoch:7	Loss:0.741	translation_Loss:0.494	token_training_loss:0.0	distillation_Loss:0.247                                                   	MRR:27.09	Hits@10:47.96	Best:27.09
2025-01-05 21:01:45,395: Snapshot:1	Epoch:8	Loss:0.688	translation_Loss:0.452	token_training_loss:0.0	distillation_Loss:0.236                                                   	MRR:27.68	Hits@10:48.49	Best:27.68
2025-01-05 21:01:47,468: Snapshot:1	Epoch:9	Loss:0.653	translation_Loss:0.428	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:28.21	Hits@10:48.95	Best:28.21
2025-01-05 21:01:49,629: Snapshot:1	Epoch:10	Loss:0.626	translation_Loss:0.41	token_training_loss:0.0	distillation_Loss:0.216                                                   	MRR:28.58	Hits@10:49.31	Best:28.58
2025-01-05 21:01:51,756: Snapshot:1	Epoch:11	Loss:0.594	translation_Loss:0.385	token_training_loss:0.0	distillation_Loss:0.209                                                   	MRR:28.85	Hits@10:49.87	Best:28.85
2025-01-05 21:01:53,865: Snapshot:1	Epoch:12	Loss:0.569	translation_Loss:0.367	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:29.06	Hits@10:50.0	Best:29.06
2025-01-05 21:01:56,087: Snapshot:1	Epoch:13	Loss:0.553	translation_Loss:0.356	token_training_loss:0.0	distillation_Loss:0.197                                                   	MRR:29.32	Hits@10:50.02	Best:29.32
2025-01-05 21:01:58,488: Snapshot:1	Epoch:14	Loss:0.537	translation_Loss:0.344	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:29.23	Hits@10:50.11	Best:29.32
2025-01-05 21:02:00,532: Snapshot:1	Epoch:15	Loss:0.526	translation_Loss:0.339	token_training_loss:0.0	distillation_Loss:0.187                                                   	MRR:29.34	Hits@10:50.29	Best:29.34
2025-01-05 21:02:02,605: Snapshot:1	Epoch:16	Loss:0.505	translation_Loss:0.321	token_training_loss:0.0	distillation_Loss:0.184                                                   	MRR:29.48	Hits@10:50.38	Best:29.48
2025-01-05 21:02:04,760: Snapshot:1	Epoch:17	Loss:0.502	translation_Loss:0.322	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:29.73	Hits@10:50.35	Best:29.73
2025-01-05 21:02:06,858: Snapshot:1	Epoch:18	Loss:0.49	translation_Loss:0.312	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:30.07	Hits@10:50.3	Best:30.07
2025-01-05 21:02:09,402: Snapshot:1	Epoch:19	Loss:0.488	translation_Loss:0.313	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:30.24	Hits@10:50.66	Best:30.24
2025-01-05 21:02:11,458: Snapshot:1	Epoch:20	Loss:0.476	translation_Loss:0.302	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:29.95	Hits@10:50.65	Best:30.24
2025-01-05 21:02:13,461: Snapshot:1	Epoch:21	Loss:0.474	translation_Loss:0.301	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:30.05	Hits@10:50.72	Best:30.24
2025-01-05 21:02:15,560: Snapshot:1	Epoch:22	Loss:0.47	translation_Loss:0.298	token_training_loss:0.0	distillation_Loss:0.172                                                   	MRR:30.08	Hits@10:50.74	Best:30.24
2025-01-05 21:02:17,694: Snapshot:1	Epoch:23	Loss:0.459	translation_Loss:0.29	token_training_loss:0.0	distillation_Loss:0.17                                                   	MRR:30.02	Hits@10:50.68	Best:30.24
2025-01-05 21:02:19,854: Early Stopping! Snapshot: 1 Epoch: 24 Best Results: 30.24
2025-01-05 21:02:19,854: Start to training tokens! Snapshot: 1 Epoch: 24 Loss:0.457 MRR:30.04 Best Results: 30.24
Token added to optimizer, embeddings excluded successfully.
2025-01-05 21:02:19,854: Snapshot:1	Epoch:24	Loss:0.457	translation_Loss:0.289	token_training_loss:0.0	distillation_Loss:0.168                                                   	MRR:30.04	Hits@10:50.54	Best:30.24
2025-01-05 21:02:21,972: Snapshot:1	Epoch:25	Loss:16.236	translation_Loss:4.447	token_training_loss:11.789	distillation_Loss:0.0                                                   	MRR:30.04	Hits@10:50.54	Best:30.24
2025-01-05 21:02:24,340: End of token training: 1 Epoch: 26 Loss:7.077 MRR:30.04 Best Results: 30.24
2025-01-05 21:02:24,340: Snapshot:1	Epoch:26	Loss:7.077	translation_Loss:4.438	token_training_loss:2.638	distillation_Loss:0.0                                                           	MRR:30.04	Hits@10:50.54	Best:30.24
2025-01-05 21:02:24,582: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-05 21:02:27,971: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.258  | 0.1506 | 0.3149 |  0.38  |  0.4545 |
|     1      | 0.2932 | 0.1885 | 0.3387 | 0.4098 |  0.5054 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 21:02:44,472: Snapshot:2	Epoch:0	Loss:15.815	translation_Loss:14.774	token_training_loss:0.0	distillation_Loss:1.04                                                   	MRR:14.15	Hits@10:28.78	Best:14.15
2025-01-05 21:02:53,371: Snapshot:2	Epoch:1	Loss:7.023	translation_Loss:5.56	token_training_loss:0.0	distillation_Loss:1.463                                                   	MRR:19.02	Hits@10:36.19	Best:19.02
2025-01-05 21:03:02,580: Snapshot:2	Epoch:2	Loss:4.569	translation_Loss:3.309	token_training_loss:0.0	distillation_Loss:1.261                                                   	MRR:20.57	Hits@10:37.46	Best:20.57
2025-01-05 21:03:11,816: Snapshot:2	Epoch:3	Loss:3.74	translation_Loss:2.624	token_training_loss:0.0	distillation_Loss:1.116                                                   	MRR:21.12	Hits@10:37.91	Best:21.12
2025-01-05 21:03:21,148: Snapshot:2	Epoch:4	Loss:3.409	translation_Loss:2.359	token_training_loss:0.0	distillation_Loss:1.05                                                   	MRR:21.51	Hits@10:38.18	Best:21.51
2025-01-05 21:03:30,262: Snapshot:2	Epoch:5	Loss:3.237	translation_Loss:2.226	token_training_loss:0.0	distillation_Loss:1.012                                                   	MRR:21.49	Hits@10:38.31	Best:21.51
2025-01-05 21:03:39,533: Snapshot:2	Epoch:6	Loss:3.156	translation_Loss:2.156	token_training_loss:0.0	distillation_Loss:1.0                                                   	MRR:21.37	Hits@10:38.24	Best:21.51
2025-01-05 21:03:48,890: Snapshot:2	Epoch:7	Loss:3.083	translation_Loss:2.094	token_training_loss:0.0	distillation_Loss:0.989                                                   	MRR:21.58	Hits@10:38.53	Best:21.58
2025-01-05 21:03:57,988: Snapshot:2	Epoch:8	Loss:3.058	translation_Loss:2.075	token_training_loss:0.0	distillation_Loss:0.983                                                   	MRR:21.72	Hits@10:38.41	Best:21.72
2025-01-05 21:04:07,180: Snapshot:2	Epoch:9	Loss:3.026	translation_Loss:2.047	token_training_loss:0.0	distillation_Loss:0.979                                                   	MRR:21.48	Hits@10:38.22	Best:21.72
2025-01-05 21:04:16,402: Snapshot:2	Epoch:10	Loss:2.992	translation_Loss:2.017	token_training_loss:0.0	distillation_Loss:0.975                                                   	MRR:21.48	Hits@10:38.39	Best:21.72
2025-01-05 21:04:25,261: Snapshot:2	Epoch:11	Loss:2.983	translation_Loss:2.01	token_training_loss:0.0	distillation_Loss:0.973                                                   	MRR:21.65	Hits@10:38.42	Best:21.72
2025-01-05 21:04:34,469: Snapshot:2	Epoch:12	Loss:2.962	translation_Loss:1.99	token_training_loss:0.0	distillation_Loss:0.973                                                   	MRR:21.65	Hits@10:38.54	Best:21.72
2025-01-05 21:04:43,588: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.72
2025-01-05 21:04:43,589: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:2.947 MRR:21.67 Best Results: 21.72
Token added to optimizer, embeddings excluded successfully.
2025-01-05 21:04:43,589: Snapshot:2	Epoch:13	Loss:2.947	translation_Loss:1.978	token_training_loss:0.0	distillation_Loss:0.97                                                   	MRR:21.67	Hits@10:38.41	Best:21.72
2025-01-05 21:04:52,350: Snapshot:2	Epoch:14	Loss:32.339	translation_Loss:17.901	token_training_loss:14.438	distillation_Loss:0.0                                                   	MRR:21.67	Hits@10:38.41	Best:21.72
2025-01-05 21:05:01,488: End of token training: 2 Epoch: 15 Loss:18.014 MRR:21.67 Best Results: 21.72
2025-01-05 21:05:01,488: Snapshot:2	Epoch:15	Loss:18.014	translation_Loss:17.887	token_training_loss:0.127	distillation_Loss:0.0                                                           	MRR:21.67	Hits@10:38.41	Best:21.72
2025-01-05 21:05:01,730: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-05 21:05:09,120: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2543 | 0.1438 | 0.3117 | 0.3785 |  0.4567 |
|     1      | 0.275  | 0.1707 | 0.3201 | 0.3844 |  0.4721 |
|     2      | 0.2172 | 0.1311 | 0.2465 | 0.3053 |  0.3857 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 21:05:28,621: Snapshot:3	Epoch:0	Loss:14.205	translation_Loss:13.428	token_training_loss:0.0	distillation_Loss:0.777                                                   	MRR:16.04	Hits@10:32.29	Best:16.04
2025-01-05 21:05:39,847: Snapshot:3	Epoch:1	Loss:5.086	translation_Loss:3.726	token_training_loss:0.0	distillation_Loss:1.359                                                   	MRR:20.23	Hits@10:38.13	Best:20.23
2025-01-05 21:05:50,768: Snapshot:3	Epoch:2	Loss:3.309	translation_Loss:1.964	token_training_loss:0.0	distillation_Loss:1.345                                                   	MRR:21.06	Hits@10:38.99	Best:21.06
2025-01-05 21:06:01,975: Snapshot:3	Epoch:3	Loss:2.779	translation_Loss:1.5	token_training_loss:0.0	distillation_Loss:1.279                                                   	MRR:21.42	Hits@10:39.65	Best:21.42
2025-01-05 21:06:13,247: Snapshot:3	Epoch:4	Loss:2.575	translation_Loss:1.332	token_training_loss:0.0	distillation_Loss:1.242                                                   	MRR:21.64	Hits@10:40.01	Best:21.64
2025-01-05 21:06:24,500: Snapshot:3	Epoch:5	Loss:2.455	translation_Loss:1.232	token_training_loss:0.0	distillation_Loss:1.223                                                   	MRR:21.78	Hits@10:39.97	Best:21.78
2025-01-05 21:06:35,718: Snapshot:3	Epoch:6	Loss:2.389	translation_Loss:1.192	token_training_loss:0.0	distillation_Loss:1.197                                                   	MRR:21.7	Hits@10:39.81	Best:21.78
2025-01-05 21:06:46,602: Snapshot:3	Epoch:7	Loss:2.343	translation_Loss:1.149	token_training_loss:0.0	distillation_Loss:1.194                                                   	MRR:21.98	Hits@10:40.07	Best:21.98
2025-01-05 21:06:57,714: Snapshot:3	Epoch:8	Loss:2.322	translation_Loss:1.138	token_training_loss:0.0	distillation_Loss:1.184                                                   	MRR:22.1	Hits@10:40.1	Best:22.1
2025-01-05 21:07:08,903: Snapshot:3	Epoch:9	Loss:2.297	translation_Loss:1.114	token_training_loss:0.0	distillation_Loss:1.183                                                   	MRR:21.85	Hits@10:39.86	Best:22.1
2025-01-05 21:07:20,121: Snapshot:3	Epoch:10	Loss:2.284	translation_Loss:1.103	token_training_loss:0.0	distillation_Loss:1.181                                                   	MRR:21.86	Hits@10:40.24	Best:22.1
2025-01-05 21:07:31,265: Snapshot:3	Epoch:11	Loss:2.268	translation_Loss:1.086	token_training_loss:0.0	distillation_Loss:1.183                                                   	MRR:21.8	Hits@10:40.05	Best:22.1
2025-01-05 21:07:42,331: Snapshot:3	Epoch:12	Loss:2.259	translation_Loss:1.081	token_training_loss:0.0	distillation_Loss:1.178                                                   	MRR:21.84	Hits@10:40.02	Best:22.1
2025-01-05 21:07:53,014: Early Stopping! Snapshot: 3 Epoch: 13 Best Results: 22.1
2025-01-05 21:07:53,015: Start to training tokens! Snapshot: 3 Epoch: 13 Loss:2.263 MRR:21.85 Best Results: 22.1
Token added to optimizer, embeddings excluded successfully.
2025-01-05 21:07:53,015: Snapshot:3	Epoch:13	Loss:2.263	translation_Loss:1.083	token_training_loss:0.0	distillation_Loss:1.181                                                   	MRR:21.85	Hits@10:39.91	Best:22.1
2025-01-05 21:08:04,091: Snapshot:3	Epoch:14	Loss:33.474	translation_Loss:18.127	token_training_loss:15.348	distillation_Loss:0.0                                                   	MRR:21.85	Hits@10:39.91	Best:22.1
2025-01-05 21:08:15,278: End of token training: 3 Epoch: 15 Loss:18.2 MRR:21.85 Best Results: 22.1
2025-01-05 21:08:15,279: Snapshot:3	Epoch:15	Loss:18.2	translation_Loss:18.129	token_training_loss:0.071	distillation_Loss:0.0                                                           	MRR:21.85	Hits@10:39.91	Best:22.1
2025-01-05 21:08:15,557: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-05 21:08:28,053: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.1547 | 0.3078 | 0.3727 |  0.4554 |
|     1      | 0.2688 | 0.1696 | 0.3044 | 0.3727 |  0.466  |
|     2      | 0.2079 | 0.1224 | 0.2364 | 0.2924 |  0.3765 |
|     3      | 0.2201 | 0.1254 | 0.2586 | 0.3199 |  0.4014 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-05 21:08:38,691: Snapshot:4	Epoch:0	Loss:7.748	translation_Loss:6.329	token_training_loss:0.0	distillation_Loss:1.419                                                   	MRR:8.06	Hits@10:16.86	Best:8.06
2025-01-05 21:08:43,228: Snapshot:4	Epoch:1	Loss:5.577	translation_Loss:4.701	token_training_loss:0.0	distillation_Loss:0.877                                                   	MRR:13.96	Hits@10:27.11	Best:13.96
2025-01-05 21:08:47,833: Snapshot:4	Epoch:2	Loss:4.556	translation_Loss:3.735	token_training_loss:0.0	distillation_Loss:0.822                                                   	MRR:17.93	Hits@10:31.81	Best:17.93
2025-01-05 21:08:52,793: Snapshot:4	Epoch:3	Loss:3.837	translation_Loss:3.107	token_training_loss:0.0	distillation_Loss:0.731                                                   	MRR:20.63	Hits@10:34.33	Best:20.63
2025-01-05 21:08:57,385: Snapshot:4	Epoch:4	Loss:3.325	translation_Loss:2.667	token_training_loss:0.0	distillation_Loss:0.658                                                   	MRR:21.72	Hits@10:35.09	Best:21.72
2025-01-05 21:09:02,096: Snapshot:4	Epoch:5	Loss:3.0	translation_Loss:2.386	token_training_loss:0.0	distillation_Loss:0.614                                                   	MRR:22.0	Hits@10:35.15	Best:22.0
2025-01-05 21:09:07,080: Snapshot:4	Epoch:6	Loss:2.812	translation_Loss:2.228	token_training_loss:0.0	distillation_Loss:0.584                                                   	MRR:21.97	Hits@10:35.12	Best:22.0
2025-01-05 21:09:11,786: Snapshot:4	Epoch:7	Loss:2.714	translation_Loss:2.146	token_training_loss:0.0	distillation_Loss:0.569                                                   	MRR:21.94	Hits@10:35.09	Best:22.0
2025-01-05 21:09:16,430: Snapshot:4	Epoch:8	Loss:2.655	translation_Loss:2.097	token_training_loss:0.0	distillation_Loss:0.558                                                   	MRR:21.87	Hits@10:35.18	Best:22.0
2025-01-05 21:09:21,459: Snapshot:4	Epoch:9	Loss:2.615	translation_Loss:2.067	token_training_loss:0.0	distillation_Loss:0.548                                                   	MRR:21.88	Hits@10:35.13	Best:22.0
2025-01-05 21:09:26,106: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 22.0
2025-01-05 21:09:26,106: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:2.592 MRR:21.78 Best Results: 22.0
Token added to optimizer, embeddings excluded successfully.
2025-01-05 21:09:26,107: Snapshot:4	Epoch:10	Loss:2.592	translation_Loss:2.052	token_training_loss:0.0	distillation_Loss:0.541                                                   	MRR:21.78	Hits@10:35.17	Best:22.0
2025-01-05 21:09:30,568: Snapshot:4	Epoch:11	Loss:24.866	translation_Loss:9.803	token_training_loss:15.063	distillation_Loss:0.0                                                   	MRR:21.78	Hits@10:35.17	Best:22.0
2025-01-05 21:09:34,997: End of token training: 4 Epoch: 12 Loss:10.72 MRR:21.78 Best Results: 22.0
2025-01-05 21:09:34,997: Snapshot:4	Epoch:12	Loss:10.72	translation_Loss:9.804	token_training_loss:0.916	distillation_Loss:0.0                                                           	MRR:21.78	Hits@10:35.17	Best:22.0
2025-01-05 21:09:35,287: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-05 21:09:49,697: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1368 | 0.2941 | 0.3624 |  0.4497 |
|     1      | 0.2651 | 0.1649 | 0.302  | 0.3685 |  0.464  |
|     2      | 0.2051 | 0.1192 | 0.2336 | 0.2908 |  0.3752 |
|     3      | 0.2148 | 0.1189 | 0.253  | 0.3163 |  0.3992 |
|     4      | 0.2196 | 0.151  | 0.2443 | 0.2926 |  0.3529 |
+------------+--------+--------+--------+--------+---------+
2025-01-05 21:09:49,699: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2573 | 0.1502 | 0.314  | 0.3778 |  0.4518 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.258  | 0.1506 | 0.3149 |  0.38  |  0.4545 |
|     1      | 0.2932 | 0.1885 | 0.3387 | 0.4098 |  0.5054 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2543 | 0.1438 | 0.3117 | 0.3785 |  0.4567 |
|     1      | 0.275  | 0.1707 | 0.3201 | 0.3844 |  0.4721 |
|     2      | 0.2172 | 0.1311 | 0.2465 | 0.3053 |  0.3857 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.1547 | 0.3078 | 0.3727 |  0.4554 |
|     1      | 0.2688 | 0.1696 | 0.3044 | 0.3727 |  0.466  |
|     2      | 0.2079 | 0.1224 | 0.2364 | 0.2924 |  0.3765 |
|     3      | 0.2201 | 0.1254 | 0.2586 | 0.3199 |  0.4014 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1368 | 0.2941 | 0.3624 |  0.4497 |
|     1      | 0.2651 | 0.1649 | 0.302  | 0.3685 |  0.464  |
|     2      | 0.2051 | 0.1192 | 0.2336 | 0.2908 |  0.3752 |
|     3      | 0.2148 | 0.1189 | 0.253  | 0.3163 |  0.3992 |
|     4      | 0.2196 | 0.151  | 0.2443 | 0.2926 |  0.3529 |
+------------+--------+--------+--------+--------+---------+]
2025-01-05 21:09:49,699: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 122.5509181022644  |   0.257   |     0.15     |    0.314     |     0.452     |
|    1     | 62.037505865097046 |   0.267   |    0.161     |    0.321     |     0.468     |
|    2     | 149.79681825637817 |   0.237   |     0.14     |    0.278     |     0.421     |
|    3     | 181.24428153038025 |   0.228   |    0.134     |    0.265     |      0.41     |
|    4     |  64.4668779373169  |   0.221   |     0.13     |    0.257     |      0.4      |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-05 21:09:49,699: Sum_Training_Time:580.0964016914368
2025-01-05 21:09:49,699: Every_Training_Time:[122.5509181022644, 62.037505865097046, 149.79681825637817, 181.24428153038025, 64.4668779373169]
2025-01-05 21:09:49,699: Forward transfer: 0.043524999999999994 Backward transfer: -0.014674999999999994