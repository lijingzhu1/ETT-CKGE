2024-12-26 23:45:09,388: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241226234435/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-26 23:45:19,679: Snapshot:0	Epoch:0	Loss:24.869	translation_Loss:24.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.15	Hits@10:15.62	Best:7.15
2024-12-26 23:45:26,541: Snapshot:0	Epoch:1	Loss:16.859	translation_Loss:16.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.6	Hits@10:25.86	Best:11.6
2024-12-26 23:45:33,839: Snapshot:0	Epoch:2	Loss:11.571	translation_Loss:11.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.35	Hits@10:32.19	Best:15.35
2024-12-26 23:45:40,800: Snapshot:0	Epoch:3	Loss:7.838	translation_Loss:7.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.52	Hits@10:35.87	Best:18.52
2024-12-26 23:45:47,870: Snapshot:0	Epoch:4	Loss:5.22	translation_Loss:5.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.98	Hits@10:37.95	Best:20.98
2024-12-26 23:45:54,801: Snapshot:0	Epoch:5	Loss:3.481	translation_Loss:3.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:39.23	Best:22.63
2024-12-26 23:46:01,686: Snapshot:0	Epoch:6	Loss:2.326	translation_Loss:2.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.7	Hits@10:39.81	Best:23.7
2024-12-26 23:46:09,207: Snapshot:0	Epoch:7	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.3	Hits@10:39.95	Best:24.3
2024-12-26 23:46:15,928: Snapshot:0	Epoch:8	Loss:1.154	translation_Loss:1.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:40.23	Best:24.57
2024-12-26 23:46:22,453: Snapshot:0	Epoch:9	Loss:0.88	translation_Loss:0.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.69	Hits@10:40.17	Best:24.69
2024-12-26 23:46:29,346: Snapshot:0	Epoch:10	Loss:0.707	translation_Loss:0.707	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:40.38	Best:24.77
2024-12-26 23:46:36,338: Snapshot:0	Epoch:11	Loss:0.583	translation_Loss:0.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.83	Hits@10:40.42	Best:24.83
2024-12-26 23:46:43,299: Snapshot:0	Epoch:12	Loss:0.5	translation_Loss:0.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:40.34	Best:24.91
2024-12-26 23:46:50,644: Snapshot:0	Epoch:13	Loss:0.44	translation_Loss:0.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:40.54	Best:25.0
2024-12-26 23:46:57,535: Snapshot:0	Epoch:14	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.92	Hits@10:40.44	Best:25.0
2024-12-26 23:47:04,395: Snapshot:0	Epoch:15	Loss:0.36	translation_Loss:0.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.84	Hits@10:40.2	Best:25.0
2024-12-26 23:47:10,418: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 25.0
2024-12-26 23:47:10,418: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.328 MRR:24.86 Best Results: 25.0
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:47:10,419: Snapshot:0	Epoch:16	Loss:0.328	translation_Loss:0.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.86	Hits@10:40.37	Best:25.0
2024-12-26 23:47:17,826: Snapshot:0	Epoch:17	Loss:33.869	translation_Loss:18.31	multi_layer_Loss:15.559	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.86	Hits@10:40.37	Best:25.0
2024-12-26 23:47:25,201: End of token training: 0 Epoch: 18 Loss:18.416 MRR:24.86 Best Results: 25.0
2024-12-26 23:47:25,201: Snapshot:0	Epoch:18	Loss:18.416	translation_Loss:18.309	multi_layer_Loss:0.107	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.86	Hits@10:40.37	Best:25.0
2024-12-26 23:47:25,465: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-26 23:47:27,858: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1565 | 0.2822 | 0.332  |  0.3944 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:47:53,074: Snapshot:1	Epoch:0	Loss:12.503	translation_Loss:10.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.844                                                   	MRR:19.82	Hits@10:33.17	Best:19.82
2024-12-26 23:48:00,562: Snapshot:1	Epoch:1	Loss:10.474	translation_Loss:8.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.845                                                   	MRR:20.54	Hits@10:34.24	Best:20.54
2024-12-26 23:48:08,080: Snapshot:1	Epoch:2	Loss:9.336	translation_Loss:7.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:21.02	Hits@10:34.7	Best:21.02
2024-12-26 23:48:15,453: Snapshot:1	Epoch:3	Loss:8.964	translation_Loss:7.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.725                                                   	MRR:21.09	Hits@10:34.92	Best:21.09
2024-12-26 23:48:22,863: Snapshot:1	Epoch:4	Loss:8.8	translation_Loss:7.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.712                                                   	MRR:21.14	Hits@10:35.0	Best:21.14
2024-12-26 23:48:30,244: Snapshot:1	Epoch:5	Loss:8.716	translation_Loss:7.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.702                                                   	MRR:21.19	Hits@10:35.02	Best:21.19
2024-12-26 23:48:37,628: Snapshot:1	Epoch:6	Loss:8.695	translation_Loss:6.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.702                                                   	MRR:21.15	Hits@10:35.05	Best:21.19
2024-12-26 23:48:44,991: Snapshot:1	Epoch:7	Loss:8.638	translation_Loss:6.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.694                                                   	MRR:21.22	Hits@10:34.93	Best:21.22
2024-12-26 23:48:52,336: Snapshot:1	Epoch:8	Loss:8.631	translation_Loss:6.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.702                                                   	MRR:21.21	Hits@10:35.09	Best:21.22
2024-12-26 23:48:59,655: Snapshot:1	Epoch:9	Loss:8.617	translation_Loss:6.922	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.696                                                   	MRR:21.14	Hits@10:34.97	Best:21.22
2024-12-26 23:49:06,994: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 21.22
2024-12-26 23:49:06,995: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:8.616 MRR:21.17 Best Results: 21.22
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:49:06,995: Snapshot:1	Epoch:10	Loss:8.616	translation_Loss:6.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.698                                                   	MRR:21.17	Hits@10:34.9	Best:21.22
2024-12-26 23:49:14,354: Snapshot:1	Epoch:11	Loss:37.097	translation_Loss:22.069	multi_layer_Loss:15.028	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.17	Hits@10:34.9	Best:21.22
2024-12-26 23:49:21,696: End of token training: 1 Epoch: 12 Loss:22.169 MRR:21.17 Best Results: 21.22
2024-12-26 23:49:21,696: Snapshot:1	Epoch:12	Loss:22.169	translation_Loss:22.062	multi_layer_Loss:0.106	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.17	Hits@10:34.9	Best:21.22
2024-12-26 23:49:22,042: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-26 23:49:29,097: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2487 | 0.1624 | 0.291  | 0.3408 |  0.4053 |
|     1      | 0.2095 | 0.1306 | 0.2482 | 0.296  |  0.3527 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:49:54,624: Snapshot:2	Epoch:0	Loss:10.131	translation_Loss:8.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.954                                                   	MRR:19.82	Hits@10:34.47	Best:19.82
2024-12-26 23:50:02,254: Snapshot:2	Epoch:1	Loss:8.974	translation_Loss:6.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.097                                                   	MRR:20.13	Hits@10:34.9	Best:20.13
2024-12-26 23:50:09,972: Snapshot:2	Epoch:2	Loss:8.474	translation_Loss:6.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.996                                                   	MRR:20.17	Hits@10:35.01	Best:20.17
2024-12-26 23:50:17,518: Snapshot:2	Epoch:3	Loss:8.391	translation_Loss:6.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.008                                                   	MRR:20.14	Hits@10:34.96	Best:20.17
2024-12-26 23:50:25,069: Snapshot:2	Epoch:4	Loss:8.346	translation_Loss:6.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.999                                                   	MRR:20.23	Hits@10:34.95	Best:20.23
2024-12-26 23:50:32,687: Snapshot:2	Epoch:5	Loss:8.347	translation_Loss:6.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.006                                                   	MRR:20.18	Hits@10:35.02	Best:20.23
2024-12-26 23:50:40,204: Snapshot:2	Epoch:6	Loss:8.319	translation_Loss:6.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.006                                                   	MRR:20.18	Hits@10:35.13	Best:20.23
2024-12-26 23:50:47,764: Early Stopping! Snapshot: 2 Epoch: 7 Best Results: 20.23
2024-12-26 23:50:47,765: Start to training tokens! Snapshot: 2 Epoch: 7 Loss:8.331 MRR:20.19 Best Results: 20.23
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:50:47,765: Snapshot:2	Epoch:7	Loss:8.331	translation_Loss:6.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.006                                                   	MRR:20.19	Hits@10:35.01	Best:20.23
2024-12-26 23:50:55,263: Snapshot:2	Epoch:8	Loss:37.861	translation_Loss:22.131	multi_layer_Loss:15.729	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.19	Hits@10:35.01	Best:20.23
2024-12-26 23:51:02,742: End of token training: 2 Epoch: 9 Loss:22.259 MRR:20.19 Best Results: 20.23
2024-12-26 23:51:02,742: Snapshot:2	Epoch:9	Loss:22.259	translation_Loss:22.148	multi_layer_Loss:0.11	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.19	Hits@10:35.01	Best:20.23
2024-12-26 23:51:03,084: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-26 23:51:12,304: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2485 | 0.161  | 0.2892 | 0.3422 |  0.4089 |
|     1      | 0.215  | 0.1348 | 0.2522 | 0.3012 |  0.3645 |
|     2      | 0.2012 | 0.1215 | 0.2366 | 0.2863 |  0.3496 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:51:37,861: Snapshot:3	Epoch:0	Loss:7.521	translation_Loss:5.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.848                                                   	MRR:18.85	Hits@10:35.38	Best:18.85
2024-12-26 23:51:45,552: Snapshot:3	Epoch:1	Loss:6.615	translation_Loss:4.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.052                                                   	MRR:19.03	Hits@10:35.5	Best:19.03
2024-12-26 23:51:53,099: Snapshot:3	Epoch:2	Loss:6.433	translation_Loss:4.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.986                                                   	MRR:18.99	Hits@10:35.5	Best:19.03
2024-12-26 23:52:00,665: Snapshot:3	Epoch:3	Loss:6.424	translation_Loss:4.415	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.009                                                   	MRR:19.15	Hits@10:35.61	Best:19.15
2024-12-26 23:52:08,264: Snapshot:3	Epoch:4	Loss:6.417	translation_Loss:4.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.005                                                   	MRR:19.01	Hits@10:35.53	Best:19.15
2024-12-26 23:52:15,841: Snapshot:3	Epoch:5	Loss:6.444	translation_Loss:4.426	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.018                                                   	MRR:19.09	Hits@10:35.51	Best:19.15
2024-12-26 23:52:23,388: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 19.15
2024-12-26 23:52:23,388: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:6.428 MRR:18.96 Best Results: 19.15
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:52:23,389: Snapshot:3	Epoch:6	Loss:6.428	translation_Loss:4.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.016                                                   	MRR:18.96	Hits@10:35.51	Best:19.15
2024-12-26 23:52:30,370: Snapshot:3	Epoch:7	Loss:36.143	translation_Loss:21.228	multi_layer_Loss:14.914	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.96	Hits@10:35.51	Best:19.15
2024-12-26 23:52:36,977: End of token training: 3 Epoch: 8 Loss:21.326 MRR:18.96 Best Results: 19.15
2024-12-26 23:52:36,977: Snapshot:3	Epoch:8	Loss:21.326	translation_Loss:21.22	multi_layer_Loss:0.106	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.96	Hits@10:35.51	Best:19.15
2024-12-26 23:52:37,309: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-26 23:52:48,790: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2431 | 0.1564 | 0.2812 | 0.3351 |  0.405  |
|     1      | 0.2141 | 0.1336 | 0.2494 | 0.3007 |  0.3666 |
|     2      | 0.2032 | 0.1214 | 0.2364 | 0.2896 |  0.3597 |
|     3      | 0.1909 | 0.1075 | 0.2208 | 0.278  |  0.3589 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:53:12,148: Snapshot:4	Epoch:0	Loss:4.221	translation_Loss:2.922	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.3                                                   	MRR:20.45	Hits@10:44.07	Best:20.45
2024-12-26 23:53:18,828: Snapshot:4	Epoch:1	Loss:2.969	translation_Loss:1.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.294                                                   	MRR:20.81	Hits@10:43.82	Best:20.81
2024-12-26 23:53:25,522: Snapshot:4	Epoch:2	Loss:2.806	translation_Loss:1.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.275                                                   	MRR:20.8	Hits@10:43.85	Best:20.81
2024-12-26 23:53:32,154: Snapshot:4	Epoch:3	Loss:2.767	translation_Loss:1.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.277                                                   	MRR:20.83	Hits@10:43.7	Best:20.83
2024-12-26 23:53:38,838: Snapshot:4	Epoch:4	Loss:2.766	translation_Loss:1.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.285                                                   	MRR:20.86	Hits@10:43.97	Best:20.86
2024-12-26 23:53:45,604: Snapshot:4	Epoch:5	Loss:2.759	translation_Loss:1.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.292                                                   	MRR:20.87	Hits@10:43.97	Best:20.87
2024-12-26 23:53:52,316: Snapshot:4	Epoch:6	Loss:2.754	translation_Loss:1.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.289                                                   	MRR:21.11	Hits@10:43.94	Best:21.11
2024-12-26 23:53:59,746: Snapshot:4	Epoch:7	Loss:2.765	translation_Loss:1.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.296                                                   	MRR:20.91	Hits@10:44.13	Best:21.11
2024-12-26 23:54:07,643: Snapshot:4	Epoch:8	Loss:2.774	translation_Loss:1.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:21.15	Hits@10:44.04	Best:21.15
2024-12-26 23:54:15,314: Snapshot:4	Epoch:9	Loss:2.768	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.305                                                   	MRR:20.98	Hits@10:43.92	Best:21.15
2024-12-26 23:54:23,028: Snapshot:4	Epoch:10	Loss:2.769	translation_Loss:1.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:21.01	Hits@10:43.9	Best:21.15
2024-12-26 23:54:30,675: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 21.15
2024-12-26 23:54:30,675: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:2.76 MRR:20.95 Best Results: 21.15
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-26 23:54:30,676: Snapshot:4	Epoch:11	Loss:2.76	translation_Loss:1.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.299                                                   	MRR:20.95	Hits@10:44.08	Best:21.15
2024-12-26 23:54:38,336: Snapshot:4	Epoch:12	Loss:32.434	translation_Loss:17.43	multi_layer_Loss:15.004	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.95	Hits@10:44.08	Best:21.15
2024-12-26 23:54:46,032: End of token training: 4 Epoch: 13 Loss:17.542 MRR:20.95 Best Results: 21.15
2024-12-26 23:54:46,032: Snapshot:4	Epoch:13	Loss:17.542	translation_Loss:17.434	multi_layer_Loss:0.108	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.95	Hits@10:44.08	Best:21.15
2024-12-26 23:54:46,373: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-26 23:55:02,267: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2334 | 0.1478 | 0.2707 | 0.323  |  0.3933 |
|     1      | 0.2046 | 0.1245 | 0.2389 | 0.2896 |  0.3571 |
|     2      | 0.1945 | 0.1145 | 0.224  | 0.2761 |  0.3521 |
|     3      | 0.1853 | 0.1004 | 0.2117 | 0.2705 |  0.3606 |
|     4      | 0.2114 | 0.1005 | 0.2485 | 0.3306 |  0.4405 |
+------------+--------+--------+--------+--------+---------+
2024-12-26 23:55:02,270: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1565 | 0.2822 | 0.332  |  0.3944 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2487 | 0.1624 | 0.291  | 0.3408 |  0.4053 |
|     1      | 0.2095 | 0.1306 | 0.2482 | 0.296  |  0.3527 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2485 | 0.161  | 0.2892 | 0.3422 |  0.4089 |
|     1      | 0.215  | 0.1348 | 0.2522 | 0.3012 |  0.3645 |
|     2      | 0.2012 | 0.1215 | 0.2366 | 0.2863 |  0.3496 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2431 | 0.1564 | 0.2812 | 0.3351 |  0.405  |
|     1      | 0.2141 | 0.1336 | 0.2494 | 0.3007 |  0.3666 |
|     2      | 0.2032 | 0.1214 | 0.2364 | 0.2896 |  0.3597 |
|     3      | 0.1909 | 0.1075 | 0.2208 | 0.278  |  0.3589 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2334 | 0.1478 | 0.2707 | 0.323  |  0.3933 |
|     1      | 0.2046 | 0.1245 | 0.2389 | 0.2896 |  0.3571 |
|     2      | 0.1945 | 0.1145 | 0.224  | 0.2761 |  0.3521 |
|     3      | 0.1853 | 0.1004 | 0.2117 | 0.2705 |  0.3606 |
|     4      | 0.2114 | 0.1005 | 0.2485 | 0.3306 |  0.4405 |
+------------+--------+--------+--------+--------+---------+]
2024-12-26 23:55:02,270: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 135.81203269958496 |   0.241   |    0.157     |    0.282     |     0.394     |
|    1     | 110.52778434753418 |   0.229   |    0.146     |     0.27     |     0.379     |
|    2     | 90.43572306632996  |   0.222   |    0.139     |    0.259     |     0.374     |
|    3     | 81.42225933074951  |   0.213   |     0.13     |    0.247     |     0.373     |
|    4     | 114.18943738937378 |   0.206   |    0.118     |    0.239     |     0.381     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-26 23:55:02,270: Sum_Training_Time:532.3872368335724
2024-12-26 23:55:02,270: Every_Training_Time:[135.81203269958496, 110.52778434753418, 90.43572306632996, 81.42225933074951, 114.18943738937378]
2024-12-26 23:55:02,270: Forward transfer: 0.166075 Backward transfer: -0.00632499999999999
2024-12-26 23:55:40,436: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241226235508/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-26 23:55:50,612: Snapshot:0	Epoch:0	Loss:24.869	translation_Loss:24.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.15	Hits@10:15.62	Best:7.15
2024-12-26 23:55:57,452: Snapshot:0	Epoch:1	Loss:16.859	translation_Loss:16.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.6	Hits@10:25.86	Best:11.6
2024-12-26 23:56:04,380: Snapshot:0	Epoch:2	Loss:11.571	translation_Loss:11.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.34	Hits@10:32.19	Best:15.34
2024-12-26 23:56:10,302: Snapshot:0	Epoch:3	Loss:7.838	translation_Loss:7.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.55	Hits@10:35.9	Best:18.55
2024-12-26 23:56:16,205: Snapshot:0	Epoch:4	Loss:5.22	translation_Loss:5.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.0	Hits@10:37.98	Best:21.0
2024-12-26 23:56:22,174: Snapshot:0	Epoch:5	Loss:3.479	translation_Loss:3.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.61	Hits@10:39.15	Best:22.61
2024-12-26 23:56:28,058: Snapshot:0	Epoch:6	Loss:2.325	translation_Loss:2.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.83	Hits@10:39.72	Best:23.83
2024-12-26 23:56:34,443: Snapshot:0	Epoch:7	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:40.03	Best:24.31
2024-12-26 23:56:40,573: Snapshot:0	Epoch:8	Loss:1.154	translation_Loss:1.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:40.2	Best:24.59
2024-12-26 23:56:46,435: Snapshot:0	Epoch:9	Loss:0.885	translation_Loss:0.885	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:40.27	Best:24.74
2024-12-26 23:56:52,422: Snapshot:0	Epoch:10	Loss:0.704	translation_Loss:0.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:40.31	Best:24.82
2024-12-26 23:56:58,271: Snapshot:0	Epoch:11	Loss:0.576	translation_Loss:0.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:40.35	Best:24.82
2024-12-26 23:57:04,111: Snapshot:0	Epoch:12	Loss:0.495	translation_Loss:0.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.79	Hits@10:40.31	Best:24.82
2024-12-26 23:57:10,443: Snapshot:0	Epoch:13	Loss:0.441	translation_Loss:0.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:40.52	Best:24.9
2024-12-26 23:57:16,331: Snapshot:0	Epoch:14	Loss:0.404	translation_Loss:0.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.85	Hits@10:40.48	Best:24.9
2024-12-26 23:57:23,072: Snapshot:0	Epoch:15	Loss:0.362	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:40.31	Best:24.9
2024-12-26 23:57:29,896: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 24.9
2024-12-26 23:57:29,896: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.329 MRR:24.84 Best Results: 24.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-26 23:57:29,897: Snapshot:0	Epoch:16	Loss:0.329	translation_Loss:0.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.84	Hits@10:40.38	Best:24.9
2024-12-26 23:57:37,298: Snapshot:0	Epoch:17	Loss:28.998	translation_Loss:18.325	multi_layer_Loss:10.673	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.84	Hits@10:40.38	Best:24.9
2024-12-26 23:57:44,689: End of token training: 0 Epoch: 18 Loss:18.409 MRR:24.84 Best Results: 24.9
2024-12-26 23:57:44,690: Snapshot:0	Epoch:18	Loss:18.409	translation_Loss:18.324	multi_layer_Loss:0.085	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.84	Hits@10:40.38	Best:24.9
2024-12-26 23:57:44,947: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-26 23:57:47,353: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2417 | 0.157  | 0.2823 | 0.3331 |  0.3936 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:58:12,537: Snapshot:1	Epoch:0	Loss:11.94	translation_Loss:10.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.543                                                   	MRR:20.39	Hits@10:34.27	Best:20.39
2024-12-26 23:58:19,327: Snapshot:1	Epoch:1	Loss:8.814	translation_Loss:6.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.217                                                   	MRR:21.43	Hits@10:35.75	Best:21.43
2024-12-26 23:58:26,840: Snapshot:1	Epoch:2	Loss:7.838	translation_Loss:5.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.229                                                   	MRR:21.81	Hits@10:36.13	Best:21.81
2024-12-26 23:58:33,441: Snapshot:1	Epoch:3	Loss:7.485	translation_Loss:5.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.242                                                   	MRR:21.94	Hits@10:36.41	Best:21.94
2024-12-26 23:58:39,835: Snapshot:1	Epoch:4	Loss:7.328	translation_Loss:5.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.227                                                   	MRR:21.91	Hits@10:36.43	Best:21.94
2024-12-26 23:58:46,216: Snapshot:1	Epoch:5	Loss:7.257	translation_Loss:5.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.228                                                   	MRR:21.95	Hits@10:36.29	Best:21.95
2024-12-26 23:58:53,144: Snapshot:1	Epoch:6	Loss:7.243	translation_Loss:5.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.225                                                   	MRR:21.89	Hits@10:36.42	Best:21.95
2024-12-26 23:59:00,507: Snapshot:1	Epoch:7	Loss:7.193	translation_Loss:4.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.227                                                   	MRR:21.94	Hits@10:36.39	Best:21.95
2024-12-26 23:59:07,902: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 21.95
2024-12-26 23:59:07,903: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:7.185 MRR:21.9 Best Results: 21.95
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-26 23:59:07,903: Snapshot:1	Epoch:8	Loss:7.185	translation_Loss:4.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.232                                                   	MRR:21.9	Hits@10:36.31	Best:21.95
2024-12-26 23:59:15,248: Snapshot:1	Epoch:9	Loss:31.849	translation_Loss:21.339	multi_layer_Loss:10.51	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.9	Hits@10:36.31	Best:21.95
2024-12-26 23:59:22,705: End of token training: 1 Epoch: 10 Loss:21.428 MRR:21.9 Best Results: 21.95
2024-12-26 23:59:22,705: Snapshot:1	Epoch:10	Loss:21.428	translation_Loss:21.338	multi_layer_Loss:0.09	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.9	Hits@10:36.31	Best:21.95
2024-12-26 23:59:23,040: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-26 23:59:28,847: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1649 | 0.2941 | 0.347  |  0.4125 |
|     1      | 0.2186 | 0.1379 | 0.2578 | 0.3056 |  0.3663 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-26 23:59:53,809: Snapshot:2	Epoch:0	Loss:8.628	translation_Loss:7.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.497                                                   	MRR:20.83	Hits@10:36.41	Best:20.83
2024-12-27 00:00:01,340: Snapshot:2	Epoch:1	Loss:6.333	translation_Loss:4.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.209                                                   	MRR:21.2	Hits@10:36.97	Best:21.2
2024-12-27 00:00:09,117: Snapshot:2	Epoch:2	Loss:6.007	translation_Loss:3.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.25                                                   	MRR:21.5	Hits@10:37.3	Best:21.5
2024-12-27 00:00:16,671: Snapshot:2	Epoch:3	Loss:5.931	translation_Loss:3.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.307                                                   	MRR:21.39	Hits@10:37.22	Best:21.5
2024-12-27 00:00:23,427: Snapshot:2	Epoch:4	Loss:5.93	translation_Loss:3.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.327                                                   	MRR:21.45	Hits@10:37.27	Best:21.5
2024-12-27 00:00:30,351: Early Stopping! Snapshot: 2 Epoch: 5 Best Results: 21.5
2024-12-27 00:00:30,351: Start to training tokens! Snapshot: 2 Epoch: 5 Loss:5.903 MRR:21.48 Best Results: 21.5
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 00:00:30,351: Snapshot:2	Epoch:5	Loss:5.903	translation_Loss:3.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.34                                                   	MRR:21.48	Hits@10:37.27	Best:21.5
2024-12-27 00:00:36,835: Snapshot:2	Epoch:6	Loss:31.85	translation_Loss:20.958	multi_layer_Loss:10.891	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.48	Hits@10:37.27	Best:21.5
2024-12-27 00:00:43,337: End of token training: 2 Epoch: 7 Loss:21.066 MRR:21.48 Best Results: 21.5
2024-12-27 00:00:43,338: Snapshot:2	Epoch:7	Loss:21.066	translation_Loss:20.977	multi_layer_Loss:0.088	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.48	Hits@10:37.27	Best:21.5
2024-12-27 00:00:43,581: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-27 00:00:51,690: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2504 | 0.1624 | 0.2907 | 0.3435 |  0.4134 |
|     1      | 0.2262 | 0.1446 | 0.2618 | 0.3139 |  0.3814 |
|     2      | 0.2173 | 0.1356 | 0.2522 | 0.3031 |  0.3715 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 00:01:14,477: Snapshot:3	Epoch:0	Loss:5.292	translation_Loss:4.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.262                                                   	MRR:20.01	Hits@10:38.0	Best:20.01
2024-12-27 00:01:21,666: Snapshot:3	Epoch:1	Loss:3.696	translation_Loss:2.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.649                                                   	MRR:20.01	Hits@10:37.96	Best:20.01
2024-12-27 00:01:29,511: Snapshot:3	Epoch:2	Loss:3.624	translation_Loss:1.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.647                                                   	MRR:19.96	Hits@10:37.89	Best:20.01
2024-12-27 00:01:37,067: Snapshot:3	Epoch:3	Loss:3.616	translation_Loss:1.915	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.701                                                   	MRR:20.03	Hits@10:38.21	Best:20.03
2024-12-27 00:01:44,701: Snapshot:3	Epoch:4	Loss:3.629	translation_Loss:1.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.721                                                   	MRR:20.06	Hits@10:38.03	Best:20.06
2024-12-27 00:01:52,349: Snapshot:3	Epoch:5	Loss:3.644	translation_Loss:1.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.74                                                   	MRR:20.14	Hits@10:37.85	Best:20.14
2024-12-27 00:01:59,889: Snapshot:3	Epoch:6	Loss:3.654	translation_Loss:1.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.747                                                   	MRR:20.11	Hits@10:38.11	Best:20.14
2024-12-27 00:02:07,823: Snapshot:3	Epoch:7	Loss:3.644	translation_Loss:1.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:20.17	Hits@10:38.21	Best:20.17
2024-12-27 00:02:15,321: Snapshot:3	Epoch:8	Loss:3.653	translation_Loss:1.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.75                                                   	MRR:20.01	Hits@10:38.17	Best:20.17
2024-12-27 00:02:22,860: Snapshot:3	Epoch:9	Loss:3.661	translation_Loss:1.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.772                                                   	MRR:20.2	Hits@10:38.34	Best:20.2
2024-12-27 00:02:29,703: Snapshot:3	Epoch:10	Loss:3.665	translation_Loss:1.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.767                                                   	MRR:20.25	Hits@10:38.07	Best:20.25
2024-12-27 00:02:37,402: Snapshot:3	Epoch:11	Loss:3.662	translation_Loss:1.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.775                                                   	MRR:20.05	Hits@10:38.07	Best:20.25
2024-12-27 00:02:45,291: Snapshot:3	Epoch:12	Loss:3.654	translation_Loss:1.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.763                                                   	MRR:20.1	Hits@10:37.97	Best:20.25
2024-12-27 00:02:52,776: Early Stopping! Snapshot: 3 Epoch: 13 Best Results: 20.25
2024-12-27 00:02:52,777: Start to training tokens! Snapshot: 3 Epoch: 13 Loss:3.676 MRR:20.05 Best Results: 20.25
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 00:02:52,777: Snapshot:3	Epoch:13	Loss:3.676	translation_Loss:1.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.775                                                   	MRR:20.05	Hits@10:38.05	Best:20.25
2024-12-27 00:03:00,237: Snapshot:3	Epoch:14	Loss:29.22	translation_Loss:19.858	multi_layer_Loss:9.362	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.05	Hits@10:38.05	Best:20.25
2024-12-27 00:03:07,765: End of token training: 3 Epoch: 15 Loss:19.95 MRR:20.05 Best Results: 20.25
2024-12-27 00:03:07,765: Snapshot:3	Epoch:15	Loss:19.95	translation_Loss:19.873	multi_layer_Loss:0.077	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.05	Hits@10:38.05	Best:20.25
2024-12-27 00:03:08,000: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-27 00:03:20,468: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2383 | 0.1526 | 0.2757 | 0.3275 |  0.399  |
|     1      | 0.2181 | 0.1378 | 0.2491 | 0.3017 |  0.3724 |
|     2      | 0.2121 | 0.1295 | 0.2421 | 0.3002 |  0.3757 |
|     3      | 0.2022 | 0.1114 | 0.2335 | 0.2967 |  0.3838 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 00:03:46,729: Snapshot:4	Epoch:0	Loss:2.535	translation_Loss:1.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.723                                                   	MRR:21.88	Hits@10:46.01	Best:21.88
2024-12-27 00:03:54,657: Snapshot:4	Epoch:1	Loss:1.332	translation_Loss:0.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.685                                                   	MRR:21.93	Hits@10:46.0	Best:21.93
2024-12-27 00:04:01,317: Snapshot:4	Epoch:2	Loss:1.225	translation_Loss:0.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.668                                                   	MRR:21.88	Hits@10:45.37	Best:21.93
2024-12-27 00:04:08,113: Snapshot:4	Epoch:3	Loss:1.183	translation_Loss:0.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.673                                                   	MRR:22.0	Hits@10:45.78	Best:22.0
2024-12-27 00:04:14,790: Snapshot:4	Epoch:4	Loss:1.198	translation_Loss:0.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.683                                                   	MRR:21.81	Hits@10:45.92	Best:22.0
2024-12-27 00:04:21,437: Snapshot:4	Epoch:5	Loss:1.195	translation_Loss:0.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.69                                                   	MRR:21.81	Hits@10:45.63	Best:22.0
2024-12-27 00:04:28,351: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 22.0
2024-12-27 00:04:28,352: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:1.196 MRR:21.98 Best Results: 22.0
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 00:04:28,352: Snapshot:4	Epoch:6	Loss:1.196	translation_Loss:0.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.693                                                   	MRR:21.98	Hits@10:45.63	Best:22.0
2024-12-27 00:04:35,017: Snapshot:4	Epoch:7	Loss:26.323	translation_Loss:16.541	multi_layer_Loss:9.781	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.98	Hits@10:45.63	Best:22.0
2024-12-27 00:04:41,572: End of token training: 4 Epoch: 8 Loss:16.633 MRR:21.98 Best Results: 22.0
2024-12-27 00:04:41,572: Snapshot:4	Epoch:8	Loss:16.633	translation_Loss:16.552	multi_layer_Loss:0.081	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.98	Hits@10:45.63	Best:22.0
2024-12-27 00:04:41,803: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-27 00:04:56,485: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2249 | 0.1407 | 0.2599 | 0.3108 |  0.3828 |
|     1      | 0.2046 | 0.1269 | 0.2334 | 0.2857 |  0.3566 |
|     2      | 0.1993 | 0.1178 | 0.2285 | 0.2834 |  0.3613 |
|     3      | 0.1906 | 0.1023 | 0.216  | 0.2806 |  0.3753 |
|     4      | 0.2211 | 0.1059 | 0.2589 | 0.3451 |  0.4585 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 00:04:56,488: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2417 | 0.157  | 0.2823 | 0.3331 |  0.3936 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1649 | 0.2941 | 0.347  |  0.4125 |
|     1      | 0.2186 | 0.1379 | 0.2578 | 0.3056 |  0.3663 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2504 | 0.1624 | 0.2907 | 0.3435 |  0.4134 |
|     1      | 0.2262 | 0.1446 | 0.2618 | 0.3139 |  0.3814 |
|     2      | 0.2173 | 0.1356 | 0.2522 | 0.3031 |  0.3715 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2383 | 0.1526 | 0.2757 | 0.3275 |  0.399  |
|     1      | 0.2181 | 0.1378 | 0.2491 | 0.3017 |  0.3724 |
|     2      | 0.2121 | 0.1295 | 0.2421 | 0.3002 |  0.3757 |
|     3      | 0.2022 | 0.1114 | 0.2335 | 0.2967 |  0.3838 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2249 | 0.1407 | 0.2599 | 0.3108 |  0.3828 |
|     1      | 0.2046 | 0.1269 | 0.2334 | 0.2857 |  0.3566 |
|     2      | 0.1993 | 0.1178 | 0.2285 | 0.2834 |  0.3613 |
|     3      | 0.1906 | 0.1023 | 0.216  | 0.2806 |  0.3753 |
|     4      | 0.2211 | 0.1059 | 0.2589 | 0.3451 |  0.4585 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 00:04:56,488: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 124.25281572341919 |   0.242   |    0.157     |    0.282     |     0.394     |
|    1     | 92.04294538497925  |   0.235   |    0.151     |    0.276     |     0.389     |
|    2     | 71.30225086212158  |   0.231   |    0.148     |    0.268     |     0.389     |
|    3     | 133.1089813709259  |   0.218   |    0.133     |     0.25     |     0.383     |
|    4     |  77.4409327507019  |   0.208   |    0.119     |    0.239     |     0.387     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 00:04:56,488: Sum_Training_Time:498.1479260921478
2024-12-27 00:04:56,488: Every_Training_Time:[124.25281572341919, 92.04294538497925, 71.30225086212158, 133.1089813709259, 77.4409327507019]
2024-12-27 00:04:56,488: Forward transfer: 0.17472500000000002 Backward transfer: -0.015099999999999995
2024-12-27 00:05:33,153: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227000500/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 00:05:43,406: Snapshot:0	Epoch:0	Loss:24.869	translation_Loss:24.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.15	Hits@10:15.62	Best:7.15
2024-12-27 00:05:50,257: Snapshot:0	Epoch:1	Loss:16.859	translation_Loss:16.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.61	Hits@10:25.86	Best:11.61
2024-12-27 00:05:57,523: Snapshot:0	Epoch:2	Loss:11.571	translation_Loss:11.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.34	Hits@10:32.18	Best:15.34
2024-12-27 00:06:04,429: Snapshot:0	Epoch:3	Loss:7.838	translation_Loss:7.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.53	Hits@10:35.94	Best:18.53
2024-12-27 00:06:11,327: Snapshot:0	Epoch:4	Loss:5.219	translation_Loss:5.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.01	Hits@10:38.05	Best:21.01
2024-12-27 00:06:18,158: Snapshot:0	Epoch:5	Loss:3.48	translation_Loss:3.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:39.22	Best:22.63
2024-12-27 00:06:24,979: Snapshot:0	Epoch:6	Loss:2.326	translation_Loss:2.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.8	Hits@10:39.8	Best:23.8
2024-12-27 00:06:32,421: Snapshot:0	Epoch:7	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:40.16	Best:24.33
2024-12-27 00:06:39,329: Snapshot:0	Epoch:8	Loss:1.153	translation_Loss:1.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.58	Hits@10:40.29	Best:24.58
2024-12-27 00:06:46,180: Snapshot:0	Epoch:9	Loss:0.881	translation_Loss:0.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:40.19	Best:24.72
2024-12-27 00:06:53,051: Snapshot:0	Epoch:10	Loss:0.702	translation_Loss:0.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.38	Best:24.78
2024-12-27 00:06:59,927: Snapshot:0	Epoch:11	Loss:0.576	translation_Loss:0.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:40.52	Best:24.78
2024-12-27 00:07:06,709: Snapshot:0	Epoch:12	Loss:0.498	translation_Loss:0.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:40.47	Best:24.8
2024-12-27 00:07:13,092: Snapshot:0	Epoch:13	Loss:0.441	translation_Loss:0.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:40.62	Best:24.91
2024-12-27 00:07:18,979: Snapshot:0	Epoch:14	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.48	Best:24.91
2024-12-27 00:07:24,827: Snapshot:0	Epoch:15	Loss:0.36	translation_Loss:0.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.37	Best:24.91
2024-12-27 00:07:30,936: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 24.91
2024-12-27 00:07:30,936: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.328 MRR:24.84 Best Results: 24.91
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 00:07:30,936: Snapshot:0	Epoch:16	Loss:0.328	translation_Loss:0.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.84	Hits@10:40.46	Best:24.91
2024-12-27 00:07:38,383: Snapshot:0	Epoch:17	Loss:35.887	translation_Loss:18.333	multi_layer_Loss:17.554	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.84	Hits@10:40.46	Best:24.91
2024-12-27 00:07:45,650: End of token training: 0 Epoch: 18 Loss:18.445 MRR:24.84 Best Results: 24.91
2024-12-27 00:07:45,650: Snapshot:0	Epoch:18	Loss:18.445	translation_Loss:18.331	multi_layer_Loss:0.114	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.84	Hits@10:40.46	Best:24.91
2024-12-27 00:07:45,910: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-27 00:07:48,444: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1554 | 0.2819 | 0.3314 |  0.3934 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 00:08:14,238: Snapshot:1	Epoch:0	Loss:12.814	translation_Loss:10.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.898                                                   	MRR:19.29	Hits@10:32.22	Best:19.29
2024-12-27 00:08:21,689: Snapshot:1	Epoch:1	Loss:11.061	translation_Loss:9.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.83                                                   	MRR:20.11	Hits@10:33.51	Best:20.11
2024-12-27 00:08:28,915: Snapshot:1	Epoch:2	Loss:10.034	translation_Loss:8.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.777                                                   	MRR:20.48	Hits@10:33.95	Best:20.48
2024-12-27 00:08:35,390: Snapshot:1	Epoch:3	Loss:9.631	translation_Loss:7.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.738                                                   	MRR:20.65	Hits@10:34.17	Best:20.65
2024-12-27 00:08:41,731: Snapshot:1	Epoch:4	Loss:9.465	translation_Loss:7.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.722                                                   	MRR:20.64	Hits@10:34.28	Best:20.65
2024-12-27 00:08:48,171: Snapshot:1	Epoch:5	Loss:9.375	translation_Loss:7.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.713                                                   	MRR:20.75	Hits@10:34.34	Best:20.75
2024-12-27 00:08:54,455: Snapshot:1	Epoch:6	Loss:9.347	translation_Loss:7.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.71                                                   	MRR:20.74	Hits@10:34.26	Best:20.75
2024-12-27 00:09:00,784: Snapshot:1	Epoch:7	Loss:9.294	translation_Loss:7.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.708                                                   	MRR:20.74	Hits@10:34.17	Best:20.75
2024-12-27 00:09:07,168: Snapshot:1	Epoch:8	Loss:9.282	translation_Loss:7.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.71                                                   	MRR:20.82	Hits@10:34.23	Best:20.82
2024-12-27 00:09:13,537: Snapshot:1	Epoch:9	Loss:9.271	translation_Loss:7.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.708                                                   	MRR:20.77	Hits@10:34.15	Best:20.82
2024-12-27 00:09:20,199: Snapshot:1	Epoch:10	Loss:9.264	translation_Loss:7.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.707                                                   	MRR:20.74	Hits@10:34.24	Best:20.82
2024-12-27 00:09:27,508: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 20.82
2024-12-27 00:09:27,508: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:9.235 MRR:20.71 Best Results: 20.82
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 00:09:27,508: Snapshot:1	Epoch:11	Loss:9.235	translation_Loss:7.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:20.71	Hits@10:34.15	Best:20.82
2024-12-27 00:09:34,762: Snapshot:1	Epoch:12	Loss:39.49	translation_Loss:22.375	multi_layer_Loss:17.115	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.71	Hits@10:34.15	Best:20.82
2024-12-27 00:09:42,515: End of token training: 1 Epoch: 13 Loss:22.479 MRR:20.71 Best Results: 20.82
2024-12-27 00:09:42,516: Snapshot:1	Epoch:13	Loss:22.479	translation_Loss:22.369	multi_layer_Loss:0.11	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.71	Hits@10:34.15	Best:20.82
2024-12-27 00:09:42,782: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-27 00:09:48,716: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2473 | 0.1635 | 0.288  | 0.3362 |  0.3993 |
|     1      | 0.2041 | 0.1262 | 0.2435 | 0.2879 |  0.3454 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 00:10:13,987: Snapshot:2	Epoch:0	Loss:10.99	translation_Loss:8.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.096                                                   	MRR:19.02	Hits@10:33.14	Best:19.02
2024-12-27 00:10:21,962: Snapshot:2	Epoch:1	Loss:10.342	translation_Loss:8.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.989                                                   	MRR:19.38	Hits@10:33.49	Best:19.38
2024-12-27 00:10:29,496: Snapshot:2	Epoch:2	Loss:9.876	translation_Loss:7.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.918                                                   	MRR:19.36	Hits@10:33.55	Best:19.38
2024-12-27 00:10:37,038: Snapshot:2	Epoch:3	Loss:9.752	translation_Loss:7.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.909                                                   	MRR:19.33	Hits@10:33.59	Best:19.38
2024-12-27 00:10:44,203: Snapshot:2	Epoch:4	Loss:9.725	translation_Loss:7.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.901                                                   	MRR:19.48	Hits@10:33.59	Best:19.48
2024-12-27 00:10:50,707: Snapshot:2	Epoch:5	Loss:9.722	translation_Loss:7.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.907                                                   	MRR:19.43	Hits@10:33.59	Best:19.48
2024-12-27 00:10:57,224: Snapshot:2	Epoch:6	Loss:9.696	translation_Loss:7.791	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.905                                                   	MRR:19.42	Hits@10:33.55	Best:19.48
2024-12-27 00:11:05,209: Early Stopping! Snapshot: 2 Epoch: 7 Best Results: 19.48
2024-12-27 00:11:05,210: Start to training tokens! Snapshot: 2 Epoch: 7 Loss:9.693 MRR:19.41 Best Results: 19.48
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 00:11:05,210: Snapshot:2	Epoch:7	Loss:9.693	translation_Loss:7.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.907                                                   	MRR:19.41	Hits@10:33.5	Best:19.48
2024-12-27 00:11:12,737: Snapshot:2	Epoch:8	Loss:40.699	translation_Loss:22.727	multi_layer_Loss:17.971	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.41	Hits@10:33.5	Best:19.48
2024-12-27 00:11:20,200: End of token training: 2 Epoch: 9 Loss:22.858 MRR:19.41 Best Results: 19.48
2024-12-27 00:11:20,201: Snapshot:2	Epoch:9	Loss:22.858	translation_Loss:22.744	multi_layer_Loss:0.115	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.41	Hits@10:33.5	Best:19.48
2024-12-27 00:11:20,462: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-27 00:11:29,477: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2464 | 0.1599 | 0.2881 | 0.3379 |  0.4038 |
|     1      | 0.2089 | 0.1296 | 0.2479 | 0.2928 |  0.3527 |
|     2      | 0.1931 | 0.1157 | 0.2279 | 0.2753 |  0.3353 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 00:11:54,987: Snapshot:3	Epoch:0	Loss:9.112	translation_Loss:6.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.139                                                   	MRR:17.69	Hits@10:32.81	Best:17.69
2024-12-27 00:12:02,634: Snapshot:3	Epoch:1	Loss:8.745	translation_Loss:6.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.027                                                   	MRR:17.95	Hits@10:33.14	Best:17.95
2024-12-27 00:12:10,595: Snapshot:3	Epoch:2	Loss:8.506	translation_Loss:6.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.962                                                   	MRR:17.97	Hits@10:33.24	Best:17.97
2024-12-27 00:12:18,203: Snapshot:3	Epoch:3	Loss:8.471	translation_Loss:6.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.966                                                   	MRR:18.06	Hits@10:33.28	Best:18.06
2024-12-27 00:12:25,768: Snapshot:3	Epoch:4	Loss:8.472	translation_Loss:6.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.97                                                   	MRR:18.04	Hits@10:33.2	Best:18.06
2024-12-27 00:12:33,347: Snapshot:3	Epoch:5	Loss:8.462	translation_Loss:6.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.976                                                   	MRR:17.97	Hits@10:33.22	Best:18.06
2024-12-27 00:12:40,877: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 18.06
2024-12-27 00:12:40,877: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:8.464 MRR:18.02 Best Results: 18.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 00:12:40,877: Snapshot:3	Epoch:6	Loss:8.464	translation_Loss:6.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.982                                                   	MRR:18.02	Hits@10:33.17	Best:18.06
2024-12-27 00:12:48,824: Snapshot:3	Epoch:7	Loss:39.875	translation_Loss:22.187	multi_layer_Loss:17.688	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.02	Hits@10:33.17	Best:18.06
2024-12-27 00:12:56,305: End of token training: 3 Epoch: 8 Loss:22.297 MRR:18.02 Best Results: 18.06
2024-12-27 00:12:56,305: Snapshot:3	Epoch:8	Loss:22.297	translation_Loss:22.184	multi_layer_Loss:0.114	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.02	Hits@10:33.17	Best:18.06
2024-12-27 00:12:56,586: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-27 00:13:08,892: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1574 | 0.2846 | 0.3362 |  0.4011 |
|     1      | 0.2091 | 0.1294 | 0.2465 | 0.2933 |  0.3557 |
|     2      | 0.1957 | 0.1162 | 0.2302 | 0.2822 |  0.3451 |
|     3      | 0.1794 | 0.1008 | 0.209  | 0.261  |  0.332  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 00:13:34,613: Snapshot:4	Epoch:0	Loss:6.165	translation_Loss:4.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.849                                                   	MRR:18.96	Hits@10:39.65	Best:18.96
2024-12-27 00:13:42,256: Snapshot:4	Epoch:1	Loss:5.07	translation_Loss:3.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.849                                                   	MRR:19.51	Hits@10:39.98	Best:19.51
2024-12-27 00:13:49,924: Snapshot:4	Epoch:2	Loss:4.841	translation_Loss:3.036	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.805                                                   	MRR:19.52	Hits@10:40.26	Best:19.52
2024-12-27 00:13:57,555: Snapshot:4	Epoch:3	Loss:4.81	translation_Loss:3.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.807                                                   	MRR:19.58	Hits@10:40.23	Best:19.58
2024-12-27 00:14:05,217: Snapshot:4	Epoch:4	Loss:4.801	translation_Loss:2.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.814                                                   	MRR:19.59	Hits@10:40.15	Best:19.59
2024-12-27 00:14:12,917: Snapshot:4	Epoch:5	Loss:4.808	translation_Loss:2.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.823                                                   	MRR:19.6	Hits@10:40.4	Best:19.6
2024-12-27 00:14:19,835: Snapshot:4	Epoch:6	Loss:4.804	translation_Loss:2.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.823                                                   	MRR:19.64	Hits@10:40.28	Best:19.64
2024-12-27 00:14:26,800: Snapshot:4	Epoch:7	Loss:4.807	translation_Loss:2.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.829                                                   	MRR:19.66	Hits@10:40.52	Best:19.66
2024-12-27 00:14:33,478: Snapshot:4	Epoch:8	Loss:4.783	translation_Loss:2.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.822                                                   	MRR:19.61	Hits@10:40.27	Best:19.66
2024-12-27 00:14:40,115: Snapshot:4	Epoch:9	Loss:4.786	translation_Loss:2.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.826                                                   	MRR:19.66	Hits@10:40.23	Best:19.66
2024-12-27 00:14:46,856: Snapshot:4	Epoch:10	Loss:4.794	translation_Loss:2.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.817                                                   	MRR:19.74	Hits@10:40.31	Best:19.74
2024-12-27 00:14:53,546: Snapshot:4	Epoch:11	Loss:4.785	translation_Loss:2.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.825                                                   	MRR:19.56	Hits@10:40.31	Best:19.74
2024-12-27 00:15:00,726: Snapshot:4	Epoch:12	Loss:4.795	translation_Loss:2.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.828                                                   	MRR:19.57	Hits@10:40.32	Best:19.74
2024-12-27 00:15:07,428: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 19.74
2024-12-27 00:15:07,428: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:4.792 MRR:19.52 Best Results: 19.74
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 00:15:07,428: Snapshot:4	Epoch:13	Loss:4.792	translation_Loss:2.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.83                                                   	MRR:19.52	Hits@10:40.28	Best:19.74
2024-12-27 00:15:14,686: Snapshot:4	Epoch:14	Loss:35.805	translation_Loss:18.491	multi_layer_Loss:17.314	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.52	Hits@10:40.28	Best:19.74
2024-12-27 00:15:22,276: End of token training: 4 Epoch: 15 Loss:18.606 MRR:19.52 Best Results: 19.74
2024-12-27 00:15:22,276: Snapshot:4	Epoch:15	Loss:18.606	translation_Loss:18.496	multi_layer_Loss:0.11	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.52	Hits@10:40.28	Best:19.74
2024-12-27 00:15:22,620: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-27 00:15:38,358: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.238  | 0.1532 | 0.2773 | 0.3271 |  0.3957 |
|     1      | 0.2054 | 0.1263 | 0.2401 | 0.2896 |  0.3511 |
|     2      | 0.1931 | 0.1153 | 0.2247 | 0.2755 |  0.3429 |
|     3      | 0.1781 | 0.0977 | 0.2038 | 0.2617 |  0.3378 |
|     4      | 0.1994 | 0.099  | 0.2323 | 0.3072 |  0.4055 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 00:15:38,361: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1554 | 0.2819 | 0.3314 |  0.3934 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2473 | 0.1635 | 0.288  | 0.3362 |  0.3993 |
|     1      | 0.2041 | 0.1262 | 0.2435 | 0.2879 |  0.3454 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2464 | 0.1599 | 0.2881 | 0.3379 |  0.4038 |
|     1      | 0.2089 | 0.1296 | 0.2479 | 0.2928 |  0.3527 |
|     2      | 0.1931 | 0.1157 | 0.2279 | 0.2753 |  0.3353 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1574 | 0.2846 | 0.3362 |  0.4011 |
|     1      | 0.2091 | 0.1294 | 0.2465 | 0.2933 |  0.3557 |
|     2      | 0.1957 | 0.1162 | 0.2302 | 0.2822 |  0.3451 |
|     3      | 0.1794 | 0.1008 | 0.209  | 0.261  |  0.332  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.238  | 0.1532 | 0.2773 | 0.3271 |  0.3957 |
|     1      | 0.2054 | 0.1263 | 0.2401 | 0.2896 |  0.3511 |
|     2      | 0.1931 | 0.1153 | 0.2247 | 0.2755 |  0.3429 |
|     3      | 0.1781 | 0.0977 | 0.2038 | 0.2617 |  0.3378 |
|     4      | 0.1994 | 0.099  | 0.2323 | 0.3072 |  0.4055 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 00:15:38,361: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 132.4963505268097  |   0.241   |    0.155     |    0.282     |     0.393     |
|    1     | 110.74014472961426 |   0.226   |    0.145     |    0.266     |     0.372     |
|    2     | 88.26287245750427  |   0.216   |    0.135     |    0.255     |     0.364     |
|    3     | 83.59750938415527  |   0.207   |    0.126     |    0.243     |     0.358     |
|    4     | 130.08431005477905 |   0.203   |    0.118     |    0.236     |     0.367     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 00:15:38,361: Sum_Training_Time:545.1811871528625
2024-12-27 00:15:38,361: Every_Training_Time:[132.4963505268097, 110.74014472961426, 88.26287245750427, 83.59750938415527, 130.08431005477905]
2024-12-27 00:15:38,361: Forward transfer: 0.15995 Backward transfer: -0.000675000000000002
