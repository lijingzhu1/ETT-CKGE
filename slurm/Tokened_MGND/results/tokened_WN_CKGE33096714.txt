2025-01-07 21:01:20,583: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107210106/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=1111, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 21:01:29,398: Snapshot:0	Epoch:0	Loss:15.33	translation_Loss:15.33	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.6	Hits@10:4.29	Best:1.6
2025-01-07 21:01:36,746: Snapshot:0	Epoch:1	Loss:8.279	translation_Loss:8.279	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:5.53	Hits@10:14.89	Best:5.53
2025-01-07 21:01:44,373: Snapshot:0	Epoch:2	Loss:3.806	translation_Loss:3.806	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.69	Hits@10:27.29	Best:10.69
2025-01-07 21:01:52,040: Snapshot:0	Epoch:3	Loss:1.582	translation_Loss:1.582	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.13	Hits@10:33.17	Best:13.13
2025-01-07 21:01:59,405: Snapshot:0	Epoch:4	Loss:0.847	translation_Loss:0.847	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.22	Hits@10:35.52	Best:14.22
2025-01-07 21:02:06,995: Snapshot:0	Epoch:5	Loss:0.519	translation_Loss:0.519	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.8	Hits@10:36.7	Best:14.8
2025-01-07 21:02:14,368: Snapshot:0	Epoch:6	Loss:0.343	translation_Loss:0.343	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.1	Hits@10:37.52	Best:15.1
2025-01-07 21:02:21,968: Snapshot:0	Epoch:7	Loss:0.249	translation_Loss:0.249	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.32	Hits@10:38.07	Best:15.32
2025-01-07 21:02:29,382: Snapshot:0	Epoch:8	Loss:0.177	translation_Loss:0.177	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.52	Hits@10:38.4	Best:15.52
2025-01-07 21:02:37,028: Snapshot:0	Epoch:9	Loss:0.138	translation_Loss:0.138	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.59	Hits@10:38.63	Best:15.59
2025-01-07 21:02:44,662: Snapshot:0	Epoch:10	Loss:0.11	translation_Loss:0.11	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.71	Hits@10:38.99	Best:15.71
2025-01-07 21:02:52,080: Snapshot:0	Epoch:11	Loss:0.086	translation_Loss:0.086	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.75	Hits@10:39.18	Best:15.75
2025-01-07 21:02:59,666: Snapshot:0	Epoch:12	Loss:0.077	translation_Loss:0.077	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.8	Hits@10:39.25	Best:15.8
2025-01-07 21:03:07,330: Snapshot:0	Epoch:13	Loss:0.066	translation_Loss:0.066	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.84	Hits@10:39.31	Best:15.84
2025-01-07 21:03:15,960: Snapshot:0	Epoch:14	Loss:0.059	translation_Loss:0.059	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.85	Hits@10:39.4	Best:15.85
2025-01-07 21:03:23,514: Snapshot:0	Epoch:15	Loss:0.049	translation_Loss:0.049	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.92	Hits@10:39.61	Best:15.92
2025-01-07 21:03:30,899: Snapshot:0	Epoch:16	Loss:0.045	translation_Loss:0.045	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.64	Best:15.96
2025-01-07 21:03:38,556: Snapshot:0	Epoch:17	Loss:0.042	translation_Loss:0.042	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.93	Best:15.98
2025-01-07 21:03:46,052: Snapshot:0	Epoch:18	Loss:0.04	translation_Loss:0.04	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.99	Hits@10:40.07	Best:15.99
2025-01-07 21:03:54,526: Snapshot:0	Epoch:19	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.97	Hits@10:40.16	Best:15.99
2025-01-07 21:04:01,832: Snapshot:0	Epoch:20	Loss:0.036	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:40.25	Best:16.03
2025-01-07 21:04:09,421: Snapshot:0	Epoch:21	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.09	Hits@10:40.3	Best:16.09
2025-01-07 21:04:16,807: Snapshot:0	Epoch:22	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.46	Best:16.12
2025-01-07 21:04:24,354: Snapshot:0	Epoch:23	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.11	Hits@10:40.55	Best:16.12
2025-01-07 21:04:32,002: Snapshot:0	Epoch:24	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.65	Best:16.14
2025-01-07 21:04:39,342: Snapshot:0	Epoch:25	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.56	Best:16.14
2025-01-07 21:04:46,907: Snapshot:0	Epoch:26	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.11	Hits@10:40.59	Best:16.14
2025-01-07 21:04:54,306: Snapshot:0	Epoch:27	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.61	Best:16.18
2025-01-07 21:05:02,849: Snapshot:0	Epoch:28	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.17	Hits@10:40.64	Best:16.18
2025-01-07 21:05:11,522: Snapshot:0	Epoch:29	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.75	Best:16.21
2025-01-07 21:05:19,025: Snapshot:0	Epoch:30	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.87	Best:16.21
2025-01-07 21:05:26,787: Snapshot:0	Epoch:31	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.24	Hits@10:40.86	Best:16.24
2025-01-07 21:05:34,431: Snapshot:0	Epoch:32	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.26	Hits@10:40.94	Best:16.26
2025-01-07 21:05:42,174: Snapshot:0	Epoch:33	Loss:0.022	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.94	Best:16.26
2025-01-07 21:05:49,629: Snapshot:0	Epoch:34	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.24	Hits@10:40.96	Best:16.26
2025-01-07 21:05:57,299: Early Stopping! Snapshot: 0 Epoch: 35 Best Results: 16.26
2025-01-07 21:05:57,299: Start to training tokens! Snapshot: 0 Epoch: 35 Loss:0.023 MRR:16.2 Best Results: 16.26
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:05:57,300: Snapshot:0	Epoch:35	Loss:0.023	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.2	Hits@10:40.95	Best:16.26
2025-01-07 21:06:05,308: Snapshot:0	Epoch:36	Loss:21.828	translation_Loss:5.535	token_training_loss:16.293	distillation_Loss:0.0                                                   	MRR:16.2	Hits@10:40.95	Best:16.26
2025-01-07 21:06:13,099: End of token training: 0 Epoch: 37 Loss:6.019 MRR:16.2 Best Results: 16.26
2025-01-07 21:06:13,099: Snapshot:0	Epoch:37	Loss:6.019	translation_Loss:5.525	token_training_loss:0.494	distillation_Loss:0.0                                                           	MRR:16.2	Hits@10:40.95	Best:16.26
2025-01-07 21:06:13,203: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 21:06:17,255: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.0058 | 0.2939 | 0.3551 |  0.4087 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,920,600
Trainable params: 2,800
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:06:21,296: Snapshot:1	Epoch:0	Loss:2.731	translation_Loss:2.648	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:2.61	Hits@10:6.69	Best:2.61
2025-01-07 21:06:22,979: Snapshot:1	Epoch:1	Loss:1.754	translation_Loss:1.608	token_training_loss:0.0	distillation_Loss:0.146                                                   	MRR:7.47	Hits@10:19.41	Best:7.47
2025-01-07 21:06:24,624: Snapshot:1	Epoch:2	Loss:0.987	translation_Loss:0.799	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:10.58	Hits@10:26.64	Best:10.58
2025-01-07 21:06:26,127: Snapshot:1	Epoch:3	Loss:0.539	translation_Loss:0.3	token_training_loss:0.0	distillation_Loss:0.238                                                   	MRR:12.21	Hits@10:30.11	Best:12.21
2025-01-07 21:06:27,678: Snapshot:1	Epoch:4	Loss:0.38	translation_Loss:0.115	token_training_loss:0.0	distillation_Loss:0.265                                                   	MRR:13.04	Hits@10:31.4	Best:13.04
2025-01-07 21:06:29,205: Snapshot:1	Epoch:5	Loss:0.309	translation_Loss:0.051	token_training_loss:0.0	distillation_Loss:0.259                                                   	MRR:13.49	Hits@10:32.5	Best:13.49
2025-01-07 21:06:30,939: Snapshot:1	Epoch:6	Loss:0.254	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.228                                                   	MRR:13.58	Hits@10:32.63	Best:13.58
2025-01-07 21:06:32,471: Snapshot:1	Epoch:7	Loss:0.204	translation_Loss:0.016	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:13.69	Hits@10:33.06	Best:13.69
2025-01-07 21:06:33,916: Snapshot:1	Epoch:8	Loss:0.162	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:13.62	Hits@10:33.25	Best:13.69
2025-01-07 21:06:35,370: Snapshot:1	Epoch:9	Loss:0.127	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:13.61	Hits@10:33.39	Best:13.69
2025-01-07 21:06:36,807: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.69
2025-01-07 21:06:36,807: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.101 MRR:13.57 Best Results: 13.69
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:06:36,808: Snapshot:1	Epoch:10	Loss:0.101	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.088                                                   	MRR:13.57	Hits@10:33.28	Best:13.69
2025-01-07 21:06:38,223: Snapshot:1	Epoch:11	Loss:8.24	translation_Loss:1.206	token_training_loss:7.034	distillation_Loss:0.0                                                   	MRR:13.57	Hits@10:33.28	Best:13.69
2025-01-07 21:06:39,642: End of token training: 1 Epoch: 12 Loss:5.623 MRR:13.57 Best Results: 13.69
2025-01-07 21:06:39,642: Snapshot:1	Epoch:12	Loss:5.623	translation_Loss:1.206	token_training_loss:4.417	distillation_Loss:0.0                                                           	MRR:13.57	Hits@10:33.28	Best:13.69
2025-01-07 21:06:39,717: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 21:06:44,549: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1651 | 0.0064 | 0.2931 | 0.3598 |  0.415  |
|     1      |  0.14  | 0.0046 |  0.25  | 0.2919 |  0.3347 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,739,200
Trainable params: 2,800
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:06:48,504: Snapshot:2	Epoch:0	Loss:2.657	translation_Loss:2.608	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:2.81	Hits@10:7.66	Best:2.81
2025-01-07 21:06:50,176: Snapshot:2	Epoch:1	Loss:1.652	translation_Loss:1.524	token_training_loss:0.0	distillation_Loss:0.128                                                   	MRR:7.61	Hits@10:19.22	Best:7.61
2025-01-07 21:06:51,827: Snapshot:2	Epoch:2	Loss:0.849	translation_Loss:0.692	token_training_loss:0.0	distillation_Loss:0.157                                                   	MRR:10.6	Hits@10:25.62	Best:10.6
2025-01-07 21:06:53,487: Snapshot:2	Epoch:3	Loss:0.402	translation_Loss:0.228	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:12.2	Hits@10:28.6	Best:12.2
2025-01-07 21:06:55,218: Snapshot:2	Epoch:4	Loss:0.268	translation_Loss:0.08	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:13.1	Hits@10:30.11	Best:13.1
2025-01-07 21:06:56,903: Snapshot:2	Epoch:5	Loss:0.226	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.191                                                   	MRR:13.48	Hits@10:30.94	Best:13.48
2025-01-07 21:06:58,564: Snapshot:2	Epoch:6	Loss:0.198	translation_Loss:0.018	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:13.64	Hits@10:31.51	Best:13.64
2025-01-07 21:07:00,192: Snapshot:2	Epoch:7	Loss:0.174	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.162                                                   	MRR:13.7	Hits@10:31.75	Best:13.7
2025-01-07 21:07:01,746: Snapshot:2	Epoch:8	Loss:0.149	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.141                                                   	MRR:13.67	Hits@10:31.64	Best:13.7
2025-01-07 21:07:03,581: Snapshot:2	Epoch:9	Loss:0.126	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.119                                                   	MRR:13.65	Hits@10:31.64	Best:13.7
2025-01-07 21:07:05,128: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 13.7
2025-01-07 21:07:05,128: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:0.105 MRR:13.55 Best Results: 13.7
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:07:05,129: Snapshot:2	Epoch:10	Loss:0.105	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.098                                                   	MRR:13.55	Hits@10:31.64	Best:13.7
2025-01-07 21:07:06,653: Snapshot:2	Epoch:11	Loss:8.336	translation_Loss:1.2	token_training_loss:7.136	distillation_Loss:0.0                                                   	MRR:13.55	Hits@10:31.64	Best:13.7
2025-01-07 21:07:08,185: End of token training: 2 Epoch: 12 Loss:5.781 MRR:13.55 Best Results: 13.7
2025-01-07 21:07:08,185: Snapshot:2	Epoch:12	Loss:5.781	translation_Loss:1.199	token_training_loss:4.582	distillation_Loss:0.0                                                           	MRR:13.55	Hits@10:31.64	Best:13.7
2025-01-07 21:07:08,278: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 21:07:13,998: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1627 | 0.007  | 0.2858 | 0.3554 |  0.4155 |
|     1      | 0.1376 | 0.0062 | 0.2417 | 0.2868 |  0.3347 |
|     2      | 0.1394 | 0.0054 | 0.2532 | 0.2863 |  0.3272 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,000
Trainable params: 2,800
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:07:18,168: Snapshot:3	Epoch:0	Loss:2.638	translation_Loss:2.556	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:2.94	Hits@10:7.5	Best:2.94
2025-01-07 21:07:19,944: Snapshot:3	Epoch:1	Loss:1.62	translation_Loss:1.447	token_training_loss:0.0	distillation_Loss:0.173                                                   	MRR:7.53	Hits@10:18.71	Best:7.53
2025-01-07 21:07:21,719: Snapshot:3	Epoch:2	Loss:0.824	translation_Loss:0.623	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:10.79	Hits@10:25.32	Best:10.79
2025-01-07 21:07:23,513: Snapshot:3	Epoch:3	Loss:0.431	translation_Loss:0.198	token_training_loss:0.0	distillation_Loss:0.233                                                   	MRR:12.23	Hits@10:27.98	Best:12.23
2025-01-07 21:07:25,296: Snapshot:3	Epoch:4	Loss:0.322	translation_Loss:0.071	token_training_loss:0.0	distillation_Loss:0.251                                                   	MRR:12.98	Hits@10:29.3	Best:12.98
2025-01-07 21:07:27,060: Snapshot:3	Epoch:5	Loss:0.275	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.242                                                   	MRR:13.25	Hits@10:29.92	Best:13.25
2025-01-07 21:07:28,846: Snapshot:3	Epoch:6	Loss:0.234	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.213                                                   	MRR:13.41	Hits@10:30.3	Best:13.41
2025-01-07 21:07:30,866: Snapshot:3	Epoch:7	Loss:0.191	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:13.49	Hits@10:30.62	Best:13.49
2025-01-07 21:07:32,522: Snapshot:3	Epoch:8	Loss:0.154	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.143                                                   	MRR:13.48	Hits@10:30.59	Best:13.49
2025-01-07 21:07:34,168: Snapshot:3	Epoch:9	Loss:0.123	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.112                                                   	MRR:13.49	Hits@10:30.81	Best:13.49
2025-01-07 21:07:35,965: Snapshot:3	Epoch:10	Loss:0.098	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:13.53	Hits@10:30.78	Best:13.53
2025-01-07 21:07:37,782: Snapshot:3	Epoch:11	Loss:0.081	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:13.57	Hits@10:30.91	Best:13.57
2025-01-07 21:07:39,457: Snapshot:3	Epoch:12	Loss:0.068	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.058                                                   	MRR:13.49	Hits@10:31.13	Best:13.57
2025-01-07 21:07:41,289: Snapshot:3	Epoch:13	Loss:0.061	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.05                                                   	MRR:13.48	Hits@10:31.32	Best:13.57
2025-01-07 21:07:43,277: Snapshot:3	Epoch:14	Loss:0.055	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.045                                                   	MRR:13.6	Hits@10:31.37	Best:13.6
2025-01-07 21:07:45,212: Snapshot:3	Epoch:15	Loss:0.05	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.041                                                   	MRR:13.63	Hits@10:31.42	Best:13.63
2025-01-07 21:07:47,033: Snapshot:3	Epoch:16	Loss:0.047	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.038                                                   	MRR:13.63	Hits@10:31.24	Best:13.63
2025-01-07 21:07:49,004: Snapshot:3	Epoch:17	Loss:0.046	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.036                                                   	MRR:13.68	Hits@10:31.34	Best:13.68
2025-01-07 21:07:51,078: Snapshot:3	Epoch:18	Loss:0.043	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.034                                                   	MRR:13.67	Hits@10:31.26	Best:13.68
2025-01-07 21:07:52,893: Snapshot:3	Epoch:19	Loss:0.041	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.033                                                   	MRR:13.74	Hits@10:31.42	Best:13.74
2025-01-07 21:07:54,697: Snapshot:3	Epoch:20	Loss:0.039	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.031                                                   	MRR:13.76	Hits@10:31.48	Best:13.76
2025-01-07 21:07:56,384: Snapshot:3	Epoch:21	Loss:0.038	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.03                                                   	MRR:13.72	Hits@10:31.64	Best:13.76
2025-01-07 21:07:58,040: Snapshot:3	Epoch:22	Loss:0.037	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:13.7	Hits@10:31.67	Best:13.76
2025-01-07 21:07:59,708: Early Stopping! Snapshot: 3 Epoch: 23 Best Results: 13.76
2025-01-07 21:07:59,708: Start to training tokens! Snapshot: 3 Epoch: 23 Loss:0.036 MRR:13.66 Best Results: 13.76
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:07:59,709: Snapshot:3	Epoch:23	Loss:0.036	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.029                                                   	MRR:13.66	Hits@10:31.67	Best:13.76
2025-01-07 21:08:01,346: Snapshot:3	Epoch:24	Loss:8.152	translation_Loss:1.176	token_training_loss:6.976	distillation_Loss:0.0                                                   	MRR:13.66	Hits@10:31.67	Best:13.76
2025-01-07 21:08:02,973: End of token training: 3 Epoch: 25 Loss:5.63 MRR:13.66 Best Results: 13.76
2025-01-07 21:08:02,973: Snapshot:3	Epoch:25	Loss:5.63	translation_Loss:1.179	token_training_loss:4.452	distillation_Loss:0.0                                                           	MRR:13.66	Hits@10:31.67	Best:13.76
2025-01-07 21:08:03,045: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 21:08:09,981: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1625 | 0.0066 | 0.2869 | 0.355  |  0.4152 |
|     1      | 0.1371 | 0.0062 | 0.2401 | 0.2874 |  0.3339 |
|     2      | 0.1387 | 0.0054 | 0.2522 | 0.2884 |  0.3261 |
|     3      | 0.1374 | 0.0075 | 0.2414 | 0.2855 |  0.3247 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,800
Trainable params: 2,800
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:08:14,346: Snapshot:4	Epoch:0	Loss:2.6	translation_Loss:2.519	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:9.94	Hits@10:24.46	Best:9.94
2025-01-07 21:08:16,248: Snapshot:4	Epoch:1	Loss:1.568	translation_Loss:1.388	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:11.81	Hits@10:28.92	Best:11.81
2025-01-07 21:08:18,189: Snapshot:4	Epoch:2	Loss:0.758	translation_Loss:0.554	token_training_loss:0.0	distillation_Loss:0.205                                                   	MRR:13.07	Hits@10:31.32	Best:13.07
2025-01-07 21:08:20,108: Snapshot:4	Epoch:3	Loss:0.38	translation_Loss:0.156	token_training_loss:0.0	distillation_Loss:0.224                                                   	MRR:13.6	Hits@10:32.04	Best:13.6
2025-01-07 21:08:21,998: Snapshot:4	Epoch:4	Loss:0.294	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.234                                                   	MRR:13.92	Hits@10:32.63	Best:13.92
2025-01-07 21:08:23,905: Snapshot:4	Epoch:5	Loss:0.253	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.223                                                   	MRR:14.03	Hits@10:32.82	Best:14.03
2025-01-07 21:08:25,997: Snapshot:4	Epoch:6	Loss:0.215	translation_Loss:0.018	token_training_loss:0.0	distillation_Loss:0.196                                                   	MRR:14.05	Hits@10:33.17	Best:14.05
2025-01-07 21:08:27,934: Snapshot:4	Epoch:7	Loss:0.178	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.166                                                   	MRR:14.08	Hits@10:33.44	Best:14.08
2025-01-07 21:08:29,869: Snapshot:4	Epoch:8	Loss:0.144	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.135                                                   	MRR:14.1	Hits@10:33.31	Best:14.1
2025-01-07 21:08:31,995: Snapshot:4	Epoch:9	Loss:0.115	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.107                                                   	MRR:14.15	Hits@10:33.39	Best:14.15
2025-01-07 21:08:33,784: Snapshot:4	Epoch:10	Loss:0.094	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.084                                                   	MRR:14.11	Hits@10:33.36	Best:14.15
2025-01-07 21:08:35,553: Snapshot:4	Epoch:11	Loss:0.077	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:14.14	Hits@10:33.44	Best:14.15
2025-01-07 21:08:37,311: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 14.15
2025-01-07 21:08:37,311: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:0.064 MRR:14.11 Best Results: 14.15
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:08:37,311: Snapshot:4	Epoch:12	Loss:0.064	translation_Loss:0.008	token_training_loss:0.0	distillation_Loss:0.056                                                   	MRR:14.11	Hits@10:33.44	Best:14.15
2025-01-07 21:08:39,044: Snapshot:4	Epoch:13	Loss:8.531	translation_Loss:1.182	token_training_loss:7.349	distillation_Loss:0.0                                                   	MRR:14.11	Hits@10:33.44	Best:14.15
2025-01-07 21:08:40,800: End of token training: 4 Epoch: 14 Loss:5.985 MRR:14.11 Best Results: 14.15
2025-01-07 21:08:40,801: Snapshot:4	Epoch:14	Loss:5.985	translation_Loss:1.178	token_training_loss:4.806	distillation_Loss:0.0                                                           	MRR:14.11	Hits@10:33.44	Best:14.15
2025-01-07 21:08:40,872: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 21:08:48,816: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1615 | 0.006  | 0.2848 | 0.3539 |  0.414  |
|     1      | 0.1364 | 0.0059 | 0.2387 | 0.2836 |  0.332  |
|     2      | 0.1382 | 0.0062 | 0.2516 | 0.2882 |  0.3282 |
|     3      | 0.1399 | 0.0089 | 0.2481 | 0.2874 |  0.3296 |
|     4      | 0.1386 | 0.0089 | 0.2445 | 0.295  |  0.3383 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,800
Trainable params: 2,800
Non-trainable params: 8,193,000
=================================================================
2025-01-07 21:08:48,818: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1639 | 0.0058 | 0.2939 | 0.3551 |  0.4087 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1651 | 0.0064 | 0.2931 | 0.3598 |  0.415  |
|     1      |  0.14  | 0.0046 |  0.25  | 0.2919 |  0.3347 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1627 | 0.007  | 0.2858 | 0.3554 |  0.4155 |
|     1      | 0.1376 | 0.0062 | 0.2417 | 0.2868 |  0.3347 |
|     2      | 0.1394 | 0.0054 | 0.2532 | 0.2863 |  0.3272 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1625 | 0.0066 | 0.2869 | 0.355  |  0.4152 |
|     1      | 0.1371 | 0.0062 | 0.2401 | 0.2874 |  0.3339 |
|     2      | 0.1387 | 0.0054 | 0.2522 | 0.2884 |  0.3261 |
|     3      | 0.1374 | 0.0075 | 0.2414 | 0.2855 |  0.3247 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1615 | 0.006  | 0.2848 | 0.3539 |  0.414  |
|     1      | 0.1364 | 0.0059 | 0.2387 | 0.2836 |  0.332  |
|     2      | 0.1382 | 0.0062 | 0.2516 | 0.2882 |  0.3282 |
|     3      | 0.1399 | 0.0089 | 0.2481 | 0.2874 |  0.3296 |
|     4      | 0.1386 | 0.0089 | 0.2445 | 0.295  |  0.3383 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 21:08:48,819: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 292.5154983997345  |   0.164   |    0.006     |    0.294     |     0.409     |
|    1     | 21.275272846221924 |   0.162   |    0.006     |    0.287     |     0.404     |
|    2     |  22.7080340385437  |   0.157   |    0.007     |    0.276     |     0.394     |
|    3     | 47.749698638916016 |   0.154   |    0.007     |    0.273     |     0.386     |
|    4     | 29.701406478881836 |   0.152   |    0.007     |    0.269     |     0.381     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 21:08:48,819: Sum_Training_Time:413.949910402298
2025-01-07 21:08:48,819: Every_Training_Time:[292.5154983997345, 21.275272846221924, 22.7080340385437, 47.749698638916016, 29.701406478881836]
2025-01-07 21:08:48,819: Forward transfer: 0.060475 Backward transfer: -0.0011750000000000024
2025-01-07 21:09:02,026: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107210852/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=2222, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 21:09:11,762: Snapshot:0	Epoch:0	Loss:15.323	translation_Loss:15.323	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.65	Hits@10:4.48	Best:1.65
2025-01-07 21:09:19,240: Snapshot:0	Epoch:1	Loss:8.283	translation_Loss:8.283	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:5.74	Hits@10:15.23	Best:5.74
2025-01-07 21:09:26,900: Snapshot:0	Epoch:2	Loss:3.854	translation_Loss:3.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.42	Hits@10:26.75	Best:10.42
2025-01-07 21:09:34,589: Snapshot:0	Epoch:3	Loss:1.642	translation_Loss:1.642	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:12.82	Hits@10:32.65	Best:12.82
2025-01-07 21:09:42,007: Snapshot:0	Epoch:4	Loss:0.884	translation_Loss:0.884	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.98	Hits@10:35.25	Best:13.98
2025-01-07 21:09:49,691: Snapshot:0	Epoch:5	Loss:0.544	translation_Loss:0.544	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.62	Hits@10:36.61	Best:14.62
2025-01-07 21:09:57,159: Snapshot:0	Epoch:6	Loss:0.355	translation_Loss:0.355	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.89	Hits@10:37.41	Best:14.89
2025-01-07 21:10:05,451: Snapshot:0	Epoch:7	Loss:0.248	translation_Loss:0.248	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.17	Hits@10:37.88	Best:15.17
2025-01-07 21:10:13,776: Snapshot:0	Epoch:8	Loss:0.182	translation_Loss:0.182	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.33	Hits@10:38.1	Best:15.33
2025-01-07 21:10:21,466: Snapshot:0	Epoch:9	Loss:0.14	translation_Loss:0.14	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.46	Hits@10:38.47	Best:15.46
2025-01-07 21:10:29,179: Snapshot:0	Epoch:10	Loss:0.113	translation_Loss:0.113	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.59	Hits@10:38.78	Best:15.59
2025-01-07 21:10:36,642: Snapshot:0	Epoch:11	Loss:0.092	translation_Loss:0.092	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.63	Hits@10:38.96	Best:15.63
2025-01-07 21:10:44,352: Snapshot:0	Epoch:12	Loss:0.077	translation_Loss:0.077	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.74	Hits@10:39.09	Best:15.74
2025-01-07 21:10:51,799: Snapshot:0	Epoch:13	Loss:0.07	translation_Loss:0.07	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.78	Hits@10:39.25	Best:15.78
2025-01-07 21:10:59,526: Snapshot:0	Epoch:14	Loss:0.06	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.87	Hits@10:39.38	Best:15.87
2025-01-07 21:11:07,228: Snapshot:0	Epoch:15	Loss:0.05	translation_Loss:0.05	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.95	Hits@10:39.49	Best:15.95
2025-01-07 21:11:14,833: Snapshot:0	Epoch:16	Loss:0.046	translation_Loss:0.046	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.68	Best:15.98
2025-01-07 21:11:22,582: Snapshot:0	Epoch:17	Loss:0.042	translation_Loss:0.042	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.0	Hits@10:39.74	Best:16.0
2025-01-07 21:11:30,627: Snapshot:0	Epoch:18	Loss:0.042	translation_Loss:0.042	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.03	Hits@10:39.89	Best:16.03
2025-01-07 21:11:39,225: Snapshot:0	Epoch:19	Loss:0.038	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.07	Hits@10:40.08	Best:16.07
2025-01-07 21:11:46,758: Snapshot:0	Epoch:20	Loss:0.036	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.11	Hits@10:40.2	Best:16.11
2025-01-07 21:11:54,515: Snapshot:0	Epoch:21	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.33	Best:16.19
2025-01-07 21:12:02,014: Snapshot:0	Epoch:22	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.22	Hits@10:40.4	Best:16.22
2025-01-07 21:12:09,732: Snapshot:0	Epoch:23	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.44	Best:16.23
2025-01-07 21:12:17,413: Snapshot:0	Epoch:24	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.41	Best:16.23
2025-01-07 21:12:24,898: Snapshot:0	Epoch:25	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.62	Best:16.25
2025-01-07 21:12:32,619: Snapshot:0	Epoch:26	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.68	Best:16.25
2025-01-07 21:12:41,016: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.3	Hits@10:40.82	Best:16.3
2025-01-07 21:12:49,189: Snapshot:0	Epoch:28	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.31	Hits@10:40.84	Best:16.31
2025-01-07 21:12:56,918: Snapshot:0	Epoch:29	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.32	Hits@10:40.85	Best:16.32
2025-01-07 21:13:04,412: Snapshot:0	Epoch:30	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.33	Hits@10:40.88	Best:16.33
2025-01-07 21:13:12,068: Snapshot:0	Epoch:31	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.28	Hits@10:40.87	Best:16.33
2025-01-07 21:13:19,527: Snapshot:0	Epoch:32	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.26	Hits@10:40.99	Best:16.33
2025-01-07 21:13:27,227: Early Stopping! Snapshot: 0 Epoch: 33 Best Results: 16.33
2025-01-07 21:13:27,228: Start to training tokens! Snapshot: 0 Epoch: 33 Loss:0.025 MRR:16.24 Best Results: 16.33
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:13:27,228: Snapshot:0	Epoch:33	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.24	Hits@10:40.95	Best:16.33
2025-01-07 21:13:35,236: Snapshot:0	Epoch:34	Loss:21.933	translation_Loss:5.548	token_training_loss:16.385	distillation_Loss:0.0                                                   	MRR:16.24	Hits@10:40.95	Best:16.33
2025-01-07 21:13:43,030: End of token training: 0 Epoch: 35 Loss:6.084 MRR:16.24 Best Results: 16.33
2025-01-07 21:13:43,031: Snapshot:0	Epoch:35	Loss:6.084	translation_Loss:5.546	token_training_loss:0.538	distillation_Loss:0.0                                                           	MRR:16.24	Hits@10:40.95	Best:16.33
2025-01-07 21:13:43,126: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 21:13:46,909: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1642 | 0.0059 | 0.2945 | 0.3576 |  0.4109 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,920,600
Trainable params: 2,800
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:13:50,759: Snapshot:1	Epoch:0	Loss:2.735	translation_Loss:2.651	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:2.42	Hits@10:6.37	Best:2.42
2025-01-07 21:13:52,452: Snapshot:1	Epoch:1	Loss:1.762	translation_Loss:1.615	token_training_loss:0.0	distillation_Loss:0.148                                                   	MRR:7.18	Hits@10:18.92	Best:7.18
2025-01-07 21:13:54,150: Snapshot:1	Epoch:2	Loss:0.997	translation_Loss:0.808	token_training_loss:0.0	distillation_Loss:0.189                                                   	MRR:10.38	Hits@10:26.45	Best:10.38
2025-01-07 21:13:55,837: Snapshot:1	Epoch:3	Loss:0.547	translation_Loss:0.308	token_training_loss:0.0	distillation_Loss:0.239                                                   	MRR:12.08	Hits@10:29.97	Best:12.08
2025-01-07 21:13:57,523: Snapshot:1	Epoch:4	Loss:0.386	translation_Loss:0.12	token_training_loss:0.0	distillation_Loss:0.266                                                   	MRR:13.05	Hits@10:31.88	Best:13.05
2025-01-07 21:13:59,478: Snapshot:1	Epoch:5	Loss:0.315	translation_Loss:0.054	token_training_loss:0.0	distillation_Loss:0.26                                                   	MRR:13.48	Hits@10:32.72	Best:13.48
2025-01-07 21:14:01,156: Snapshot:1	Epoch:6	Loss:0.258	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.23                                                   	MRR:13.62	Hits@10:33.17	Best:13.62
2025-01-07 21:14:02,714: Snapshot:1	Epoch:7	Loss:0.209	translation_Loss:0.018	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:13.65	Hits@10:33.33	Best:13.65
2025-01-07 21:14:04,146: Snapshot:1	Epoch:8	Loss:0.166	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.151                                                   	MRR:13.64	Hits@10:33.55	Best:13.65
2025-01-07 21:14:05,604: Snapshot:1	Epoch:9	Loss:0.131	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.117                                                   	MRR:13.61	Hits@10:33.66	Best:13.65
2025-01-07 21:14:07,038: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.65
2025-01-07 21:14:07,038: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.106 MRR:13.6 Best Results: 13.65
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:14:07,038: Snapshot:1	Epoch:10	Loss:0.106	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.091                                                   	MRR:13.6	Hits@10:33.76	Best:13.65
2025-01-07 21:14:08,473: Snapshot:1	Epoch:11	Loss:8.276	translation_Loss:1.185	token_training_loss:7.091	distillation_Loss:0.0                                                   	MRR:13.6	Hits@10:33.76	Best:13.65
2025-01-07 21:14:09,880: End of token training: 1 Epoch: 12 Loss:5.747 MRR:13.6 Best Results: 13.65
2025-01-07 21:14:09,880: Snapshot:1	Epoch:12	Loss:5.747	translation_Loss:1.187	token_training_loss:4.56	distillation_Loss:0.0                                                           	MRR:13.6	Hits@10:33.76	Best:13.65
2025-01-07 21:14:09,951: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 21:14:14,791: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1653 | 0.007  | 0.2945 | 0.3617 |  0.4162 |
|     1      | 0.1378 | 0.0035 |  0.25  | 0.2887 |  0.3309 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,739,200
Trainable params: 2,800
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:14:18,695: Snapshot:2	Epoch:0	Loss:2.653	translation_Loss:2.604	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:2.83	Hits@10:8.09	Best:2.83
2025-01-07 21:14:20,370: Snapshot:2	Epoch:1	Loss:1.656	translation_Loss:1.527	token_training_loss:0.0	distillation_Loss:0.129                                                   	MRR:7.35	Hits@10:18.98	Best:7.35
2025-01-07 21:14:22,040: Snapshot:2	Epoch:2	Loss:0.854	translation_Loss:0.695	token_training_loss:0.0	distillation_Loss:0.158                                                   	MRR:10.66	Hits@10:25.81	Best:10.66
2025-01-07 21:14:23,705: Snapshot:2	Epoch:3	Loss:0.405	translation_Loss:0.23	token_training_loss:0.0	distillation_Loss:0.175                                                   	MRR:12.17	Hits@10:28.92	Best:12.17
2025-01-07 21:14:25,392: Snapshot:2	Epoch:4	Loss:0.27	translation_Loss:0.08	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:13.01	Hits@10:30.03	Best:13.01
2025-01-07 21:14:27,068: Snapshot:2	Epoch:5	Loss:0.227	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:13.24	Hits@10:30.43	Best:13.24
2025-01-07 21:14:28,733: Snapshot:2	Epoch:6	Loss:0.2	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.181                                                   	MRR:13.4	Hits@10:30.78	Best:13.4
2025-01-07 21:14:30,400: Snapshot:2	Epoch:7	Loss:0.176	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.163                                                   	MRR:13.49	Hits@10:31.34	Best:13.49
2025-01-07 21:14:32,303: Snapshot:2	Epoch:8	Loss:0.151	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.142                                                   	MRR:13.5	Hits@10:31.37	Best:13.5
2025-01-07 21:14:33,987: Snapshot:2	Epoch:9	Loss:0.127	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.12                                                   	MRR:13.53	Hits@10:31.4	Best:13.53
2025-01-07 21:14:35,549: Snapshot:2	Epoch:10	Loss:0.106	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.1                                                   	MRR:13.48	Hits@10:31.48	Best:13.53
2025-01-07 21:14:37,119: Snapshot:2	Epoch:11	Loss:0.088	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:13.41	Hits@10:31.32	Best:13.53
2025-01-07 21:14:38,704: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 13.53
2025-01-07 21:14:38,705: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:0.074 MRR:13.35 Best Results: 13.53
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:14:38,705: Snapshot:2	Epoch:12	Loss:0.074	translation_Loss:0.006	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:13.35	Hits@10:31.32	Best:13.53
2025-01-07 21:14:40,241: Snapshot:2	Epoch:13	Loss:8.167	translation_Loss:1.154	token_training_loss:7.013	distillation_Loss:0.0                                                   	MRR:13.35	Hits@10:31.32	Best:13.53
2025-01-07 21:14:41,774: End of token training: 2 Epoch: 14 Loss:5.601 MRR:13.35 Best Results: 13.53
2025-01-07 21:14:41,775: Snapshot:2	Epoch:14	Loss:5.601	translation_Loss:1.158	token_training_loss:4.443	distillation_Loss:0.0                                                           	MRR:13.35	Hits@10:31.32	Best:13.53
2025-01-07 21:14:41,847: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 21:14:47,590: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1637 | 0.007  | 0.288  | 0.3598 |  0.4174 |
|     1      | 0.137  | 0.0048 | 0.2427 | 0.2855 |  0.3304 |
|     2      | 0.1407 | 0.0062 | 0.254  | 0.2909 |  0.3274 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,000
Trainable params: 2,800
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:14:51,709: Snapshot:3	Epoch:0	Loss:2.645	translation_Loss:2.563	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:2.78	Hits@10:7.12	Best:2.78
2025-01-07 21:14:53,483: Snapshot:3	Epoch:1	Loss:1.626	translation_Loss:1.454	token_training_loss:0.0	distillation_Loss:0.173                                                   	MRR:7.41	Hits@10:18.76	Best:7.41
2025-01-07 21:14:55,254: Snapshot:3	Epoch:2	Loss:0.831	translation_Loss:0.63	token_training_loss:0.0	distillation_Loss:0.201                                                   	MRR:10.61	Hits@10:25.3	Best:10.61
2025-01-07 21:14:57,342: Snapshot:3	Epoch:3	Loss:0.433	translation_Loss:0.201	token_training_loss:0.0	distillation_Loss:0.233                                                   	MRR:12.07	Hits@10:27.88	Best:12.07
2025-01-07 21:14:59,280: Snapshot:3	Epoch:4	Loss:0.326	translation_Loss:0.075	token_training_loss:0.0	distillation_Loss:0.251                                                   	MRR:12.89	Hits@10:29.3	Best:12.89
2025-01-07 21:15:01,242: Snapshot:3	Epoch:5	Loss:0.277	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.242                                                   	MRR:13.2	Hits@10:29.87	Best:13.2
2025-01-07 21:15:03,196: Snapshot:3	Epoch:6	Loss:0.234	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.214                                                   	MRR:13.3	Hits@10:30.19	Best:13.3
2025-01-07 21:15:05,175: Snapshot:3	Epoch:7	Loss:0.194	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:13.39	Hits@10:30.38	Best:13.39
2025-01-07 21:15:07,218: Snapshot:3	Epoch:8	Loss:0.156	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:13.4	Hits@10:30.67	Best:13.4
2025-01-07 21:15:08,962: Snapshot:3	Epoch:9	Loss:0.124	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:13.33	Hits@10:30.86	Best:13.4
2025-01-07 21:15:10,584: Snapshot:3	Epoch:10	Loss:0.1	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.088                                                   	MRR:13.34	Hits@10:30.91	Best:13.4
2025-01-07 21:15:12,224: Early Stopping! Snapshot: 3 Epoch: 11 Best Results: 13.4
2025-01-07 21:15:12,225: Start to training tokens! Snapshot: 3 Epoch: 11 Loss:0.082 MRR:13.33 Best Results: 13.4
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:15:12,225: Snapshot:3	Epoch:11	Loss:0.082	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:13.33	Hits@10:30.97	Best:13.4
2025-01-07 21:15:13,849: Snapshot:3	Epoch:12	Loss:8.334	translation_Loss:1.177	token_training_loss:7.157	distillation_Loss:0.0                                                   	MRR:13.33	Hits@10:30.97	Best:13.4
2025-01-07 21:15:15,558: End of token training: 3 Epoch: 13 Loss:5.771 MRR:13.33 Best Results: 13.4
2025-01-07 21:15:15,558: Snapshot:3	Epoch:13	Loss:5.771	translation_Loss:1.181	token_training_loss:4.591	distillation_Loss:0.0                                                           	MRR:13.33	Hits@10:30.97	Best:13.4
2025-01-07 21:15:15,683: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 21:15:22,685: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1629 | 0.0076 | 0.2853 | 0.3575 |  0.4152 |
|     1      | 0.1355 | 0.0048 | 0.2409 | 0.2847 |  0.3277 |
|     2      | 0.1389 | 0.0067 | 0.2478 | 0.2879 |  0.3258 |
|     3      | 0.1348 | 0.007  | 0.2382 | 0.279  |  0.3177 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,800
Trainable params: 2,800
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:15:27,055: Snapshot:4	Epoch:0	Loss:2.582	translation_Loss:2.5	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:10.11	Hits@10:24.46	Best:10.11
2025-01-07 21:15:28,990: Snapshot:4	Epoch:1	Loss:1.568	translation_Loss:1.388	token_training_loss:0.0	distillation_Loss:0.18                                                   	MRR:11.98	Hits@10:28.58	Best:11.98
2025-01-07 21:15:30,912: Snapshot:4	Epoch:2	Loss:0.769	translation_Loss:0.565	token_training_loss:0.0	distillation_Loss:0.205                                                   	MRR:12.98	Hits@10:30.91	Best:12.98
2025-01-07 21:15:32,859: Snapshot:4	Epoch:3	Loss:0.386	translation_Loss:0.161	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:13.51	Hits@10:31.83	Best:13.51
2025-01-07 21:15:34,994: Snapshot:4	Epoch:4	Loss:0.297	translation_Loss:0.062	token_training_loss:0.0	distillation_Loss:0.235                                                   	MRR:13.78	Hits@10:32.18	Best:13.78
2025-01-07 21:15:37,075: Snapshot:4	Epoch:5	Loss:0.258	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.225                                                   	MRR:13.91	Hits@10:32.69	Best:13.91
2025-01-07 21:15:38,996: Snapshot:4	Epoch:6	Loss:0.221	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.198                                                   	MRR:14.03	Hits@10:32.98	Best:14.03
2025-01-07 21:15:40,924: Snapshot:4	Epoch:7	Loss:0.182	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.168                                                   	MRR:14.14	Hits@10:33.28	Best:14.14
2025-01-07 21:15:42,866: Snapshot:4	Epoch:8	Loss:0.149	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:14.16	Hits@10:33.39	Best:14.16
2025-01-07 21:15:44,778: Snapshot:4	Epoch:9	Loss:0.12	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.11                                                   	MRR:14.19	Hits@10:33.41	Best:14.19
2025-01-07 21:15:46,527: Snapshot:4	Epoch:10	Loss:0.097	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.087                                                   	MRR:14.17	Hits@10:33.28	Best:14.19
2025-01-07 21:15:48,284: Snapshot:4	Epoch:11	Loss:0.08	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.07                                                   	MRR:14.15	Hits@10:33.41	Best:14.19
2025-01-07 21:15:50,031: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 14.19
2025-01-07 21:15:50,031: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:0.069 MRR:14.19 Best Results: 14.19
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:15:50,032: Snapshot:4	Epoch:12	Loss:0.069	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.058                                                   	MRR:14.19	Hits@10:33.28	Best:14.19
2025-01-07 21:15:51,777: Snapshot:4	Epoch:13	Loss:8.403	translation_Loss:1.157	token_training_loss:7.246	distillation_Loss:0.0                                                   	MRR:14.19	Hits@10:33.28	Best:14.19
2025-01-07 21:15:53,509: End of token training: 4 Epoch: 14 Loss:5.855 MRR:14.19 Best Results: 14.19
2025-01-07 21:15:53,509: Snapshot:4	Epoch:14	Loss:5.855	translation_Loss:1.157	token_training_loss:4.697	distillation_Loss:0.0                                                           	MRR:14.19	Hits@10:33.28	Best:14.19
2025-01-07 21:15:53,583: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 21:16:01,532: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1621 | 0.0073 | 0.2842 | 0.3554 |  0.4129 |
|     1      | 0.1338 | 0.004  | 0.2371 | 0.2831 |  0.3298 |
|     2      | 0.1377 | 0.0067 | 0.2473 | 0.2841 |  0.3255 |
|     3      | 0.1351 | 0.007  | 0.2382 | 0.2804 |  0.3202 |
|     4      | 0.1382 | 0.0081 | 0.2493 | 0.2934 |  0.3361 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,800
Trainable params: 2,800
Non-trainable params: 8,193,000
=================================================================
2025-01-07 21:16:01,537: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1642 | 0.0059 | 0.2945 | 0.3576 |  0.4109 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1653 | 0.007  | 0.2945 | 0.3617 |  0.4162 |
|     1      | 0.1378 | 0.0035 |  0.25  | 0.2887 |  0.3309 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1637 | 0.007  | 0.288  | 0.3598 |  0.4174 |
|     1      | 0.137  | 0.0048 | 0.2427 | 0.2855 |  0.3304 |
|     2      | 0.1407 | 0.0062 | 0.254  | 0.2909 |  0.3274 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1629 | 0.0076 | 0.2853 | 0.3575 |  0.4152 |
|     1      | 0.1355 | 0.0048 | 0.2409 | 0.2847 |  0.3277 |
|     2      | 0.1389 | 0.0067 | 0.2478 | 0.2879 |  0.3258 |
|     3      | 0.1348 | 0.007  | 0.2382 | 0.279  |  0.3177 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1621 | 0.0073 | 0.2842 | 0.3554 |  0.4129 |
|     1      | 0.1338 | 0.004  | 0.2371 | 0.2831 |  0.3298 |
|     2      | 0.1377 | 0.0067 | 0.2473 | 0.2841 |  0.3255 |
|     3      | 0.1351 | 0.007  | 0.2382 | 0.2804 |  0.3202 |
|     4      | 0.1382 | 0.0081 | 0.2493 | 0.2934 |  0.3361 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 21:16:01,538: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 281.0042202472687  |   0.164   |    0.006     |    0.294     |     0.411     |
|    1     | 21.929363250732422 |   0.161   |    0.007     |    0.288     |     0.404     |
|    2     | 26.06288480758667  |   0.157   |    0.007     |    0.278     |     0.395     |
|    3     | 26.751248359680176 |   0.154   |    0.007     |    0.271     |     0.385     |
|    4     | 29.510669231414795 |   0.152   |    0.007     |    0.268     |     0.379     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 21:16:01,538: Sum_Training_Time:385.25838589668274
2025-01-07 21:16:01,538: Every_Training_Time:[281.0042202472687, 21.929363250732422, 26.06288480758667, 26.751248359680176, 29.510669231414795]
2025-01-07 21:16:01,538: Forward transfer: 0.060274999999999995 Backward transfer: -0.0022000000000000075
2025-01-07 21:16:14,181: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107211605/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3333, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 21:16:23,446: Snapshot:0	Epoch:0	Loss:15.337	translation_Loss:15.337	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.62	Hits@10:4.43	Best:1.62
2025-01-07 21:16:30,846: Snapshot:0	Epoch:1	Loss:8.282	translation_Loss:8.282	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:5.62	Hits@10:15.16	Best:5.62
2025-01-07 21:16:38,457: Snapshot:0	Epoch:2	Loss:3.832	translation_Loss:3.832	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.29	Hits@10:26.63	Best:10.29
2025-01-07 21:16:46,104: Snapshot:0	Epoch:3	Loss:1.628	translation_Loss:1.628	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:12.85	Hits@10:32.67	Best:12.85
2025-01-07 21:16:53,631: Snapshot:0	Epoch:4	Loss:0.877	translation_Loss:0.877	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.04	Hits@10:35.35	Best:14.04
2025-01-07 21:17:01,151: Snapshot:0	Epoch:5	Loss:0.551	translation_Loss:0.551	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.68	Hits@10:36.59	Best:14.68
2025-01-07 21:17:08,513: Snapshot:0	Epoch:6	Loss:0.354	translation_Loss:0.354	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.01	Hits@10:37.27	Best:15.01
2025-01-07 21:17:16,183: Snapshot:0	Epoch:7	Loss:0.253	translation_Loss:0.253	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.25	Hits@10:37.75	Best:15.25
2025-01-07 21:17:24,290: Snapshot:0	Epoch:8	Loss:0.183	translation_Loss:0.183	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.42	Hits@10:38.16	Best:15.42
2025-01-07 21:17:32,259: Snapshot:0	Epoch:9	Loss:0.14	translation_Loss:0.14	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.61	Hits@10:38.4	Best:15.61
2025-01-07 21:17:39,892: Snapshot:0	Epoch:10	Loss:0.112	translation_Loss:0.112	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.69	Hits@10:38.58	Best:15.69
2025-01-07 21:17:47,231: Snapshot:0	Epoch:11	Loss:0.095	translation_Loss:0.095	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.69	Hits@10:38.82	Best:15.69
2025-01-07 21:17:54,904: Snapshot:0	Epoch:12	Loss:0.078	translation_Loss:0.078	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.77	Hits@10:39.03	Best:15.77
2025-01-07 21:18:02,305: Snapshot:0	Epoch:13	Loss:0.067	translation_Loss:0.067	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.85	Hits@10:39.25	Best:15.85
2025-01-07 21:18:10,222: Snapshot:0	Epoch:14	Loss:0.06	translation_Loss:0.06	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.89	Hits@10:39.33	Best:15.89
2025-01-07 21:18:18,750: Snapshot:0	Epoch:15	Loss:0.055	translation_Loss:0.055	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.59	Best:15.96
2025-01-07 21:18:26,199: Snapshot:0	Epoch:16	Loss:0.048	translation_Loss:0.048	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.98	Hits@10:39.71	Best:15.98
2025-01-07 21:18:33,994: Snapshot:0	Epoch:17	Loss:0.044	translation_Loss:0.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:39.84	Best:16.04
2025-01-07 21:18:41,563: Snapshot:0	Epoch:18	Loss:0.04	translation_Loss:0.04	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:39.94	Best:16.06
2025-01-07 21:18:49,295: Snapshot:0	Epoch:19	Loss:0.037	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.1	Hits@10:39.96	Best:16.1
2025-01-07 21:18:56,897: Snapshot:0	Epoch:20	Loss:0.036	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.08	Hits@10:40.0	Best:16.1
2025-01-07 21:19:04,720: Snapshot:0	Epoch:21	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.09	Best:16.13
2025-01-07 21:19:12,231: Snapshot:0	Epoch:22	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.26	Best:16.14
2025-01-07 21:19:20,051: Snapshot:0	Epoch:23	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:40.29	Best:16.14
2025-01-07 21:19:27,729: Snapshot:0	Epoch:24	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.39	Best:16.16
2025-01-07 21:19:36,144: Snapshot:0	Epoch:25	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.4	Best:16.16
2025-01-07 21:19:44,593: Snapshot:0	Epoch:26	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.53	Best:16.16
2025-01-07 21:19:52,075: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.48	Best:16.21
2025-01-07 21:19:59,907: Snapshot:0	Epoch:28	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.22	Hits@10:40.58	Best:16.22
2025-01-07 21:20:07,740: Snapshot:0	Epoch:29	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.22	Hits@10:40.66	Best:16.22
2025-01-07 21:20:15,248: Snapshot:0	Epoch:30	Loss:0.028	translation_Loss:0.028	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.66	Best:16.23
2025-01-07 21:20:23,053: Snapshot:0	Epoch:31	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.68	Best:16.23
2025-01-07 21:20:30,486: Snapshot:0	Epoch:32	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.21	Hits@10:40.7	Best:16.23
2025-01-07 21:20:39,089: Snapshot:0	Epoch:33	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.26	Hits@10:40.77	Best:16.26
2025-01-07 21:20:47,325: Snapshot:0	Epoch:34	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.74	Best:16.26
2025-01-07 21:20:55,009: Snapshot:0	Epoch:35	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.25	Hits@10:40.72	Best:16.26
2025-01-07 21:21:02,574: Snapshot:0	Epoch:36	Loss:0.023	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.27	Hits@10:40.78	Best:16.27
2025-01-07 21:21:10,490: Snapshot:0	Epoch:37	Loss:0.023	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.29	Hits@10:40.79	Best:16.29
2025-01-07 21:21:17,928: Snapshot:0	Epoch:38	Loss:0.023	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.29	Hits@10:40.92	Best:16.29
2025-01-07 21:21:25,732: Snapshot:0	Epoch:39	Loss:0.022	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.3	Hits@10:41.0	Best:16.3
2025-01-07 21:21:33,514: Snapshot:0	Epoch:40	Loss:0.023	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.33	Hits@10:40.92	Best:16.33
2025-01-07 21:21:41,162: Snapshot:0	Epoch:41	Loss:0.023	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.34	Hits@10:40.9	Best:16.34
2025-01-07 21:21:49,493: Snapshot:0	Epoch:42	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.34	Hits@10:40.98	Best:16.34
2025-01-07 21:21:57,909: Snapshot:0	Epoch:43	Loss:0.021	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.38	Hits@10:40.96	Best:16.38
2025-01-07 21:22:05,730: Snapshot:0	Epoch:44	Loss:0.022	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.36	Hits@10:41.03	Best:16.38
2025-01-07 21:22:13,248: Snapshot:0	Epoch:45	Loss:0.021	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.35	Hits@10:41.06	Best:16.38
2025-01-07 21:22:20,962: Early Stopping! Snapshot: 0 Epoch: 46 Best Results: 16.38
2025-01-07 21:22:20,963: Start to training tokens! Snapshot: 0 Epoch: 46 Loss:0.02 MRR:16.32 Best Results: 16.38
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:22:20,963: Snapshot:0	Epoch:46	Loss:0.02	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.32	Hits@10:41.08	Best:16.38
2025-01-07 21:22:29,304: Snapshot:0	Epoch:47	Loss:22.037	translation_Loss:5.537	token_training_loss:16.5	distillation_Loss:0.0                                                   	MRR:16.32	Hits@10:41.08	Best:16.38
2025-01-07 21:22:36,795: End of token training: 0 Epoch: 48 Loss:6.039 MRR:16.32 Best Results: 16.38
2025-01-07 21:22:36,796: Snapshot:0	Epoch:48	Loss:6.039	translation_Loss:5.533	token_training_loss:0.506	distillation_Loss:0.0                                                           	MRR:16.32	Hits@10:41.08	Best:16.38
2025-01-07 21:22:36,890: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 21:22:41,242: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.165 | 0.0063 | 0.2934 | 0.3566 |  0.4139 |
+------------+-------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,920,600
Trainable params: 2,800
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:22:45,333: Snapshot:1	Epoch:0	Loss:2.745	translation_Loss:2.664	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:2.6	Hits@10:6.94	Best:2.6
2025-01-07 21:22:47,041: Snapshot:1	Epoch:1	Loss:1.774	translation_Loss:1.635	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:7.36	Hits@10:18.52	Best:7.36
2025-01-07 21:22:48,761: Snapshot:1	Epoch:2	Loss:1.012	translation_Loss:0.83	token_training_loss:0.0	distillation_Loss:0.183                                                   	MRR:10.31	Hits@10:26.64	Best:10.31
2025-01-07 21:22:50,433: Snapshot:1	Epoch:3	Loss:0.558	translation_Loss:0.327	token_training_loss:0.0	distillation_Loss:0.232                                                   	MRR:12.14	Hits@10:30.08	Best:12.14
2025-01-07 21:22:52,312: Snapshot:1	Epoch:4	Loss:0.387	translation_Loss:0.129	token_training_loss:0.0	distillation_Loss:0.258                                                   	MRR:13.23	Hits@10:32.02	Best:13.23
2025-01-07 21:22:53,822: Snapshot:1	Epoch:5	Loss:0.309	translation_Loss:0.057	token_training_loss:0.0	distillation_Loss:0.252                                                   	MRR:13.7	Hits@10:33.17	Best:13.7
2025-01-07 21:22:55,345: Snapshot:1	Epoch:6	Loss:0.253	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.222                                                   	MRR:13.87	Hits@10:33.44	Best:13.87
2025-01-07 21:22:56,882: Snapshot:1	Epoch:7	Loss:0.202	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.183                                                   	MRR:13.88	Hits@10:33.58	Best:13.88
2025-01-07 21:22:58,343: Snapshot:1	Epoch:8	Loss:0.159	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:13.83	Hits@10:33.95	Best:13.88
2025-01-07 21:22:59,804: Snapshot:1	Epoch:9	Loss:0.125	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.111                                                   	MRR:13.77	Hits@10:34.01	Best:13.88
2025-01-07 21:23:01,265: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.88
2025-01-07 21:23:01,266: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.101 MRR:13.7 Best Results: 13.88
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:23:01,266: Snapshot:1	Epoch:10	Loss:0.101	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.086                                                   	MRR:13.7	Hits@10:34.14	Best:13.88
2025-01-07 21:23:02,733: Snapshot:1	Epoch:11	Loss:8.452	translation_Loss:1.197	token_training_loss:7.256	distillation_Loss:0.0                                                   	MRR:13.7	Hits@10:34.14	Best:13.88
2025-01-07 21:23:04,388: End of token training: 1 Epoch: 12 Loss:5.913 MRR:13.7 Best Results: 13.88
2025-01-07 21:23:04,388: Snapshot:1	Epoch:12	Loss:5.913	translation_Loss:1.197	token_training_loss:4.716	distillation_Loss:0.0                                                           	MRR:13.7	Hits@10:34.14	Best:13.88
2025-01-07 21:23:04,463: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 21:23:09,011: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1666 | 0.0073 | 0.2943 | 0.3625 |  0.4206 |
|     1      | 0.1384 | 0.0054 | 0.2481 | 0.2898 |  0.3312 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,739,200
Trainable params: 2,800
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:23:12,938: Snapshot:2	Epoch:0	Loss:2.666	translation_Loss:2.618	token_training_loss:0.0	distillation_Loss:0.048                                                   	MRR:3.15	Hits@10:7.98	Best:3.15
2025-01-07 21:23:14,633: Snapshot:2	Epoch:1	Loss:1.671	translation_Loss:1.548	token_training_loss:0.0	distillation_Loss:0.124                                                   	MRR:7.72	Hits@10:19.81	Best:7.72
2025-01-07 21:23:16,326: Snapshot:2	Epoch:2	Loss:0.866	translation_Loss:0.714	token_training_loss:0.0	distillation_Loss:0.151                                                   	MRR:10.59	Hits@10:25.86	Best:10.59
2025-01-07 21:23:18,305: Snapshot:2	Epoch:3	Loss:0.409	translation_Loss:0.239	token_training_loss:0.0	distillation_Loss:0.17                                                   	MRR:12.28	Hits@10:28.82	Best:12.28
2025-01-07 21:23:19,944: Snapshot:2	Epoch:4	Loss:0.274	translation_Loss:0.088	token_training_loss:0.0	distillation_Loss:0.186                                                   	MRR:13.1	Hits@10:30.11	Best:13.1
2025-01-07 21:23:21,603: Snapshot:2	Epoch:5	Loss:0.226	translation_Loss:0.037	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:13.52	Hits@10:30.73	Best:13.52
2025-01-07 21:23:23,290: Snapshot:2	Epoch:6	Loss:0.197	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.178                                                   	MRR:13.69	Hits@10:31.34	Best:13.69
2025-01-07 21:23:24,840: Snapshot:2	Epoch:7	Loss:0.171	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.16                                                   	MRR:13.66	Hits@10:31.69	Best:13.69
2025-01-07 21:23:26,411: Snapshot:2	Epoch:8	Loss:0.147	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.138                                                   	MRR:13.64	Hits@10:31.94	Best:13.69
2025-01-07 21:23:27,987: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 13.69
2025-01-07 21:23:27,987: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:0.123 MRR:13.59 Best Results: 13.69
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:23:27,988: Snapshot:2	Epoch:9	Loss:0.123	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.116                                                   	MRR:13.59	Hits@10:32.07	Best:13.69
2025-01-07 21:23:29,539: Snapshot:2	Epoch:10	Loss:8.186	translation_Loss:1.196	token_training_loss:6.99	distillation_Loss:0.0                                                   	MRR:13.59	Hits@10:32.07	Best:13.69
2025-01-07 21:23:31,100: End of token training: 2 Epoch: 11 Loss:5.648 MRR:13.59 Best Results: 13.69
2025-01-07 21:23:31,100: Snapshot:2	Epoch:11	Loss:5.648	translation_Loss:1.195	token_training_loss:4.453	distillation_Loss:0.0                                                           	MRR:13.59	Hits@10:32.07	Best:13.69
2025-01-07 21:23:31,190: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 21:23:37,125: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1645 | 0.0078 | 0.2866 | 0.3572 |  0.4211 |
|     1      | 0.1354 | 0.0067 | 0.2382 | 0.2825 |  0.3331 |
|     2      | 0.139  | 0.0065 | 0.2489 | 0.2847 |  0.3245 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,000
Trainable params: 2,800
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:23:41,056: Snapshot:3	Epoch:0	Loss:2.65	translation_Loss:2.568	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:2.98	Hits@10:7.45	Best:2.98
2025-01-07 21:23:42,869: Snapshot:3	Epoch:1	Loss:1.639	translation_Loss:1.47	token_training_loss:0.0	distillation_Loss:0.168                                                   	MRR:7.69	Hits@10:19.17	Best:7.69
2025-01-07 21:23:44,961: Snapshot:3	Epoch:2	Loss:0.843	translation_Loss:0.646	token_training_loss:0.0	distillation_Loss:0.197                                                   	MRR:10.56	Hits@10:25.16	Best:10.56
2025-01-07 21:23:46,774: Snapshot:3	Epoch:3	Loss:0.436	translation_Loss:0.208	token_training_loss:0.0	distillation_Loss:0.229                                                   	MRR:12.07	Hits@10:27.77	Best:12.07
2025-01-07 21:23:48,566: Snapshot:3	Epoch:4	Loss:0.325	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.246                                                   	MRR:12.86	Hits@10:29.11	Best:12.86
2025-01-07 21:23:50,334: Snapshot:3	Epoch:5	Loss:0.276	translation_Loss:0.039	token_training_loss:0.0	distillation_Loss:0.238                                                   	MRR:13.21	Hits@10:29.92	Best:13.21
2025-01-07 21:23:52,106: Snapshot:3	Epoch:6	Loss:0.231	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.209                                                   	MRR:13.38	Hits@10:30.22	Best:13.38
2025-01-07 21:23:53,893: Snapshot:3	Epoch:7	Loss:0.189	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.174                                                   	MRR:13.48	Hits@10:30.38	Best:13.48
2025-01-07 21:23:55,684: Snapshot:3	Epoch:8	Loss:0.151	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:13.5	Hits@10:30.65	Best:13.5
2025-01-07 21:23:57,365: Snapshot:3	Epoch:9	Loss:0.119	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.109                                                   	MRR:13.44	Hits@10:30.75	Best:13.5
2025-01-07 21:23:59,041: Snapshot:3	Epoch:10	Loss:0.095	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.085                                                   	MRR:13.44	Hits@10:30.67	Best:13.5
2025-01-07 21:24:00,803: Early Stopping! Snapshot: 3 Epoch: 11 Best Results: 13.5
2025-01-07 21:24:00,803: Start to training tokens! Snapshot: 3 Epoch: 11 Loss:0.077 MRR:13.39 Best Results: 13.5
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:24:00,804: Snapshot:3	Epoch:11	Loss:0.077	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.067                                                   	MRR:13.39	Hits@10:30.7	Best:13.5
2025-01-07 21:24:02,614: Snapshot:3	Epoch:12	Loss:8.203	translation_Loss:1.204	token_training_loss:6.999	distillation_Loss:0.0                                                   	MRR:13.39	Hits@10:30.7	Best:13.5
2025-01-07 21:24:04,421: End of token training: 3 Epoch: 13 Loss:5.595 MRR:13.39 Best Results: 13.5
2025-01-07 21:24:04,421: Snapshot:3	Epoch:13	Loss:5.595	translation_Loss:1.204	token_training_loss:4.391	distillation_Loss:0.0                                                           	MRR:13.39	Hits@10:30.7	Best:13.5
2025-01-07 21:24:04,498: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 21:24:11,822: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1638 | 0.0079 | 0.285  | 0.3564 |  0.4193 |
|     1      | 0.1338 | 0.007  | 0.2341 | 0.2774 |  0.3309 |
|     2      | 0.1377 | 0.0067 | 0.2476 | 0.2812 |  0.3231 |
|     3      | 0.1368 | 0.0078 | 0.239  | 0.2858 |  0.3239 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,800
Trainable params: 2,800
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:24:16,207: Snapshot:4	Epoch:0	Loss:2.597	translation_Loss:2.517	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:10.2	Hits@10:24.68	Best:10.2
2025-01-07 21:24:18,123: Snapshot:4	Epoch:1	Loss:1.575	translation_Loss:1.399	token_training_loss:0.0	distillation_Loss:0.176                                                   	MRR:12.09	Hits@10:29.01	Best:12.09
2025-01-07 21:24:20,237: Snapshot:4	Epoch:2	Loss:0.778	translation_Loss:0.576	token_training_loss:0.0	distillation_Loss:0.202                                                   	MRR:13.15	Hits@10:31.29	Best:13.15
2025-01-07 21:24:22,121: Snapshot:4	Epoch:3	Loss:0.393	translation_Loss:0.171	token_training_loss:0.0	distillation_Loss:0.221                                                   	MRR:13.73	Hits@10:32.2	Best:13.73
2025-01-07 21:24:24,011: Snapshot:4	Epoch:4	Loss:0.295	translation_Loss:0.064	token_training_loss:0.0	distillation_Loss:0.231                                                   	MRR:14.04	Hits@10:32.85	Best:14.04
2025-01-07 21:24:25,919: Snapshot:4	Epoch:5	Loss:0.255	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.22                                                   	MRR:14.2	Hits@10:33.23	Best:14.2
2025-01-07 21:24:27,702: Snapshot:4	Epoch:6	Loss:0.215	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.194                                                   	MRR:14.19	Hits@10:33.39	Best:14.2
2025-01-07 21:24:29,665: Snapshot:4	Epoch:7	Loss:0.178	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.164                                                   	MRR:14.28	Hits@10:33.47	Best:14.28
2025-01-07 21:24:31,590: Snapshot:4	Epoch:8	Loss:0.145	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.133                                                   	MRR:14.29	Hits@10:33.49	Best:14.29
2025-01-07 21:24:33,375: Snapshot:4	Epoch:9	Loss:0.115	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.106                                                   	MRR:14.24	Hits@10:33.58	Best:14.29
2025-01-07 21:24:35,158: Snapshot:4	Epoch:10	Loss:0.094	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.084                                                   	MRR:14.23	Hits@10:33.58	Best:14.29
2025-01-07 21:24:36,918: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 14.29
2025-01-07 21:24:36,918: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:0.078 MRR:14.17 Best Results: 14.29
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:24:36,918: Snapshot:4	Epoch:11	Loss:0.078	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:14.17	Hits@10:33.47	Best:14.29
2025-01-07 21:24:38,655: Snapshot:4	Epoch:12	Loss:8.347	translation_Loss:1.183	token_training_loss:7.163	distillation_Loss:0.0                                                   	MRR:14.17	Hits@10:33.47	Best:14.29
2025-01-07 21:24:40,401: End of token training: 4 Epoch: 13 Loss:5.797 MRR:14.17 Best Results: 14.29
2025-01-07 21:24:40,401: Snapshot:4	Epoch:13	Loss:5.797	translation_Loss:1.185	token_training_loss:4.613	distillation_Loss:0.0                                                           	MRR:14.17	Hits@10:33.47	Best:14.29
2025-01-07 21:24:40,476: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 21:24:48,838: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1626 | 0.0079 | 0.2821 | 0.3548 |  0.4174 |
|     1      | 0.1327 | 0.0073 | 0.2312 | 0.2785 |  0.3298 |
|     2      | 0.1361 | 0.0075 | 0.2409 | 0.2812 |  0.3228 |
|     3      | 0.1374 | 0.0083 | 0.2401 | 0.2849 |  0.3258 |
|     4      | 0.1376 | 0.0083 | 0.2434 | 0.2926 |  0.3388 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,800
Trainable params: 2,800
Non-trainable params: 8,193,000
=================================================================
2025-01-07 21:24:48,841: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.165 | 0.0063 | 0.2934 | 0.3566 |  0.4139 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1666 | 0.0073 | 0.2943 | 0.3625 |  0.4206 |
|     1      | 0.1384 | 0.0054 | 0.2481 | 0.2898 |  0.3312 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1645 | 0.0078 | 0.2866 | 0.3572 |  0.4211 |
|     1      | 0.1354 | 0.0067 | 0.2382 | 0.2825 |  0.3331 |
|     2      | 0.139  | 0.0065 | 0.2489 | 0.2847 |  0.3245 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1638 | 0.0079 | 0.285  | 0.3564 |  0.4193 |
|     1      | 0.1338 | 0.007  | 0.2341 | 0.2774 |  0.3309 |
|     2      | 0.1377 | 0.0067 | 0.2476 | 0.2812 |  0.3231 |
|     3      | 0.1368 | 0.0078 | 0.239  | 0.2858 |  0.3239 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1626 | 0.0079 | 0.2821 | 0.3548 |  0.4174 |
|     1      | 0.1327 | 0.0073 | 0.2312 | 0.2785 |  0.3298 |
|     2      | 0.1361 | 0.0075 | 0.2409 | 0.2812 |  0.3228 |
|     3      | 0.1374 | 0.0083 | 0.2401 | 0.2849 |  0.3258 |
|     4      | 0.1376 | 0.0083 | 0.2434 | 0.2926 |  0.3388 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 21:24:48,841: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 382.61373591423035 |   0.165   |    0.006     |    0.293     |     0.414     |
|    1     | 22.239137887954712 |   0.163   |    0.007     |    0.288     |     0.408     |
|    2     | 20.974860191345215 |   0.158   |    0.008     |    0.276     |     0.398     |
|    3     | 26.27584719657898  |   0.155   |    0.008     |     0.27     |     0.388     |
|    4     | 27.243494749069214 |   0.152   |    0.008     |    0.265     |     0.382     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 21:24:48,841: Sum_Training_Time:479.34707593917847
2025-01-07 21:24:48,842: Every_Training_Time:[382.61373591423035, 22.239137887954712, 20.974860191345215, 26.27584719657898, 27.243494749069214]
2025-01-07 21:24:48,842: Forward transfer: 0.059825 Backward transfer: -0.002600000000000005
2025-01-07 21:25:01,466: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107212452/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=4444, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 21:25:11,394: Snapshot:0	Epoch:0	Loss:15.319	translation_Loss:15.319	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.4	Hits@10:3.92	Best:1.4
2025-01-07 21:25:18,949: Snapshot:0	Epoch:1	Loss:8.22	translation_Loss:8.22	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:4.88	Hits@10:13.29	Best:4.88
2025-01-07 21:25:26,647: Snapshot:0	Epoch:2	Loss:3.734	translation_Loss:3.734	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.84	Hits@10:25.5	Best:9.84
2025-01-07 21:25:34,601: Snapshot:0	Epoch:3	Loss:1.583	translation_Loss:1.583	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:12.52	Hits@10:31.76	Best:12.52
2025-01-07 21:25:42,424: Snapshot:0	Epoch:4	Loss:0.863	translation_Loss:0.863	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.75	Hits@10:34.65	Best:13.75
2025-01-07 21:25:50,318: Snapshot:0	Epoch:5	Loss:0.537	translation_Loss:0.537	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.41	Hits@10:36.1	Best:14.41
2025-01-07 21:25:57,839: Snapshot:0	Epoch:6	Loss:0.367	translation_Loss:0.367	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.85	Hits@10:36.79	Best:14.85
2025-01-07 21:26:05,698: Snapshot:0	Epoch:7	Loss:0.261	translation_Loss:0.261	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.1	Hits@10:37.46	Best:15.1
2025-01-07 21:26:13,193: Snapshot:0	Epoch:8	Loss:0.192	translation_Loss:0.192	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.26	Hits@10:37.78	Best:15.26
2025-01-07 21:26:20,968: Snapshot:0	Epoch:9	Loss:0.153	translation_Loss:0.153	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.35	Hits@10:38.3	Best:15.35
2025-01-07 21:26:28,734: Snapshot:0	Epoch:10	Loss:0.123	translation_Loss:0.123	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.49	Hits@10:38.53	Best:15.49
2025-01-07 21:26:36,240: Snapshot:0	Epoch:11	Loss:0.102	translation_Loss:0.102	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.56	Hits@10:38.69	Best:15.56
2025-01-07 21:26:44,752: Snapshot:0	Epoch:12	Loss:0.091	translation_Loss:0.091	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.61	Hits@10:38.9	Best:15.61
2025-01-07 21:26:52,906: Snapshot:0	Epoch:13	Loss:0.074	translation_Loss:0.074	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.72	Hits@10:39.14	Best:15.72
2025-01-07 21:27:00,773: Snapshot:0	Epoch:14	Loss:0.064	translation_Loss:0.064	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.8	Hits@10:39.27	Best:15.8
2025-01-07 21:27:08,441: Snapshot:0	Epoch:15	Loss:0.058	translation_Loss:0.058	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.8	Hits@10:39.41	Best:15.8
2025-01-07 21:27:16,041: Snapshot:0	Epoch:16	Loss:0.054	translation_Loss:0.054	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.84	Hits@10:39.47	Best:15.84
2025-01-07 21:27:23,773: Snapshot:0	Epoch:17	Loss:0.047	translation_Loss:0.047	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.86	Hits@10:39.62	Best:15.86
2025-01-07 21:27:31,615: Snapshot:0	Epoch:18	Loss:0.044	translation_Loss:0.044	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.89	Hits@10:39.71	Best:15.89
2025-01-07 21:27:40,226: Snapshot:0	Epoch:19	Loss:0.042	translation_Loss:0.042	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.94	Hits@10:39.8	Best:15.94
2025-01-07 21:27:47,873: Snapshot:0	Epoch:20	Loss:0.039	translation_Loss:0.039	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.88	Best:15.96
2025-01-07 21:27:55,572: Snapshot:0	Epoch:21	Loss:0.039	translation_Loss:0.039	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:39.89	Best:16.04
2025-01-07 21:28:03,124: Snapshot:0	Epoch:22	Loss:0.036	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.99	Hits@10:39.98	Best:16.04
2025-01-07 21:28:10,834: Snapshot:0	Epoch:23	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.01	Hits@10:40.12	Best:16.04
2025-01-07 21:28:18,636: Snapshot:0	Epoch:24	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.06	Hits@10:40.26	Best:16.06
2025-01-07 21:28:26,705: Snapshot:0	Epoch:25	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.12	Hits@10:40.43	Best:16.12
2025-01-07 21:28:35,373: Snapshot:0	Epoch:26	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.15	Hits@10:40.51	Best:16.15
2025-01-07 21:28:42,750: Snapshot:0	Epoch:27	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.13	Hits@10:40.59	Best:16.15
2025-01-07 21:28:50,657: Snapshot:0	Epoch:28	Loss:0.029	translation_Loss:0.029	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.17	Hits@10:40.67	Best:16.17
2025-01-07 21:28:59,279: Snapshot:0	Epoch:29	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.16	Hits@10:40.63	Best:16.17
2025-01-07 21:29:06,986: Snapshot:0	Epoch:30	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:40.7	Best:16.18
2025-01-07 21:29:14,778: Snapshot:0	Epoch:31	Loss:0.027	translation_Loss:0.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.19	Hits@10:40.81	Best:16.19
2025-01-07 21:29:22,318: Snapshot:0	Epoch:32	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.22	Hits@10:40.86	Best:16.22
2025-01-07 21:29:30,051: Snapshot:0	Epoch:33	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.23	Hits@10:40.91	Best:16.23
2025-01-07 21:29:37,583: Snapshot:0	Epoch:34	Loss:0.026	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.28	Hits@10:40.84	Best:16.28
2025-01-07 21:29:45,320: Snapshot:0	Epoch:35	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.34	Hits@10:41.01	Best:16.34
2025-01-07 21:29:52,789: Snapshot:0	Epoch:36	Loss:0.025	translation_Loss:0.025	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.29	Hits@10:41.0	Best:16.34
2025-01-07 21:30:00,500: Snapshot:0	Epoch:37	Loss:0.024	translation_Loss:0.024	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.28	Hits@10:40.95	Best:16.34
2025-01-07 21:30:07,917: Early Stopping! Snapshot: 0 Epoch: 38 Best Results: 16.34
2025-01-07 21:30:07,917: Start to training tokens! Snapshot: 0 Epoch: 38 Loss:0.022 MRR:16.3 Best Results: 16.34
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:30:07,918: Snapshot:0	Epoch:38	Loss:0.022	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.3	Hits@10:41.09	Best:16.34
2025-01-07 21:30:17,023: Snapshot:0	Epoch:39	Loss:22.233	translation_Loss:5.699	token_training_loss:16.535	distillation_Loss:0.0                                                   	MRR:16.3	Hits@10:41.09	Best:16.34
2025-01-07 21:30:25,408: End of token training: 0 Epoch: 40 Loss:6.209 MRR:16.3 Best Results: 16.34
2025-01-07 21:30:25,409: Snapshot:0	Epoch:40	Loss:6.209	translation_Loss:5.695	token_training_loss:0.515	distillation_Loss:0.0                                                           	MRR:16.3	Hits@10:41.09	Best:16.34
2025-01-07 21:30:25,501: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 21:30:29,456: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1651 | 0.0071 | 0.296  | 0.359  |  0.4117 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,920,600
Trainable params: 2,800
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:30:33,213: Snapshot:1	Epoch:0	Loss:2.727	translation_Loss:2.644	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:2.47	Hits@10:6.32	Best:2.47
2025-01-07 21:30:34,740: Snapshot:1	Epoch:1	Loss:1.757	translation_Loss:1.612	token_training_loss:0.0	distillation_Loss:0.145                                                   	MRR:7.17	Hits@10:18.49	Best:7.17
2025-01-07 21:30:36,282: Snapshot:1	Epoch:2	Loss:0.996	translation_Loss:0.809	token_training_loss:0.0	distillation_Loss:0.187                                                   	MRR:10.31	Hits@10:25.97	Best:10.31
2025-01-07 21:30:37,808: Snapshot:1	Epoch:3	Loss:0.547	translation_Loss:0.309	token_training_loss:0.0	distillation_Loss:0.237                                                   	MRR:12.03	Hits@10:29.49	Best:12.03
2025-01-07 21:30:39,554: Snapshot:1	Epoch:4	Loss:0.383	translation_Loss:0.12	token_training_loss:0.0	distillation_Loss:0.264                                                   	MRR:12.9	Hits@10:31.13	Best:12.9
2025-01-07 21:30:41,137: Snapshot:1	Epoch:5	Loss:0.313	translation_Loss:0.056	token_training_loss:0.0	distillation_Loss:0.257                                                   	MRR:13.43	Hits@10:32.2	Best:13.43
2025-01-07 21:30:42,647: Snapshot:1	Epoch:6	Loss:0.257	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.227                                                   	MRR:13.57	Hits@10:32.82	Best:13.57
2025-01-07 21:30:44,172: Snapshot:1	Epoch:7	Loss:0.205	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.187                                                   	MRR:13.66	Hits@10:32.98	Best:13.66
2025-01-07 21:30:45,855: Snapshot:1	Epoch:8	Loss:0.164	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:13.52	Hits@10:33.09	Best:13.66
2025-01-07 21:30:47,322: Snapshot:1	Epoch:9	Loss:0.128	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.113                                                   	MRR:13.47	Hits@10:33.28	Best:13.66
2025-01-07 21:30:48,784: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.66
2025-01-07 21:30:48,785: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.105 MRR:13.48 Best Results: 13.66
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:30:48,785: Snapshot:1	Epoch:10	Loss:0.105	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.088                                                   	MRR:13.48	Hits@10:33.31	Best:13.66
2025-01-07 21:30:50,225: Snapshot:1	Epoch:11	Loss:8.164	translation_Loss:1.225	token_training_loss:6.939	distillation_Loss:0.0                                                   	MRR:13.48	Hits@10:33.31	Best:13.66
2025-01-07 21:30:51,651: End of token training: 1 Epoch: 12 Loss:5.576 MRR:13.48 Best Results: 13.66
2025-01-07 21:30:51,651: Snapshot:1	Epoch:12	Loss:5.576	translation_Loss:1.227	token_training_loss:4.349	distillation_Loss:0.0                                                           	MRR:13.48	Hits@10:33.31	Best:13.66
2025-01-07 21:30:51,724: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 21:30:56,447: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1665 | 0.0076 | 0.2969 | 0.3617 |  0.4181 |
|     1      | 0.1378 | 0.0046 | 0.2457 | 0.2917 |  0.3304 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,739,200
Trainable params: 2,800
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:31:00,440: Snapshot:2	Epoch:0	Loss:2.643	translation_Loss:2.594	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:2.73	Hits@10:7.18	Best:2.73
2025-01-07 21:31:02,122: Snapshot:2	Epoch:1	Loss:1.645	translation_Loss:1.518	token_training_loss:0.0	distillation_Loss:0.127                                                   	MRR:7.61	Hits@10:19.41	Best:7.61
2025-01-07 21:31:03,917: Snapshot:2	Epoch:2	Loss:0.84	translation_Loss:0.684	token_training_loss:0.0	distillation_Loss:0.156                                                   	MRR:10.5	Hits@10:25.7	Best:10.5
2025-01-07 21:31:05,628: Snapshot:2	Epoch:3	Loss:0.398	translation_Loss:0.224	token_training_loss:0.0	distillation_Loss:0.173                                                   	MRR:12.12	Hits@10:28.23	Best:12.12
2025-01-07 21:31:07,280: Snapshot:2	Epoch:4	Loss:0.268	translation_Loss:0.081	token_training_loss:0.0	distillation_Loss:0.188                                                   	MRR:12.87	Hits@10:29.6	Best:12.87
2025-01-07 21:31:09,241: Snapshot:2	Epoch:5	Loss:0.224	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.19                                                   	MRR:13.28	Hits@10:30.32	Best:13.28
2025-01-07 21:31:10,912: Snapshot:2	Epoch:6	Loss:0.198	translation_Loss:0.019	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:13.45	Hits@10:30.81	Best:13.45
2025-01-07 21:31:12,564: Snapshot:2	Epoch:7	Loss:0.173	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.161                                                   	MRR:13.53	Hits@10:31.13	Best:13.53
2025-01-07 21:31:14,450: Snapshot:2	Epoch:8	Loss:0.149	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.14                                                   	MRR:13.62	Hits@10:31.45	Best:13.62
2025-01-07 21:31:16,023: Snapshot:2	Epoch:9	Loss:0.125	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.118                                                   	MRR:13.6	Hits@10:31.61	Best:13.62
2025-01-07 21:31:17,826: Snapshot:2	Epoch:10	Loss:0.104	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.098                                                   	MRR:13.55	Hits@10:31.45	Best:13.62
2025-01-07 21:31:19,391: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 13.62
2025-01-07 21:31:19,391: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:0.087 MRR:13.51 Best Results: 13.62
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:31:19,391: Snapshot:2	Epoch:11	Loss:0.087	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.08                                                   	MRR:13.51	Hits@10:31.48	Best:13.62
2025-01-07 21:31:20,975: Snapshot:2	Epoch:12	Loss:8.466	translation_Loss:1.194	token_training_loss:7.272	distillation_Loss:0.0                                                   	MRR:13.51	Hits@10:31.48	Best:13.62
2025-01-07 21:31:22,504: End of token training: 2 Epoch: 13 Loss:5.924 MRR:13.51 Best Results: 13.62
2025-01-07 21:31:22,505: Snapshot:2	Epoch:13	Loss:5.924	translation_Loss:1.192	token_training_loss:4.732	distillation_Loss:0.0                                                           	MRR:13.51	Hits@10:31.48	Best:13.62
2025-01-07 21:31:22,580: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 21:31:28,397: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.164  | 0.0075 | 0.2912 | 0.3578 |  0.4165 |
|     1      | 0.1358 | 0.0051 | 0.2411 | 0.2841 |  0.3285 |
|     2      | 0.1419 | 0.0075 | 0.257  | 0.2944 |  0.3309 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,000
Trainable params: 2,800
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:31:32,582: Snapshot:3	Epoch:0	Loss:2.64	translation_Loss:2.558	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:2.82	Hits@10:7.58	Best:2.82
2025-01-07 21:31:34,365: Snapshot:3	Epoch:1	Loss:1.622	translation_Loss:1.452	token_training_loss:0.0	distillation_Loss:0.17                                                   	MRR:7.58	Hits@10:18.79	Best:7.58
2025-01-07 21:31:36,140: Snapshot:3	Epoch:2	Loss:0.826	translation_Loss:0.626	token_training_loss:0.0	distillation_Loss:0.199                                                   	MRR:10.85	Hits@10:25.35	Best:10.85
2025-01-07 21:31:38,339: Snapshot:3	Epoch:3	Loss:0.428	translation_Loss:0.197	token_training_loss:0.0	distillation_Loss:0.232                                                   	MRR:12.32	Hits@10:27.45	Best:12.32
2025-01-07 21:31:40,128: Snapshot:3	Epoch:4	Loss:0.319	translation_Loss:0.07	token_training_loss:0.0	distillation_Loss:0.249                                                   	MRR:13.07	Hits@10:28.79	Best:13.07
2025-01-07 21:31:41,929: Snapshot:3	Epoch:5	Loss:0.275	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.24                                                   	MRR:13.45	Hits@10:29.49	Best:13.45
2025-01-07 21:31:43,757: Snapshot:3	Epoch:6	Loss:0.232	translation_Loss:0.021	token_training_loss:0.0	distillation_Loss:0.211                                                   	MRR:13.54	Hits@10:30.08	Best:13.54
2025-01-07 21:31:45,721: Snapshot:3	Epoch:7	Loss:0.191	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.176                                                   	MRR:13.54	Hits@10:30.35	Best:13.54
2025-01-07 21:31:47,531: Snapshot:3	Epoch:8	Loss:0.153	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.142                                                   	MRR:13.57	Hits@10:30.48	Best:13.57
2025-01-07 21:31:49,379: Snapshot:3	Epoch:9	Loss:0.122	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.111                                                   	MRR:13.65	Hits@10:30.46	Best:13.65
2025-01-07 21:31:51,020: Snapshot:3	Epoch:10	Loss:0.098	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.086                                                   	MRR:13.62	Hits@10:30.56	Best:13.65
2025-01-07 21:31:52,659: Snapshot:3	Epoch:11	Loss:0.08	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.069                                                   	MRR:13.59	Hits@10:30.51	Best:13.65
2025-01-07 21:31:54,457: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 13.65
2025-01-07 21:31:54,457: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:0.068 MRR:13.6 Best Results: 13.65
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:31:54,458: Snapshot:3	Epoch:12	Loss:0.068	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.057                                                   	MRR:13.6	Hits@10:30.56	Best:13.65
2025-01-07 21:31:56,249: Snapshot:3	Epoch:13	Loss:8.253	translation_Loss:1.209	token_training_loss:7.044	distillation_Loss:0.0                                                   	MRR:13.6	Hits@10:30.56	Best:13.65
2025-01-07 21:31:58,064: End of token training: 3 Epoch: 14 Loss:5.732 MRR:13.6 Best Results: 13.65
2025-01-07 21:31:58,064: Snapshot:3	Epoch:14	Loss:5.732	translation_Loss:1.214	token_training_loss:4.519	distillation_Loss:0.0                                                           	MRR:13.6	Hits@10:30.56	Best:13.65
2025-01-07 21:31:58,169: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 21:32:05,539: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1632 | 0.0075 | 0.2879 | 0.3574 |  0.4157 |
|     1      | 0.1335 | 0.004  | 0.236  | 0.279  |  0.3272 |
|     2      | 0.1388 | 0.0059 | 0.2495 | 0.2909 |  0.328  |
|     3      | 0.1358 | 0.0081 | 0.2409 | 0.2831 |  0.3183 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,800
Trainable params: 2,800
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:32:09,918: Snapshot:4	Epoch:0	Loss:2.573	translation_Loss:2.492	token_training_loss:0.0	distillation_Loss:0.081                                                   	MRR:9.96	Hits@10:24.7	Best:9.96
2025-01-07 21:32:11,807: Snapshot:4	Epoch:1	Loss:1.553	translation_Loss:1.374	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:11.88	Hits@10:28.68	Best:11.88
2025-01-07 21:32:13,722: Snapshot:4	Epoch:2	Loss:0.76	translation_Loss:0.557	token_training_loss:0.0	distillation_Loss:0.203                                                   	MRR:12.91	Hits@10:30.91	Best:12.91
2025-01-07 21:32:15,669: Snapshot:4	Epoch:3	Loss:0.383	translation_Loss:0.16	token_training_loss:0.0	distillation_Loss:0.223                                                   	MRR:13.45	Hits@10:31.75	Best:13.45
2025-01-07 21:32:17,541: Snapshot:4	Epoch:4	Loss:0.294	translation_Loss:0.061	token_training_loss:0.0	distillation_Loss:0.233                                                   	MRR:13.82	Hits@10:32.15	Best:13.82
2025-01-07 21:32:19,440: Snapshot:4	Epoch:5	Loss:0.254	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.222                                                   	MRR:13.98	Hits@10:32.74	Best:13.98
2025-01-07 21:32:21,318: Snapshot:4	Epoch:6	Loss:0.217	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.195                                                   	MRR:14.07	Hits@10:33.01	Best:14.07
2025-01-07 21:32:23,230: Snapshot:4	Epoch:7	Loss:0.179	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.164                                                   	MRR:14.15	Hits@10:33.23	Best:14.15
2025-01-07 21:32:25,350: Snapshot:4	Epoch:8	Loss:0.146	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.134                                                   	MRR:14.17	Hits@10:33.31	Best:14.17
2025-01-07 21:32:27,209: Snapshot:4	Epoch:9	Loss:0.117	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.107                                                   	MRR:14.22	Hits@10:33.39	Best:14.22
2025-01-07 21:32:28,974: Snapshot:4	Epoch:10	Loss:0.095	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.085                                                   	MRR:14.19	Hits@10:33.39	Best:14.22
2025-01-07 21:32:30,887: Snapshot:4	Epoch:11	Loss:0.079	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.068                                                   	MRR:14.24	Hits@10:33.31	Best:14.24
2025-01-07 21:32:32,765: Snapshot:4	Epoch:12	Loss:0.066	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.057                                                   	MRR:14.25	Hits@10:33.39	Best:14.25
2025-01-07 21:32:34,505: Snapshot:4	Epoch:13	Loss:0.058	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:14.23	Hits@10:33.47	Best:14.25
2025-01-07 21:32:36,302: Snapshot:4	Epoch:14	Loss:0.053	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:14.24	Hits@10:33.41	Best:14.25
2025-01-07 21:32:38,112: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 14.25
2025-01-07 21:32:38,113: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:0.048 MRR:14.19 Best Results: 14.25
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:32:38,113: Snapshot:4	Epoch:15	Loss:0.048	translation_Loss:0.009	token_training_loss:0.0	distillation_Loss:0.039                                                   	MRR:14.19	Hits@10:33.44	Best:14.25
2025-01-07 21:32:39,858: Snapshot:4	Epoch:16	Loss:8.238	translation_Loss:1.188	token_training_loss:7.05	distillation_Loss:0.0                                                   	MRR:14.19	Hits@10:33.44	Best:14.25
2025-01-07 21:32:41,649: End of token training: 4 Epoch: 17 Loss:5.638 MRR:14.19 Best Results: 14.25
2025-01-07 21:32:41,649: Snapshot:4	Epoch:17	Loss:5.638	translation_Loss:1.184	token_training_loss:4.455	distillation_Loss:0.0                                                           	MRR:14.19	Hits@10:33.44	Best:14.25
2025-01-07 21:32:41,726: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 21:32:49,667: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1631 | 0.0078 | 0.288  | 0.3565 |  0.4154 |
|     1      | 0.1328 | 0.003  | 0.2384 | 0.2772 |  0.325  |
|     2      | 0.1382 | 0.0056 | 0.2481 | 0.289  |  0.3301 |
|     3      | 0.1348 | 0.0065 | 0.2398 | 0.2849 |  0.3218 |
|     4      | 0.1407 | 0.0083 | 0.2504 | 0.2955 |  0.3404 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,800
Trainable params: 2,800
Non-trainable params: 8,193,000
=================================================================
2025-01-07 21:32:49,675: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1651 | 0.0071 | 0.296  | 0.359  |  0.4117 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1665 | 0.0076 | 0.2969 | 0.3617 |  0.4181 |
|     1      | 0.1378 | 0.0046 | 0.2457 | 0.2917 |  0.3304 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.164  | 0.0075 | 0.2912 | 0.3578 |  0.4165 |
|     1      | 0.1358 | 0.0051 | 0.2411 | 0.2841 |  0.3285 |
|     2      | 0.1419 | 0.0075 | 0.257  | 0.2944 |  0.3309 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1632 | 0.0075 | 0.2879 | 0.3574 |  0.4157 |
|     1      | 0.1335 | 0.004  | 0.236  | 0.279  |  0.3272 |
|     2      | 0.1388 | 0.0059 | 0.2495 | 0.2909 |  0.328  |
|     3      | 0.1358 | 0.0081 | 0.2409 | 0.2831 |  0.3183 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1631 | 0.0078 | 0.288  | 0.3565 |  0.4154 |
|     1      | 0.1328 | 0.003  | 0.2384 | 0.2772 |  0.325  |
|     2      | 0.1382 | 0.0056 | 0.2481 | 0.289  |  0.3301 |
|     3      | 0.1348 | 0.0065 | 0.2398 | 0.2849 |  0.3218 |
|     4      | 0.1407 | 0.0083 | 0.2504 | 0.2955 |  0.3404 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 21:32:49,676: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 323.9419975280762  |   0.165   |    0.007     |    0.296     |     0.412     |
|    1     | 21.344873189926147 |   0.162   |    0.007     |     0.29     |     0.406     |
|    2     | 25.13149094581604  |   0.158   |    0.007     |    0.281     |     0.395     |
|    3     | 28.421833515167236 |   0.154   |    0.007     |    0.273     |     0.385     |
|    4     | 34.969122648239136 |   0.153   |    0.007     |     0.27     |     0.381     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 21:32:49,676: Sum_Training_Time:433.80931782722473
2025-01-07 21:32:49,676: Every_Training_Time:[323.9419975280762, 21.344873189926147, 25.13149094581604, 28.421833515167236, 34.969122648239136]
2025-01-07 21:32:49,676: Forward transfer: 0.060375000000000005 Backward transfer: -0.002925000000000004
2025-01-07 21:33:01,922: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/WN_CKGE/', dataset='WN_CKGE', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250107213253/WN_CKGE', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=5555, record=False, reply_loss_weight=0.1, save_path='./checkpoint/WN_CKGE', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-07 21:33:11,648: Snapshot:0	Epoch:0	Loss:15.291	translation_Loss:15.291	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:1.72	Hits@10:4.8	Best:1.72
2025-01-07 21:33:19,455: Snapshot:0	Epoch:1	Loss:8.213	translation_Loss:8.213	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.44	Hits@10:16.87	Best:6.44
2025-01-07 21:33:27,383: Snapshot:0	Epoch:2	Loss:3.736	translation_Loss:3.736	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:11.23	Hits@10:28.12	Best:11.23
2025-01-07 21:33:35,101: Snapshot:0	Epoch:3	Loss:1.57	translation_Loss:1.57	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.45	Hits@10:33.29	Best:13.45
2025-01-07 21:33:42,435: Snapshot:0	Epoch:4	Loss:0.848	translation_Loss:0.848	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.44	Hits@10:35.54	Best:14.44
2025-01-07 21:33:50,194: Snapshot:0	Epoch:5	Loss:0.532	translation_Loss:0.532	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:14.92	Hits@10:36.59	Best:14.92
2025-01-07 21:33:57,772: Snapshot:0	Epoch:6	Loss:0.358	translation_Loss:0.358	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.26	Hits@10:37.44	Best:15.26
2025-01-07 21:34:06,379: Snapshot:0	Epoch:7	Loss:0.254	translation_Loss:0.254	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.47	Hits@10:37.91	Best:15.47
2025-01-07 21:34:13,838: Snapshot:0	Epoch:8	Loss:0.186	translation_Loss:0.186	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.63	Hits@10:38.13	Best:15.63
2025-01-07 21:34:21,464: Snapshot:0	Epoch:9	Loss:0.143	translation_Loss:0.143	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.74	Hits@10:38.52	Best:15.74
2025-01-07 21:34:29,108: Snapshot:0	Epoch:10	Loss:0.117	translation_Loss:0.117	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.81	Hits@10:38.75	Best:15.81
2025-01-07 21:34:36,548: Snapshot:0	Epoch:11	Loss:0.097	translation_Loss:0.097	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.89	Hits@10:38.93	Best:15.89
2025-01-07 21:34:44,190: Snapshot:0	Epoch:12	Loss:0.082	translation_Loss:0.082	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.96	Hits@10:39.02	Best:15.96
2025-01-07 21:34:51,693: Snapshot:0	Epoch:13	Loss:0.067	translation_Loss:0.067	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.04	Hits@10:39.25	Best:16.04
2025-01-07 21:34:59,489: Snapshot:0	Epoch:14	Loss:0.061	translation_Loss:0.061	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.1	Hits@10:39.3	Best:16.1
2025-01-07 21:35:07,289: Snapshot:0	Epoch:15	Loss:0.054	translation_Loss:0.054	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.14	Hits@10:39.53	Best:16.14
2025-01-07 21:35:15,013: Snapshot:0	Epoch:16	Loss:0.049	translation_Loss:0.049	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.22	Hits@10:39.78	Best:16.22
2025-01-07 21:35:23,768: Snapshot:0	Epoch:17	Loss:0.043	translation_Loss:0.043	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.18	Hits@10:39.82	Best:16.22
2025-01-07 21:35:31,724: Snapshot:0	Epoch:18	Loss:0.043	translation_Loss:0.043	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.2	Hits@10:39.8	Best:16.22
2025-01-07 21:35:39,594: Snapshot:0	Epoch:19	Loss:0.038	translation_Loss:0.038	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.24	Hits@10:39.93	Best:16.24
2025-01-07 21:35:47,144: Snapshot:0	Epoch:20	Loss:0.035	translation_Loss:0.035	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.26	Hits@10:39.98	Best:16.26
2025-01-07 21:35:55,031: Snapshot:0	Epoch:21	Loss:0.034	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.27	Hits@10:39.97	Best:16.27
2025-01-07 21:36:02,630: Snapshot:0	Epoch:22	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.3	Hits@10:40.05	Best:16.3
2025-01-07 21:36:10,496: Snapshot:0	Epoch:23	Loss:0.033	translation_Loss:0.033	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.33	Hits@10:40.15	Best:16.33
2025-01-07 21:36:18,191: Snapshot:0	Epoch:24	Loss:0.032	translation_Loss:0.032	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.29	Hits@10:40.32	Best:16.33
2025-01-07 21:36:25,639: Snapshot:0	Epoch:25	Loss:0.031	translation_Loss:0.031	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.26	Hits@10:40.33	Best:16.33
2025-01-07 21:36:33,523: Early Stopping! Snapshot: 0 Epoch: 26 Best Results: 16.33
2025-01-07 21:36:33,524: Start to training tokens! Snapshot: 0 Epoch: 26 Loss:0.03 MRR:16.33 Best Results: 16.33
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:36:33,524: Snapshot:0	Epoch:26	Loss:0.03	translation_Loss:0.03	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:16.33	Hits@10:40.38	Best:16.33
2025-01-07 21:36:42,543: Snapshot:0	Epoch:27	Loss:22.051	translation_Loss:5.699	token_training_loss:16.352	distillation_Loss:0.0                                                   	MRR:16.33	Hits@10:40.38	Best:16.33
2025-01-07 21:36:50,276: End of token training: 0 Epoch: 28 Loss:6.209 MRR:16.33 Best Results: 16.33
2025-01-07 21:36:50,276: Snapshot:0	Epoch:28	Loss:6.209	translation_Loss:5.698	token_training_loss:0.511	distillation_Loss:0.0                                                           	MRR:16.33	Hits@10:40.38	Best:16.33
2025-01-07 21:36:50,378: => loading checkpoint './checkpoint/WN_CKGE/0model_best.tar'
2025-01-07 21:36:54,141: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.165 | 0.0071 | 0.2989 | 0.357  |  0.4052 |
+------------+-------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (4,913,400)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 4,920,600
Trainable params: 2,800
Non-trainable params: 4,917,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:36:58,043: Snapshot:1	Epoch:0	Loss:2.718	translation_Loss:2.634	token_training_loss:0.0	distillation_Loss:0.085                                                   	MRR:2.41	Hits@10:6.32	Best:2.41
2025-01-07 21:36:59,594: Snapshot:1	Epoch:1	Loss:1.737	translation_Loss:1.584	token_training_loss:0.0	distillation_Loss:0.153                                                   	MRR:7.34	Hits@10:19.33	Best:7.34
2025-01-07 21:37:01,103: Snapshot:1	Epoch:2	Loss:0.969	translation_Loss:0.775	token_training_loss:0.0	distillation_Loss:0.194                                                   	MRR:10.65	Hits@10:26.34	Best:10.65
2025-01-07 21:37:02,851: Snapshot:1	Epoch:3	Loss:0.533	translation_Loss:0.288	token_training_loss:0.0	distillation_Loss:0.245                                                   	MRR:12.44	Hits@10:29.97	Best:12.44
2025-01-07 21:37:04,439: Snapshot:1	Epoch:4	Loss:0.384	translation_Loss:0.111	token_training_loss:0.0	distillation_Loss:0.273                                                   	MRR:13.33	Hits@10:31.61	Best:13.33
2025-01-07 21:37:05,986: Snapshot:1	Epoch:5	Loss:0.315	translation_Loss:0.049	token_training_loss:0.0	distillation_Loss:0.266                                                   	MRR:13.79	Hits@10:32.47	Best:13.79
2025-01-07 21:37:07,517: Snapshot:1	Epoch:6	Loss:0.26	translation_Loss:0.026	token_training_loss:0.0	distillation_Loss:0.235                                                   	MRR:13.94	Hits@10:33.04	Best:13.94
2025-01-07 21:37:09,043: Snapshot:1	Epoch:7	Loss:0.211	translation_Loss:0.018	token_training_loss:0.0	distillation_Loss:0.193                                                   	MRR:13.97	Hits@10:33.2	Best:13.97
2025-01-07 21:37:10,724: Snapshot:1	Epoch:8	Loss:0.167	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.152                                                   	MRR:13.87	Hits@10:33.25	Best:13.97
2025-01-07 21:37:12,210: Snapshot:1	Epoch:9	Loss:0.131	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.117                                                   	MRR:13.87	Hits@10:33.25	Best:13.97
2025-01-07 21:37:13,651: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 13.97
2025-01-07 21:37:13,652: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:0.107 MRR:13.79 Best Results: 13.97
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:37:13,652: Snapshot:1	Epoch:10	Loss:0.107	translation_Loss:0.017	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:13.79	Hits@10:33.52	Best:13.97
2025-01-07 21:37:15,070: Snapshot:1	Epoch:11	Loss:8.342	translation_Loss:1.225	token_training_loss:7.117	distillation_Loss:0.0                                                   	MRR:13.79	Hits@10:33.52	Best:13.97
2025-01-07 21:37:16,501: End of token training: 1 Epoch: 12 Loss:5.792 MRR:13.79 Best Results: 13.97
2025-01-07 21:37:16,501: Snapshot:1	Epoch:12	Loss:5.792	translation_Loss:1.226	token_training_loss:4.565	distillation_Loss:0.0                                                           	MRR:13.79	Hits@10:33.52	Best:13.97
2025-01-07 21:37:16,573: => loading checkpoint './checkpoint/WN_CKGE/1model_best.tar'
2025-01-07 21:37:21,349: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1659 | 0.0073 | 0.2986 | 0.3614 |  0.4131 |
|     1      | 0.1389 | 0.0054 | 0.2508 | 0.2879 |  0.3317 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (5,732,000)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 5,739,200
Trainable params: 2,800
Non-trainable params: 5,736,400
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:37:25,301: Snapshot:2	Epoch:0	Loss:2.624	translation_Loss:2.575	token_training_loss:0.0	distillation_Loss:0.049                                                   	MRR:2.74	Hits@10:7.39	Best:2.74
2025-01-07 21:37:26,964: Snapshot:2	Epoch:1	Loss:1.623	translation_Loss:1.491	token_training_loss:0.0	distillation_Loss:0.132                                                   	MRR:7.63	Hits@10:19.22	Best:7.63
2025-01-07 21:37:28,642: Snapshot:2	Epoch:2	Loss:0.823	translation_Loss:0.66	token_training_loss:0.0	distillation_Loss:0.163                                                   	MRR:10.67	Hits@10:25.56	Best:10.67
2025-01-07 21:37:30,302: Snapshot:2	Epoch:3	Loss:0.395	translation_Loss:0.216	token_training_loss:0.0	distillation_Loss:0.179                                                   	MRR:12.14	Hits@10:27.77	Best:12.14
2025-01-07 21:37:31,999: Snapshot:2	Epoch:4	Loss:0.271	translation_Loss:0.079	token_training_loss:0.0	distillation_Loss:0.192                                                   	MRR:12.95	Hits@10:28.84	Best:12.95
2025-01-07 21:37:33,675: Snapshot:2	Epoch:5	Loss:0.23	translation_Loss:0.036	token_training_loss:0.0	distillation_Loss:0.194                                                   	MRR:13.33	Hits@10:29.57	Best:13.33
2025-01-07 21:37:35,532: Snapshot:2	Epoch:6	Loss:0.203	translation_Loss:0.02	token_training_loss:0.0	distillation_Loss:0.183                                                   	MRR:13.46	Hits@10:30.03	Best:13.46
2025-01-07 21:37:37,482: Snapshot:2	Epoch:7	Loss:0.178	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.165                                                   	MRR:13.52	Hits@10:30.3	Best:13.52
2025-01-07 21:37:39,422: Snapshot:2	Epoch:8	Loss:0.155	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.144                                                   	MRR:13.56	Hits@10:30.46	Best:13.56
2025-01-07 21:37:41,007: Snapshot:2	Epoch:9	Loss:0.13	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.122                                                   	MRR:13.55	Hits@10:30.67	Best:13.56
2025-01-07 21:37:42,555: Snapshot:2	Epoch:10	Loss:0.108	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.102                                                   	MRR:13.5	Hits@10:30.67	Best:13.56
2025-01-07 21:37:44,173: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 13.56
2025-01-07 21:37:44,173: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:0.09 MRR:13.46 Best Results: 13.56
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:37:44,173: Snapshot:2	Epoch:11	Loss:0.09	translation_Loss:0.007	token_training_loss:0.0	distillation_Loss:0.084                                                   	MRR:13.46	Hits@10:30.56	Best:13.56
2025-01-07 21:37:45,881: Snapshot:2	Epoch:12	Loss:8.485	translation_Loss:1.204	token_training_loss:7.281	distillation_Loss:0.0                                                   	MRR:13.46	Hits@10:30.56	Best:13.56
2025-01-07 21:37:47,591: End of token training: 2 Epoch: 13 Loss:5.972 MRR:13.46 Best Results: 13.56
2025-01-07 21:37:47,591: Snapshot:2	Epoch:13	Loss:5.972	translation_Loss:1.205	token_training_loss:4.767	distillation_Loss:0.0                                                           	MRR:13.46	Hits@10:30.56	Best:13.56
2025-01-07 21:37:47,669: => loading checkpoint './checkpoint/WN_CKGE/2model_best.tar'
2025-01-07 21:37:53,760: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1628 | 0.0075 | 0.2899 | 0.355  |  0.4087 |
|     1      | 0.1366 | 0.0059 | 0.2444 | 0.2874 |  0.3301 |
|     2      | 0.1401 | 0.0073 | 0.254  | 0.2844 |  0.3218 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (6,550,800)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 6,558,000
Trainable params: 2,800
Non-trainable params: 6,555,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:37:57,980: Snapshot:3	Epoch:0	Loss:2.614	translation_Loss:2.531	token_training_loss:0.0	distillation_Loss:0.083                                                   	MRR:2.97	Hits@10:7.5	Best:2.97
2025-01-07 21:38:00,030: Snapshot:3	Epoch:1	Loss:1.597	translation_Loss:1.421	token_training_loss:0.0	distillation_Loss:0.177                                                   	MRR:7.45	Hits@10:19.03	Best:7.45
2025-01-07 21:38:01,815: Snapshot:3	Epoch:2	Loss:0.805	translation_Loss:0.6	token_training_loss:0.0	distillation_Loss:0.205                                                   	MRR:10.68	Hits@10:24.92	Best:10.68
2025-01-07 21:38:03,806: Snapshot:3	Epoch:3	Loss:0.423	translation_Loss:0.186	token_training_loss:0.0	distillation_Loss:0.236                                                   	MRR:12.17	Hits@10:27.74	Best:12.17
2025-01-07 21:38:05,578: Snapshot:3	Epoch:4	Loss:0.325	translation_Loss:0.071	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:12.88	Hits@10:28.66	Best:12.88
2025-01-07 21:38:07,373: Snapshot:3	Epoch:5	Loss:0.28	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.245                                                   	MRR:13.24	Hits@10:29.52	Best:13.24
2025-01-07 21:38:09,161: Snapshot:3	Epoch:6	Loss:0.239	translation_Loss:0.023	token_training_loss:0.0	distillation_Loss:0.216                                                   	MRR:13.42	Hits@10:29.78	Best:13.42
2025-01-07 21:38:10,818: Snapshot:3	Epoch:7	Loss:0.198	translation_Loss:0.016	token_training_loss:0.0	distillation_Loss:0.182                                                   	MRR:13.4	Hits@10:29.95	Best:13.42
2025-01-07 21:38:12,671: Snapshot:3	Epoch:8	Loss:0.16	translation_Loss:0.014	token_training_loss:0.0	distillation_Loss:0.147                                                   	MRR:13.44	Hits@10:30.05	Best:13.44
2025-01-07 21:38:14,466: Snapshot:3	Epoch:9	Loss:0.126	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.115                                                   	MRR:13.46	Hits@10:30.22	Best:13.46
2025-01-07 21:38:16,138: Snapshot:3	Epoch:10	Loss:0.102	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.09                                                   	MRR:13.46	Hits@10:30.27	Best:13.46
2025-01-07 21:38:17,905: Snapshot:3	Epoch:11	Loss:0.083	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.072                                                   	MRR:13.48	Hits@10:30.35	Best:13.48
2025-01-07 21:38:19,677: Snapshot:3	Epoch:12	Loss:0.073	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.06                                                   	MRR:13.49	Hits@10:30.35	Best:13.49
2025-01-07 21:38:21,469: Snapshot:3	Epoch:13	Loss:0.064	translation_Loss:0.013	token_training_loss:0.0	distillation_Loss:0.052                                                   	MRR:13.56	Hits@10:30.48	Best:13.56
2025-01-07 21:38:23,368: Snapshot:3	Epoch:14	Loss:0.058	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.047                                                   	MRR:13.55	Hits@10:30.54	Best:13.56
2025-01-07 21:38:25,080: Snapshot:3	Epoch:15	Loss:0.053	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.043                                                   	MRR:13.51	Hits@10:30.59	Best:13.56
2025-01-07 21:38:26,705: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 13.56
2025-01-07 21:38:26,705: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:0.051 MRR:13.51 Best Results: 13.56
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:38:26,705: Snapshot:3	Epoch:16	Loss:0.051	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.041                                                   	MRR:13.51	Hits@10:30.56	Best:13.56
2025-01-07 21:38:28,331: Snapshot:3	Epoch:17	Loss:8.267	translation_Loss:1.215	token_training_loss:7.052	distillation_Loss:0.0                                                   	MRR:13.51	Hits@10:30.56	Best:13.56
2025-01-07 21:38:29,947: End of token training: 3 Epoch: 18 Loss:5.719 MRR:13.51 Best Results: 13.56
2025-01-07 21:38:29,947: Snapshot:3	Epoch:18	Loss:5.719	translation_Loss:1.217	token_training_loss:4.502	distillation_Loss:0.0                                                           	MRR:13.51	Hits@10:30.56	Best:13.56
2025-01-07 21:38:30,036: => loading checkpoint './checkpoint/WN_CKGE/3model_best.tar'
2025-01-07 21:38:37,125: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1623 | 0.0076 | 0.2878 | 0.3558 |  0.4092 |
|     1      | 0.1367 | 0.0054 | 0.243  | 0.289  |  0.3306 |
|     2      | 0.1394 | 0.007  | 0.2519 | 0.2852 |  0.321  |
|     3      | 0.1367 | 0.0078 | 0.2444 | 0.2806 |  0.3161 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (7,369,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 7,376,800
Trainable params: 2,800
Non-trainable params: 7,374,000
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-07 21:38:41,641: Snapshot:4	Epoch:0	Loss:2.566	translation_Loss:2.483	token_training_loss:0.0	distillation_Loss:0.082                                                   	MRR:10.03	Hits@10:24.14	Best:10.03
2025-01-07 21:38:43,698: Snapshot:4	Epoch:1	Loss:1.542	translation_Loss:1.358	token_training_loss:0.0	distillation_Loss:0.184                                                   	MRR:11.76	Hits@10:27.93	Best:11.76
2025-01-07 21:38:45,728: Snapshot:4	Epoch:2	Loss:0.748	translation_Loss:0.539	token_training_loss:0.0	distillation_Loss:0.209                                                   	MRR:12.91	Hits@10:30.19	Best:12.91
2025-01-07 21:38:47,628: Snapshot:4	Epoch:3	Loss:0.378	translation_Loss:0.151	token_training_loss:0.0	distillation_Loss:0.228                                                   	MRR:13.39	Hits@10:30.89	Best:13.39
2025-01-07 21:38:49,764: Snapshot:4	Epoch:4	Loss:0.296	translation_Loss:0.058	token_training_loss:0.0	distillation_Loss:0.238                                                   	MRR:13.72	Hits@10:31.42	Best:13.72
2025-01-07 21:38:51,744: Snapshot:4	Epoch:5	Loss:0.26	translation_Loss:0.034	token_training_loss:0.0	distillation_Loss:0.226                                                   	MRR:13.86	Hits@10:31.64	Best:13.86
2025-01-07 21:38:54,021: Snapshot:4	Epoch:6	Loss:0.222	translation_Loss:0.022	token_training_loss:0.0	distillation_Loss:0.2                                                   	MRR:13.89	Hits@10:32.02	Best:13.89
2025-01-07 21:38:55,945: Snapshot:4	Epoch:7	Loss:0.185	translation_Loss:0.015	token_training_loss:0.0	distillation_Loss:0.17                                                   	MRR:13.96	Hits@10:32.12	Best:13.96
2025-01-07 21:38:57,905: Snapshot:4	Epoch:8	Loss:0.151	translation_Loss:0.012	token_training_loss:0.0	distillation_Loss:0.139                                                   	MRR:13.99	Hits@10:32.47	Best:13.99
2025-01-07 21:38:59,816: Snapshot:4	Epoch:9	Loss:0.122	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.111                                                   	MRR:14.05	Hits@10:32.55	Best:14.05
2025-01-07 21:39:01,717: Snapshot:4	Epoch:10	Loss:0.099	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.088                                                   	MRR:14.09	Hits@10:32.47	Best:14.09
2025-01-07 21:39:03,661: Snapshot:4	Epoch:11	Loss:0.082	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.071                                                   	MRR:14.15	Hits@10:32.47	Best:14.15
2025-01-07 21:39:05,447: Snapshot:4	Epoch:12	Loss:0.07	translation_Loss:0.011	token_training_loss:0.0	distillation_Loss:0.059                                                   	MRR:14.12	Hits@10:32.45	Best:14.15
2025-01-07 21:39:07,369: Snapshot:4	Epoch:13	Loss:0.061	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.051                                                   	MRR:14.17	Hits@10:32.39	Best:14.17
2025-01-07 21:39:09,132: Snapshot:4	Epoch:14	Loss:0.056	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.046                                                   	MRR:14.15	Hits@10:32.47	Best:14.17
2025-01-07 21:39:10,923: Snapshot:4	Epoch:15	Loss:0.052	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.042                                                   	MRR:14.14	Hits@10:32.55	Best:14.17
2025-01-07 21:39:12,930: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 14.17
2025-01-07 21:39:12,931: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:0.049 MRR:14.13 Best Results: 14.17
Token added to optimizer, embeddings excluded successfully.
2025-01-07 21:39:12,931: Snapshot:4	Epoch:16	Loss:0.049	translation_Loss:0.01	token_training_loss:0.0	distillation_Loss:0.039                                                   	MRR:14.13	Hits@10:32.55	Best:14.17
2025-01-07 21:39:14,681: Snapshot:4	Epoch:17	Loss:8.281	translation_Loss:1.2	token_training_loss:7.082	distillation_Loss:0.0                                                   	MRR:14.13	Hits@10:32.55	Best:14.17
2025-01-07 21:39:16,473: End of token training: 4 Epoch: 18 Loss:5.709 MRR:14.13 Best Results: 14.17
2025-01-07 21:39:16,473: Snapshot:4	Epoch:18	Loss:5.709	translation_Loss:1.201	token_training_loss:4.508	distillation_Loss:0.0                                                           	MRR:14.13	Hits@10:32.55	Best:14.17
2025-01-07 21:39:16,544: => loading checkpoint './checkpoint/WN_CKGE/4model_best.tar'
2025-01-07 21:39:24,784: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.162  | 0.0073 | 0.2873 | 0.3551 |   0.41  |
|     1      | 0.1357 | 0.0043 | 0.2433 | 0.2863 |  0.329  |
|     2      | 0.1385 | 0.0059 | 0.2527 | 0.2825 |  0.3204 |
|     3      | 0.1362 | 0.007  | 0.2454 | 0.2825 |  0.3153 |
|     4      | 0.1399 | 0.0078 | 0.2523 | 0.2955 |  0.3332 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (8,188,600)
├─Embedding: 1-2                         (4,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 8,195,800
Trainable params: 2,800
Non-trainable params: 8,193,000
=================================================================
2025-01-07 21:39:24,786: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.165 | 0.0071 | 0.2989 | 0.357  |  0.4052 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1659 | 0.0073 | 0.2986 | 0.3614 |  0.4131 |
|     1      | 0.1389 | 0.0054 | 0.2508 | 0.2879 |  0.3317 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1628 | 0.0075 | 0.2899 | 0.355  |  0.4087 |
|     1      | 0.1366 | 0.0059 | 0.2444 | 0.2874 |  0.3301 |
|     2      | 0.1401 | 0.0073 | 0.254  | 0.2844 |  0.3218 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1623 | 0.0076 | 0.2878 | 0.3558 |  0.4092 |
|     1      | 0.1367 | 0.0054 | 0.243  | 0.289  |  0.3306 |
|     2      | 0.1394 | 0.007  | 0.2519 | 0.2852 |  0.321  |
|     3      | 0.1367 | 0.0078 | 0.2444 | 0.2806 |  0.3161 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.162  | 0.0073 | 0.2873 | 0.3551 |   0.41  |
|     1      | 0.1357 | 0.0043 | 0.2433 | 0.2863 |  0.329  |
|     2      | 0.1385 | 0.0059 | 0.2527 | 0.2825 |  0.3204 |
|     3      | 0.1362 | 0.007  | 0.2454 | 0.2825 |  0.3153 |
|     4      | 0.1399 | 0.0078 | 0.2523 | 0.2955 |  0.3332 |
+------------+--------+--------+--------+--------+---------+]
2025-01-07 21:39:24,787: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 228.35416889190674 |   0.165   |    0.007     |    0.299     |     0.405     |
|    1     | 21.326042413711548 |   0.162   |    0.007     |    0.292     |     0.401     |
|    2     | 25.130569458007812 |   0.157   |    0.007     |     0.28     |     0.388     |
|    3     | 34.88357377052307  |   0.154   |    0.007     |    0.274     |      0.38     |
|    4     | 38.161887884140015 |   0.152   |    0.007     |    0.272     |     0.376     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-07 21:39:24,787: Sum_Training_Time:347.8562424182892
2025-01-07 21:39:24,787: Every_Training_Time:[228.35416889190674, 21.326042413711548, 25.130569458007812, 34.88357377052307, 38.161887884140015]
2025-01-07 21:39:24,787: Forward transfer: 0.060875 Backward transfer: -0.0020750000000000005
