2025-01-06 22:23:29,490: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106222313/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=1111, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[3000.0, 15000.0, 80000.0, 80000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:23:46,877: Snapshot:0	Epoch:0	Loss:24.059	translation_Loss:24.059	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:11.1	Hits@10:26.27	Best:11.1
2025-01-06 22:24:00,074: Snapshot:0	Epoch:1	Loss:14.887	translation_Loss:14.887	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.9	Hits@10:40.17	Best:18.9
2025-01-06 22:24:13,300: Snapshot:0	Epoch:2	Loss:8.522	translation_Loss:8.522	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.91	Hits@10:45.08	Best:23.91
2025-01-06 22:24:26,006: Snapshot:0	Epoch:3	Loss:4.749	translation_Loss:4.749	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:26.47	Hits@10:47.22	Best:26.47
2025-01-06 22:24:39,128: Snapshot:0	Epoch:4	Loss:2.781	translation_Loss:2.781	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.37	Hits@10:47.99	Best:27.37
2025-01-06 22:24:52,386: Snapshot:0	Epoch:5	Loss:1.837	translation_Loss:1.837	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.66	Hits@10:48.23	Best:27.66
2025-01-06 22:25:05,226: Snapshot:0	Epoch:6	Loss:1.368	translation_Loss:1.368	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.79	Hits@10:48.43	Best:27.79
2025-01-06 22:25:18,464: Snapshot:0	Epoch:7	Loss:1.121	translation_Loss:1.121	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.68	Hits@10:48.26	Best:27.79
2025-01-06 22:25:31,556: Snapshot:0	Epoch:8	Loss:0.969	translation_Loss:0.969	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.66	Hits@10:48.11	Best:27.79
2025-01-06 22:25:44,267: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 27.79
2025-01-06 22:25:44,268: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:0.859 MRR:27.77 Best Results: 27.79
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:25:44,268: Snapshot:0	Epoch:9	Loss:0.859	translation_Loss:0.859	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.77	Hits@10:48.1	Best:27.79
2025-01-06 22:25:57,983: Snapshot:0	Epoch:10	Loss:31.168	translation_Loss:16.444	token_training_loss:14.724	distillation_Loss:0.0                                                   	MRR:27.77	Hits@10:48.1	Best:27.79
2025-01-06 22:26:11,257: End of token training: 0 Epoch: 11 Loss:16.52 MRR:27.77 Best Results: 27.79
2025-01-06 22:26:11,262: Snapshot:0	Epoch:11	Loss:16.52	translation_Loss:16.436	token_training_loss:0.084	distillation_Loss:0.0                                                           	MRR:27.77	Hits@10:48.1	Best:27.79
2025-01-06 22:26:11,583: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2025-01-06 22:26:17,170: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2806 | 0.1687 | 0.3476 | 0.4154 |  0.4872 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,312,200)
├─Embedding: 1-2                         (19,600)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,333,800
Trainable params: 2,000
Non-trainable params: 2,331,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:26:37,896: Snapshot:1	Epoch:0	Loss:20.244	translation_Loss:19.014	token_training_loss:0.0	distillation_Loss:1.23                                                   	MRR:10.19	Hits@10:24.89	Best:10.19
2025-01-06 22:26:50,157: Snapshot:1	Epoch:1	Loss:10.659	translation_Loss:8.413	token_training_loss:0.0	distillation_Loss:2.246                                                   	MRR:15.77	Hits@10:32.2	Best:15.77
2025-01-06 22:27:02,047: Snapshot:1	Epoch:2	Loss:6.838	translation_Loss:4.396	token_training_loss:0.0	distillation_Loss:2.442                                                   	MRR:16.83	Hits@10:33.74	Best:16.83
2025-01-06 22:27:14,347: Snapshot:1	Epoch:3	Loss:5.442	translation_Loss:3.134	token_training_loss:0.0	distillation_Loss:2.308                                                   	MRR:17.07	Hits@10:33.64	Best:17.07
2025-01-06 22:27:26,686: Snapshot:1	Epoch:4	Loss:4.917	translation_Loss:2.737	token_training_loss:0.0	distillation_Loss:2.181                                                   	MRR:17.14	Hits@10:33.33	Best:17.14
2025-01-06 22:27:38,435: Snapshot:1	Epoch:5	Loss:4.67	translation_Loss:2.567	token_training_loss:0.0	distillation_Loss:2.104                                                   	MRR:17.09	Hits@10:33.08	Best:17.14
2025-01-06 22:27:50,666: Snapshot:1	Epoch:6	Loss:4.568	translation_Loss:2.511	token_training_loss:0.0	distillation_Loss:2.057                                                   	MRR:16.9	Hits@10:32.8	Best:17.14
2025-01-06 22:28:02,800: Early Stopping! Snapshot: 1 Epoch: 7 Best Results: 17.14
2025-01-06 22:28:02,800: Start to training tokens! Snapshot: 1 Epoch: 7 Loss:4.491 MRR:16.91 Best Results: 17.14
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:28:02,800: Snapshot:1	Epoch:7	Loss:4.491	translation_Loss:2.466	token_training_loss:0.0	distillation_Loss:2.025                                                   	MRR:16.91	Hits@10:32.87	Best:17.14
2025-01-06 22:28:14,335: Snapshot:1	Epoch:8	Loss:34.69	translation_Loss:19.511	token_training_loss:15.18	distillation_Loss:0.0                                                   	MRR:16.91	Hits@10:32.87	Best:17.14
2025-01-06 22:28:26,297: End of token training: 1 Epoch: 9 Loss:19.6 MRR:16.91 Best Results: 17.14
2025-01-06 22:28:26,297: Snapshot:1	Epoch:9	Loss:19.6	translation_Loss:19.493	token_training_loss:0.107	distillation_Loss:0.0                                                           	MRR:16.91	Hits@10:32.87	Best:17.14
2025-01-06 22:28:26,604: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2025-01-06 22:28:37,097: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2497 | 0.1404 | 0.3111 | 0.3797 |  0.4527 |
|     1      | 0.1732 | 0.0887 | 0.2068 | 0.2629 |  0.3331 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,668,800)
├─Embedding: 1-2                         (38,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,709,200
Trainable params: 2,000
Non-trainable params: 2,707,200
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:28:53,801: Snapshot:2	Epoch:0	Loss:13.678	translation_Loss:11.88	token_training_loss:0.0	distillation_Loss:1.798                                                   	MRR:10.93	Hits@10:26.41	Best:10.93
2025-01-06 22:29:02,907: Snapshot:2	Epoch:1	Loss:7.733	translation_Loss:5.147	token_training_loss:0.0	distillation_Loss:2.586                                                   	MRR:17.85	Hits@10:35.1	Best:17.85
2025-01-06 22:29:11,604: Snapshot:2	Epoch:2	Loss:5.429	translation_Loss:3.45	token_training_loss:0.0	distillation_Loss:1.979                                                   	MRR:19.75	Hits@10:35.68	Best:19.75
2025-01-06 22:29:20,721: Snapshot:2	Epoch:3	Loss:4.292	translation_Loss:2.673	token_training_loss:0.0	distillation_Loss:1.619                                                   	MRR:20.9	Hits@10:36.81	Best:20.9
2025-01-06 22:29:29,459: Snapshot:2	Epoch:4	Loss:3.694	translation_Loss:2.317	token_training_loss:0.0	distillation_Loss:1.377                                                   	MRR:21.09	Hits@10:37.08	Best:21.09
2025-01-06 22:29:38,627: Snapshot:2	Epoch:5	Loss:3.403	translation_Loss:2.159	token_training_loss:0.0	distillation_Loss:1.244                                                   	MRR:21.4	Hits@10:37.2	Best:21.4
2025-01-06 22:29:47,360: Snapshot:2	Epoch:6	Loss:3.254	translation_Loss:2.085	token_training_loss:0.0	distillation_Loss:1.17                                                   	MRR:21.18	Hits@10:36.76	Best:21.4
2025-01-06 22:29:56,486: Snapshot:2	Epoch:7	Loss:3.182	translation_Loss:2.043	token_training_loss:0.0	distillation_Loss:1.138                                                   	MRR:21.17	Hits@10:36.66	Best:21.4
2025-01-06 22:30:05,269: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 21.4
2025-01-06 22:30:05,270: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:3.124 MRR:20.96 Best Results: 21.4
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:30:05,270: Snapshot:2	Epoch:8	Loss:3.124	translation_Loss:2.012	token_training_loss:0.0	distillation_Loss:1.113                                                   	MRR:20.96	Hits@10:36.69	Best:21.4
2025-01-06 22:30:13,892: Snapshot:2	Epoch:9	Loss:30.161	translation_Loss:15.062	token_training_loss:15.099	distillation_Loss:0.0                                                   	MRR:20.96	Hits@10:36.69	Best:21.4
2025-01-06 22:30:22,847: End of token training: 2 Epoch: 10 Loss:15.363 MRR:20.96 Best Results: 21.4
2025-01-06 22:30:22,847: Snapshot:2	Epoch:10	Loss:15.363	translation_Loss:15.074	token_training_loss:0.29	distillation_Loss:0.0                                                           	MRR:20.96	Hits@10:36.69	Best:21.4
2025-01-06 22:30:23,150: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2025-01-06 22:30:37,082: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.218  | 0.117  | 0.276  | 0.3386 |  0.4045 |
|     1      | 0.1618 | 0.0779 | 0.1946 | 0.2508 |  0.3209 |
|     2      | 0.2114 | 0.1324 | 0.2367 | 0.2912 |  0.3678 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,751,000)
├─Embedding: 1-2                         (57,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,810,200
Trainable params: 2,000
Non-trainable params: 2,808,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:30:47,458: Snapshot:3	Epoch:0	Loss:6.95	translation_Loss:6.259	token_training_loss:0.0	distillation_Loss:0.691                                                   	MRR:6.34	Hits@10:14.81	Best:6.34
2025-01-06 22:30:51,622: Snapshot:3	Epoch:1	Loss:4.99	translation_Loss:4.287	token_training_loss:0.0	distillation_Loss:0.703                                                   	MRR:13.7	Hits@10:29.13	Best:13.7
2025-01-06 22:30:56,097: Snapshot:3	Epoch:2	Loss:3.684	translation_Loss:3.078	token_training_loss:0.0	distillation_Loss:0.606                                                   	MRR:17.09	Hits@10:33.95	Best:17.09
2025-01-06 22:31:00,172: Snapshot:3	Epoch:3	Loss:2.936	translation_Loss:2.413	token_training_loss:0.0	distillation_Loss:0.522                                                   	MRR:20.29	Hits@10:36.17	Best:20.29
2025-01-06 22:31:04,269: Snapshot:3	Epoch:4	Loss:2.504	translation_Loss:2.027	token_training_loss:0.0	distillation_Loss:0.477                                                   	MRR:21.94	Hits@10:37.32	Best:21.94
2025-01-06 22:31:08,304: Snapshot:3	Epoch:5	Loss:2.203	translation_Loss:1.769	token_training_loss:0.0	distillation_Loss:0.434                                                   	MRR:23.14	Hits@10:38.11	Best:23.14
2025-01-06 22:31:12,882: Snapshot:3	Epoch:6	Loss:1.995	translation_Loss:1.596	token_training_loss:0.0	distillation_Loss:0.399                                                   	MRR:23.71	Hits@10:38.7	Best:23.71
2025-01-06 22:31:16,943: Snapshot:3	Epoch:7	Loss:1.823	translation_Loss:1.456	token_training_loss:0.0	distillation_Loss:0.367                                                   	MRR:24.19	Hits@10:38.88	Best:24.19
2025-01-06 22:31:20,949: Snapshot:3	Epoch:8	Loss:1.703	translation_Loss:1.359	token_training_loss:0.0	distillation_Loss:0.344                                                   	MRR:24.73	Hits@10:39.18	Best:24.73
2025-01-06 22:31:24,987: Snapshot:3	Epoch:9	Loss:1.621	translation_Loss:1.295	token_training_loss:0.0	distillation_Loss:0.327                                                   	MRR:25.12	Hits@10:39.39	Best:25.12
2025-01-06 22:31:29,074: Snapshot:3	Epoch:10	Loss:1.558	translation_Loss:1.244	token_training_loss:0.0	distillation_Loss:0.313                                                   	MRR:25.37	Hits@10:39.39	Best:25.37
2025-01-06 22:31:33,223: Snapshot:3	Epoch:11	Loss:1.516	translation_Loss:1.211	token_training_loss:0.0	distillation_Loss:0.306                                                   	MRR:25.42	Hits@10:39.52	Best:25.42
2025-01-06 22:31:37,672: Snapshot:3	Epoch:12	Loss:1.482	translation_Loss:1.185	token_training_loss:0.0	distillation_Loss:0.297                                                   	MRR:25.54	Hits@10:39.58	Best:25.54
2025-01-06 22:31:41,742: Snapshot:3	Epoch:13	Loss:1.448	translation_Loss:1.156	token_training_loss:0.0	distillation_Loss:0.292                                                   	MRR:25.61	Hits@10:39.8	Best:25.61
2025-01-06 22:31:45,906: Snapshot:3	Epoch:14	Loss:1.442	translation_Loss:1.152	token_training_loss:0.0	distillation_Loss:0.29                                                   	MRR:25.88	Hits@10:39.66	Best:25.88
2025-01-06 22:31:49,887: Snapshot:3	Epoch:15	Loss:1.423	translation_Loss:1.133	token_training_loss:0.0	distillation_Loss:0.29                                                   	MRR:25.52	Hits@10:39.73	Best:25.88
2025-01-06 22:31:54,296: Snapshot:3	Epoch:16	Loss:1.414	translation_Loss:1.127	token_training_loss:0.0	distillation_Loss:0.287                                                   	MRR:25.7	Hits@10:39.65	Best:25.88
2025-01-06 22:31:58,289: Early Stopping! Snapshot: 3 Epoch: 17 Best Results: 25.88
2025-01-06 22:31:58,289: Start to training tokens! Snapshot: 3 Epoch: 17 Loss:1.408 MRR:25.67 Best Results: 25.88
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:31:58,289: Snapshot:3	Epoch:17	Loss:1.408	translation_Loss:1.124	token_training_loss:0.0	distillation_Loss:0.284                                                   	MRR:25.67	Hits@10:39.48	Best:25.88
2025-01-06 22:32:02,214: Snapshot:3	Epoch:18	Loss:18.748	translation_Loss:5.775	token_training_loss:12.973	distillation_Loss:0.0                                                   	MRR:25.67	Hits@10:39.48	Best:25.88
2025-01-06 22:32:06,096: End of token training: 3 Epoch: 19 Loss:7.267 MRR:25.67 Best Results: 25.88
2025-01-06 22:32:06,096: Snapshot:3	Epoch:19	Loss:7.267	translation_Loss:5.785	token_training_loss:1.482	distillation_Loss:0.0                                                           	MRR:25.67	Hits@10:39.48	Best:25.88
2025-01-06 22:32:06,410: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2025-01-06 22:32:22,587: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2181 | 0.1174 | 0.276  | 0.3378 |  0.404  |
|     1      | 0.1619 | 0.0777 | 0.1946 | 0.2502 |  0.3219 |
|     2      | 0.2045 | 0.1238 | 0.2294 | 0.2869 |  0.3671 |
|     3      | 0.2541 | 0.177  | 0.2839 | 0.3325 |  0.3957 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,877,400)
├─Embedding: 1-2                         (76,000)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,955,400
Trainable params: 2,000
Non-trainable params: 2,953,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:32:31,346: Snapshot:4	Epoch:0	Loss:4.627	translation_Loss:4.108	token_training_loss:0.0	distillation_Loss:0.519                                                   	MRR:6.77	Hits@10:17.97	Best:6.77
2025-01-06 22:32:34,247: Snapshot:4	Epoch:1	Loss:3.495	translation_Loss:2.752	token_training_loss:0.0	distillation_Loss:0.742                                                   	MRR:10.77	Hits@10:27.76	Best:10.77
2025-01-06 22:32:37,161: Snapshot:4	Epoch:2	Loss:2.856	translation_Loss:2.107	token_training_loss:0.0	distillation_Loss:0.749                                                   	MRR:14.65	Hits@10:33.94	Best:14.65
2025-01-06 22:32:40,090: Snapshot:4	Epoch:3	Loss:2.401	translation_Loss:1.701	token_training_loss:0.0	distillation_Loss:0.7                                                   	MRR:17.42	Hits@10:36.95	Best:17.42
2025-01-06 22:32:43,011: Snapshot:4	Epoch:4	Loss:2.091	translation_Loss:1.452	token_training_loss:0.0	distillation_Loss:0.639                                                   	MRR:19.64	Hits@10:38.98	Best:19.64
2025-01-06 22:32:46,282: Snapshot:4	Epoch:5	Loss:1.87	translation_Loss:1.273	token_training_loss:0.0	distillation_Loss:0.597                                                   	MRR:20.76	Hits@10:40.3	Best:20.76
2025-01-06 22:32:49,168: Snapshot:4	Epoch:6	Loss:1.681	translation_Loss:1.12	token_training_loss:0.0	distillation_Loss:0.561                                                   	MRR:21.88	Hits@10:41.56	Best:21.88
2025-01-06 22:32:52,063: Snapshot:4	Epoch:7	Loss:1.54	translation_Loss:1.012	token_training_loss:0.0	distillation_Loss:0.528                                                   	MRR:22.37	Hits@10:42.3	Best:22.37
2025-01-06 22:32:54,934: Snapshot:4	Epoch:8	Loss:1.426	translation_Loss:0.922	token_training_loss:0.0	distillation_Loss:0.505                                                   	MRR:22.61	Hits@10:42.44	Best:22.61
2025-01-06 22:32:57,849: Snapshot:4	Epoch:9	Loss:1.343	translation_Loss:0.863	token_training_loss:0.0	distillation_Loss:0.48                                                   	MRR:22.76	Hits@10:42.57	Best:22.76
2025-01-06 22:33:00,705: Snapshot:4	Epoch:10	Loss:1.277	translation_Loss:0.814	token_training_loss:0.0	distillation_Loss:0.462                                                   	MRR:22.9	Hits@10:42.67	Best:22.9
2025-01-06 22:33:03,542: Snapshot:4	Epoch:11	Loss:1.224	translation_Loss:0.774	token_training_loss:0.0	distillation_Loss:0.45                                                   	MRR:22.86	Hits@10:42.37	Best:22.9
2025-01-06 22:33:06,411: Snapshot:4	Epoch:12	Loss:1.188	translation_Loss:0.751	token_training_loss:0.0	distillation_Loss:0.437                                                   	MRR:22.95	Hits@10:42.56	Best:22.95
2025-01-06 22:33:09,745: Snapshot:4	Epoch:13	Loss:1.156	translation_Loss:0.726	token_training_loss:0.0	distillation_Loss:0.431                                                   	MRR:23.08	Hits@10:42.46	Best:23.08
2025-01-06 22:33:12,584: Snapshot:4	Epoch:14	Loss:1.14	translation_Loss:0.715	token_training_loss:0.0	distillation_Loss:0.425                                                   	MRR:22.92	Hits@10:42.34	Best:23.08
2025-01-06 22:33:15,412: Snapshot:4	Epoch:15	Loss:1.12	translation_Loss:0.699	token_training_loss:0.0	distillation_Loss:0.421                                                   	MRR:22.76	Hits@10:42.52	Best:23.08
2025-01-06 22:33:18,233: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 23.08
2025-01-06 22:33:18,233: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:1.112 MRR:22.61 Best Results: 23.08
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:33:18,234: Snapshot:4	Epoch:16	Loss:1.112	translation_Loss:0.697	token_training_loss:0.0	distillation_Loss:0.415                                                   	MRR:22.61	Hits@10:42.38	Best:23.08
2025-01-06 22:33:21,034: Snapshot:4	Epoch:17	Loss:15.898	translation_Loss:4.034	token_training_loss:11.864	distillation_Loss:0.0                                                   	MRR:22.61	Hits@10:42.38	Best:23.08
2025-01-06 22:33:23,826: End of token training: 4 Epoch: 18 Loss:6.698 MRR:22.61 Best Results: 23.08
2025-01-06 22:33:23,826: Snapshot:4	Epoch:18	Loss:6.698	translation_Loss:4.033	token_training_loss:2.665	distillation_Loss:0.0                                                           	MRR:22.61	Hits@10:42.38	Best:23.08
2025-01-06 22:33:24,183: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2025-01-06 22:33:41,856: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2172 | 0.1169 | 0.2746 | 0.3361 |  0.403  |
|     1      | 0.1619 | 0.0785 | 0.1947 | 0.2493 |  0.3208 |
|     2      | 0.1962 | 0.1191 | 0.2181 | 0.2716 |  0.3493 |
|     3      | 0.2512 | 0.1723 | 0.2819 | 0.332  |  0.3993 |
|     4      | 0.2217 | 0.1238 | 0.245  |  0.32  |  0.4193 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,000
Trainable params: 2,000
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:33:41,858: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2806 | 0.1687 | 0.3476 | 0.4154 |  0.4872 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2497 | 0.1404 | 0.3111 | 0.3797 |  0.4527 |
|     1      | 0.1732 | 0.0887 | 0.2068 | 0.2629 |  0.3331 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.218  | 0.117  | 0.276  | 0.3386 |  0.4045 |
|     1      | 0.1618 | 0.0779 | 0.1946 | 0.2508 |  0.3209 |
|     2      | 0.2114 | 0.1324 | 0.2367 | 0.2912 |  0.3678 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2181 | 0.1174 | 0.276  | 0.3378 |  0.404  |
|     1      | 0.1619 | 0.0777 | 0.1946 | 0.2502 |  0.3219 |
|     2      | 0.2045 | 0.1238 | 0.2294 | 0.2869 |  0.3671 |
|     3      | 0.2541 | 0.177  | 0.2839 | 0.3325 |  0.3957 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2172 | 0.1169 | 0.2746 | 0.3361 |  0.403  |
|     1      | 0.1619 | 0.0785 | 0.1947 | 0.2493 |  0.3208 |
|     2      | 0.1962 | 0.1191 | 0.2181 | 0.2716 |  0.3493 |
|     3      | 0.2512 | 0.1723 | 0.2819 | 0.332  |  0.3993 |
|     4      | 0.2217 | 0.1238 | 0.245  |  0.32  |  0.4193 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:33:41,859: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 161.77153205871582 |   0.281   |    0.169     |    0.348     |     0.487     |
|    1     |  124.501225233078  |   0.212   |    0.115     |     0.26     |     0.395     |
|    2     | 102.01697778701782 |   0.196   |    0.107     |    0.236     |     0.365     |
|    3     | 86.98863291740417  |   0.201   |    0.112     |     0.24     |     0.368     |
|    4     | 59.673356771469116 |    0.2    |    0.112     |    0.237     |     0.368     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:33:41,859: Sum_Training_Time:534.9517247676849
2025-01-06 22:33:41,859: Every_Training_Time:[161.77153205871582, 124.501225233078, 102.01697778701782, 86.98863291740417, 59.673356771469116]
2025-01-06 22:33:41,859: Forward transfer: 0.0154 Backward transfer: -0.023200000000000005
2025-01-06 22:34:02,434: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106223346/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=2222, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[3000.0, 15000.0, 80000.0, 80000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:34:19,710: Snapshot:0	Epoch:0	Loss:24.085	translation_Loss:24.085	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:11.08	Hits@10:26.25	Best:11.08
2025-01-06 22:34:32,791: Snapshot:0	Epoch:1	Loss:14.914	translation_Loss:14.914	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.87	Hits@10:39.8	Best:18.87
2025-01-06 22:34:45,856: Snapshot:0	Epoch:2	Loss:8.699	translation_Loss:8.699	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.58	Hits@10:44.57	Best:23.58
2025-01-06 22:34:58,460: Snapshot:0	Epoch:3	Loss:4.949	translation_Loss:4.949	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.94	Hits@10:46.49	Best:25.94
2025-01-06 22:35:11,544: Snapshot:0	Epoch:4	Loss:2.913	translation_Loss:2.913	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:26.88	Hits@10:47.33	Best:26.88
2025-01-06 22:35:24,660: Snapshot:0	Epoch:5	Loss:1.937	translation_Loss:1.937	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.17	Hits@10:47.82	Best:27.17
2025-01-06 22:35:37,266: Snapshot:0	Epoch:6	Loss:1.434	translation_Loss:1.434	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.34	Hits@10:47.82	Best:27.34
2025-01-06 22:35:50,316: Snapshot:0	Epoch:7	Loss:1.159	translation_Loss:1.159	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.23	Hits@10:47.8	Best:27.34
2025-01-06 22:36:03,288: Snapshot:0	Epoch:8	Loss:0.984	translation_Loss:0.984	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.29	Hits@10:47.69	Best:27.34
2025-01-06 22:36:15,929: Snapshot:0	Epoch:9	Loss:0.889	translation_Loss:0.889	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.35	Hits@10:47.48	Best:27.35
2025-01-06 22:36:28,984: Snapshot:0	Epoch:10	Loss:0.809	translation_Loss:0.809	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.23	Hits@10:47.55	Best:27.35
2025-01-06 22:36:41,990: Snapshot:0	Epoch:11	Loss:0.742	translation_Loss:0.742	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.08	Hits@10:47.69	Best:27.35
2025-01-06 22:36:54,588: Early Stopping! Snapshot: 0 Epoch: 12 Best Results: 27.35
2025-01-06 22:36:54,589: Start to training tokens! Snapshot: 0 Epoch: 12 Loss:0.694 MRR:27.19 Best Results: 27.35
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:36:54,589: Snapshot:0	Epoch:12	Loss:0.694	translation_Loss:0.694	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.19	Hits@10:47.47	Best:27.35
2025-01-06 22:37:08,204: Snapshot:0	Epoch:13	Loss:31.37	translation_Loss:16.367	token_training_loss:15.003	distillation_Loss:0.0                                                   	MRR:27.19	Hits@10:47.47	Best:27.35
2025-01-06 22:37:21,435: End of token training: 0 Epoch: 14 Loss:16.456 MRR:27.19 Best Results: 27.35
2025-01-06 22:37:21,436: Snapshot:0	Epoch:14	Loss:16.456	translation_Loss:16.368	token_training_loss:0.088	distillation_Loss:0.0                                                           	MRR:27.19	Hits@10:47.47	Best:27.35
2025-01-06 22:37:21,732: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2025-01-06 22:37:27,415: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2748 | 0.1652 | 0.3387 | 0.4055 |  0.4781 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,312,200)
├─Embedding: 1-2                         (19,600)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,333,800
Trainable params: 2,000
Non-trainable params: 2,331,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:37:48,114: Snapshot:1	Epoch:0	Loss:19.849	translation_Loss:18.638	token_training_loss:0.0	distillation_Loss:1.211                                                   	MRR:10.37	Hits@10:24.32	Best:10.37
2025-01-06 22:38:00,337: Snapshot:1	Epoch:1	Loss:10.165	translation_Loss:7.977	token_training_loss:0.0	distillation_Loss:2.188                                                   	MRR:16.0	Hits@10:31.75	Best:16.0
2025-01-06 22:38:12,092: Snapshot:1	Epoch:2	Loss:6.444	translation_Loss:4.12	token_training_loss:0.0	distillation_Loss:2.324                                                   	MRR:16.37	Hits@10:33.06	Best:16.37
2025-01-06 22:38:24,187: Snapshot:1	Epoch:3	Loss:5.083	translation_Loss:2.921	token_training_loss:0.0	distillation_Loss:2.162                                                   	MRR:16.64	Hits@10:32.9	Best:16.64
2025-01-06 22:38:36,265: Snapshot:1	Epoch:4	Loss:4.591	translation_Loss:2.563	token_training_loss:0.0	distillation_Loss:2.028                                                   	MRR:16.57	Hits@10:32.68	Best:16.64
2025-01-06 22:38:48,027: Snapshot:1	Epoch:5	Loss:4.373	translation_Loss:2.413	token_training_loss:0.0	distillation_Loss:1.961                                                   	MRR:16.69	Hits@10:32.57	Best:16.69
2025-01-06 22:39:00,152: Snapshot:1	Epoch:6	Loss:4.246	translation_Loss:2.329	token_training_loss:0.0	distillation_Loss:1.917                                                   	MRR:16.6	Hits@10:32.48	Best:16.69
2025-01-06 22:39:11,743: Snapshot:1	Epoch:7	Loss:4.196	translation_Loss:2.303	token_training_loss:0.0	distillation_Loss:1.893                                                   	MRR:16.35	Hits@10:32.45	Best:16.69
2025-01-06 22:39:23,862: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 16.69
2025-01-06 22:39:23,863: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:4.146 MRR:16.48 Best Results: 16.69
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:39:23,863: Snapshot:1	Epoch:8	Loss:4.146	translation_Loss:2.269	token_training_loss:0.0	distillation_Loss:1.877                                                   	MRR:16.48	Hits@10:32.34	Best:16.69
2025-01-06 22:39:35,738: Snapshot:1	Epoch:9	Loss:34.259	translation_Loss:19.225	token_training_loss:15.034	distillation_Loss:0.0                                                   	MRR:16.48	Hits@10:32.34	Best:16.69
2025-01-06 22:39:47,213: End of token training: 1 Epoch: 10 Loss:19.332 MRR:16.48 Best Results: 16.69
2025-01-06 22:39:47,214: Snapshot:1	Epoch:10	Loss:19.332	translation_Loss:19.226	token_training_loss:0.106	distillation_Loss:0.0                                                           	MRR:16.48	Hits@10:32.34	Best:16.69
2025-01-06 22:39:47,508: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2025-01-06 22:39:58,346: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1373 | 0.3039 | 0.3713 |  0.4491 |
|     1      | 0.1686 | 0.087  |  0.2   | 0.2559 |  0.3244 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,668,800)
├─Embedding: 1-2                         (38,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,709,200
Trainable params: 2,000
Non-trainable params: 2,707,200
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:40:15,436: Snapshot:2	Epoch:0	Loss:13.58	translation_Loss:11.796	token_training_loss:0.0	distillation_Loss:1.784                                                   	MRR:12.17	Hits@10:27.59	Best:12.17
2025-01-06 22:40:24,533: Snapshot:2	Epoch:1	Loss:7.46	translation_Loss:4.879	token_training_loss:0.0	distillation_Loss:2.581                                                   	MRR:18.82	Hits@10:35.59	Best:18.82
2025-01-06 22:40:33,153: Snapshot:2	Epoch:2	Loss:5.229	translation_Loss:3.272	token_training_loss:0.0	distillation_Loss:1.956                                                   	MRR:20.67	Hits@10:36.59	Best:20.67
2025-01-06 22:40:42,286: Snapshot:2	Epoch:3	Loss:4.099	translation_Loss:2.514	token_training_loss:0.0	distillation_Loss:1.585                                                   	MRR:21.1	Hits@10:36.9	Best:21.1
2025-01-06 22:40:50,899: Snapshot:2	Epoch:4	Loss:3.522	translation_Loss:2.182	token_training_loss:0.0	distillation_Loss:1.34                                                   	MRR:21.64	Hits@10:36.99	Best:21.64
2025-01-06 22:40:59,526: Snapshot:2	Epoch:5	Loss:3.22	translation_Loss:2.017	token_training_loss:0.0	distillation_Loss:1.203                                                   	MRR:21.33	Hits@10:36.62	Best:21.64
2025-01-06 22:41:08,453: Snapshot:2	Epoch:6	Loss:3.083	translation_Loss:1.948	token_training_loss:0.0	distillation_Loss:1.136                                                   	MRR:21.27	Hits@10:36.68	Best:21.64
2025-01-06 22:41:17,168: Early Stopping! Snapshot: 2 Epoch: 7 Best Results: 21.64
2025-01-06 22:41:17,168: Start to training tokens! Snapshot: 2 Epoch: 7 Loss:2.994 MRR:21.02 Best Results: 21.64
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:41:17,169: Snapshot:2	Epoch:7	Loss:2.994	translation_Loss:1.904	token_training_loss:0.0	distillation_Loss:1.09                                                   	MRR:21.02	Hits@10:36.26	Best:21.64
2025-01-06 22:41:26,068: Snapshot:2	Epoch:8	Loss:30.55	translation_Loss:14.819	token_training_loss:15.731	distillation_Loss:0.0                                                   	MRR:21.02	Hits@10:36.26	Best:21.64
2025-01-06 22:41:34,476: End of token training: 2 Epoch: 9 Loss:15.124 MRR:21.02 Best Results: 21.64
2025-01-06 22:41:34,476: Snapshot:2	Epoch:9	Loss:15.124	translation_Loss:14.817	token_training_loss:0.307	distillation_Loss:0.0                                                           	MRR:21.02	Hits@10:36.26	Best:21.64
2025-01-06 22:41:34,803: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2025-01-06 22:41:48,956: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2119 | 0.1131 | 0.2633 | 0.3262 |  0.4001 |
|     1      | 0.1574 | 0.0764 | 0.1881 | 0.2435 |  0.3121 |
|     2      | 0.2138 | 0.1369 | 0.2364 | 0.2901 |  0.3674 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,751,000)
├─Embedding: 1-2                         (57,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,810,200
Trainable params: 2,000
Non-trainable params: 2,808,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:41:59,283: Snapshot:3	Epoch:0	Loss:6.769	translation_Loss:6.088	token_training_loss:0.0	distillation_Loss:0.681                                                   	MRR:5.92	Hits@10:14.13	Best:5.92
2025-01-06 22:42:03,324: Snapshot:3	Epoch:1	Loss:4.825	translation_Loss:4.137	token_training_loss:0.0	distillation_Loss:0.689                                                   	MRR:13.47	Hits@10:30.04	Best:13.47
2025-01-06 22:42:07,353: Snapshot:3	Epoch:2	Loss:3.544	translation_Loss:2.972	token_training_loss:0.0	distillation_Loss:0.572                                                   	MRR:16.77	Hits@10:33.84	Best:16.77
2025-01-06 22:42:11,743: Snapshot:3	Epoch:3	Loss:2.818	translation_Loss:2.339	token_training_loss:0.0	distillation_Loss:0.48                                                   	MRR:19.77	Hits@10:35.99	Best:19.77
2025-01-06 22:42:15,764: Snapshot:3	Epoch:4	Loss:2.396	translation_Loss:1.966	token_training_loss:0.0	distillation_Loss:0.431                                                   	MRR:21.74	Hits@10:37.44	Best:21.74
2025-01-06 22:42:19,781: Snapshot:3	Epoch:5	Loss:2.1	translation_Loss:1.713	token_training_loss:0.0	distillation_Loss:0.386                                                   	MRR:22.63	Hits@10:38.05	Best:22.63
2025-01-06 22:42:23,823: Snapshot:3	Epoch:6	Loss:1.884	translation_Loss:1.535	token_training_loss:0.0	distillation_Loss:0.349                                                   	MRR:23.21	Hits@10:38.66	Best:23.21
2025-01-06 22:42:27,821: Snapshot:3	Epoch:7	Loss:1.736	translation_Loss:1.412	token_training_loss:0.0	distillation_Loss:0.324                                                   	MRR:23.68	Hits@10:38.8	Best:23.68
2025-01-06 22:42:32,247: Snapshot:3	Epoch:8	Loss:1.616	translation_Loss:1.315	token_training_loss:0.0	distillation_Loss:0.301                                                   	MRR:24.08	Hits@10:38.7	Best:24.08
2025-01-06 22:42:36,279: Snapshot:3	Epoch:9	Loss:1.535	translation_Loss:1.249	token_training_loss:0.0	distillation_Loss:0.286                                                   	MRR:24.29	Hits@10:38.74	Best:24.29
2025-01-06 22:42:40,281: Snapshot:3	Epoch:10	Loss:1.477	translation_Loss:1.203	token_training_loss:0.0	distillation_Loss:0.274                                                   	MRR:24.4	Hits@10:38.91	Best:24.4
2025-01-06 22:42:44,282: Snapshot:3	Epoch:11	Loss:1.438	translation_Loss:1.17	token_training_loss:0.0	distillation_Loss:0.268                                                   	MRR:24.43	Hits@10:38.41	Best:24.43
2025-01-06 22:42:48,307: Snapshot:3	Epoch:12	Loss:1.405	translation_Loss:1.143	token_training_loss:0.0	distillation_Loss:0.262                                                   	MRR:24.44	Hits@10:38.66	Best:24.44
2025-01-06 22:42:52,302: Snapshot:3	Epoch:13	Loss:1.389	translation_Loss:1.13	token_training_loss:0.0	distillation_Loss:0.26                                                   	MRR:24.6	Hits@10:38.74	Best:24.6
2025-01-06 22:42:56,815: Snapshot:3	Epoch:14	Loss:1.362	translation_Loss:1.107	token_training_loss:0.0	distillation_Loss:0.255                                                   	MRR:24.69	Hits@10:38.72	Best:24.69
2025-01-06 22:43:00,844: Snapshot:3	Epoch:15	Loss:1.353	translation_Loss:1.099	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:24.91	Hits@10:38.61	Best:24.91
2025-01-06 22:43:04,817: Snapshot:3	Epoch:16	Loss:1.34	translation_Loss:1.09	token_training_loss:0.0	distillation_Loss:0.25                                                   	MRR:24.96	Hits@10:38.9	Best:24.96
2025-01-06 22:43:08,742: Snapshot:3	Epoch:17	Loss:1.331	translation_Loss:1.082	token_training_loss:0.0	distillation_Loss:0.25                                                   	MRR:24.92	Hits@10:38.67	Best:24.96
2025-01-06 22:43:13,075: Snapshot:3	Epoch:18	Loss:1.317	translation_Loss:1.069	token_training_loss:0.0	distillation_Loss:0.248                                                   	MRR:24.93	Hits@10:38.75	Best:24.96
2025-01-06 22:43:16,986: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 24.96
2025-01-06 22:43:16,986: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:1.317 MRR:24.87 Best Results: 24.96
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:43:16,986: Snapshot:3	Epoch:19	Loss:1.317	translation_Loss:1.07	token_training_loss:0.0	distillation_Loss:0.247                                                   	MRR:24.87	Hits@10:38.5	Best:24.96
2025-01-06 22:43:20,864: Snapshot:3	Epoch:20	Loss:18.71	translation_Loss:5.81	token_training_loss:12.9	distillation_Loss:0.0                                                   	MRR:24.87	Hits@10:38.5	Best:24.96
2025-01-06 22:43:24,744: End of token training: 3 Epoch: 21 Loss:7.297 MRR:24.87 Best Results: 24.96
2025-01-06 22:43:24,744: Snapshot:3	Epoch:21	Loss:7.297	translation_Loss:5.828	token_training_loss:1.47	distillation_Loss:0.0                                                           	MRR:24.87	Hits@10:38.5	Best:24.96
2025-01-06 22:43:25,050: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2025-01-06 22:43:41,157: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2124 | 0.1129 | 0.2636 | 0.3282 |  0.4032 |
|     1      | 0.1569 | 0.0752 | 0.1878 | 0.2441 |  0.3137 |
|     2      | 0.2075 | 0.1284 | 0.2327 | 0.2878 |  0.3663 |
|     3      | 0.2504 | 0.1758 | 0.2802 | 0.3227 |  0.3855 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,877,400)
├─Embedding: 1-2                         (76,000)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,955,400
Trainable params: 2,000
Non-trainable params: 2,953,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:43:49,971: Snapshot:4	Epoch:0	Loss:4.648	translation_Loss:4.135	token_training_loss:0.0	distillation_Loss:0.513                                                   	MRR:5.8	Hits@10:15.65	Best:5.8
2025-01-06 22:43:52,847: Snapshot:4	Epoch:1	Loss:3.539	translation_Loss:2.806	token_training_loss:0.0	distillation_Loss:0.733                                                   	MRR:10.31	Hits@10:26.48	Best:10.31
2025-01-06 22:43:55,762: Snapshot:4	Epoch:2	Loss:2.893	translation_Loss:2.168	token_training_loss:0.0	distillation_Loss:0.725                                                   	MRR:13.79	Hits@10:32.32	Best:13.79
2025-01-06 22:43:58,616: Snapshot:4	Epoch:3	Loss:2.415	translation_Loss:1.738	token_training_loss:0.0	distillation_Loss:0.677                                                   	MRR:17.49	Hits@10:35.77	Best:17.49
2025-01-06 22:44:01,465: Snapshot:4	Epoch:4	Loss:2.08	translation_Loss:1.455	token_training_loss:0.0	distillation_Loss:0.625                                                   	MRR:19.33	Hits@10:38.03	Best:19.33
2025-01-06 22:44:04,324: Snapshot:4	Epoch:5	Loss:1.851	translation_Loss:1.275	token_training_loss:0.0	distillation_Loss:0.576                                                   	MRR:20.78	Hits@10:39.66	Best:20.78
2025-01-06 22:44:07,253: Snapshot:4	Epoch:6	Loss:1.654	translation_Loss:1.116	token_training_loss:0.0	distillation_Loss:0.538                                                   	MRR:21.61	Hits@10:40.46	Best:21.61
2025-01-06 22:44:10,566: Snapshot:4	Epoch:7	Loss:1.507	translation_Loss:1.003	token_training_loss:0.0	distillation_Loss:0.504                                                   	MRR:22.13	Hits@10:40.72	Best:22.13
2025-01-06 22:44:13,447: Snapshot:4	Epoch:8	Loss:1.389	translation_Loss:0.914	token_training_loss:0.0	distillation_Loss:0.476                                                   	MRR:22.58	Hits@10:41.05	Best:22.58
2025-01-06 22:44:16,252: Snapshot:4	Epoch:9	Loss:1.3	translation_Loss:0.844	token_training_loss:0.0	distillation_Loss:0.456                                                   	MRR:22.45	Hits@10:41.22	Best:22.58
2025-01-06 22:44:19,044: Snapshot:4	Epoch:10	Loss:1.235	translation_Loss:0.801	token_training_loss:0.0	distillation_Loss:0.435                                                   	MRR:22.53	Hits@10:41.32	Best:22.58
2025-01-06 22:44:21,836: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 22.58
2025-01-06 22:44:21,836: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:1.189 MRR:22.54 Best Results: 22.58
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:44:21,836: Snapshot:4	Epoch:11	Loss:1.189	translation_Loss:0.765	token_training_loss:0.0	distillation_Loss:0.424                                                   	MRR:22.54	Hits@10:41.42	Best:22.58
2025-01-06 22:44:24,566: Snapshot:4	Epoch:12	Loss:14.858	translation_Loss:3.998	token_training_loss:10.86	distillation_Loss:0.0                                                   	MRR:22.54	Hits@10:41.42	Best:22.58
2025-01-06 22:44:27,302: End of token training: 4 Epoch: 13 Loss:6.152 MRR:22.54 Best Results: 22.58
2025-01-06 22:44:27,302: Snapshot:4	Epoch:13	Loss:6.152	translation_Loss:3.989	token_training_loss:2.163	distillation_Loss:0.0                                                           	MRR:22.54	Hits@10:41.42	Best:22.58
2025-01-06 22:44:27,598: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2025-01-06 22:44:45,322: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2079 | 0.1103 | 0.2584 | 0.3198 |  0.3932 |
|     1      | 0.1562 | 0.0749 | 0.1872 | 0.2423 |  0.3129 |
|     2      | 0.1971 | 0.1221 | 0.2192 | 0.2695 |  0.3458 |
|     3      |  0.24  | 0.1636 | 0.2651 | 0.3147 |  0.3831 |
|     4      | 0.2212 | 0.1256 | 0.2437 | 0.3165 |  0.4165 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,000
Trainable params: 2,000
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:44:45,325: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2748 | 0.1652 | 0.3387 | 0.4055 |  0.4781 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1373 | 0.3039 | 0.3713 |  0.4491 |
|     1      | 0.1686 | 0.087  |  0.2   | 0.2559 |  0.3244 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2119 | 0.1131 | 0.2633 | 0.3262 |  0.4001 |
|     1      | 0.1574 | 0.0764 | 0.1881 | 0.2435 |  0.3121 |
|     2      | 0.2138 | 0.1369 | 0.2364 | 0.2901 |  0.3674 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2124 | 0.1129 | 0.2636 | 0.3282 |  0.4032 |
|     1      | 0.1569 | 0.0752 | 0.1878 | 0.2441 |  0.3137 |
|     2      | 0.2075 | 0.1284 | 0.2327 | 0.2878 |  0.3663 |
|     3      | 0.2504 | 0.1758 | 0.2802 | 0.3227 |  0.3855 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2079 | 0.1103 | 0.2584 | 0.3198 |  0.3932 |
|     1      | 0.1562 | 0.0749 | 0.1872 | 0.2423 |  0.3129 |
|     2      | 0.1971 | 0.1221 | 0.2192 | 0.2695 |  0.3458 |
|     3      |  0.24  | 0.1636 | 0.2651 | 0.3147 |  0.3831 |
|     4      | 0.2212 | 0.1256 | 0.2437 | 0.3165 |  0.4165 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:44:45,326: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 199.00081586837769 |   0.275   |    0.165     |    0.339     |     0.478     |
|    1     | 135.1798541545868  |   0.208   |    0.113     |    0.253     |     0.388     |
|    2     | 92.08121705055237  |   0.193   |    0.106     |    0.229     |      0.36     |
|    3     | 93.76946091651917  |   0.197   |    0.111     |    0.234     |     0.364     |
|    4     | 44.548274517059326 |   0.194   |    0.108     |    0.228     |      0.36     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:44:45,326: Sum_Training_Time:564.5796225070953
2025-01-06 22:44:45,326: Every_Training_Time:[199.00081586837769, 135.1798541545868, 92.08121705055237, 93.76946091651917, 44.548274517059326]
2025-01-06 22:44:45,326: Forward transfer: 0.015625 Backward transfer: -0.0266
2025-01-06 22:45:06,148: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106224450/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3333, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[3000.0, 15000.0, 80000.0, 80000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:45:23,439: Snapshot:0	Epoch:0	Loss:24.114	translation_Loss:24.114	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:11.15	Hits@10:26.79	Best:11.15
2025-01-06 22:45:36,505: Snapshot:0	Epoch:1	Loss:14.854	translation_Loss:14.854	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:19.08	Hits@10:40.81	Best:19.08
2025-01-06 22:45:49,514: Snapshot:0	Epoch:2	Loss:8.506	translation_Loss:8.506	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.84	Hits@10:45.18	Best:23.84
2025-01-06 22:46:02,104: Snapshot:0	Epoch:3	Loss:4.716	translation_Loss:4.716	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:26.05	Hits@10:46.86	Best:26.05
2025-01-06 22:46:15,185: Snapshot:0	Epoch:4	Loss:2.731	translation_Loss:2.731	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.0	Hits@10:47.7	Best:27.0
2025-01-06 22:46:28,240: Snapshot:0	Epoch:5	Loss:1.8	translation_Loss:1.8	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.29	Hits@10:47.77	Best:27.29
2025-01-06 22:46:40,804: Snapshot:0	Epoch:6	Loss:1.341	translation_Loss:1.341	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.28	Hits@10:47.98	Best:27.29
2025-01-06 22:46:53,876: Snapshot:0	Epoch:7	Loss:1.097	translation_Loss:1.097	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.35	Hits@10:47.87	Best:27.35
2025-01-06 22:47:06,905: Snapshot:0	Epoch:8	Loss:0.944	translation_Loss:0.944	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.33	Hits@10:47.73	Best:27.35
2025-01-06 22:47:19,559: Snapshot:0	Epoch:9	Loss:0.83	translation_Loss:0.83	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.4	Hits@10:47.62	Best:27.4
2025-01-06 22:47:32,583: Snapshot:0	Epoch:10	Loss:0.772	translation_Loss:0.772	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.23	Hits@10:47.55	Best:27.4
2025-01-06 22:47:45,645: Snapshot:0	Epoch:11	Loss:0.708	translation_Loss:0.708	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.17	Hits@10:47.34	Best:27.4
2025-01-06 22:47:58,241: Early Stopping! Snapshot: 0 Epoch: 12 Best Results: 27.4
2025-01-06 22:47:58,242: Start to training tokens! Snapshot: 0 Epoch: 12 Loss:0.667 MRR:27.03 Best Results: 27.4
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:47:58,242: Snapshot:0	Epoch:12	Loss:0.667	translation_Loss:0.667	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.03	Hits@10:47.13	Best:27.4
2025-01-06 22:48:11,745: Snapshot:0	Epoch:13	Loss:31.571	translation_Loss:16.16	token_training_loss:15.411	distillation_Loss:0.0                                                   	MRR:27.03	Hits@10:47.13	Best:27.4
2025-01-06 22:48:24,758: End of token training: 0 Epoch: 14 Loss:16.276 MRR:27.03 Best Results: 27.4
2025-01-06 22:48:24,758: Snapshot:0	Epoch:14	Loss:16.276	translation_Loss:16.185	token_training_loss:0.091	distillation_Loss:0.0                                                           	MRR:27.03	Hits@10:47.13	Best:27.4
2025-01-06 22:48:25,052: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2025-01-06 22:48:30,717: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2773 | 0.1682 | 0.3398 | 0.4076 |  0.4802 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,312,200)
├─Embedding: 1-2                         (19,600)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,333,800
Trainable params: 2,000
Non-trainable params: 2,331,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:48:51,374: Snapshot:1	Epoch:0	Loss:20.005	translation_Loss:18.792	token_training_loss:0.0	distillation_Loss:1.212                                                   	MRR:9.96	Hits@10:24.58	Best:9.96
2025-01-06 22:49:03,552: Snapshot:1	Epoch:1	Loss:10.308	translation_Loss:8.145	token_training_loss:0.0	distillation_Loss:2.164                                                   	MRR:15.96	Hits@10:32.17	Best:15.96
2025-01-06 22:49:15,236: Snapshot:1	Epoch:2	Loss:6.614	translation_Loss:4.302	token_training_loss:0.0	distillation_Loss:2.312                                                   	MRR:16.48	Hits@10:33.34	Best:16.48
2025-01-06 22:49:27,273: Snapshot:1	Epoch:3	Loss:5.286	translation_Loss:3.06	token_training_loss:0.0	distillation_Loss:2.225                                                   	MRR:16.75	Hits@10:33.26	Best:16.75
2025-01-06 22:49:39,298: Snapshot:1	Epoch:4	Loss:4.785	translation_Loss:2.675	token_training_loss:0.0	distillation_Loss:2.111                                                   	MRR:16.92	Hits@10:33.17	Best:16.92
2025-01-06 22:49:50,931: Snapshot:1	Epoch:5	Loss:4.568	translation_Loss:2.52	token_training_loss:0.0	distillation_Loss:2.048                                                   	MRR:16.76	Hits@10:32.91	Best:16.92
2025-01-06 22:50:02,969: Snapshot:1	Epoch:6	Loss:4.463	translation_Loss:2.453	token_training_loss:0.0	distillation_Loss:2.01                                                   	MRR:16.81	Hits@10:32.97	Best:16.92
2025-01-06 22:50:14,672: Early Stopping! Snapshot: 1 Epoch: 7 Best Results: 16.92
2025-01-06 22:50:14,673: Start to training tokens! Snapshot: 1 Epoch: 7 Loss:4.404 MRR:16.52 Best Results: 16.92
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:50:14,673: Snapshot:1	Epoch:7	Loss:4.404	translation_Loss:2.413	token_training_loss:0.0	distillation_Loss:1.991                                                   	MRR:16.52	Hits@10:32.73	Best:16.92
2025-01-06 22:50:26,551: Snapshot:1	Epoch:8	Loss:34.318	translation_Loss:19.447	token_training_loss:14.87	distillation_Loss:0.0                                                   	MRR:16.52	Hits@10:32.73	Best:16.92
2025-01-06 22:50:38,387: End of token training: 1 Epoch: 9 Loss:19.554 MRR:16.52 Best Results: 16.92
2025-01-06 22:50:38,387: Snapshot:1	Epoch:9	Loss:19.554	translation_Loss:19.446	token_training_loss:0.109	distillation_Loss:0.0                                                           	MRR:16.52	Hits@10:32.73	Best:16.92
2025-01-06 22:50:38,687: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2025-01-06 22:50:49,839: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2474 | 0.1418 | 0.3038 | 0.3721 |  0.4478 |
|     1      | 0.1698 | 0.0862 | 0.2014 | 0.2564 |  0.331  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,668,800)
├─Embedding: 1-2                         (38,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,709,200
Trainable params: 2,000
Non-trainable params: 2,707,200
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:51:06,519: Snapshot:2	Epoch:0	Loss:13.374	translation_Loss:11.573	token_training_loss:0.0	distillation_Loss:1.801                                                   	MRR:11.22	Hits@10:26.07	Best:11.22
2025-01-06 22:51:15,623: Snapshot:2	Epoch:1	Loss:7.475	translation_Loss:5.007	token_training_loss:0.0	distillation_Loss:2.468                                                   	MRR:18.4	Hits@10:34.51	Best:18.4
2025-01-06 22:51:24,261: Snapshot:2	Epoch:2	Loss:5.194	translation_Loss:3.325	token_training_loss:0.0	distillation_Loss:1.869                                                   	MRR:20.04	Hits@10:35.63	Best:20.04
2025-01-06 22:51:33,370: Snapshot:2	Epoch:3	Loss:4.061	translation_Loss:2.553	token_training_loss:0.0	distillation_Loss:1.507                                                   	MRR:21.07	Hits@10:36.6	Best:21.07
2025-01-06 22:51:41,989: Snapshot:2	Epoch:4	Loss:3.472	translation_Loss:2.193	token_training_loss:0.0	distillation_Loss:1.279                                                   	MRR:21.3	Hits@10:36.82	Best:21.3
2025-01-06 22:51:51,059: Snapshot:2	Epoch:5	Loss:3.185	translation_Loss:2.035	token_training_loss:0.0	distillation_Loss:1.15                                                   	MRR:21.08	Hits@10:36.67	Best:21.3
2025-01-06 22:51:59,690: Snapshot:2	Epoch:6	Loss:3.058	translation_Loss:1.968	token_training_loss:0.0	distillation_Loss:1.09                                                   	MRR:21.11	Hits@10:36.44	Best:21.3
2025-01-06 22:52:08,344: Early Stopping! Snapshot: 2 Epoch: 7 Best Results: 21.3
2025-01-06 22:52:08,344: Start to training tokens! Snapshot: 2 Epoch: 7 Loss:2.983 MRR:21.13 Best Results: 21.3
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:52:08,345: Snapshot:2	Epoch:7	Loss:2.983	translation_Loss:1.926	token_training_loss:0.0	distillation_Loss:1.058                                                   	MRR:21.13	Hits@10:36.42	Best:21.3
2025-01-06 22:52:17,233: Snapshot:2	Epoch:8	Loss:30.376	translation_Loss:14.964	token_training_loss:15.412	distillation_Loss:0.0                                                   	MRR:21.13	Hits@10:36.42	Best:21.3
2025-01-06 22:52:25,633: End of token training: 2 Epoch: 9 Loss:15.271 MRR:21.13 Best Results: 21.3
2025-01-06 22:52:25,638: Snapshot:2	Epoch:9	Loss:15.271	translation_Loss:14.977	token_training_loss:0.295	distillation_Loss:0.0                                                           	MRR:21.13	Hits@10:36.42	Best:21.3
2025-01-06 22:52:25,944: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2025-01-06 22:52:40,106: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2153 | 0.1177 | 0.2657 | 0.329  |  0.4003 |
|     1      | 0.1589 | 0.0766 | 0.1867 | 0.2436 |   0.32  |
|     2      | 0.2113 | 0.133  | 0.2353 | 0.2905 |  0.3663 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,751,000)
├─Embedding: 1-2                         (57,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,810,200
Trainable params: 2,000
Non-trainable params: 2,808,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:52:50,441: Snapshot:3	Epoch:0	Loss:6.841	translation_Loss:6.159	token_training_loss:0.0	distillation_Loss:0.682                                                   	MRR:5.59	Hits@10:13.28	Best:5.59
2025-01-06 22:52:54,471: Snapshot:3	Epoch:1	Loss:4.877	translation_Loss:4.189	token_training_loss:0.0	distillation_Loss:0.688                                                   	MRR:12.67	Hits@10:29.1	Best:12.67
2025-01-06 22:52:58,484: Snapshot:3	Epoch:2	Loss:3.559	translation_Loss:2.978	token_training_loss:0.0	distillation_Loss:0.581                                                   	MRR:16.59	Hits@10:34.16	Best:16.59
2025-01-06 22:53:02,890: Snapshot:3	Epoch:3	Loss:2.812	translation_Loss:2.32	token_training_loss:0.0	distillation_Loss:0.492                                                   	MRR:20.06	Hits@10:36.4	Best:20.06
2025-01-06 22:53:06,921: Snapshot:3	Epoch:4	Loss:2.394	translation_Loss:1.954	token_training_loss:0.0	distillation_Loss:0.44                                                   	MRR:22.43	Hits@10:37.95	Best:22.43
2025-01-06 22:53:10,884: Snapshot:3	Epoch:5	Loss:2.104	translation_Loss:1.708	token_training_loss:0.0	distillation_Loss:0.397                                                   	MRR:23.42	Hits@10:38.31	Best:23.42
2025-01-06 22:53:14,887: Snapshot:3	Epoch:6	Loss:1.892	translation_Loss:1.53	token_training_loss:0.0	distillation_Loss:0.362                                                   	MRR:24.06	Hits@10:38.73	Best:24.06
2025-01-06 22:53:18,905: Snapshot:3	Epoch:7	Loss:1.739	translation_Loss:1.405	token_training_loss:0.0	distillation_Loss:0.333                                                   	MRR:24.59	Hits@10:39.18	Best:24.59
2025-01-06 22:53:22,919: Snapshot:3	Epoch:8	Loss:1.624	translation_Loss:1.31	token_training_loss:0.0	distillation_Loss:0.314                                                   	MRR:24.73	Hits@10:39.02	Best:24.73
2025-01-06 22:53:27,342: Snapshot:3	Epoch:9	Loss:1.554	translation_Loss:1.255	token_training_loss:0.0	distillation_Loss:0.298                                                   	MRR:24.98	Hits@10:39.07	Best:24.98
2025-01-06 22:53:31,371: Snapshot:3	Epoch:10	Loss:1.498	translation_Loss:1.208	token_training_loss:0.0	distillation_Loss:0.29                                                   	MRR:25.07	Hits@10:38.89	Best:25.07
2025-01-06 22:53:35,418: Snapshot:3	Epoch:11	Loss:1.456	translation_Loss:1.174	token_training_loss:0.0	distillation_Loss:0.281                                                   	MRR:25.18	Hits@10:38.97	Best:25.18
2025-01-06 22:53:39,466: Snapshot:3	Epoch:12	Loss:1.429	translation_Loss:1.154	token_training_loss:0.0	distillation_Loss:0.276                                                   	MRR:25.26	Hits@10:39.12	Best:25.26
2025-01-06 22:53:43,989: Snapshot:3	Epoch:13	Loss:1.405	translation_Loss:1.133	token_training_loss:0.0	distillation_Loss:0.272                                                   	MRR:25.54	Hits@10:39.13	Best:25.54
2025-01-06 22:53:47,897: Snapshot:3	Epoch:14	Loss:1.385	translation_Loss:1.117	token_training_loss:0.0	distillation_Loss:0.268                                                   	MRR:25.41	Hits@10:39.26	Best:25.54
2025-01-06 22:53:51,869: Snapshot:3	Epoch:15	Loss:1.37	translation_Loss:1.105	token_training_loss:0.0	distillation_Loss:0.265                                                   	MRR:25.56	Hits@10:39.35	Best:25.56
2025-01-06 22:53:55,883: Snapshot:3	Epoch:16	Loss:1.356	translation_Loss:1.092	token_training_loss:0.0	distillation_Loss:0.264                                                   	MRR:25.52	Hits@10:39.25	Best:25.56
2025-01-06 22:53:59,816: Snapshot:3	Epoch:17	Loss:1.351	translation_Loss:1.09	token_training_loss:0.0	distillation_Loss:0.261                                                   	MRR:25.55	Hits@10:39.3	Best:25.56
2025-01-06 22:54:03,767: Early Stopping! Snapshot: 3 Epoch: 18 Best Results: 25.56
2025-01-06 22:54:03,768: Start to training tokens! Snapshot: 3 Epoch: 18 Loss:1.345 MRR:25.49 Best Results: 25.56
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:54:03,768: Snapshot:3	Epoch:18	Loss:1.345	translation_Loss:1.086	token_training_loss:0.0	distillation_Loss:0.26                                                   	MRR:25.49	Hits@10:39.42	Best:25.56
2025-01-06 22:54:08,063: Snapshot:3	Epoch:19	Loss:19.178	translation_Loss:5.762	token_training_loss:13.416	distillation_Loss:0.0                                                   	MRR:25.49	Hits@10:39.42	Best:25.56
2025-01-06 22:54:12,003: End of token training: 3 Epoch: 20 Loss:7.448 MRR:25.49 Best Results: 25.56
2025-01-06 22:54:12,004: Snapshot:3	Epoch:20	Loss:7.448	translation_Loss:5.761	token_training_loss:1.687	distillation_Loss:0.0                                                           	MRR:25.49	Hits@10:39.42	Best:25.56
2025-01-06 22:54:12,308: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2025-01-06 22:54:28,152: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2159 | 0.1177 | 0.2665 | 0.3304 |  0.4035 |
|     1      | 0.1596 | 0.0774 | 0.1871 | 0.2442 |  0.321  |
|     2      | 0.2064 | 0.1273 | 0.2294 | 0.287  |  0.3654 |
|     3      | 0.2521 | 0.1759 | 0.2829 | 0.326  |  0.3899 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,877,400)
├─Embedding: 1-2                         (76,000)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,955,400
Trainable params: 2,000
Non-trainable params: 2,953,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:54:36,781: Snapshot:4	Epoch:0	Loss:4.597	translation_Loss:4.086	token_training_loss:0.0	distillation_Loss:0.511                                                   	MRR:5.36	Hits@10:14.37	Best:5.36
2025-01-06 22:54:40,089: Snapshot:4	Epoch:1	Loss:3.485	translation_Loss:2.758	token_training_loss:0.0	distillation_Loss:0.727                                                   	MRR:9.65	Hits@10:24.39	Best:9.65
2025-01-06 22:54:42,960: Snapshot:4	Epoch:2	Loss:2.856	translation_Loss:2.131	token_training_loss:0.0	distillation_Loss:0.726                                                   	MRR:12.54	Hits@10:30.96	Best:12.54
2025-01-06 22:54:45,859: Snapshot:4	Epoch:3	Loss:2.385	translation_Loss:1.706	token_training_loss:0.0	distillation_Loss:0.68                                                   	MRR:16.09	Hits@10:34.44	Best:16.09
2025-01-06 22:54:48,705: Snapshot:4	Epoch:4	Loss:2.048	translation_Loss:1.425	token_training_loss:0.0	distillation_Loss:0.623                                                   	MRR:18.2	Hits@10:37.36	Best:18.2
2025-01-06 22:54:51,573: Snapshot:4	Epoch:5	Loss:1.816	translation_Loss:1.237	token_training_loss:0.0	distillation_Loss:0.58                                                   	MRR:20.02	Hits@10:38.91	Best:20.02
2025-01-06 22:54:54,435: Snapshot:4	Epoch:6	Loss:1.636	translation_Loss:1.092	token_training_loss:0.0	distillation_Loss:0.544                                                   	MRR:21.11	Hits@10:40.09	Best:21.11
2025-01-06 22:54:57,310: Snapshot:4	Epoch:7	Loss:1.499	translation_Loss:0.985	token_training_loss:0.0	distillation_Loss:0.514                                                   	MRR:21.85	Hits@10:41.09	Best:21.85
2025-01-06 22:55:00,684: Snapshot:4	Epoch:8	Loss:1.384	translation_Loss:0.89	token_training_loss:0.0	distillation_Loss:0.494                                                   	MRR:22.2	Hits@10:41.24	Best:22.2
2025-01-06 22:55:03,564: Snapshot:4	Epoch:9	Loss:1.299	translation_Loss:0.824	token_training_loss:0.0	distillation_Loss:0.475                                                   	MRR:22.39	Hits@10:41.41	Best:22.39
2025-01-06 22:55:06,430: Snapshot:4	Epoch:10	Loss:1.23	translation_Loss:0.773	token_training_loss:0.0	distillation_Loss:0.457                                                   	MRR:22.38	Hits@10:41.66	Best:22.39
2025-01-06 22:55:09,201: Snapshot:4	Epoch:11	Loss:1.185	translation_Loss:0.739	token_training_loss:0.0	distillation_Loss:0.447                                                   	MRR:22.32	Hits@10:41.53	Best:22.39
2025-01-06 22:55:11,964: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 22.39
2025-01-06 22:55:11,965: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:1.149 MRR:22.26 Best Results: 22.39
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:55:11,965: Snapshot:4	Epoch:12	Loss:1.149	translation_Loss:0.713	token_training_loss:0.0	distillation_Loss:0.435                                                   	MRR:22.26	Hits@10:41.72	Best:22.39
2025-01-06 22:55:14,733: Snapshot:4	Epoch:13	Loss:15.919	translation_Loss:3.954	token_training_loss:11.965	distillation_Loss:0.0                                                   	MRR:22.26	Hits@10:41.72	Best:22.39
2025-01-06 22:55:17,458: End of token training: 4 Epoch: 14 Loss:6.701 MRR:22.26 Best Results: 22.39
2025-01-06 22:55:17,458: Snapshot:4	Epoch:14	Loss:6.701	translation_Loss:3.961	token_training_loss:2.74	distillation_Loss:0.0                                                           	MRR:22.26	Hits@10:41.72	Best:22.39
2025-01-06 22:55:17,788: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2025-01-06 22:55:35,454: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2115 | 0.1149 | 0.2603 | 0.3226 |  0.3973 |
|     1      | 0.1588 | 0.0767 | 0.1864 | 0.2439 |  0.3194 |
|     2      | 0.1975 | 0.1219 | 0.2179 | 0.2716 |  0.3486 |
|     3      | 0.2473 | 0.1696 | 0.2771 | 0.3235 |  0.3907 |
|     4      | 0.2239 | 0.128  | 0.244  | 0.3143 |  0.4204 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,000
Trainable params: 2,000
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:55:35,457: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2773 | 0.1682 | 0.3398 | 0.4076 |  0.4802 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2474 | 0.1418 | 0.3038 | 0.3721 |  0.4478 |
|     1      | 0.1698 | 0.0862 | 0.2014 | 0.2564 |  0.331  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2153 | 0.1177 | 0.2657 | 0.329  |  0.4003 |
|     1      | 0.1589 | 0.0766 | 0.1867 | 0.2436 |   0.32  |
|     2      | 0.2113 | 0.133  | 0.2353 | 0.2905 |  0.3663 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2159 | 0.1177 | 0.2665 | 0.3304 |  0.4035 |
|     1      | 0.1596 | 0.0774 | 0.1871 | 0.2442 |  0.321  |
|     2      | 0.2064 | 0.1273 | 0.2294 | 0.287  |  0.3654 |
|     3      | 0.2521 | 0.1759 | 0.2829 | 0.326  |  0.3899 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2115 | 0.1149 | 0.2603 | 0.3226 |  0.3973 |
|     1      | 0.1588 | 0.0767 | 0.1864 | 0.2439 |  0.3194 |
|     2      | 0.1975 | 0.1219 | 0.2179 | 0.2716 |  0.3486 |
|     3      | 0.2473 | 0.1696 | 0.2771 | 0.3235 |  0.3907 |
|     4      | 0.2239 | 0.128  | 0.244  | 0.3143 |  0.4204 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:55:35,458: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 198.60942959785461 |   0.277   |    0.168     |     0.34     |      0.48     |
|    1     | 123.04583168029785 |    0.21   |    0.115     |    0.254     |     0.391     |
|    2     | 92.05997252464294  |   0.194   |    0.107     |    0.229     |     0.363     |
|    3     | 89.87045979499817  |   0.199   |    0.113     |    0.234     |     0.367     |
|    4     | 47.76488280296326  |   0.197   |    0.111     |    0.229     |     0.364     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:55:35,458: Sum_Training_Time:551.3505764007568
2025-01-06 22:55:35,458: Every_Training_Time:[198.60942959785461, 123.04583168029785, 92.05997252464294, 89.87045979499817, 47.76488280296326]
2025-01-06 22:55:35,458: Forward transfer: 0.01555 Backward transfer: -0.023849999999999996
2025-01-06 22:55:56,737: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/RELATION/', dataset='RELATION', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106225540/RELATION', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=4444, record=False, reply_loss_weight=0.1, save_path='./checkpoint/RELATION', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[3000.0, 15000.0, 80000.0, 80000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:56:14,134: Snapshot:0	Epoch:0	Loss:24.176	translation_Loss:24.176	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:10.89	Hits@10:26.5	Best:10.89
2025-01-06 22:56:27,375: Snapshot:0	Epoch:1	Loss:15.102	translation_Loss:15.102	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.78	Hits@10:40.24	Best:18.78
2025-01-06 22:56:40,514: Snapshot:0	Epoch:2	Loss:8.766	translation_Loss:8.766	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.6	Hits@10:44.8	Best:23.6
2025-01-06 22:56:53,264: Snapshot:0	Epoch:3	Loss:4.995	translation_Loss:4.995	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:26.07	Hits@10:46.73	Best:26.07
2025-01-06 22:57:06,416: Snapshot:0	Epoch:4	Loss:2.953	translation_Loss:2.953	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.03	Hits@10:47.52	Best:27.03
2025-01-06 22:57:19,678: Snapshot:0	Epoch:5	Loss:1.949	translation_Loss:1.949	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.26	Hits@10:47.73	Best:27.26
2025-01-06 22:57:32,415: Snapshot:0	Epoch:6	Loss:1.454	translation_Loss:1.454	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.37	Hits@10:47.88	Best:27.37
2025-01-06 22:57:45,622: Snapshot:0	Epoch:7	Loss:1.165	translation_Loss:1.165	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.42	Hits@10:47.86	Best:27.42
2025-01-06 22:57:58,739: Snapshot:0	Epoch:8	Loss:0.989	translation_Loss:0.989	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.53	Hits@10:47.88	Best:27.53
2025-01-06 22:58:11,473: Snapshot:0	Epoch:9	Loss:0.877	translation_Loss:0.877	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.53	Hits@10:47.98	Best:27.53
2025-01-06 22:58:24,611: Snapshot:0	Epoch:10	Loss:0.792	translation_Loss:0.792	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.51	Hits@10:47.92	Best:27.53
2025-01-06 22:58:37,733: Early Stopping! Snapshot: 0 Epoch: 11 Best Results: 27.53
2025-01-06 22:58:37,733: Start to training tokens! Snapshot: 0 Epoch: 11 Loss:0.738 MRR:27.48 Best Results: 27.53
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:58:37,733: Snapshot:0	Epoch:11	Loss:0.738	translation_Loss:0.738	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:27.48	Hits@10:47.85	Best:27.53
2025-01-06 22:58:50,970: Snapshot:0	Epoch:12	Loss:30.568	translation_Loss:16.555	token_training_loss:14.013	distillation_Loss:0.0                                                   	MRR:27.48	Hits@10:47.85	Best:27.53
2025-01-06 22:59:04,195: End of token training: 0 Epoch: 13 Loss:16.622 MRR:27.48 Best Results: 27.53
2025-01-06 22:59:04,195: Snapshot:0	Epoch:13	Loss:16.622	translation_Loss:16.543	token_training_loss:0.079	distillation_Loss:0.0                                                           	MRR:27.48	Hits@10:47.85	Best:27.53
2025-01-06 22:59:04,484: => loading checkpoint './checkpoint/RELATION/0model_best.tar'
2025-01-06 22:59:09,878: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2789 | 0.1676 | 0.3443 | 0.4131 |  0.4846 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,312,200)
├─Embedding: 1-2                         (19,600)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,333,800
Trainable params: 2,000
Non-trainable params: 2,331,800
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:59:30,920: Snapshot:1	Epoch:0	Loss:20.009	translation_Loss:18.797	token_training_loss:0.0	distillation_Loss:1.212                                                   	MRR:10.56	Hits@10:24.9	Best:10.56
2025-01-06 22:59:43,148: Snapshot:1	Epoch:1	Loss:10.314	translation_Loss:8.13	token_training_loss:0.0	distillation_Loss:2.185                                                   	MRR:16.05	Hits@10:31.97	Best:16.05
2025-01-06 22:59:54,957: Snapshot:1	Epoch:2	Loss:6.541	translation_Loss:4.204	token_training_loss:0.0	distillation_Loss:2.337                                                   	MRR:16.84	Hits@10:33.06	Best:16.84
2025-01-06 23:00:07,317: Snapshot:1	Epoch:3	Loss:5.154	translation_Loss:2.961	token_training_loss:0.0	distillation_Loss:2.193                                                   	MRR:16.82	Hits@10:33.11	Best:16.84
2025-01-06 23:00:19,210: Snapshot:1	Epoch:4	Loss:4.634	translation_Loss:2.567	token_training_loss:0.0	distillation_Loss:2.067                                                   	MRR:16.89	Hits@10:33.0	Best:16.89
2025-01-06 23:00:31,283: Snapshot:1	Epoch:5	Loss:4.423	translation_Loss:2.43	token_training_loss:0.0	distillation_Loss:1.993                                                   	MRR:16.85	Hits@10:32.55	Best:16.89
2025-01-06 23:00:43,461: Snapshot:1	Epoch:6	Loss:4.314	translation_Loss:2.357	token_training_loss:0.0	distillation_Loss:1.957                                                   	MRR:16.94	Hits@10:32.68	Best:16.94
2025-01-06 23:00:55,243: Snapshot:1	Epoch:7	Loss:4.253	translation_Loss:2.319	token_training_loss:0.0	distillation_Loss:1.933                                                   	MRR:16.69	Hits@10:32.54	Best:16.94
2025-01-06 23:01:07,502: Snapshot:1	Epoch:8	Loss:4.215	translation_Loss:2.297	token_training_loss:0.0	distillation_Loss:1.919                                                   	MRR:16.55	Hits@10:32.32	Best:16.94
2025-01-06 23:01:19,800: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 16.94
2025-01-06 23:01:19,800: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:4.189 MRR:16.56 Best Results: 16.94
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:01:19,801: Snapshot:1	Epoch:9	Loss:4.189	translation_Loss:2.277	token_training_loss:0.0	distillation_Loss:1.912                                                   	MRR:16.56	Hits@10:32.5	Best:16.94
2025-01-06 23:01:31,353: Snapshot:1	Epoch:10	Loss:33.49	translation_Loss:19.53	token_training_loss:13.961	distillation_Loss:0.0                                                   	MRR:16.56	Hits@10:32.5	Best:16.94
2025-01-06 23:01:43,311: End of token training: 1 Epoch: 11 Loss:19.618 MRR:16.56 Best Results: 16.94
2025-01-06 23:01:43,312: Snapshot:1	Epoch:11	Loss:19.618	translation_Loss:19.514	token_training_loss:0.104	distillation_Loss:0.0                                                           	MRR:16.56	Hits@10:32.5	Best:16.94
2025-01-06 23:01:43,607: => loading checkpoint './checkpoint/RELATION/1model_best.tar'
2025-01-06 23:01:53,665: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.138  | 0.3086 | 0.3795 |  0.461  |
|     1      | 0.1718 | 0.0896 | 0.2031 | 0.258  |  0.3298 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,668,800)
├─Embedding: 1-2                         (38,400)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,709,200
Trainable params: 2,000
Non-trainable params: 2,707,200
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:02:10,422: Snapshot:2	Epoch:0	Loss:14.048	translation_Loss:12.245	token_training_loss:0.0	distillation_Loss:1.803                                                   	MRR:11.0	Hits@10:26.3	Best:11.0
2025-01-06 23:02:19,676: Snapshot:2	Epoch:1	Loss:7.832	translation_Loss:5.186	token_training_loss:0.0	distillation_Loss:2.646                                                   	MRR:18.75	Hits@10:35.02	Best:18.75
2025-01-06 23:02:28,349: Snapshot:2	Epoch:2	Loss:5.401	translation_Loss:3.399	token_training_loss:0.0	distillation_Loss:2.002                                                   	MRR:20.01	Hits@10:36.06	Best:20.01
2025-01-06 23:02:37,479: Snapshot:2	Epoch:3	Loss:4.241	translation_Loss:2.62	token_training_loss:0.0	distillation_Loss:1.621                                                   	MRR:21.08	Hits@10:36.76	Best:21.08
2025-01-06 23:02:46,183: Snapshot:2	Epoch:4	Loss:3.639	translation_Loss:2.262	token_training_loss:0.0	distillation_Loss:1.377                                                   	MRR:21.51	Hits@10:36.89	Best:21.51
2025-01-06 23:02:55,214: Snapshot:2	Epoch:5	Loss:3.334	translation_Loss:2.098	token_training_loss:0.0	distillation_Loss:1.236                                                   	MRR:21.18	Hits@10:36.59	Best:21.51
2025-01-06 23:03:03,869: Snapshot:2	Epoch:6	Loss:3.19	translation_Loss:2.027	token_training_loss:0.0	distillation_Loss:1.163                                                   	MRR:21.22	Hits@10:36.56	Best:21.51
2025-01-06 23:03:12,491: Early Stopping! Snapshot: 2 Epoch: 7 Best Results: 21.51
2025-01-06 23:03:12,491: Start to training tokens! Snapshot: 2 Epoch: 7 Loss:3.113 MRR:21.23 Best Results: 21.51
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:03:12,491: Snapshot:2	Epoch:7	Loss:3.113	translation_Loss:1.988	token_training_loss:0.0	distillation_Loss:1.124                                                   	MRR:21.23	Hits@10:36.49	Best:21.51
2025-01-06 23:03:21,459: Snapshot:2	Epoch:8	Loss:29.301	translation_Loss:14.893	token_training_loss:14.408	distillation_Loss:0.0                                                   	MRR:21.23	Hits@10:36.49	Best:21.51
2025-01-06 23:03:29,934: End of token training: 2 Epoch: 9 Loss:15.172 MRR:21.23 Best Results: 21.51
2025-01-06 23:03:29,935: Snapshot:2	Epoch:9	Loss:15.172	translation_Loss:14.889	token_training_loss:0.283	distillation_Loss:0.0                                                           	MRR:21.23	Hits@10:36.49	Best:21.51
2025-01-06 23:03:30,257: => loading checkpoint './checkpoint/RELATION/2model_best.tar'
2025-01-06 23:03:44,432: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2146 | 0.114  | 0.2687 | 0.3313 |  0.4048 |
|     1      | 0.1606 | 0.0806 | 0.1891 | 0.2437 |  0.3152 |
|     2      | 0.2117 | 0.1348 | 0.2341 | 0.2884 |  0.3648 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,751,000)
├─Embedding: 1-2                         (57,200)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,810,200
Trainable params: 2,000
Non-trainable params: 2,808,200
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:03:54,821: Snapshot:3	Epoch:0	Loss:6.807	translation_Loss:6.128	token_training_loss:0.0	distillation_Loss:0.68                                                   	MRR:6.03	Hits@10:13.27	Best:6.03
2025-01-06 23:03:58,909: Snapshot:3	Epoch:1	Loss:4.837	translation_Loss:4.145	token_training_loss:0.0	distillation_Loss:0.691                                                   	MRR:14.24	Hits@10:30.34	Best:14.24
2025-01-06 23:04:02,909: Snapshot:3	Epoch:2	Loss:3.53	translation_Loss:2.946	token_training_loss:0.0	distillation_Loss:0.584                                                   	MRR:18.11	Hits@10:34.38	Best:18.11
2025-01-06 23:04:06,924: Snapshot:3	Epoch:3	Loss:2.779	translation_Loss:2.284	token_training_loss:0.0	distillation_Loss:0.495                                                   	MRR:21.28	Hits@10:36.79	Best:21.28
2025-01-06 23:04:11,021: Snapshot:3	Epoch:4	Loss:2.365	translation_Loss:1.92	token_training_loss:0.0	distillation_Loss:0.444                                                   	MRR:22.97	Hits@10:37.91	Best:22.97
2025-01-06 23:04:15,518: Snapshot:3	Epoch:5	Loss:2.083	translation_Loss:1.685	token_training_loss:0.0	distillation_Loss:0.398                                                   	MRR:24.0	Hits@10:38.33	Best:24.0
2025-01-06 23:04:19,577: Snapshot:3	Epoch:6	Loss:1.873	translation_Loss:1.51	token_training_loss:0.0	distillation_Loss:0.363                                                   	MRR:24.63	Hits@10:38.7	Best:24.63
2025-01-06 23:04:23,538: Snapshot:3	Epoch:7	Loss:1.717	translation_Loss:1.384	token_training_loss:0.0	distillation_Loss:0.333                                                   	MRR:24.63	Hits@10:38.62	Best:24.63
2025-01-06 23:04:27,545: Snapshot:3	Epoch:8	Loss:1.608	translation_Loss:1.296	token_training_loss:0.0	distillation_Loss:0.312                                                   	MRR:25.03	Hits@10:38.8	Best:25.03
2025-01-06 23:04:31,568: Snapshot:3	Epoch:9	Loss:1.524	translation_Loss:1.227	token_training_loss:0.0	distillation_Loss:0.296                                                   	MRR:25.09	Hits@10:38.75	Best:25.09
2025-01-06 23:04:36,110: Snapshot:3	Epoch:10	Loss:1.468	translation_Loss:1.184	token_training_loss:0.0	distillation_Loss:0.284                                                   	MRR:25.21	Hits@10:39.09	Best:25.21
2025-01-06 23:04:40,114: Snapshot:3	Epoch:11	Loss:1.425	translation_Loss:1.149	token_training_loss:0.0	distillation_Loss:0.276                                                   	MRR:25.33	Hits@10:39.03	Best:25.33
2025-01-06 23:04:44,125: Snapshot:3	Epoch:12	Loss:1.398	translation_Loss:1.127	token_training_loss:0.0	distillation_Loss:0.271                                                   	MRR:25.39	Hits@10:38.71	Best:25.39
2025-01-06 23:04:48,085: Snapshot:3	Epoch:13	Loss:1.375	translation_Loss:1.108	token_training_loss:0.0	distillation_Loss:0.267                                                   	MRR:25.27	Hits@10:38.86	Best:25.39
2025-01-06 23:04:52,151: Snapshot:3	Epoch:14	Loss:1.365	translation_Loss:1.102	token_training_loss:0.0	distillation_Loss:0.263                                                   	MRR:25.24	Hits@10:38.93	Best:25.39
2025-01-06 23:04:56,645: Snapshot:3	Epoch:15	Loss:1.346	translation_Loss:1.082	token_training_loss:0.0	distillation_Loss:0.264                                                   	MRR:25.4	Hits@10:38.96	Best:25.4
2025-01-06 23:05:00,664: Snapshot:3	Epoch:16	Loss:1.334	translation_Loss:1.074	token_training_loss:0.0	distillation_Loss:0.26                                                   	MRR:25.46	Hits@10:39.04	Best:25.46
2025-01-06 23:05:04,679: Snapshot:3	Epoch:17	Loss:1.322	translation_Loss:1.064	token_training_loss:0.0	distillation_Loss:0.258                                                   	MRR:25.31	Hits@10:39.33	Best:25.46
2025-01-06 23:05:08,621: Snapshot:3	Epoch:18	Loss:1.307	translation_Loss:1.051	token_training_loss:0.0	distillation_Loss:0.256                                                   	MRR:25.21	Hits@10:39.4	Best:25.46
2025-01-06 23:05:12,655: Snapshot:3	Epoch:19	Loss:1.308	translation_Loss:1.052	token_training_loss:0.0	distillation_Loss:0.256                                                   	MRR:25.53	Hits@10:39.32	Best:25.53
2025-01-06 23:05:17,125: Snapshot:3	Epoch:20	Loss:1.302	translation_Loss:1.048	token_training_loss:0.0	distillation_Loss:0.254                                                   	MRR:25.52	Hits@10:39.17	Best:25.53
2025-01-06 23:05:21,096: Snapshot:3	Epoch:21	Loss:1.294	translation_Loss:1.038	token_training_loss:0.0	distillation_Loss:0.256                                                   	MRR:25.4	Hits@10:39.02	Best:25.53
2025-01-06 23:05:25,108: Snapshot:3	Epoch:22	Loss:1.293	translation_Loss:1.04	token_training_loss:0.0	distillation_Loss:0.253                                                   	MRR:25.54	Hits@10:39.16	Best:25.54
2025-01-06 23:05:29,088: Snapshot:3	Epoch:23	Loss:1.288	translation_Loss:1.034	token_training_loss:0.0	distillation_Loss:0.253                                                   	MRR:25.38	Hits@10:39.12	Best:25.54
2025-01-06 23:05:33,156: Snapshot:3	Epoch:24	Loss:1.285	translation_Loss:1.033	token_training_loss:0.0	distillation_Loss:0.252                                                   	MRR:25.36	Hits@10:39.4	Best:25.54
2025-01-06 23:05:37,100: Early Stopping! Snapshot: 3 Epoch: 25 Best Results: 25.54
2025-01-06 23:05:37,100: Start to training tokens! Snapshot: 3 Epoch: 25 Loss:1.28 MRR:25.22 Best Results: 25.54
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:05:37,101: Snapshot:3	Epoch:25	Loss:1.28	translation_Loss:1.028	token_training_loss:0.0	distillation_Loss:0.252                                                   	MRR:25.22	Hits@10:39.24	Best:25.54
2025-01-06 23:05:41,440: Snapshot:3	Epoch:26	Loss:19.297	translation_Loss:5.89	token_training_loss:13.407	distillation_Loss:0.0                                                   	MRR:25.22	Hits@10:39.24	Best:25.54
2025-01-06 23:05:45,354: End of token training: 3 Epoch: 27 Loss:7.514 MRR:25.22 Best Results: 25.54
2025-01-06 23:05:45,354: Snapshot:3	Epoch:27	Loss:7.514	translation_Loss:5.889	token_training_loss:1.625	distillation_Loss:0.0                                                           	MRR:25.22	Hits@10:39.24	Best:25.54
2025-01-06 23:05:45,668: => loading checkpoint './checkpoint/RELATION/3model_best.tar'
2025-01-06 23:06:01,485: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2154 | 0.1139 | 0.2694 | 0.3334 |  0.407  |
|     1      | 0.1612 | 0.0807 | 0.1899 | 0.245  |  0.3165 |
|     2      | 0.2085 | 0.1301 | 0.2317 | 0.2873 |  0.3631 |
|     3      | 0.2539 | 0.177  | 0.2867 |  0.33  |  0.3925 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,877,400)
├─Embedding: 1-2                         (76,000)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,955,400
Trainable params: 2,000
Non-trainable params: 2,953,400
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:06:10,515: Snapshot:4	Epoch:0	Loss:4.691	translation_Loss:4.176	token_training_loss:0.0	distillation_Loss:0.515                                                   	MRR:5.32	Hits@10:14.49	Best:5.32
2025-01-06 23:06:13,422: Snapshot:4	Epoch:1	Loss:3.583	translation_Loss:2.848	token_training_loss:0.0	distillation_Loss:0.735                                                   	MRR:9.65	Hits@10:25.47	Best:9.65
2025-01-06 23:06:16,428: Snapshot:4	Epoch:2	Loss:2.937	translation_Loss:2.202	token_training_loss:0.0	distillation_Loss:0.735                                                   	MRR:13.32	Hits@10:31.94	Best:13.32
2025-01-06 23:06:19,316: Snapshot:4	Epoch:3	Loss:2.446	translation_Loss:1.76	token_training_loss:0.0	distillation_Loss:0.686                                                   	MRR:17.0	Hits@10:35.6	Best:17.0
2025-01-06 23:06:22,163: Snapshot:4	Epoch:4	Loss:2.108	translation_Loss:1.479	token_training_loss:0.0	distillation_Loss:0.629                                                   	MRR:18.84	Hits@10:38.45	Best:18.84
2025-01-06 23:06:25,067: Snapshot:4	Epoch:5	Loss:1.866	translation_Loss:1.283	token_training_loss:0.0	distillation_Loss:0.583                                                   	MRR:19.88	Hits@10:40.01	Best:19.88
2025-01-06 23:06:28,035: Snapshot:4	Epoch:6	Loss:1.664	translation_Loss:1.122	token_training_loss:0.0	distillation_Loss:0.542                                                   	MRR:21.21	Hits@10:40.73	Best:21.21
2025-01-06 23:06:30,927: Snapshot:4	Epoch:7	Loss:1.508	translation_Loss:1.002	token_training_loss:0.0	distillation_Loss:0.506                                                   	MRR:21.97	Hits@10:41.32	Best:21.97
2025-01-06 23:06:34,201: Snapshot:4	Epoch:8	Loss:1.386	translation_Loss:0.912	token_training_loss:0.0	distillation_Loss:0.474                                                   	MRR:22.29	Hits@10:41.66	Best:22.29
2025-01-06 23:06:37,111: Snapshot:4	Epoch:9	Loss:1.293	translation_Loss:0.841	token_training_loss:0.0	distillation_Loss:0.452                                                   	MRR:22.49	Hits@10:42.08	Best:22.49
2025-01-06 23:06:39,976: Snapshot:4	Epoch:10	Loss:1.229	translation_Loss:0.796	token_training_loss:0.0	distillation_Loss:0.433                                                   	MRR:22.35	Hits@10:41.95	Best:22.49
2025-01-06 23:06:42,780: Snapshot:4	Epoch:11	Loss:1.172	translation_Loss:0.753	token_training_loss:0.0	distillation_Loss:0.418                                                   	MRR:22.22	Hits@10:41.86	Best:22.49
2025-01-06 23:06:45,594: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 22.49
2025-01-06 23:06:45,594: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:1.133 MRR:22.44 Best Results: 22.49
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:06:45,595: Snapshot:4	Epoch:12	Loss:1.133	translation_Loss:0.725	token_training_loss:0.0	distillation_Loss:0.408                                                   	MRR:22.44	Hits@10:41.95	Best:22.49
2025-01-06 23:06:48,391: Snapshot:4	Epoch:13	Loss:15.473	translation_Loss:4.013	token_training_loss:11.46	distillation_Loss:0.0                                                   	MRR:22.44	Hits@10:41.95	Best:22.49
2025-01-06 23:06:51,155: End of token training: 4 Epoch: 14 Loss:6.465 MRR:22.44 Best Results: 22.49
2025-01-06 23:06:51,155: Snapshot:4	Epoch:14	Loss:6.465	translation_Loss:4.02	token_training_loss:2.445	distillation_Loss:0.0                                                           	MRR:22.44	Hits@10:41.95	Best:22.49
2025-01-06 23:06:51,484: => loading checkpoint './checkpoint/RELATION/4model_best.tar'
2025-01-06 23:07:09,248: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2115 | 0.1115 | 0.2651 | 0.3267 |  0.4005 |
|     1      | 0.1603 | 0.0802 | 0.189  | 0.2437 |  0.315  |
|     2      | 0.1977 | 0.1224 | 0.2184 | 0.2711 |  0.3473 |
|     3      | 0.2453 | 0.1678 | 0.2731 | 0.3184 |  0.3919 |
|     4      | 0.2169 | 0.1189 | 0.2428 | 0.3134 |  0.4128 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,000
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,000
Trainable params: 2,000
Non-trainable params: 3,003,000
=================================================================
2025-01-06 23:07:09,251: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2789 | 0.1676 | 0.3443 | 0.4131 |  0.4846 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2492 | 0.138  | 0.3086 | 0.3795 |  0.461  |
|     1      | 0.1718 | 0.0896 | 0.2031 | 0.258  |  0.3298 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2146 | 0.114  | 0.2687 | 0.3313 |  0.4048 |
|     1      | 0.1606 | 0.0806 | 0.1891 | 0.2437 |  0.3152 |
|     2      | 0.2117 | 0.1348 | 0.2341 | 0.2884 |  0.3648 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2154 | 0.1139 | 0.2694 | 0.3334 |  0.407  |
|     1      | 0.1612 | 0.0807 | 0.1899 | 0.245  |  0.3165 |
|     2      | 0.2085 | 0.1301 | 0.2317 | 0.2873 |  0.3631 |
|     3      | 0.2539 | 0.177  | 0.2867 |  0.33  |  0.3925 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2115 | 0.1115 | 0.2651 | 0.3267 |  0.4005 |
|     1      | 0.1603 | 0.0802 | 0.189  | 0.2437 |  0.315  |
|     2      | 0.1977 | 0.1224 | 0.2184 | 0.2711 |  0.3473 |
|     3      | 0.2453 | 0.1678 | 0.2731 | 0.3184 |  0.3919 |
|     4      | 0.2169 | 0.1189 | 0.2428 | 0.3134 |  0.4128 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 23:07:09,252: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 187.45733451843262 |   0.279   |    0.168     |    0.344     |     0.485     |
|    1     | 148.82069897651672 |   0.212   |    0.114     |    0.257     |     0.397     |
|    2     | 92.50564503669739  |   0.194   |    0.107     |    0.231     |     0.362     |
|    3     | 118.88111162185669 |    0.2    |    0.113     |    0.237     |     0.366     |
|    4     | 48.12868523597717  |   0.197   |     0.11     |    0.231     |     0.363     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 23:07:09,252: Sum_Training_Time:595.7934753894806
2025-01-06 23:07:09,252: Every_Training_Time:[187.45733451843262, 148.82069897651672, 92.50564503669739, 118.88111162185669, 48.12868523597717]
2025-01-06 23:07:09,252: Forward transfer: 0.015449999999999998 Backward transfer: -0.02537500000000001
