2024-12-27 03:53:38,500: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227035303/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=50000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 03:53:48,105: Snapshot:0	Epoch:0	Loss:51.848	translation_Loss:51.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-27 03:53:54,385: Snapshot:0	Epoch:1	Loss:48.341	translation_Loss:48.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.35	Hits@10:5.97	Best:3.35
2024-12-27 03:54:00,585: Snapshot:0	Epoch:2	Loss:45.084	translation_Loss:45.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.51	Hits@10:8.56	Best:4.51
2024-12-27 03:54:06,868: Snapshot:0	Epoch:3	Loss:42.069	translation_Loss:42.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.4	Hits@10:10.8	Best:5.4
2024-12-27 03:54:13,565: Snapshot:0	Epoch:4	Loss:39.208	translation_Loss:39.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.07	Best:6.23
2024-12-27 03:54:19,831: Snapshot:0	Epoch:5	Loss:36.473	translation_Loss:36.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.09	Hits@10:15.44	Best:7.09
2024-12-27 03:54:26,107: Snapshot:0	Epoch:6	Loss:33.877	translation_Loss:33.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.91	Hits@10:17.71	Best:7.91
2024-12-27 03:54:32,418: Snapshot:0	Epoch:7	Loss:31.404	translation_Loss:31.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.84	Hits@10:19.88	Best:8.84
2024-12-27 03:54:38,674: Snapshot:0	Epoch:8	Loss:29.038	translation_Loss:29.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.74	Hits@10:22.0	Best:9.74
2024-12-27 03:54:44,947: Snapshot:0	Epoch:9	Loss:26.816	translation_Loss:26.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.66	Hits@10:23.92	Best:10.66
2024-12-27 03:54:51,162: Snapshot:0	Epoch:10	Loss:24.706	translation_Loss:24.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.54	Hits@10:25.58	Best:11.54
2024-12-27 03:54:57,549: Snapshot:0	Epoch:11	Loss:22.703	translation_Loss:22.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.32	Hits@10:27.17	Best:12.32
2024-12-27 03:55:03,882: Snapshot:0	Epoch:12	Loss:20.861	translation_Loss:20.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.05	Hits@10:28.55	Best:13.05
2024-12-27 03:55:10,262: Snapshot:0	Epoch:13	Loss:19.111	translation_Loss:19.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.69	Hits@10:29.77	Best:13.69
2024-12-27 03:55:16,674: Snapshot:0	Epoch:14	Loss:17.523	translation_Loss:17.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.37	Hits@10:30.81	Best:14.37
2024-12-27 03:55:23,015: Snapshot:0	Epoch:15	Loss:16.057	translation_Loss:16.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.97	Hits@10:31.85	Best:14.97
2024-12-27 03:55:29,399: Snapshot:0	Epoch:16	Loss:14.671	translation_Loss:14.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.59	Hits@10:32.68	Best:15.59
2024-12-27 03:55:35,712: Snapshot:0	Epoch:17	Loss:13.412	translation_Loss:13.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.14	Hits@10:33.5	Best:16.14
2024-12-27 03:55:42,097: Snapshot:0	Epoch:18	Loss:12.255	translation_Loss:12.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.71	Hits@10:34.34	Best:16.71
2024-12-27 03:55:48,450: Snapshot:0	Epoch:19	Loss:11.17	translation_Loss:11.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.29	Hits@10:34.96	Best:17.29
2024-12-27 03:55:54,825: Snapshot:0	Epoch:20	Loss:10.172	translation_Loss:10.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.79	Hits@10:35.4	Best:17.79
2024-12-27 03:56:01,160: Snapshot:0	Epoch:21	Loss:9.287	translation_Loss:9.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.29	Hits@10:35.98	Best:18.29
2024-12-27 03:56:07,490: Snapshot:0	Epoch:22	Loss:8.473	translation_Loss:8.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.77	Hits@10:36.44	Best:18.77
2024-12-27 03:56:13,743: Snapshot:0	Epoch:23	Loss:7.692	translation_Loss:7.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.2	Hits@10:36.87	Best:19.2
2024-12-27 03:56:20,538: Snapshot:0	Epoch:24	Loss:6.988	translation_Loss:6.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.64	Hits@10:37.29	Best:19.64
2024-12-27 03:56:26,822: Snapshot:0	Epoch:25	Loss:6.369	translation_Loss:6.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.06	Hits@10:37.48	Best:20.06
2024-12-27 03:56:33,081: Snapshot:0	Epoch:26	Loss:5.776	translation_Loss:5.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.39	Hits@10:37.8	Best:20.39
2024-12-27 03:56:39,349: Snapshot:0	Epoch:27	Loss:5.251	translation_Loss:5.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.69	Hits@10:37.97	Best:20.69
2024-12-27 03:56:45,642: Snapshot:0	Epoch:28	Loss:4.767	translation_Loss:4.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.01	Hits@10:38.25	Best:21.01
2024-12-27 03:56:51,959: Snapshot:0	Epoch:29	Loss:4.309	translation_Loss:4.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.32	Hits@10:38.41	Best:21.32
2024-12-27 03:56:58,288: Snapshot:0	Epoch:30	Loss:3.939	translation_Loss:3.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.58	Hits@10:38.48	Best:21.58
2024-12-27 03:57:04,669: Snapshot:0	Epoch:31	Loss:3.586	translation_Loss:3.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.78	Hits@10:38.65	Best:21.78
2024-12-27 03:57:11,089: Snapshot:0	Epoch:32	Loss:3.273	translation_Loss:3.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.0	Hits@10:38.84	Best:22.0
2024-12-27 03:57:17,439: Snapshot:0	Epoch:33	Loss:2.974	translation_Loss:2.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.22	Hits@10:38.93	Best:22.22
2024-12-27 03:57:23,700: Snapshot:0	Epoch:34	Loss:2.715	translation_Loss:2.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.45	Hits@10:39.05	Best:22.45
2024-12-27 03:57:30,424: Snapshot:0	Epoch:35	Loss:2.495	translation_Loss:2.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.68	Hits@10:39.16	Best:22.68
2024-12-27 03:57:36,695: Snapshot:0	Epoch:36	Loss:2.284	translation_Loss:2.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.82	Hits@10:39.24	Best:22.82
2024-12-27 03:57:42,920: Snapshot:0	Epoch:37	Loss:2.099	translation_Loss:2.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.91	Hits@10:39.23	Best:22.91
2024-12-27 03:57:49,174: Snapshot:0	Epoch:38	Loss:1.924	translation_Loss:1.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.07	Hits@10:39.38	Best:23.07
2024-12-27 03:57:55,436: Snapshot:0	Epoch:39	Loss:1.787	translation_Loss:1.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.11	Hits@10:39.45	Best:23.11
2024-12-27 03:58:01,788: Snapshot:0	Epoch:40	Loss:1.654	translation_Loss:1.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:39.46	Best:23.22
2024-12-27 03:58:08,102: Snapshot:0	Epoch:41	Loss:1.538	translation_Loss:1.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.3	Hits@10:39.46	Best:23.3
2024-12-27 03:58:14,372: Snapshot:0	Epoch:42	Loss:1.42	translation_Loss:1.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.42	Hits@10:39.57	Best:23.42
2024-12-27 03:58:20,590: Snapshot:0	Epoch:43	Loss:1.322	translation_Loss:1.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.49	Hits@10:39.6	Best:23.49
2024-12-27 03:58:26,873: Snapshot:0	Epoch:44	Loss:1.237	translation_Loss:1.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.6	Hits@10:39.73	Best:23.6
2024-12-27 03:58:33,084: Snapshot:0	Epoch:45	Loss:1.165	translation_Loss:1.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.61	Hits@10:39.74	Best:23.61
2024-12-27 03:58:39,359: Snapshot:0	Epoch:46	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.74	Best:23.69
2024-12-27 03:58:45,641: Snapshot:0	Epoch:47	Loss:1.018	translation_Loss:1.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:39.88	Best:23.74
2024-12-27 03:58:51,880: Snapshot:0	Epoch:48	Loss:0.963	translation_Loss:0.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:39.89	Best:23.75
2024-12-27 03:58:58,159: Snapshot:0	Epoch:49	Loss:0.905	translation_Loss:0.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.82	Hits@10:39.81	Best:23.82
2024-12-27 03:59:04,447: Snapshot:0	Epoch:50	Loss:0.858	translation_Loss:0.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.9	Hits@10:39.91	Best:23.9
2024-12-27 03:59:10,740: Snapshot:0	Epoch:51	Loss:0.812	translation_Loss:0.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.91	Hits@10:40.02	Best:23.91
2024-12-27 03:59:17,004: Snapshot:0	Epoch:52	Loss:0.773	translation_Loss:0.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:39.91	Best:23.91
2024-12-27 03:59:23,348: Snapshot:0	Epoch:53	Loss:0.733	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.97	Hits@10:39.96	Best:23.97
2024-12-27 03:59:30,107: Snapshot:0	Epoch:54	Loss:0.705	translation_Loss:0.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:39.99	Best:23.99
2024-12-27 03:59:36,437: Snapshot:0	Epoch:55	Loss:0.665	translation_Loss:0.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.97	Hits@10:40.08	Best:23.99
2024-12-27 03:59:42,793: Snapshot:0	Epoch:56	Loss:0.647	translation_Loss:0.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.01	Hits@10:40.15	Best:24.01
2024-12-27 03:59:49,136: Snapshot:0	Epoch:57	Loss:0.618	translation_Loss:0.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.01	Hits@10:40.1	Best:24.01
2024-12-27 03:59:55,475: Snapshot:0	Epoch:58	Loss:0.598	translation_Loss:0.598	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.07	Hits@10:40.08	Best:24.07
2024-12-27 04:00:01,857: Snapshot:0	Epoch:59	Loss:0.566	translation_Loss:0.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.01	Hits@10:40.08	Best:24.07
2024-12-27 04:00:08,340: Snapshot:0	Epoch:60	Loss:0.544	translation_Loss:0.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.02	Best:24.16
2024-12-27 04:00:14,640: Snapshot:0	Epoch:61	Loss:0.526	translation_Loss:0.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:40.05	Best:24.16
2024-12-27 04:00:20,865: Snapshot:0	Epoch:62	Loss:0.512	translation_Loss:0.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:40.08	Best:24.16
2024-12-27 04:00:27,151: Early Stopping! Snapshot: 0 Epoch: 63 Best Results: 24.16
2024-12-27 04:00:27,152: Start to training tokens! Snapshot: 0 Epoch: 63 Loss:0.495 MRR:24.11 Best Results: 24.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 04:00:27,152: Snapshot:0	Epoch:63	Loss:0.495	translation_Loss:0.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.0	Best:24.16
2024-12-27 04:00:34,522: Snapshot:0	Epoch:64	Loss:135.428	translation_Loss:35.965	multi_layer_Loss:99.463	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.0	Best:24.16
2024-12-27 04:00:40,821: End of token training: 0 Epoch: 65 Loss:61.343 MRR:24.11 Best Results: 24.16
2024-12-27 04:00:40,821: Snapshot:0	Epoch:65	Loss:61.343	translation_Loss:35.975	multi_layer_Loss:25.369	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.11	Hits@10:40.0	Best:24.16
2024-12-27 04:00:41,088: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-27 04:00:43,874: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2342 | 0.1463 | 0.2789 | 0.3286 |  0.3904 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:01:08,183: Snapshot:1	Epoch:0	Loss:25.221	translation_Loss:24.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.687                                                   	MRR:17.73	Hits@10:30.89	Best:17.73
2024-12-27 04:01:15,138: Snapshot:1	Epoch:1	Loss:23.968	translation_Loss:22.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.23                                                   	MRR:18.07	Hits@10:31.3	Best:18.07
2024-12-27 04:01:22,075: Snapshot:1	Epoch:2	Loss:23.189	translation_Loss:21.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.404                                                   	MRR:18.29	Hits@10:31.59	Best:18.29
2024-12-27 04:01:28,917: Snapshot:1	Epoch:3	Loss:22.455	translation_Loss:20.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.47                                                   	MRR:18.43	Hits@10:31.84	Best:18.43
2024-12-27 04:01:35,794: Snapshot:1	Epoch:4	Loss:21.661	translation_Loss:20.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.502                                                   	MRR:18.68	Hits@10:32.14	Best:18.68
2024-12-27 04:01:42,654: Snapshot:1	Epoch:5	Loss:20.923	translation_Loss:19.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.516                                                   	MRR:18.84	Hits@10:32.33	Best:18.84
2024-12-27 04:01:49,529: Snapshot:1	Epoch:6	Loss:20.242	translation_Loss:18.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.525                                                   	MRR:19.02	Hits@10:32.66	Best:19.02
2024-12-27 04:01:56,470: Snapshot:1	Epoch:7	Loss:19.654	translation_Loss:18.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.53                                                   	MRR:19.16	Hits@10:32.85	Best:19.16
2024-12-27 04:02:03,477: Snapshot:1	Epoch:8	Loss:19.132	translation_Loss:17.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.532                                                   	MRR:19.34	Hits@10:33.01	Best:19.34
2024-12-27 04:02:10,390: Snapshot:1	Epoch:9	Loss:18.668	translation_Loss:17.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.532                                                   	MRR:19.47	Hits@10:33.09	Best:19.47
2024-12-27 04:02:17,263: Snapshot:1	Epoch:10	Loss:18.31	translation_Loss:16.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.524                                                   	MRR:19.53	Hits@10:33.25	Best:19.53
2024-12-27 04:02:24,144: Snapshot:1	Epoch:11	Loss:18.032	translation_Loss:16.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.519                                                   	MRR:19.62	Hits@10:33.4	Best:19.62
2024-12-27 04:02:31,097: Snapshot:1	Epoch:12	Loss:17.787	translation_Loss:16.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.511                                                   	MRR:19.72	Hits@10:33.55	Best:19.72
2024-12-27 04:02:38,033: Snapshot:1	Epoch:13	Loss:17.597	translation_Loss:16.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.503                                                   	MRR:19.8	Hits@10:33.63	Best:19.8
2024-12-27 04:02:44,915: Snapshot:1	Epoch:14	Loss:17.454	translation_Loss:15.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.496                                                   	MRR:19.83	Hits@10:33.67	Best:19.83
2024-12-27 04:02:51,826: Snapshot:1	Epoch:15	Loss:17.318	translation_Loss:15.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.491                                                   	MRR:19.9	Hits@10:33.76	Best:19.9
2024-12-27 04:02:58,703: Snapshot:1	Epoch:16	Loss:17.223	translation_Loss:15.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.483                                                   	MRR:19.91	Hits@10:33.76	Best:19.91
2024-12-27 04:03:05,621: Snapshot:1	Epoch:17	Loss:17.123	translation_Loss:15.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.478                                                   	MRR:19.97	Hits@10:33.88	Best:19.97
2024-12-27 04:03:12,531: Snapshot:1	Epoch:18	Loss:17.081	translation_Loss:15.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.472                                                   	MRR:19.97	Hits@10:33.83	Best:19.97
2024-12-27 04:03:19,387: Snapshot:1	Epoch:19	Loss:17.026	translation_Loss:15.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.469                                                   	MRR:19.99	Hits@10:33.82	Best:19.99
2024-12-27 04:03:26,309: Snapshot:1	Epoch:20	Loss:16.953	translation_Loss:15.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:20.01	Hits@10:33.86	Best:20.01
2024-12-27 04:03:33,235: Snapshot:1	Epoch:21	Loss:16.912	translation_Loss:15.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.463                                                   	MRR:20.05	Hits@10:33.85	Best:20.05
2024-12-27 04:03:40,054: Snapshot:1	Epoch:22	Loss:16.889	translation_Loss:15.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.462                                                   	MRR:20.02	Hits@10:33.9	Best:20.05
2024-12-27 04:03:46,910: Snapshot:1	Epoch:23	Loss:16.849	translation_Loss:15.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.46                                                   	MRR:20.05	Hits@10:33.92	Best:20.05
2024-12-27 04:03:53,784: Snapshot:1	Epoch:24	Loss:16.851	translation_Loss:15.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.456                                                   	MRR:20.07	Hits@10:33.86	Best:20.07
2024-12-27 04:04:00,710: Snapshot:1	Epoch:25	Loss:16.773	translation_Loss:15.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.455                                                   	MRR:20.03	Hits@10:33.87	Best:20.07
2024-12-27 04:04:07,656: Snapshot:1	Epoch:26	Loss:16.771	translation_Loss:15.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.454                                                   	MRR:20.05	Hits@10:33.92	Best:20.07
2024-12-27 04:04:14,552: Snapshot:1	Epoch:27	Loss:16.752	translation_Loss:15.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.452                                                   	MRR:20.08	Hits@10:33.95	Best:20.08
2024-12-27 04:04:21,388: Snapshot:1	Epoch:28	Loss:16.762	translation_Loss:15.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.456                                                   	MRR:20.09	Hits@10:33.98	Best:20.09
2024-12-27 04:04:28,222: Snapshot:1	Epoch:29	Loss:16.757	translation_Loss:15.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:20.06	Hits@10:33.91	Best:20.09
2024-12-27 04:04:35,043: Snapshot:1	Epoch:30	Loss:16.708	translation_Loss:15.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:20.06	Hits@10:33.94	Best:20.09
2024-12-27 04:04:41,974: Early Stopping! Snapshot: 1 Epoch: 31 Best Results: 20.09
2024-12-27 04:04:41,974: Start to training tokens! Snapshot: 1 Epoch: 31 Loss:16.692 MRR:20.07 Best Results: 20.09
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 04:04:41,974: Snapshot:1	Epoch:31	Loss:16.692	translation_Loss:15.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.45                                                   	MRR:20.07	Hits@10:33.93	Best:20.09
2024-12-27 04:04:48,745: Snapshot:1	Epoch:32	Loss:141.679	translation_Loss:44.409	multi_layer_Loss:97.27	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.07	Hits@10:33.93	Best:20.09
2024-12-27 04:04:55,537: End of token training: 1 Epoch: 33 Loss:67.375 MRR:20.07 Best Results: 20.09
2024-12-27 04:04:55,537: Snapshot:1	Epoch:33	Loss:67.375	translation_Loss:44.383	multi_layer_Loss:22.993	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.07	Hits@10:33.93	Best:20.09
2024-12-27 04:04:55,884: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-27 04:05:01,672: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1514 | 0.284  | 0.3336 |  0.3969 |
|     1      | 0.199  | 0.1188 | 0.2415 | 0.2865 |  0.3424 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:05:25,939: Snapshot:2	Epoch:0	Loss:21.398	translation_Loss:20.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.732                                                   	MRR:17.87	Hits@10:31.95	Best:17.87
2024-12-27 04:05:33,518: Snapshot:2	Epoch:1	Loss:20.301	translation_Loss:18.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.482                                                   	MRR:18.11	Hits@10:32.47	Best:18.11
2024-12-27 04:05:40,574: Snapshot:2	Epoch:2	Loss:19.895	translation_Loss:18.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.794                                                   	MRR:18.24	Hits@10:32.7	Best:18.24
2024-12-27 04:05:47,687: Snapshot:2	Epoch:3	Loss:19.505	translation_Loss:17.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.916                                                   	MRR:18.35	Hits@10:32.88	Best:18.35
2024-12-27 04:05:54,833: Snapshot:2	Epoch:4	Loss:19.231	translation_Loss:17.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.973                                                   	MRR:18.43	Hits@10:32.93	Best:18.43
2024-12-27 04:06:01,896: Snapshot:2	Epoch:5	Loss:18.865	translation_Loss:16.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.0                                                   	MRR:18.5	Hits@10:33.01	Best:18.5
2024-12-27 04:06:08,987: Snapshot:2	Epoch:6	Loss:18.59	translation_Loss:16.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.015                                                   	MRR:18.55	Hits@10:33.1	Best:18.55
2024-12-27 04:06:16,072: Snapshot:2	Epoch:7	Loss:18.326	translation_Loss:16.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.022                                                   	MRR:18.63	Hits@10:33.23	Best:18.63
2024-12-27 04:06:23,145: Snapshot:2	Epoch:8	Loss:18.133	translation_Loss:16.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.026                                                   	MRR:18.66	Hits@10:33.31	Best:18.66
2024-12-27 04:06:30,213: Snapshot:2	Epoch:9	Loss:18.01	translation_Loss:15.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.029                                                   	MRR:18.73	Hits@10:33.36	Best:18.73
2024-12-27 04:06:37,348: Snapshot:2	Epoch:10	Loss:17.871	translation_Loss:15.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.026                                                   	MRR:18.77	Hits@10:33.37	Best:18.77
2024-12-27 04:06:44,835: Snapshot:2	Epoch:11	Loss:17.776	translation_Loss:15.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.022                                                   	MRR:18.77	Hits@10:33.37	Best:18.77
2024-12-27 04:06:51,899: Snapshot:2	Epoch:12	Loss:17.705	translation_Loss:15.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.017                                                   	MRR:18.81	Hits@10:33.46	Best:18.81
2024-12-27 04:06:59,090: Snapshot:2	Epoch:13	Loss:17.621	translation_Loss:15.605	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.015                                                   	MRR:18.82	Hits@10:33.45	Best:18.82
2024-12-27 04:07:06,265: Snapshot:2	Epoch:14	Loss:17.574	translation_Loss:15.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.012                                                   	MRR:18.86	Hits@10:33.5	Best:18.86
2024-12-27 04:07:13,432: Snapshot:2	Epoch:15	Loss:17.561	translation_Loss:15.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.009                                                   	MRR:18.82	Hits@10:33.56	Best:18.86
2024-12-27 04:07:20,515: Snapshot:2	Epoch:16	Loss:17.526	translation_Loss:15.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.005                                                   	MRR:18.81	Hits@10:33.44	Best:18.86
2024-12-27 04:07:27,545: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 18.86
2024-12-27 04:07:27,545: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:17.503 MRR:18.86 Best Results: 18.86
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 04:07:27,546: Snapshot:2	Epoch:17	Loss:17.503	translation_Loss:15.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.003                                                   	MRR:18.86	Hits@10:33.39	Best:18.86
2024-12-27 04:07:34,559: Snapshot:2	Epoch:18	Loss:145.647	translation_Loss:45.055	multi_layer_Loss:100.592	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.86	Hits@10:33.39	Best:18.86
2024-12-27 04:07:41,514: End of token training: 2 Epoch: 19 Loss:70.172 MRR:18.86 Best Results: 18.86
2024-12-27 04:07:41,514: Snapshot:2	Epoch:19	Loss:70.172	translation_Loss:45.1	multi_layer_Loss:25.072	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.86	Hits@10:33.39	Best:18.86
2024-12-27 04:07:41,825: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-27 04:07:50,923: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2411 | 0.1521 | 0.2862 | 0.337  |  0.4018 |
|     1      | 0.2043 | 0.1229 | 0.2465 | 0.2928 |  0.3502 |
|     2      | 0.1868 | 0.1059 | 0.2271 | 0.2735 |  0.3326 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:08:15,342: Snapshot:3	Epoch:0	Loss:17.956	translation_Loss:17.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.751                                                   	MRR:16.27	Hits@10:31.32	Best:16.27
2024-12-27 04:08:22,477: Snapshot:3	Epoch:1	Loss:16.672	translation_Loss:15.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.653                                                   	MRR:16.65	Hits@10:32.12	Best:16.65
2024-12-27 04:08:29,658: Snapshot:3	Epoch:2	Loss:16.355	translation_Loss:14.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.095                                                   	MRR:16.88	Hits@10:32.38	Best:16.88
2024-12-27 04:08:36,916: Snapshot:3	Epoch:3	Loss:16.139	translation_Loss:13.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.293                                                   	MRR:16.93	Hits@10:32.54	Best:16.93
2024-12-27 04:08:44,155: Snapshot:3	Epoch:4	Loss:15.955	translation_Loss:13.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.388                                                   	MRR:17.08	Hits@10:32.72	Best:17.08
2024-12-27 04:08:51,361: Snapshot:3	Epoch:5	Loss:15.818	translation_Loss:13.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.43                                                   	MRR:17.16	Hits@10:32.81	Best:17.16
2024-12-27 04:08:58,514: Snapshot:3	Epoch:6	Loss:15.712	translation_Loss:13.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.451                                                   	MRR:17.18	Hits@10:32.84	Best:17.18
2024-12-27 04:09:05,654: Snapshot:3	Epoch:7	Loss:15.637	translation_Loss:13.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.467                                                   	MRR:17.23	Hits@10:32.82	Best:17.23
2024-12-27 04:09:12,849: Snapshot:3	Epoch:8	Loss:15.537	translation_Loss:13.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.475                                                   	MRR:17.32	Hits@10:32.96	Best:17.32
2024-12-27 04:09:19,979: Snapshot:3	Epoch:9	Loss:15.509	translation_Loss:13.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.481                                                   	MRR:17.29	Hits@10:33.0	Best:17.32
2024-12-27 04:09:27,106: Snapshot:3	Epoch:10	Loss:15.47	translation_Loss:12.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.484                                                   	MRR:17.29	Hits@10:33.04	Best:17.32
2024-12-27 04:09:34,239: Snapshot:3	Epoch:11	Loss:15.46	translation_Loss:12.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.482                                                   	MRR:17.38	Hits@10:33.06	Best:17.38
2024-12-27 04:09:41,407: Snapshot:3	Epoch:12	Loss:15.425	translation_Loss:12.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.48                                                   	MRR:17.38	Hits@10:33.03	Best:17.38
2024-12-27 04:09:48,562: Snapshot:3	Epoch:13	Loss:15.416	translation_Loss:12.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.479                                                   	MRR:17.36	Hits@10:33.08	Best:17.38
2024-12-27 04:09:56,156: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 17.38
2024-12-27 04:09:56,156: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:15.382 MRR:17.36 Best Results: 17.38
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 04:09:56,157: Snapshot:3	Epoch:14	Loss:15.382	translation_Loss:12.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.479                                                   	MRR:17.36	Hits@10:33.14	Best:17.38
2024-12-27 04:10:03,197: Snapshot:3	Epoch:15	Loss:141.526	translation_Loss:44.044	multi_layer_Loss:97.482	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.36	Hits@10:33.14	Best:17.38
2024-12-27 04:10:10,442: End of token training: 3 Epoch: 16 Loss:66.35 MRR:17.36 Best Results: 17.38
2024-12-27 04:10:10,442: Snapshot:3	Epoch:16	Loss:66.35	translation_Loss:44.032	multi_layer_Loss:22.318	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.36	Hits@10:33.14	Best:17.38
2024-12-27 04:10:10,781: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-27 04:10:23,200: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1514 | 0.2821 | 0.3349 |  0.4005 |
|     1      | 0.205  | 0.1231 | 0.2446 | 0.2945 |  0.3554 |
|     2      | 0.1911 | 0.1089 | 0.2286 | 0.2798 |  0.3435 |
|     3      | 0.1746 | 0.0938 | 0.2053 | 0.2584 |  0.3314 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:10:47,297: Snapshot:4	Epoch:0	Loss:13.781	translation_Loss:13.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.724                                                   	MRR:16.8	Hits@10:36.35	Best:16.8
2024-12-27 04:10:54,894: Snapshot:4	Epoch:1	Loss:11.387	translation_Loss:9.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.734                                                   	MRR:17.97	Hits@10:38.97	Best:17.97
2024-12-27 04:11:02,096: Snapshot:4	Epoch:2	Loss:10.629	translation_Loss:8.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.288                                                   	MRR:18.41	Hits@10:40.27	Best:18.41
2024-12-27 04:11:09,350: Snapshot:4	Epoch:3	Loss:10.078	translation_Loss:7.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.546                                                   	MRR:18.7	Hits@10:40.58	Best:18.7
2024-12-27 04:11:16,635: Snapshot:4	Epoch:4	Loss:9.64	translation_Loss:6.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.672                                                   	MRR:18.98	Hits@10:40.91	Best:18.98
2024-12-27 04:11:23,881: Snapshot:4	Epoch:5	Loss:9.36	translation_Loss:6.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.73                                                   	MRR:19.16	Hits@10:41.09	Best:19.16
2024-12-27 04:11:31,160: Snapshot:4	Epoch:6	Loss:9.095	translation_Loss:6.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.765                                                   	MRR:19.33	Hits@10:41.09	Best:19.33
2024-12-27 04:11:38,361: Snapshot:4	Epoch:7	Loss:9.021	translation_Loss:6.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.773                                                   	MRR:19.54	Hits@10:41.39	Best:19.54
2024-12-27 04:11:45,585: Snapshot:4	Epoch:8	Loss:8.914	translation_Loss:6.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.784                                                   	MRR:19.76	Hits@10:41.48	Best:19.76
2024-12-27 04:11:52,699: Snapshot:4	Epoch:9	Loss:8.858	translation_Loss:6.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.787                                                   	MRR:19.74	Hits@10:41.26	Best:19.76
2024-12-27 04:11:59,862: Snapshot:4	Epoch:10	Loss:8.808	translation_Loss:6.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.787                                                   	MRR:19.81	Hits@10:41.48	Best:19.81
2024-12-27 04:12:07,485: Snapshot:4	Epoch:11	Loss:8.774	translation_Loss:5.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.785                                                   	MRR:19.75	Hits@10:41.38	Best:19.81
2024-12-27 04:12:14,810: Snapshot:4	Epoch:12	Loss:8.777	translation_Loss:5.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.793                                                   	MRR:19.82	Hits@10:41.3	Best:19.82
2024-12-27 04:12:22,044: Snapshot:4	Epoch:13	Loss:8.748	translation_Loss:5.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.799                                                   	MRR:19.89	Hits@10:41.39	Best:19.89
2024-12-27 04:12:29,291: Snapshot:4	Epoch:14	Loss:8.74	translation_Loss:5.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.795                                                   	MRR:19.91	Hits@10:41.46	Best:19.91
2024-12-27 04:12:36,424: Snapshot:4	Epoch:15	Loss:8.746	translation_Loss:5.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.799                                                   	MRR:19.9	Hits@10:41.5	Best:19.91
2024-12-27 04:12:43,550: Snapshot:4	Epoch:16	Loss:8.705	translation_Loss:5.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.791                                                   	MRR:19.89	Hits@10:41.53	Best:19.91
2024-12-27 04:12:50,685: Early Stopping! Snapshot: 4 Epoch: 17 Best Results: 19.91
2024-12-27 04:12:50,685: Start to training tokens! Snapshot: 4 Epoch: 17 Loss:8.712 MRR:19.85 Best Results: 19.91
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 04:12:50,686: Snapshot:4	Epoch:17	Loss:8.712	translation_Loss:5.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.793                                                   	MRR:19.85	Hits@10:41.44	Best:19.91
2024-12-27 04:12:57,712: Snapshot:4	Epoch:18	Loss:134.996	translation_Loss:36.995	multi_layer_Loss:98.001	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.85	Hits@10:41.44	Best:19.91
2024-12-27 04:13:04,801: End of token training: 4 Epoch: 19 Loss:58.571 MRR:19.85 Best Results: 19.91
2024-12-27 04:13:04,801: Snapshot:4	Epoch:19	Loss:58.571	translation_Loss:36.971	multi_layer_Loss:21.6	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.85	Hits@10:41.44	Best:19.91
2024-12-27 04:13:05,147: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-27 04:13:20,859: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2345 | 0.1476 | 0.2757 | 0.3262 |  0.393  |
|     1      | 0.2004 | 0.1197 | 0.2382 | 0.2868 |  0.3516 |
|     2      | 0.1872 | 0.1058 | 0.2231 | 0.2739 |  0.341  |
|     3      | 0.1751 | 0.0921 | 0.2044 | 0.2606 |  0.3392 |
|     4      | 0.2004 | 0.0951 | 0.2372 | 0.3147 |  0.4139 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 04:13:20,879: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2342 | 0.1463 | 0.2789 | 0.3286 |  0.3904 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1514 | 0.284  | 0.3336 |  0.3969 |
|     1      | 0.199  | 0.1188 | 0.2415 | 0.2865 |  0.3424 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2411 | 0.1521 | 0.2862 | 0.337  |  0.4018 |
|     1      | 0.2043 | 0.1229 | 0.2465 | 0.2928 |  0.3502 |
|     2      | 0.1868 | 0.1059 | 0.2271 | 0.2735 |  0.3326 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1514 | 0.2821 | 0.3349 |  0.4005 |
|     1      | 0.205  | 0.1231 | 0.2446 | 0.2945 |  0.3554 |
|     2      | 0.1911 | 0.1089 | 0.2286 | 0.2798 |  0.3435 |
|     3      | 0.1746 | 0.0938 | 0.2053 | 0.2584 |  0.3314 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2345 | 0.1476 | 0.2757 | 0.3262 |  0.393  |
|     1      | 0.2004 | 0.1197 | 0.2382 | 0.2868 |  0.3516 |
|     2      | 0.1872 | 0.1058 | 0.2231 | 0.2739 |  0.341  |
|     3      | 0.1751 | 0.0921 | 0.2044 | 0.2606 |  0.3392 |
|     4      | 0.2004 | 0.0951 | 0.2372 | 0.3147 |  0.4139 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 04:13:20,880: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 422.3208758831024  |   0.234   |    0.146     |    0.279     |      0.39     |
|    1     | 248.90107417106628 |   0.219   |    0.135     |    0.263     |      0.37     |
|    2     | 156.25624823570251 |   0.211   |    0.127     |    0.253     |     0.362     |
|    3     | 136.21070957183838 |   0.203   |    0.119     |     0.24     |     0.358     |
|    4     | 158.54434847831726 |    0.2    |    0.112     |    0.236     |     0.368     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 04:13:20,880: Sum_Training_Time:1122.2332563400269
2024-12-27 04:13:20,880: Every_Training_Time:[422.3208758831024, 248.90107417106628, 156.25624823570251, 136.21070957183838, 158.54434847831726]
2024-12-27 04:13:20,880: Forward transfer: 0.15439999999999998 Backward transfer: 0.0006499999999999978
2024-12-27 04:13:58,057: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227041325/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 04:14:07,770: Snapshot:0	Epoch:0	Loss:51.848	translation_Loss:51.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-27 04:14:14,005: Snapshot:0	Epoch:1	Loss:48.341	translation_Loss:48.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.35	Hits@10:5.97	Best:3.35
2024-12-27 04:14:20,304: Snapshot:0	Epoch:2	Loss:45.084	translation_Loss:45.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.51	Hits@10:8.56	Best:4.51
2024-12-27 04:14:26,589: Snapshot:0	Epoch:3	Loss:42.069	translation_Loss:42.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.4	Hits@10:10.8	Best:5.4
2024-12-27 04:14:33,267: Snapshot:0	Epoch:4	Loss:39.208	translation_Loss:39.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.07	Best:6.23
2024-12-27 04:14:39,569: Snapshot:0	Epoch:5	Loss:36.473	translation_Loss:36.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.09	Hits@10:15.45	Best:7.09
2024-12-27 04:14:45,835: Snapshot:0	Epoch:6	Loss:33.877	translation_Loss:33.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.92	Hits@10:17.71	Best:7.92
2024-12-27 04:14:52,120: Snapshot:0	Epoch:7	Loss:31.404	translation_Loss:31.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.84	Hits@10:19.87	Best:8.84
2024-12-27 04:14:58,432: Snapshot:0	Epoch:8	Loss:29.038	translation_Loss:29.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.75	Hits@10:22.01	Best:9.75
2024-12-27 04:15:04,865: Snapshot:0	Epoch:9	Loss:26.816	translation_Loss:26.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.64	Hits@10:23.91	Best:10.64
2024-12-27 04:15:11,352: Snapshot:0	Epoch:10	Loss:24.706	translation_Loss:24.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.54	Hits@10:25.55	Best:11.54
2024-12-27 04:15:17,670: Snapshot:0	Epoch:11	Loss:22.703	translation_Loss:22.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.19	Best:12.31
2024-12-27 04:15:24,029: Snapshot:0	Epoch:12	Loss:20.861	translation_Loss:20.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.04	Hits@10:28.57	Best:13.04
2024-12-27 04:15:30,351: Snapshot:0	Epoch:13	Loss:19.112	translation_Loss:19.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.69	Hits@10:29.78	Best:13.69
2024-12-27 04:15:36,662: Snapshot:0	Epoch:14	Loss:17.523	translation_Loss:17.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.36	Hits@10:30.87	Best:14.36
2024-12-27 04:15:42,968: Snapshot:0	Epoch:15	Loss:16.057	translation_Loss:16.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.99	Hits@10:31.89	Best:14.99
2024-12-27 04:15:49,294: Snapshot:0	Epoch:16	Loss:14.672	translation_Loss:14.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.6	Hits@10:32.73	Best:15.6
2024-12-27 04:15:55,591: Snapshot:0	Epoch:17	Loss:13.413	translation_Loss:13.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.11	Hits@10:33.51	Best:16.11
2024-12-27 04:16:01,959: Snapshot:0	Epoch:18	Loss:12.256	translation_Loss:12.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.72	Hits@10:34.32	Best:16.72
2024-12-27 04:16:08,313: Snapshot:0	Epoch:19	Loss:11.171	translation_Loss:11.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.28	Hits@10:34.94	Best:17.28
2024-12-27 04:16:14,619: Snapshot:0	Epoch:20	Loss:10.173	translation_Loss:10.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.77	Hits@10:35.43	Best:17.77
2024-12-27 04:16:20,935: Snapshot:0	Epoch:21	Loss:9.289	translation_Loss:9.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.28	Hits@10:35.89	Best:18.28
2024-12-27 04:16:27,243: Snapshot:0	Epoch:22	Loss:8.473	translation_Loss:8.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.78	Hits@10:36.44	Best:18.78
2024-12-27 04:16:33,608: Snapshot:0	Epoch:23	Loss:7.692	translation_Loss:7.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.22	Hits@10:36.84	Best:19.22
2024-12-27 04:16:40,486: Snapshot:0	Epoch:24	Loss:6.989	translation_Loss:6.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.66	Hits@10:37.24	Best:19.66
2024-12-27 04:16:46,850: Snapshot:0	Epoch:25	Loss:6.37	translation_Loss:6.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.06	Hits@10:37.45	Best:20.06
2024-12-27 04:16:53,251: Snapshot:0	Epoch:26	Loss:5.775	translation_Loss:5.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.38	Hits@10:37.81	Best:20.38
2024-12-27 04:16:59,569: Snapshot:0	Epoch:27	Loss:5.251	translation_Loss:5.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.71	Hits@10:37.96	Best:20.71
2024-12-27 04:17:06,004: Snapshot:0	Epoch:28	Loss:4.767	translation_Loss:4.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.02	Hits@10:38.22	Best:21.02
2024-12-27 04:17:12,453: Snapshot:0	Epoch:29	Loss:4.306	translation_Loss:4.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.29	Hits@10:38.42	Best:21.29
2024-12-27 04:17:18,741: Snapshot:0	Epoch:30	Loss:3.938	translation_Loss:3.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.57	Hits@10:38.48	Best:21.57
2024-12-27 04:17:25,050: Snapshot:0	Epoch:31	Loss:3.589	translation_Loss:3.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.79	Hits@10:38.67	Best:21.79
2024-12-27 04:17:31,449: Snapshot:0	Epoch:32	Loss:3.271	translation_Loss:3.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.03	Hits@10:38.88	Best:22.03
2024-12-27 04:17:37,932: Snapshot:0	Epoch:33	Loss:2.974	translation_Loss:2.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.23	Hits@10:39.0	Best:22.23
2024-12-27 04:17:44,274: Snapshot:0	Epoch:34	Loss:2.715	translation_Loss:2.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.43	Hits@10:39.03	Best:22.43
2024-12-27 04:17:51,098: Snapshot:0	Epoch:35	Loss:2.496	translation_Loss:2.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.66	Hits@10:39.2	Best:22.66
2024-12-27 04:17:57,448: Snapshot:0	Epoch:36	Loss:2.283	translation_Loss:2.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.81	Hits@10:39.25	Best:22.81
2024-12-27 04:18:03,802: Snapshot:0	Epoch:37	Loss:2.099	translation_Loss:2.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.9	Hits@10:39.27	Best:22.9
2024-12-27 04:18:10,201: Snapshot:0	Epoch:38	Loss:1.925	translation_Loss:1.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.07	Hits@10:39.34	Best:23.07
2024-12-27 04:18:16,537: Snapshot:0	Epoch:39	Loss:1.787	translation_Loss:1.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.09	Hits@10:39.34	Best:23.09
2024-12-27 04:18:22,875: Snapshot:0	Epoch:40	Loss:1.654	translation_Loss:1.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.23	Hits@10:39.36	Best:23.23
2024-12-27 04:18:29,318: Snapshot:0	Epoch:41	Loss:1.538	translation_Loss:1.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.26	Hits@10:39.46	Best:23.26
2024-12-27 04:18:35,679: Snapshot:0	Epoch:42	Loss:1.421	translation_Loss:1.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:39.6	Best:23.43
2024-12-27 04:18:42,063: Snapshot:0	Epoch:43	Loss:1.324	translation_Loss:1.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:39.65	Best:23.51
2024-12-27 04:18:48,408: Snapshot:0	Epoch:44	Loss:1.236	translation_Loss:1.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.71	Best:23.58
2024-12-27 04:18:54,734: Snapshot:0	Epoch:45	Loss:1.164	translation_Loss:1.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.6	Hits@10:39.7	Best:23.6
2024-12-27 04:19:01,120: Snapshot:0	Epoch:46	Loss:1.083	translation_Loss:1.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.68	Hits@10:39.71	Best:23.68
2024-12-27 04:19:07,544: Snapshot:0	Epoch:47	Loss:1.017	translation_Loss:1.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.79	Best:23.69
2024-12-27 04:19:13,988: Snapshot:0	Epoch:48	Loss:0.962	translation_Loss:0.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.79	Hits@10:39.9	Best:23.79
2024-12-27 04:19:20,305: Snapshot:0	Epoch:49	Loss:0.905	translation_Loss:0.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.81	Hits@10:39.96	Best:23.81
2024-12-27 04:19:26,725: Snapshot:0	Epoch:50	Loss:0.859	translation_Loss:0.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.83	Hits@10:39.94	Best:23.83
2024-12-27 04:19:33,060: Snapshot:0	Epoch:51	Loss:0.812	translation_Loss:0.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.93	Hits@10:39.89	Best:23.93
2024-12-27 04:19:39,379: Snapshot:0	Epoch:52	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.93	Hits@10:39.84	Best:23.93
2024-12-27 04:19:45,687: Snapshot:0	Epoch:53	Loss:0.733	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:39.89	Best:23.99
2024-12-27 04:19:52,520: Snapshot:0	Epoch:54	Loss:0.704	translation_Loss:0.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.03	Hits@10:39.96	Best:24.03
2024-12-27 04:19:58,911: Snapshot:0	Epoch:55	Loss:0.665	translation_Loss:0.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:40.08	Best:24.05
2024-12-27 04:20:05,388: Snapshot:0	Epoch:56	Loss:0.647	translation_Loss:0.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:40.12	Best:24.06
2024-12-27 04:20:11,873: Snapshot:0	Epoch:57	Loss:0.617	translation_Loss:0.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:40.1	Best:24.08
2024-12-27 04:20:18,179: Snapshot:0	Epoch:58	Loss:0.601	translation_Loss:0.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:40.1	Best:24.12
2024-12-27 04:20:24,494: Snapshot:0	Epoch:59	Loss:0.566	translation_Loss:0.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:40.0	Best:24.12
2024-12-27 04:20:30,856: Snapshot:0	Epoch:60	Loss:0.544	translation_Loss:0.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.15	Hits@10:40.02	Best:24.15
2024-12-27 04:20:37,161: Snapshot:0	Epoch:61	Loss:0.527	translation_Loss:0.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:40.07	Best:24.15
2024-12-27 04:20:43,668: Snapshot:0	Epoch:62	Loss:0.511	translation_Loss:0.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:40.11	Best:24.18
2024-12-27 04:20:49,978: Snapshot:0	Epoch:63	Loss:0.493	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.11	Best:24.18
2024-12-27 04:20:56,782: Snapshot:0	Epoch:64	Loss:0.468	translation_Loss:0.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.02	Best:24.18
2024-12-27 04:21:03,218: Early Stopping! Snapshot: 0 Epoch: 65 Best Results: 24.18
2024-12-27 04:21:03,218: Start to training tokens! Snapshot: 0 Epoch: 65 Loss:0.457 MRR:24.16 Best Results: 24.18
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 04:21:03,219: Snapshot:0	Epoch:65	Loss:0.457	translation_Loss:0.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.02	Best:24.18
2024-12-27 04:21:10,045: Snapshot:0	Epoch:66	Loss:114.903	translation_Loss:35.931	multi_layer_Loss:78.972	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.02	Best:24.18
2024-12-27 04:21:16,457: End of token training: 0 Epoch: 67 Loss:48.272 MRR:24.16 Best Results: 24.18
2024-12-27 04:21:16,457: Snapshot:0	Epoch:67	Loss:48.272	translation_Loss:35.892	multi_layer_Loss:12.38	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.16	Hits@10:40.02	Best:24.18
2024-12-27 04:21:16,791: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-27 04:21:19,614: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2351 | 0.1484 | 0.2793 | 0.3287 |  0.3909 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:21:43,460: Snapshot:1	Epoch:0	Loss:24.191	translation_Loss:24.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.133                                                   	MRR:18.21	Hits@10:31.23	Best:18.21
2024-12-27 04:21:50,325: Snapshot:1	Epoch:1	Loss:20.183	translation_Loss:19.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:19.03	Hits@10:32.53	Best:19.03
2024-12-27 04:21:57,207: Snapshot:1	Epoch:2	Loss:17.271	translation_Loss:16.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.745                                                   	MRR:19.65	Hits@10:33.66	Best:19.65
2024-12-27 04:22:04,128: Snapshot:1	Epoch:3	Loss:14.948	translation_Loss:13.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.133                                                   	MRR:20.23	Hits@10:34.49	Best:20.23
2024-12-27 04:22:11,089: Snapshot:1	Epoch:4	Loss:13.114	translation_Loss:11.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.518                                                   	MRR:20.75	Hits@10:35.17	Best:20.75
2024-12-27 04:22:18,100: Snapshot:1	Epoch:5	Loss:11.595	translation_Loss:9.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.875                                                   	MRR:21.14	Hits@10:35.85	Best:21.14
2024-12-27 04:22:25,421: Snapshot:1	Epoch:6	Loss:10.33	translation_Loss:8.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.195                                                   	MRR:21.55	Hits@10:36.35	Best:21.55
2024-12-27 04:22:32,377: Snapshot:1	Epoch:7	Loss:9.384	translation_Loss:6.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.469                                                   	MRR:21.8	Hits@10:36.79	Best:21.8
2024-12-27 04:22:39,297: Snapshot:1	Epoch:8	Loss:8.611	translation_Loss:5.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.703                                                   	MRR:22.04	Hits@10:37.13	Best:22.04
2024-12-27 04:22:46,302: Snapshot:1	Epoch:9	Loss:8.044	translation_Loss:5.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.892                                                   	MRR:22.24	Hits@10:37.34	Best:22.24
2024-12-27 04:22:53,321: Snapshot:1	Epoch:10	Loss:7.573	translation_Loss:4.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.042                                                   	MRR:22.41	Hits@10:37.67	Best:22.41
2024-12-27 04:23:00,343: Snapshot:1	Epoch:11	Loss:7.264	translation_Loss:4.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.166                                                   	MRR:22.56	Hits@10:37.88	Best:22.56
2024-12-27 04:23:07,373: Snapshot:1	Epoch:12	Loss:6.979	translation_Loss:3.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.263                                                   	MRR:22.61	Hits@10:37.96	Best:22.61
2024-12-27 04:23:14,429: Snapshot:1	Epoch:13	Loss:6.82	translation_Loss:3.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.339                                                   	MRR:22.66	Hits@10:38.06	Best:22.66
2024-12-27 04:23:21,459: Snapshot:1	Epoch:14	Loss:6.675	translation_Loss:3.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.402                                                   	MRR:22.73	Hits@10:38.28	Best:22.73
2024-12-27 04:23:28,415: Snapshot:1	Epoch:15	Loss:6.551	translation_Loss:3.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.454                                                   	MRR:22.78	Hits@10:38.3	Best:22.78
2024-12-27 04:23:35,816: Snapshot:1	Epoch:16	Loss:6.444	translation_Loss:2.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.498                                                   	MRR:22.88	Hits@10:38.49	Best:22.88
2024-12-27 04:23:42,803: Snapshot:1	Epoch:17	Loss:6.384	translation_Loss:2.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.534                                                   	MRR:22.9	Hits@10:38.57	Best:22.9
2024-12-27 04:23:49,686: Snapshot:1	Epoch:18	Loss:6.321	translation_Loss:2.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.566                                                   	MRR:22.9	Hits@10:38.54	Best:22.9
2024-12-27 04:23:56,644: Snapshot:1	Epoch:19	Loss:6.27	translation_Loss:2.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.592                                                   	MRR:22.93	Hits@10:38.63	Best:22.93
2024-12-27 04:24:03,608: Snapshot:1	Epoch:20	Loss:6.205	translation_Loss:2.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.613                                                   	MRR:22.99	Hits@10:38.69	Best:22.99
2024-12-27 04:24:10,557: Snapshot:1	Epoch:21	Loss:6.193	translation_Loss:2.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.633                                                   	MRR:22.98	Hits@10:38.76	Best:22.99
2024-12-27 04:24:17,467: Snapshot:1	Epoch:22	Loss:6.163	translation_Loss:2.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.653                                                   	MRR:23.03	Hits@10:38.75	Best:23.03
2024-12-27 04:24:24,318: Snapshot:1	Epoch:23	Loss:6.114	translation_Loss:2.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.667                                                   	MRR:23.01	Hits@10:38.69	Best:23.03
2024-12-27 04:24:31,250: Snapshot:1	Epoch:24	Loss:6.074	translation_Loss:2.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.682                                                   	MRR:23.02	Hits@10:38.72	Best:23.03
2024-12-27 04:24:38,138: Early Stopping! Snapshot: 1 Epoch: 25 Best Results: 23.03
2024-12-27 04:24:38,139: Start to training tokens! Snapshot: 1 Epoch: 25 Loss:6.055 MRR:23.0 Best Results: 23.03
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 04:24:38,139: Snapshot:1	Epoch:25	Loss:6.055	translation_Loss:2.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.692                                                   	MRR:23.0	Hits@10:38.73	Best:23.03
2024-12-27 04:24:44,929: Snapshot:1	Epoch:26	Loss:117.583	translation_Loss:39.317	multi_layer_Loss:78.266	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.0	Hits@10:38.73	Best:23.03
2024-12-27 04:24:51,843: End of token training: 1 Epoch: 27 Loss:49.177 MRR:23.0 Best Results: 23.03
2024-12-27 04:24:51,843: Snapshot:1	Epoch:27	Loss:49.177	translation_Loss:39.313	multi_layer_Loss:9.863	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.0	Hits@10:38.73	Best:23.03
2024-12-27 04:24:52,191: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-27 04:24:58,131: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2489 | 0.1569 | 0.2916 | 0.3497 |  0.4207 |
|     1      | 0.2316 | 0.1461 | 0.2733 | 0.3231 |  0.386  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:25:22,558: Snapshot:2	Epoch:0	Loss:14.46	translation_Loss:14.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.135                                                   	MRR:20.63	Hits@10:36.32	Best:20.63
2024-12-27 04:25:29,737: Snapshot:2	Epoch:1	Loss:10.817	translation_Loss:10.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.365                                                   	MRR:21.24	Hits@10:37.15	Best:21.24
2024-12-27 04:25:36,866: Snapshot:2	Epoch:2	Loss:8.601	translation_Loss:7.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.613                                                   	MRR:21.51	Hits@10:37.72	Best:21.51
2024-12-27 04:25:43,982: Snapshot:2	Epoch:3	Loss:7.066	translation_Loss:6.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.858                                                   	MRR:21.86	Hits@10:38.29	Best:21.86
2024-12-27 04:25:51,001: Snapshot:2	Epoch:4	Loss:5.987	translation_Loss:4.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.079                                                   	MRR:22.06	Hits@10:38.69	Best:22.06
2024-12-27 04:25:58,083: Snapshot:2	Epoch:5	Loss:5.175	translation_Loss:3.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.262                                                   	MRR:22.28	Hits@10:38.82	Best:22.28
2024-12-27 04:26:05,246: Snapshot:2	Epoch:6	Loss:4.583	translation_Loss:3.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.408                                                   	MRR:22.3	Hits@10:38.98	Best:22.3
2024-12-27 04:26:12,445: Snapshot:2	Epoch:7	Loss:4.206	translation_Loss:2.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.524                                                   	MRR:22.34	Hits@10:39.07	Best:22.34
2024-12-27 04:26:20,048: Snapshot:2	Epoch:8	Loss:3.948	translation_Loss:2.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.616                                                   	MRR:22.4	Hits@10:39.18	Best:22.4
2024-12-27 04:26:27,129: Snapshot:2	Epoch:9	Loss:3.745	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.684                                                   	MRR:22.5	Hits@10:39.16	Best:22.5
2024-12-27 04:26:34,161: Snapshot:2	Epoch:10	Loss:3.644	translation_Loss:1.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.739                                                   	MRR:22.48	Hits@10:39.29	Best:22.5
2024-12-27 04:26:41,280: Snapshot:2	Epoch:11	Loss:3.547	translation_Loss:1.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.782                                                   	MRR:22.51	Hits@10:39.27	Best:22.51
2024-12-27 04:26:48,400: Snapshot:2	Epoch:12	Loss:3.454	translation_Loss:1.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.817                                                   	MRR:22.52	Hits@10:39.22	Best:22.52
2024-12-27 04:26:55,439: Snapshot:2	Epoch:13	Loss:3.434	translation_Loss:1.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.845                                                   	MRR:22.49	Hits@10:39.36	Best:22.52
2024-12-27 04:27:02,509: Snapshot:2	Epoch:14	Loss:3.386	translation_Loss:1.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.869                                                   	MRR:22.43	Hits@10:39.33	Best:22.52
2024-12-27 04:27:09,612: Early Stopping! Snapshot: 2 Epoch: 15 Best Results: 22.52
2024-12-27 04:27:09,612: Start to training tokens! Snapshot: 2 Epoch: 15 Loss:3.353 MRR:22.46 Best Results: 22.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 04:27:09,612: Snapshot:2	Epoch:15	Loss:3.353	translation_Loss:1.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.892                                                   	MRR:22.46	Hits@10:39.38	Best:22.52
2024-12-27 04:27:16,680: Snapshot:2	Epoch:16	Loss:119.572	translation_Loss:38.911	multi_layer_Loss:80.661	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.46	Hits@10:39.38	Best:22.52
2024-12-27 04:27:23,747: End of token training: 2 Epoch: 17 Loss:49.145 MRR:22.46 Best Results: 22.52
2024-12-27 04:27:23,748: Snapshot:2	Epoch:17	Loss:49.145	translation_Loss:38.891	multi_layer_Loss:10.254	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.46	Hits@10:39.38	Best:22.52
2024-12-27 04:27:24,093: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-27 04:27:33,275: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2377 | 0.1484 | 0.2769 | 0.3326 |  0.4097 |
|     1      | 0.2293 | 0.143  | 0.2653 | 0.3201 |  0.3959 |
|     2      | 0.2262 | 0.1387 | 0.2636 | 0.318  |  0.3945 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:27:57,506: Snapshot:3	Epoch:0	Loss:7.165	translation_Loss:7.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.125                                                   	MRR:19.77	Hits@10:37.42	Best:19.77
2024-12-27 04:28:04,691: Snapshot:3	Epoch:1	Loss:4.749	translation_Loss:4.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.297                                                   	MRR:20.22	Hits@10:38.67	Best:20.22
2024-12-27 04:28:11,903: Snapshot:3	Epoch:2	Loss:3.559	translation_Loss:3.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.428                                                   	MRR:20.41	Hits@10:39.11	Best:20.41
2024-12-27 04:28:19,156: Snapshot:3	Epoch:3	Loss:2.856	translation_Loss:2.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.532                                                   	MRR:20.63	Hits@10:39.36	Best:20.63
2024-12-27 04:28:26,425: Snapshot:3	Epoch:4	Loss:2.393	translation_Loss:1.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.613                                                   	MRR:20.71	Hits@10:39.42	Best:20.71
2024-12-27 04:28:34,133: Snapshot:3	Epoch:5	Loss:2.103	translation_Loss:1.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.676                                                   	MRR:20.77	Hits@10:39.51	Best:20.77
2024-12-27 04:28:41,320: Snapshot:3	Epoch:6	Loss:1.916	translation_Loss:1.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.722                                                   	MRR:20.81	Hits@10:39.61	Best:20.81
2024-12-27 04:28:48,480: Snapshot:3	Epoch:7	Loss:1.81	translation_Loss:1.049	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.761                                                   	MRR:20.86	Hits@10:39.73	Best:20.86
2024-12-27 04:28:55,695: Snapshot:3	Epoch:8	Loss:1.736	translation_Loss:0.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.793                                                   	MRR:20.8	Hits@10:39.79	Best:20.86
2024-12-27 04:29:02,902: Snapshot:3	Epoch:9	Loss:1.666	translation_Loss:0.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.816                                                   	MRR:20.97	Hits@10:39.76	Best:20.97
2024-12-27 04:29:10,086: Snapshot:3	Epoch:10	Loss:1.64	translation_Loss:0.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.831                                                   	MRR:20.82	Hits@10:39.79	Best:20.97
2024-12-27 04:29:17,316: Snapshot:3	Epoch:11	Loss:1.617	translation_Loss:0.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.848                                                   	MRR:20.84	Hits@10:39.61	Best:20.97
2024-12-27 04:29:24,379: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 20.97
2024-12-27 04:29:24,379: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:1.591 MRR:20.83 Best Results: 20.97
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 04:29:24,380: Snapshot:3	Epoch:12	Loss:1.591	translation_Loss:0.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.858                                                   	MRR:20.83	Hits@10:39.67	Best:20.97
2024-12-27 04:29:31,506: Snapshot:3	Epoch:13	Loss:107.122	translation_Loss:37.418	multi_layer_Loss:69.704	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.83	Hits@10:39.67	Best:20.97
2024-12-27 04:29:38,718: End of token training: 3 Epoch: 14 Loss:45.995 MRR:20.83 Best Results: 20.97
2024-12-27 04:29:38,719: Snapshot:3	Epoch:14	Loss:45.995	translation_Loss:37.413	multi_layer_Loss:8.582	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.83	Hits@10:39.67	Best:20.97
2024-12-27 04:29:39,067: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-27 04:29:51,562: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2201 | 0.1361 | 0.2521 | 0.3082 |  0.3831 |
|     1      | 0.213  | 0.1303 | 0.244  | 0.2966 |  0.3739 |
|     2      | 0.213  | 0.1255 | 0.2454 | 0.3056 |  0.3876 |
|     3      | 0.2111 | 0.1157 | 0.2446 | 0.3106 |  0.4005 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:30:16,486: Snapshot:4	Epoch:0	Loss:4.214	translation_Loss:4.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.099                                                   	MRR:20.21	Hits@10:44.84	Best:20.21
2024-12-27 04:30:23,845: Snapshot:4	Epoch:1	Loss:2.815	translation_Loss:2.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.189                                                   	MRR:21.14	Hits@10:46.71	Best:21.14
2024-12-27 04:30:31,564: Snapshot:4	Epoch:2	Loss:2.078	translation_Loss:1.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.217                                                   	MRR:21.68	Hits@10:46.94	Best:21.68
2024-12-27 04:30:38,885: Snapshot:4	Epoch:3	Loss:1.545	translation_Loss:1.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:22.03	Hits@10:47.67	Best:22.03
2024-12-27 04:30:46,240: Snapshot:4	Epoch:4	Loss:1.154	translation_Loss:0.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.255                                                   	MRR:22.23	Hits@10:47.78	Best:22.23
2024-12-27 04:30:53,600: Snapshot:4	Epoch:5	Loss:0.894	translation_Loss:0.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.26                                                   	MRR:22.39	Hits@10:47.8	Best:22.39
2024-12-27 04:31:00,916: Snapshot:4	Epoch:6	Loss:0.734	translation_Loss:0.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:22.45	Hits@10:48.15	Best:22.45
2024-12-27 04:31:08,242: Snapshot:4	Epoch:7	Loss:0.64	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:22.85	Hits@10:48.38	Best:22.85
2024-12-27 04:31:15,532: Snapshot:4	Epoch:8	Loss:0.587	translation_Loss:0.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:22.84	Hits@10:48.44	Best:22.85
2024-12-27 04:31:22,883: Snapshot:4	Epoch:9	Loss:0.54	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:22.86	Hits@10:48.19	Best:22.86
2024-12-27 04:31:30,221: Snapshot:4	Epoch:10	Loss:0.508	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:22.88	Hits@10:48.14	Best:22.88
2024-12-27 04:31:37,464: Snapshot:4	Epoch:11	Loss:0.496	translation_Loss:0.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.256                                                   	MRR:22.82	Hits@10:48.15	Best:22.88
2024-12-27 04:31:45,295: Snapshot:4	Epoch:12	Loss:0.481	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.257                                                   	MRR:23.22	Hits@10:48.68	Best:23.22
2024-12-27 04:31:52,586: Snapshot:4	Epoch:13	Loss:0.47	translation_Loss:0.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.259                                                   	MRR:22.98	Hits@10:48.4	Best:23.22
2024-12-27 04:31:59,836: Snapshot:4	Epoch:14	Loss:0.47	translation_Loss:0.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:22.92	Hits@10:48.52	Best:23.22
2024-12-27 04:32:07,063: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 23.22
2024-12-27 04:32:07,064: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:0.458 MRR:22.79 Best Results: 23.22
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 04:32:07,064: Snapshot:4	Epoch:15	Loss:0.458	translation_Loss:0.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:22.79	Hits@10:48.6	Best:23.22
2024-12-27 04:32:14,216: Snapshot:4	Epoch:16	Loss:104.865	translation_Loss:31.664	multi_layer_Loss:73.201	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.79	Hits@10:48.6	Best:23.22
2024-12-27 04:32:21,445: End of token training: 4 Epoch: 17 Loss:40.338 MRR:22.79 Best Results: 23.22
2024-12-27 04:32:21,445: Snapshot:4	Epoch:17	Loss:40.338	translation_Loss:31.717	multi_layer_Loss:8.621	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.79	Hits@10:48.6	Best:23.22
2024-12-27 04:32:21,788: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-27 04:32:37,723: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2015 | 0.1199 | 0.231  | 0.284  |  0.361  |
|     1      | 0.1942 | 0.1158 | 0.2197 | 0.2726 |  0.3486 |
|     2      | 0.1908 | 0.1068 | 0.219  | 0.2773 |   0.36  |
|     3      | 0.1886 | 0.0971 | 0.2171 | 0.283  |  0.3778 |
|     4      | 0.2326 | 0.1114 | 0.2748 | 0.3613 |  0.4834 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 04:32:37,725: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2351 | 0.1484 | 0.2793 | 0.3287 |  0.3909 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2489 | 0.1569 | 0.2916 | 0.3497 |  0.4207 |
|     1      | 0.2316 | 0.1461 | 0.2733 | 0.3231 |  0.386  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2377 | 0.1484 | 0.2769 | 0.3326 |  0.4097 |
|     1      | 0.2293 | 0.143  | 0.2653 | 0.3201 |  0.3959 |
|     2      | 0.2262 | 0.1387 | 0.2636 | 0.318  |  0.3945 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2201 | 0.1361 | 0.2521 | 0.3082 |  0.3831 |
|     1      | 0.213  | 0.1303 | 0.244  | 0.2966 |  0.3739 |
|     2      | 0.213  | 0.1255 | 0.2454 | 0.3056 |  0.3876 |
|     3      | 0.2111 | 0.1157 | 0.2446 | 0.3106 |  0.4005 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2015 | 0.1199 | 0.231  | 0.284  |  0.361  |
|     1      | 0.1942 | 0.1158 | 0.2197 | 0.2726 |  0.3486 |
|     2      | 0.1908 | 0.1068 | 0.219  | 0.2773 |   0.36  |
|     3      | 0.1886 | 0.0971 | 0.2171 | 0.283  |  0.3778 |
|     4      | 0.2326 | 0.1114 | 0.2748 | 0.3613 |  0.4834 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 04:32:37,725: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 438.39904403686523 |   0.235   |    0.148     |    0.279     |     0.391     |
|    1     | 209.1499104499817  |    0.24   |    0.151     |    0.282     |     0.403     |
|    2     | 142.3581986427307  |   0.231   |    0.143     |    0.269     |      0.4      |
|    3     | 121.84636974334717 |   0.214   |    0.127     |    0.247     |     0.386     |
|    4     | 146.20094084739685 |   0.202   |     0.11     |    0.232     |     0.386     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 04:32:37,725: Sum_Training_Time:1057.9544637203217
2024-12-27 04:32:37,725: Every_Training_Time:[438.39904403686523, 209.1499104499817, 142.3581986427307, 121.84636974334717, 146.20094084739685]
2024-12-27 04:32:37,725: Forward transfer: 0.176725 Backward transfer: -0.032225000000000004
2024-12-27 04:33:15,112: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227043241/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 04:33:24,843: Snapshot:0	Epoch:0	Loss:51.848	translation_Loss:51.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-27 04:33:31,153: Snapshot:0	Epoch:1	Loss:48.341	translation_Loss:48.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.35	Hits@10:5.97	Best:3.35
2024-12-27 04:33:37,445: Snapshot:0	Epoch:2	Loss:45.084	translation_Loss:45.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.51	Hits@10:8.56	Best:4.51
2024-12-27 04:33:43,754: Snapshot:0	Epoch:3	Loss:42.069	translation_Loss:42.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.4	Hits@10:10.8	Best:5.4
2024-12-27 04:33:50,490: Snapshot:0	Epoch:4	Loss:39.208	translation_Loss:39.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.07	Best:6.23
2024-12-27 04:33:56,785: Snapshot:0	Epoch:5	Loss:36.473	translation_Loss:36.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.09	Hits@10:15.45	Best:7.09
2024-12-27 04:34:03,085: Snapshot:0	Epoch:6	Loss:33.877	translation_Loss:33.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.92	Hits@10:17.71	Best:7.92
2024-12-27 04:34:09,406: Snapshot:0	Epoch:7	Loss:31.404	translation_Loss:31.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.84	Hits@10:19.87	Best:8.84
2024-12-27 04:34:15,662: Snapshot:0	Epoch:8	Loss:29.038	translation_Loss:29.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.75	Hits@10:22.01	Best:9.75
2024-12-27 04:34:21,933: Snapshot:0	Epoch:9	Loss:26.816	translation_Loss:26.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.65	Hits@10:23.9	Best:10.65
2024-12-27 04:34:28,226: Snapshot:0	Epoch:10	Loss:24.706	translation_Loss:24.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.54	Hits@10:25.55	Best:11.54
2024-12-27 04:34:34,514: Snapshot:0	Epoch:11	Loss:22.703	translation_Loss:22.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.18	Best:12.31
2024-12-27 04:34:40,781: Snapshot:0	Epoch:12	Loss:20.861	translation_Loss:20.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.04	Hits@10:28.59	Best:13.04
2024-12-27 04:34:47,060: Snapshot:0	Epoch:13	Loss:19.112	translation_Loss:19.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.7	Hits@10:29.75	Best:13.7
2024-12-27 04:34:53,364: Snapshot:0	Epoch:14	Loss:17.522	translation_Loss:17.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.35	Hits@10:30.87	Best:14.35
2024-12-27 04:34:59,619: Snapshot:0	Epoch:15	Loss:16.058	translation_Loss:16.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.98	Hits@10:31.86	Best:14.98
2024-12-27 04:35:05,980: Snapshot:0	Epoch:16	Loss:14.672	translation_Loss:14.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.61	Hits@10:32.72	Best:15.61
2024-12-27 04:35:12,279: Snapshot:0	Epoch:17	Loss:13.413	translation_Loss:13.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.11	Hits@10:33.54	Best:16.11
2024-12-27 04:35:18,545: Snapshot:0	Epoch:18	Loss:12.257	translation_Loss:12.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.72	Hits@10:34.32	Best:16.72
2024-12-27 04:35:24,790: Snapshot:0	Epoch:19	Loss:11.171	translation_Loss:11.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.28	Hits@10:34.91	Best:17.28
2024-12-27 04:35:31,134: Snapshot:0	Epoch:20	Loss:10.172	translation_Loss:10.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.79	Hits@10:35.43	Best:17.79
2024-12-27 04:35:37,410: Snapshot:0	Epoch:21	Loss:9.288	translation_Loss:9.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.3	Hits@10:35.91	Best:18.3
2024-12-27 04:35:43,681: Snapshot:0	Epoch:22	Loss:8.473	translation_Loss:8.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.77	Hits@10:36.45	Best:18.77
2024-12-27 04:35:49,952: Snapshot:0	Epoch:23	Loss:7.693	translation_Loss:7.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.21	Hits@10:36.81	Best:19.21
2024-12-27 04:35:56,728: Snapshot:0	Epoch:24	Loss:6.988	translation_Loss:6.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.64	Hits@10:37.23	Best:19.64
2024-12-27 04:36:03,029: Snapshot:0	Epoch:25	Loss:6.369	translation_Loss:6.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.05	Hits@10:37.44	Best:20.05
2024-12-27 04:36:09,285: Snapshot:0	Epoch:26	Loss:5.776	translation_Loss:5.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.37	Hits@10:37.78	Best:20.37
2024-12-27 04:36:15,590: Snapshot:0	Epoch:27	Loss:5.252	translation_Loss:5.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.71	Hits@10:38.0	Best:20.71
2024-12-27 04:36:21,875: Snapshot:0	Epoch:28	Loss:4.767	translation_Loss:4.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.99	Hits@10:38.2	Best:20.99
2024-12-27 04:36:28,182: Snapshot:0	Epoch:29	Loss:4.306	translation_Loss:4.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.29	Hits@10:38.38	Best:21.29
2024-12-27 04:36:34,475: Snapshot:0	Epoch:30	Loss:3.937	translation_Loss:3.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.6	Hits@10:38.55	Best:21.6
2024-12-27 04:36:40,707: Snapshot:0	Epoch:31	Loss:3.586	translation_Loss:3.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.77	Hits@10:38.64	Best:21.77
2024-12-27 04:36:47,011: Snapshot:0	Epoch:32	Loss:3.273	translation_Loss:3.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.01	Hits@10:38.77	Best:22.01
2024-12-27 04:36:53,397: Snapshot:0	Epoch:33	Loss:2.975	translation_Loss:2.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.22	Hits@10:38.86	Best:22.22
2024-12-27 04:36:59,709: Snapshot:0	Epoch:34	Loss:2.714	translation_Loss:2.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.46	Hits@10:38.98	Best:22.46
2024-12-27 04:37:06,497: Snapshot:0	Epoch:35	Loss:2.494	translation_Loss:2.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:39.21	Best:22.63
2024-12-27 04:37:12,830: Snapshot:0	Epoch:36	Loss:2.285	translation_Loss:2.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.75	Hits@10:39.23	Best:22.75
2024-12-27 04:37:19,129: Snapshot:0	Epoch:37	Loss:2.097	translation_Loss:2.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.91	Hits@10:39.24	Best:22.91
2024-12-27 04:37:25,500: Snapshot:0	Epoch:38	Loss:1.923	translation_Loss:1.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.03	Hits@10:39.39	Best:23.03
2024-12-27 04:37:31,844: Snapshot:0	Epoch:39	Loss:1.786	translation_Loss:1.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.14	Hits@10:39.43	Best:23.14
2024-12-27 04:37:38,148: Snapshot:0	Epoch:40	Loss:1.654	translation_Loss:1.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.2	Hits@10:39.42	Best:23.2
2024-12-27 04:37:44,437: Snapshot:0	Epoch:41	Loss:1.537	translation_Loss:1.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.31	Hits@10:39.5	Best:23.31
2024-12-27 04:37:50,766: Snapshot:0	Epoch:42	Loss:1.419	translation_Loss:1.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:39.65	Best:23.43
2024-12-27 04:37:56,976: Snapshot:0	Epoch:43	Loss:1.325	translation_Loss:1.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.46	Hits@10:39.65	Best:23.46
2024-12-27 04:38:03,297: Snapshot:0	Epoch:44	Loss:1.237	translation_Loss:1.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.69	Best:23.58
2024-12-27 04:38:09,584: Snapshot:0	Epoch:45	Loss:1.166	translation_Loss:1.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.71	Best:23.63
2024-12-27 04:38:15,870: Snapshot:0	Epoch:46	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.76	Best:23.65
2024-12-27 04:38:22,142: Snapshot:0	Epoch:47	Loss:1.019	translation_Loss:1.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.78	Best:23.71
2024-12-27 04:38:28,468: Snapshot:0	Epoch:48	Loss:0.961	translation_Loss:0.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.73	Hits@10:39.89	Best:23.73
2024-12-27 04:38:34,821: Snapshot:0	Epoch:49	Loss:0.905	translation_Loss:0.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.82	Hits@10:39.88	Best:23.82
2024-12-27 04:38:41,114: Snapshot:0	Epoch:50	Loss:0.858	translation_Loss:0.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:39.87	Best:23.87
2024-12-27 04:38:47,408: Snapshot:0	Epoch:51	Loss:0.811	translation_Loss:0.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:39.92	Best:23.89
2024-12-27 04:38:53,716: Snapshot:0	Epoch:52	Loss:0.774	translation_Loss:0.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.91	Hits@10:39.8	Best:23.91
2024-12-27 04:38:59,987: Snapshot:0	Epoch:53	Loss:0.734	translation_Loss:0.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.93	Hits@10:39.9	Best:23.93
2024-12-27 04:39:06,845: Snapshot:0	Epoch:54	Loss:0.703	translation_Loss:0.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:39.98	Best:23.96
2024-12-27 04:39:13,156: Snapshot:0	Epoch:55	Loss:0.665	translation_Loss:0.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.01	Hits@10:40.05	Best:24.01
2024-12-27 04:39:19,382: Snapshot:0	Epoch:56	Loss:0.648	translation_Loss:0.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:40.02	Best:24.01
2024-12-27 04:39:25,629: Snapshot:0	Epoch:57	Loss:0.619	translation_Loss:0.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:40.05	Best:24.01
2024-12-27 04:39:32,028: Snapshot:0	Epoch:58	Loss:0.599	translation_Loss:0.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.03	Hits@10:39.93	Best:24.03
2024-12-27 04:39:38,395: Snapshot:0	Epoch:59	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.07	Hits@10:39.9	Best:24.07
2024-12-27 04:39:44,714: Snapshot:0	Epoch:60	Loss:0.545	translation_Loss:0.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:40.06	Best:24.13
2024-12-27 04:39:50,999: Snapshot:0	Epoch:61	Loss:0.526	translation_Loss:0.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:40.0	Best:24.13
2024-12-27 04:39:57,239: Snapshot:0	Epoch:62	Loss:0.512	translation_Loss:0.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:40.0	Best:24.13
2024-12-27 04:40:03,565: Early Stopping! Snapshot: 0 Epoch: 63 Best Results: 24.13
2024-12-27 04:40:03,565: Start to training tokens! Snapshot: 0 Epoch: 63 Loss:0.497 MRR:24.11 Best Results: 24.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:40:03,565: Snapshot:0	Epoch:63	Loss:0.497	translation_Loss:0.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.02	Best:24.13
2024-12-27 04:40:11,065: Snapshot:0	Epoch:64	Loss:141.005	translation_Loss:35.949	multi_layer_Loss:105.056	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.02	Best:24.13
2024-12-27 04:40:17,410: End of token training: 0 Epoch: 65 Loss:66.919 MRR:24.11 Best Results: 24.13
2024-12-27 04:40:17,410: Snapshot:0	Epoch:65	Loss:66.919	translation_Loss:35.958	multi_layer_Loss:30.961	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.11	Hits@10:40.02	Best:24.13
2024-12-27 04:40:17,677: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-27 04:40:20,355: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2341 | 0.1466 | 0.2784 | 0.3291 |  0.3912 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:40:44,777: Snapshot:1	Epoch:0	Loss:24.696	translation_Loss:24.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.339                                                   	MRR:18.02	Hits@10:31.11	Best:18.02
2024-12-27 04:40:51,815: Snapshot:1	Epoch:1	Loss:21.731	translation_Loss:20.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.993                                                   	MRR:18.69	Hits@10:32.16	Best:18.69
2024-12-27 04:40:58,827: Snapshot:1	Epoch:2	Loss:20.116	translation_Loss:18.426	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.69                                                   	MRR:19.12	Hits@10:32.87	Best:19.12
2024-12-27 04:41:05,848: Snapshot:1	Epoch:3	Loss:18.975	translation_Loss:16.727	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.248                                                   	MRR:19.42	Hits@10:33.41	Best:19.42
2024-12-27 04:41:12,861: Snapshot:1	Epoch:4	Loss:17.972	translation_Loss:15.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.669                                                   	MRR:19.73	Hits@10:33.78	Best:19.73
2024-12-27 04:41:19,939: Snapshot:1	Epoch:5	Loss:17.132	translation_Loss:14.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.979                                                   	MRR:19.98	Hits@10:34.2	Best:19.98
2024-12-27 04:41:26,942: Snapshot:1	Epoch:6	Loss:16.387	translation_Loss:13.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.208                                                   	MRR:20.21	Hits@10:34.51	Best:20.21
2024-12-27 04:41:33,964: Snapshot:1	Epoch:7	Loss:15.778	translation_Loss:12.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.375                                                   	MRR:20.41	Hits@10:34.76	Best:20.41
2024-12-27 04:41:41,040: Snapshot:1	Epoch:8	Loss:15.251	translation_Loss:11.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.495                                                   	MRR:20.56	Hits@10:34.99	Best:20.56
2024-12-27 04:41:47,950: Snapshot:1	Epoch:9	Loss:14.779	translation_Loss:11.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.583                                                   	MRR:20.67	Hits@10:35.2	Best:20.67
2024-12-27 04:41:54,997: Snapshot:1	Epoch:10	Loss:14.445	translation_Loss:10.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.647                                                   	MRR:20.79	Hits@10:35.37	Best:20.79
2024-12-27 04:42:01,935: Snapshot:1	Epoch:11	Loss:14.18	translation_Loss:10.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.689                                                   	MRR:20.85	Hits@10:35.56	Best:20.85
2024-12-27 04:42:08,894: Snapshot:1	Epoch:12	Loss:13.955	translation_Loss:10.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.715                                                   	MRR:20.93	Hits@10:35.7	Best:20.93
2024-12-27 04:42:15,953: Snapshot:1	Epoch:13	Loss:13.791	translation_Loss:10.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.731                                                   	MRR:21.04	Hits@10:35.79	Best:21.04
2024-12-27 04:42:22,985: Snapshot:1	Epoch:14	Loss:13.661	translation_Loss:9.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.742                                                   	MRR:21.13	Hits@10:35.78	Best:21.13
2024-12-27 04:42:30,008: Snapshot:1	Epoch:15	Loss:13.546	translation_Loss:9.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.75                                                   	MRR:21.15	Hits@10:35.99	Best:21.15
2024-12-27 04:42:37,000: Snapshot:1	Epoch:16	Loss:13.456	translation_Loss:9.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.751                                                   	MRR:21.17	Hits@10:36.0	Best:21.17
2024-12-27 04:42:44,040: Snapshot:1	Epoch:17	Loss:13.37	translation_Loss:9.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.751                                                   	MRR:21.19	Hits@10:36.1	Best:21.19
2024-12-27 04:42:51,063: Snapshot:1	Epoch:18	Loss:13.338	translation_Loss:9.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.752                                                   	MRR:21.21	Hits@10:35.97	Best:21.21
2024-12-27 04:42:58,041: Snapshot:1	Epoch:19	Loss:13.291	translation_Loss:9.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.751                                                   	MRR:21.26	Hits@10:35.97	Best:21.26
2024-12-27 04:43:05,115: Snapshot:1	Epoch:20	Loss:13.225	translation_Loss:9.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.743                                                   	MRR:21.31	Hits@10:35.95	Best:21.31
2024-12-27 04:43:12,107: Snapshot:1	Epoch:21	Loss:13.188	translation_Loss:9.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.742                                                   	MRR:21.29	Hits@10:35.96	Best:21.31
2024-12-27 04:43:18,977: Snapshot:1	Epoch:22	Loss:13.167	translation_Loss:9.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.74                                                   	MRR:21.24	Hits@10:36.01	Best:21.31
2024-12-27 04:43:25,948: Early Stopping! Snapshot: 1 Epoch: 23 Best Results: 21.31
2024-12-27 04:43:25,948: Start to training tokens! Snapshot: 1 Epoch: 23 Loss:13.132 MRR:21.27 Best Results: 21.31
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:43:25,949: Snapshot:1	Epoch:23	Loss:13.132	translation_Loss:9.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.741                                                   	MRR:21.27	Hits@10:36.13	Best:21.31
2024-12-27 04:43:32,777: Snapshot:1	Epoch:24	Loss:145.562	translation_Loss:42.322	multi_layer_Loss:103.24	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.27	Hits@10:36.13	Best:21.31
2024-12-27 04:43:39,591: End of token training: 1 Epoch: 25 Loss:72.334 MRR:21.27 Best Results: 21.31
2024-12-27 04:43:39,592: Snapshot:1	Epoch:25	Loss:72.334	translation_Loss:42.305	multi_layer_Loss:30.029	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.27	Hits@10:36.13	Best:21.31
2024-12-27 04:43:39,946: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-27 04:43:45,479: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2459 | 0.1558 | 0.2892 | 0.3431 |  0.4091 |
|     1      | 0.2117 | 0.1275 | 0.2548 | 0.3043 |  0.3628 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:44:09,716: Snapshot:2	Epoch:0	Loss:18.234	translation_Loss:17.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.339                                                   	MRR:19.27	Hits@10:34.09	Best:19.27
2024-12-27 04:44:16,895: Snapshot:2	Epoch:1	Loss:15.365	translation_Loss:14.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.963                                                   	MRR:19.73	Hits@10:35.04	Best:19.73
2024-12-27 04:44:24,055: Snapshot:2	Epoch:2	Loss:13.971	translation_Loss:12.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.65                                                   	MRR:20.08	Hits@10:35.41	Best:20.08
2024-12-27 04:44:31,338: Snapshot:2	Epoch:3	Loss:13.045	translation_Loss:10.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.222                                                   	MRR:20.33	Hits@10:35.84	Best:20.33
2024-12-27 04:44:38,590: Snapshot:2	Epoch:4	Loss:12.38	translation_Loss:9.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.662                                                   	MRR:20.54	Hits@10:36.2	Best:20.54
2024-12-27 04:44:45,815: Snapshot:2	Epoch:5	Loss:11.941	translation_Loss:8.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.992                                                   	MRR:20.72	Hits@10:36.28	Best:20.72
2024-12-27 04:44:53,040: Snapshot:2	Epoch:6	Loss:11.529	translation_Loss:8.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.24                                                   	MRR:20.85	Hits@10:36.58	Best:20.85
2024-12-27 04:45:00,200: Snapshot:2	Epoch:7	Loss:11.28	translation_Loss:7.86	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.42                                                   	MRR:20.92	Hits@10:36.69	Best:20.92
2024-12-27 04:45:07,962: Snapshot:2	Epoch:8	Loss:11.044	translation_Loss:7.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.551                                                   	MRR:21.03	Hits@10:36.81	Best:21.03
2024-12-27 04:45:15,189: Snapshot:2	Epoch:9	Loss:10.89	translation_Loss:7.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.65                                                   	MRR:21.1	Hits@10:36.94	Best:21.1
2024-12-27 04:45:22,420: Snapshot:2	Epoch:10	Loss:10.781	translation_Loss:7.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.723                                                   	MRR:21.14	Hits@10:37.03	Best:21.14
2024-12-27 04:45:29,527: Snapshot:2	Epoch:11	Loss:10.7	translation_Loss:6.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.774                                                   	MRR:21.14	Hits@10:37.14	Best:21.14
2024-12-27 04:45:36,638: Snapshot:2	Epoch:12	Loss:10.636	translation_Loss:6.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.819                                                   	MRR:21.21	Hits@10:37.04	Best:21.21
2024-12-27 04:45:43,791: Snapshot:2	Epoch:13	Loss:10.61	translation_Loss:6.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.851                                                   	MRR:21.21	Hits@10:37.04	Best:21.21
2024-12-27 04:45:50,902: Snapshot:2	Epoch:14	Loss:10.575	translation_Loss:6.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.881                                                   	MRR:21.22	Hits@10:37.15	Best:21.22
2024-12-27 04:45:57,993: Snapshot:2	Epoch:15	Loss:10.537	translation_Loss:6.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.903                                                   	MRR:21.22	Hits@10:37.25	Best:21.22
2024-12-27 04:46:05,053: Snapshot:2	Epoch:16	Loss:10.494	translation_Loss:6.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.92                                                   	MRR:21.2	Hits@10:37.23	Best:21.22
2024-12-27 04:46:12,240: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 21.22
2024-12-27 04:46:12,240: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:10.49 MRR:21.19 Best Results: 21.22
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:46:12,241: Snapshot:2	Epoch:17	Loss:10.49	translation_Loss:6.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.933                                                   	MRR:21.19	Hits@10:37.12	Best:21.22
2024-12-27 04:46:19,590: Snapshot:2	Epoch:18	Loss:147.786	translation_Loss:41.404	multi_layer_Loss:106.382	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.19	Hits@10:37.12	Best:21.22
2024-12-27 04:46:26,480: End of token training: 2 Epoch: 19 Loss:73.94 MRR:21.19 Best Results: 21.22
2024-12-27 04:46:26,480: Snapshot:2	Epoch:19	Loss:73.94	translation_Loss:41.403	multi_layer_Loss:32.536	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.19	Hits@10:37.12	Best:21.22
2024-12-27 04:46:26,746: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-27 04:46:35,765: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2457 | 0.1561 | 0.2877 | 0.3427 |  0.4137 |
|     1      | 0.2203 | 0.1346 | 0.2601 | 0.313  |  0.3817 |
|     2      | 0.2114 | 0.1254 | 0.2505 | 0.3041 |  0.3732 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:46:59,753: Snapshot:3	Epoch:0	Loss:11.354	translation_Loss:11.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.332                                                   	MRR:18.66	Hits@10:35.67	Best:18.66
2024-12-27 04:47:07,513: Snapshot:3	Epoch:1	Loss:8.819	translation_Loss:7.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.843                                                   	MRR:19.2	Hits@10:36.71	Best:19.2
2024-12-27 04:47:14,880: Snapshot:3	Epoch:2	Loss:7.709	translation_Loss:6.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.331                                                   	MRR:19.5	Hits@10:37.42	Best:19.5
2024-12-27 04:47:22,177: Snapshot:3	Epoch:3	Loss:7.126	translation_Loss:5.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.71                                                   	MRR:19.71	Hits@10:37.72	Best:19.71
2024-12-27 04:47:29,449: Snapshot:3	Epoch:4	Loss:6.715	translation_Loss:4.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.98                                                   	MRR:19.84	Hits@10:37.84	Best:19.84
2024-12-27 04:47:36,691: Snapshot:3	Epoch:5	Loss:6.466	translation_Loss:4.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.174                                                   	MRR:19.96	Hits@10:38.19	Best:19.96
2024-12-27 04:47:43,868: Snapshot:3	Epoch:6	Loss:6.319	translation_Loss:4.009	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.31                                                   	MRR:19.93	Hits@10:38.11	Best:19.96
2024-12-27 04:47:51,140: Snapshot:3	Epoch:7	Loss:6.206	translation_Loss:3.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.41                                                   	MRR:19.99	Hits@10:38.13	Best:19.99
2024-12-27 04:47:58,387: Snapshot:3	Epoch:8	Loss:6.159	translation_Loss:3.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.483                                                   	MRR:20.1	Hits@10:38.22	Best:20.1
2024-12-27 04:48:05,669: Snapshot:3	Epoch:9	Loss:6.101	translation_Loss:3.568	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.533                                                   	MRR:20.13	Hits@10:38.18	Best:20.13
2024-12-27 04:48:12,916: Snapshot:3	Epoch:10	Loss:6.085	translation_Loss:3.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.572                                                   	MRR:20.15	Hits@10:38.38	Best:20.15
2024-12-27 04:48:20,681: Snapshot:3	Epoch:11	Loss:6.076	translation_Loss:3.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.604                                                   	MRR:20.13	Hits@10:38.34	Best:20.15
2024-12-27 04:48:27,934: Snapshot:3	Epoch:12	Loss:6.022	translation_Loss:3.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.631                                                   	MRR:20.14	Hits@10:38.41	Best:20.15
2024-12-27 04:48:35,204: Snapshot:3	Epoch:13	Loss:6.023	translation_Loss:3.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.648                                                   	MRR:20.16	Hits@10:38.24	Best:20.16
2024-12-27 04:48:42,469: Snapshot:3	Epoch:14	Loss:6.023	translation_Loss:3.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.67                                                   	MRR:20.21	Hits@10:38.41	Best:20.21
2024-12-27 04:48:49,680: Snapshot:3	Epoch:15	Loss:6.036	translation_Loss:3.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.687                                                   	MRR:20.13	Hits@10:38.36	Best:20.21
2024-12-27 04:48:56,821: Snapshot:3	Epoch:16	Loss:5.995	translation_Loss:3.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.7                                                   	MRR:20.19	Hits@10:38.39	Best:20.21
2024-12-27 04:49:04,039: Snapshot:3	Epoch:17	Loss:5.992	translation_Loss:3.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.705                                                   	MRR:20.26	Hits@10:38.43	Best:20.26
2024-12-27 04:49:11,342: Snapshot:3	Epoch:18	Loss:5.994	translation_Loss:3.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.709                                                   	MRR:20.16	Hits@10:38.23	Best:20.26
2024-12-27 04:49:18,531: Snapshot:3	Epoch:19	Loss:6.006	translation_Loss:3.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.716                                                   	MRR:20.17	Hits@10:38.25	Best:20.26
2024-12-27 04:49:25,803: Early Stopping! Snapshot: 3 Epoch: 20 Best Results: 20.26
2024-12-27 04:49:25,803: Start to training tokens! Snapshot: 3 Epoch: 20 Loss:6.014 MRR:20.22 Best Results: 20.26
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:49:25,803: Snapshot:3	Epoch:20	Loss:6.014	translation_Loss:3.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.725                                                   	MRR:20.22	Hits@10:38.3	Best:20.26
2024-12-27 04:49:32,812: Snapshot:3	Epoch:21	Loss:144.972	translation_Loss:39.253	multi_layer_Loss:105.719	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.22	Hits@10:38.3	Best:20.26
2024-12-27 04:49:39,790: End of token training: 3 Epoch: 22 Loss:70.43 MRR:20.22 Best Results: 20.26
2024-12-27 04:49:39,790: Snapshot:3	Epoch:22	Loss:70.43	translation_Loss:39.272	multi_layer_Loss:31.159	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.22	Hits@10:38.3	Best:20.26
2024-12-27 04:49:40,136: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-27 04:49:52,504: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.235  | 0.1478 | 0.2717 | 0.3259 |  0.399  |
|     1      | 0.213  | 0.1273 | 0.2506 | 0.3036 |  0.3762 |
|     2      |  0.21  | 0.1218 | 0.2465 | 0.3052 |  0.3813 |
|     3      | 0.2029 | 0.1093 | 0.2389 | 0.302  |  0.3869 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 04:50:17,420: Snapshot:4	Epoch:0	Loss:6.104	translation_Loss:5.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:20.1	Hits@10:43.92	Best:20.1
2024-12-27 04:50:24,816: Snapshot:4	Epoch:1	Loss:4.177	translation_Loss:3.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.48                                                   	MRR:21.08	Hits@10:46.36	Best:21.08
2024-12-27 04:50:32,568: Snapshot:4	Epoch:2	Loss:3.408	translation_Loss:2.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.621                                                   	MRR:21.35	Hits@10:46.91	Best:21.35
2024-12-27 04:50:39,928: Snapshot:4	Epoch:3	Loss:2.864	translation_Loss:2.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.712                                                   	MRR:21.51	Hits@10:46.98	Best:21.51
2024-12-27 04:50:47,265: Snapshot:4	Epoch:4	Loss:2.443	translation_Loss:1.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.768                                                   	MRR:21.85	Hits@10:47.31	Best:21.85
2024-12-27 04:50:54,635: Snapshot:4	Epoch:5	Loss:2.151	translation_Loss:1.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.8                                                   	MRR:22.05	Hits@10:47.27	Best:22.05
2024-12-27 04:51:02,064: Snapshot:4	Epoch:6	Loss:1.987	translation_Loss:1.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.818                                                   	MRR:22.13	Hits@10:47.16	Best:22.13
2024-12-27 04:51:09,452: Snapshot:4	Epoch:7	Loss:1.878	translation_Loss:1.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.83                                                   	MRR:22.41	Hits@10:47.57	Best:22.41
2024-12-27 04:51:16,800: Snapshot:4	Epoch:8	Loss:1.805	translation_Loss:0.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.832                                                   	MRR:22.4	Hits@10:47.74	Best:22.41
2024-12-27 04:51:24,216: Snapshot:4	Epoch:9	Loss:1.776	translation_Loss:0.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.838                                                   	MRR:22.42	Hits@10:47.53	Best:22.42
2024-12-27 04:51:31,487: Snapshot:4	Epoch:10	Loss:1.747	translation_Loss:0.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.844                                                   	MRR:22.36	Hits@10:47.59	Best:22.42
2024-12-27 04:51:38,789: Snapshot:4	Epoch:11	Loss:1.718	translation_Loss:0.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:22.46	Hits@10:48.02	Best:22.46
2024-12-27 04:51:46,079: Snapshot:4	Epoch:12	Loss:1.692	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.847                                                   	MRR:22.68	Hits@10:47.71	Best:22.68
2024-12-27 04:51:53,405: Snapshot:4	Epoch:13	Loss:1.708	translation_Loss:0.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:22.75	Hits@10:47.95	Best:22.75
2024-12-27 04:52:00,652: Snapshot:4	Epoch:14	Loss:1.688	translation_Loss:0.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.85                                                   	MRR:22.69	Hits@10:47.87	Best:22.75
2024-12-27 04:52:07,861: Snapshot:4	Epoch:15	Loss:1.673	translation_Loss:0.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.844                                                   	MRR:22.48	Hits@10:47.86	Best:22.75
2024-12-27 04:52:15,121: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 22.75
2024-12-27 04:52:15,121: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:1.673 MRR:22.62 Best Results: 22.75
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 04:52:15,122: Snapshot:4	Epoch:16	Loss:1.673	translation_Loss:0.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.845                                                   	MRR:22.62	Hits@10:47.94	Best:22.75
2024-12-27 04:52:22,225: Snapshot:4	Epoch:17	Loss:137.162	translation_Loss:33.102	multi_layer_Loss:104.06	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.62	Hits@10:47.94	Best:22.75
2024-12-27 04:52:29,311: End of token training: 4 Epoch: 18 Loss:63.457 MRR:22.62 Best Results: 22.75
2024-12-27 04:52:29,311: Snapshot:4	Epoch:18	Loss:63.457	translation_Loss:33.075	multi_layer_Loss:30.382	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.62	Hits@10:47.94	Best:22.75
2024-12-27 04:52:29,662: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-27 04:52:46,147: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.223  | 0.138  | 0.2577 | 0.311  |  0.3831 |
|     1      | 0.2019 |  0.12  | 0.2341 | 0.287  |  0.3591 |
|     2      | 0.1959 | 0.1099 | 0.2276 | 0.2854 |  0.3655 |
|     3      | 0.1899 | 0.0968 | 0.2209 | 0.2864 |  0.3815 |
|     4      | 0.2263 | 0.1052 | 0.2672 | 0.3608 |  0.4776 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 04:52:46,149: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2341 | 0.1466 | 0.2784 | 0.3291 |  0.3912 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2459 | 0.1558 | 0.2892 | 0.3431 |  0.4091 |
|     1      | 0.2117 | 0.1275 | 0.2548 | 0.3043 |  0.3628 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2457 | 0.1561 | 0.2877 | 0.3427 |  0.4137 |
|     1      | 0.2203 | 0.1346 | 0.2601 | 0.313  |  0.3817 |
|     2      | 0.2114 | 0.1254 | 0.2505 | 0.3041 |  0.3732 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.235  | 0.1478 | 0.2717 | 0.3259 |  0.399  |
|     1      | 0.213  | 0.1273 | 0.2506 | 0.3036 |  0.3762 |
|     2      |  0.21  | 0.1218 | 0.2465 | 0.3052 |  0.3813 |
|     3      | 0.2029 | 0.1093 | 0.2389 | 0.302  |  0.3869 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.223  | 0.138  | 0.2577 | 0.311  |  0.3831 |
|     1      | 0.2019 |  0.12  | 0.2341 | 0.287  |  0.3591 |
|     2      | 0.1959 | 0.1099 | 0.2276 | 0.2854 |  0.3655 |
|     3      | 0.1899 | 0.0968 | 0.2209 | 0.2864 |  0.3815 |
|     4      | 0.2263 | 0.1052 | 0.2672 | 0.3608 |  0.4776 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 04:52:46,150: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 422.2971405982971  |   0.234   |    0.147     |    0.278     |     0.391     |
|    1     | 196.4735462665558  |   0.229   |    0.142     |    0.272     |     0.386     |
|    2     | 157.7551462650299  |   0.226   |    0.139     |    0.266     |      0.39     |
|    3     | 180.69280362129211 |   0.215   |    0.127     |    0.252     |     0.386     |
|    4     | 153.1402587890625  |   0.207   |    0.114     |    0.241     |     0.393     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 04:52:46,150: Sum_Training_Time:1110.3588955402374
2024-12-27 04:52:46,150: Every_Training_Time:[422.2971405982971, 196.4735462665558, 157.7551462650299, 180.69280362129211, 153.1402587890625]
2024-12-27 04:52:46,150: Forward transfer: 0.16949999999999998 Backward transfer: -0.01235
