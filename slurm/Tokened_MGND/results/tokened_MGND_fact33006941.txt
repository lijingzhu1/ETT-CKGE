2024-12-27 17:46:09,354: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227174528/FACTfact_0.01_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 17:46:19,634: Snapshot:0	Epoch:0	Loss:86.416	translation_Loss:86.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.42	Hits@10:28.53	Best:14.42
2024-12-27 17:46:26,221: Snapshot:0	Epoch:1	Loss:47.561	translation_Loss:47.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.44	Hits@10:35.96	Best:21.44
2024-12-27 17:46:32,842: Snapshot:0	Epoch:2	Loss:23.655	translation_Loss:23.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.91	Hits@10:38.29	Best:23.91
2024-12-27 17:46:39,418: Snapshot:0	Epoch:3	Loss:11.722	translation_Loss:11.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:39.18	Best:24.66
2024-12-27 17:46:46,661: Snapshot:0	Epoch:4	Loss:6.42	translation_Loss:6.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:39.43	Best:24.77
2024-12-27 17:46:53,800: Snapshot:0	Epoch:5	Loss:4.204	translation_Loss:4.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.47	Hits@10:39.25	Best:24.77
2024-12-27 17:47:00,923: Snapshot:0	Epoch:6	Loss:3.253	translation_Loss:3.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:39.39	Best:24.77
2024-12-27 17:47:08,020: Snapshot:0	Epoch:7	Loss:2.662	translation_Loss:2.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.45	Hits@10:39.17	Best:24.77
2024-12-27 17:47:15,619: Snapshot:0	Epoch:8	Loss:2.329	translation_Loss:2.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:39.21	Best:24.77
2024-12-27 17:47:22,749: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 24.77
2024-12-27 17:47:22,750: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:2.114 MRR:24.37 Best Results: 24.77
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:47:22,750: Snapshot:0	Epoch:9	Loss:2.114	translation_Loss:2.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:39.32	Best:24.77
2024-12-27 17:47:30,581: Snapshot:0	Epoch:10	Loss:73.24	translation_Loss:73.203	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:39.32	Best:24.77
2024-12-27 17:47:37,938: End of token training: 0 Epoch: 11 Loss:73.167 MRR:24.37 Best Results: 24.77
2024-12-27 17:47:37,939: Snapshot:0	Epoch:11	Loss:73.167	translation_Loss:73.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.37	Hits@10:39.32	Best:24.77
2024-12-27 17:47:38,250: => loading checkpoint './checkpoint/FACTfact_0.01_512_1000/0model_best.tar'
2024-12-27 17:47:41,110: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2401 | 0.1584 | 0.2796 | 0.3273 |  0.3883 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:48:06,732: Snapshot:1	Epoch:0	Loss:54.011	translation_Loss:39.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.142                                                   	MRR:19.6	Hits@10:32.41	Best:19.6
2024-12-27 17:48:14,219: Snapshot:1	Epoch:1	Loss:46.344	translation_Loss:32.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.312                                                   	MRR:20.08	Hits@10:32.93	Best:20.08
2024-12-27 17:48:21,777: Snapshot:1	Epoch:2	Loss:43.877	translation_Loss:29.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.121                                                   	MRR:20.22	Hits@10:33.03	Best:20.22
2024-12-27 17:48:29,312: Snapshot:1	Epoch:3	Loss:43.131	translation_Loss:29.103	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.028                                                   	MRR:20.22	Hits@10:33.09	Best:20.22
2024-12-27 17:48:36,835: Snapshot:1	Epoch:4	Loss:42.823	translation_Loss:28.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.067                                                   	MRR:20.29	Hits@10:33.25	Best:20.29
2024-12-27 17:48:44,335: Snapshot:1	Epoch:5	Loss:42.585	translation_Loss:28.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.0                                                   	MRR:20.25	Hits@10:33.07	Best:20.29
2024-12-27 17:48:51,852: Snapshot:1	Epoch:6	Loss:42.417	translation_Loss:28.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.009                                                   	MRR:20.2	Hits@10:33.26	Best:20.29
2024-12-27 17:48:59,301: Snapshot:1	Epoch:7	Loss:42.413	translation_Loss:28.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.056                                                   	MRR:20.22	Hits@10:33.23	Best:20.29
2024-12-27 17:49:06,756: Snapshot:1	Epoch:8	Loss:42.378	translation_Loss:28.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.068                                                   	MRR:20.25	Hits@10:33.07	Best:20.29
2024-12-27 17:49:14,194: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 20.29
2024-12-27 17:49:14,194: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:42.176 MRR:20.21 Best Results: 20.29
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:49:14,195: Snapshot:1	Epoch:9	Loss:42.176	translation_Loss:28.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.0                                                   	MRR:20.21	Hits@10:33.17	Best:20.29
2024-12-27 17:49:21,456: Snapshot:1	Epoch:10	Loss:90.036	translation_Loss:90.0	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.21	Hits@10:33.17	Best:20.29
2024-12-27 17:49:28,692: End of token training: 1 Epoch: 11 Loss:90.064 MRR:20.21 Best Results: 20.29
2024-12-27 17:49:28,693: Snapshot:1	Epoch:11	Loss:90.064	translation_Loss:90.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.21	Hits@10:33.17	Best:20.29
2024-12-27 17:49:29,055: => loading checkpoint './checkpoint/FACTfact_0.01_512_1000/1model_best.tar'
2024-12-27 17:49:36,177: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1619 | 0.281  | 0.3295 |  0.3891 |
|     1      | 0.2015 | 0.1302 | 0.235  | 0.2761 |  0.3299 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:50:02,086: Snapshot:2	Epoch:0	Loss:50.528	translation_Loss:35.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.307                                                   	MRR:18.33	Hits@10:31.05	Best:18.33
2024-12-27 17:50:10,049: Snapshot:2	Epoch:1	Loss:47.868	translation_Loss:32.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.691                                                   	MRR:18.39	Hits@10:31.24	Best:18.39
2024-12-27 17:50:17,763: Snapshot:2	Epoch:2	Loss:47.299	translation_Loss:31.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.72                                                   	MRR:18.44	Hits@10:31.23	Best:18.44
2024-12-27 17:50:25,427: Snapshot:2	Epoch:3	Loss:47.128	translation_Loss:31.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.716                                                   	MRR:18.41	Hits@10:31.31	Best:18.44
2024-12-27 17:50:33,096: Snapshot:2	Epoch:4	Loss:47.015	translation_Loss:31.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.732                                                   	MRR:18.41	Hits@10:31.18	Best:18.44
2024-12-27 17:50:40,872: Snapshot:2	Epoch:5	Loss:47.117	translation_Loss:31.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.792                                                   	MRR:18.49	Hits@10:31.24	Best:18.49
2024-12-27 17:50:48,651: Snapshot:2	Epoch:6	Loss:47.0	translation_Loss:31.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.804                                                   	MRR:18.52	Hits@10:31.21	Best:18.52
2024-12-27 17:50:56,431: Snapshot:2	Epoch:7	Loss:46.974	translation_Loss:31.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.802                                                   	MRR:18.44	Hits@10:31.23	Best:18.52
2024-12-27 17:51:04,098: Snapshot:2	Epoch:8	Loss:46.91	translation_Loss:31.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.79                                                   	MRR:18.47	Hits@10:31.32	Best:18.52
2024-12-27 17:51:11,754: Snapshot:2	Epoch:9	Loss:47.018	translation_Loss:31.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.84                                                   	MRR:18.47	Hits@10:31.24	Best:18.52
2024-12-27 17:51:19,436: Snapshot:2	Epoch:10	Loss:46.948	translation_Loss:31.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.785                                                   	MRR:18.46	Hits@10:31.19	Best:18.52
2024-12-27 17:51:27,170: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 18.52
2024-12-27 17:51:27,170: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:46.917 MRR:18.51 Best Results: 18.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:51:27,171: Snapshot:2	Epoch:11	Loss:46.917	translation_Loss:31.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.806                                                   	MRR:18.51	Hits@10:31.22	Best:18.52
2024-12-27 17:51:35,121: Snapshot:2	Epoch:12	Loss:92.457	translation_Loss:92.42	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.51	Hits@10:31.22	Best:18.52
2024-12-27 17:51:42,605: End of token training: 2 Epoch: 13 Loss:92.355 MRR:18.51 Best Results: 18.52
2024-12-27 17:51:42,606: Snapshot:2	Epoch:13	Loss:92.355	translation_Loss:92.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.51	Hits@10:31.22	Best:18.52
2024-12-27 17:51:42,880: => loading checkpoint './checkpoint/FACTfact_0.01_512_1000/2model_best.tar'
2024-12-27 17:51:52,131: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1599 | 0.2801 | 0.328  |  0.3886 |
|     1      | 0.2018 | 0.1285 | 0.2378 | 0.2771 |  0.331  |
|     2      | 0.1823 | 0.1119 | 0.2116 | 0.2553 |  0.3118 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:52:18,075: Snapshot:3	Epoch:0	Loss:46.711	translation_Loss:31.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.902                                                   	MRR:16.01	Hits@10:29.09	Best:16.01
2024-12-27 17:52:25,886: Snapshot:3	Epoch:1	Loss:45.84	translation_Loss:30.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.158                                                   	MRR:16.08	Hits@10:28.96	Best:16.08
2024-12-27 17:52:33,633: Snapshot:3	Epoch:2	Loss:45.683	translation_Loss:30.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.231                                                   	MRR:16.03	Hits@10:28.88	Best:16.08
2024-12-27 17:52:41,448: Snapshot:3	Epoch:3	Loss:45.689	translation_Loss:30.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.282                                                   	MRR:16.07	Hits@10:29.02	Best:16.08
2024-12-27 17:52:49,257: Snapshot:3	Epoch:4	Loss:45.751	translation_Loss:30.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.318                                                   	MRR:16.03	Hits@10:29.03	Best:16.08
2024-12-27 17:52:57,071: Snapshot:3	Epoch:5	Loss:45.672	translation_Loss:30.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.31                                                   	MRR:16.02	Hits@10:28.96	Best:16.08
2024-12-27 17:53:04,760: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 16.08
2024-12-27 17:53:04,760: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:45.717 MRR:15.98 Best Results: 16.08
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:53:04,761: Snapshot:3	Epoch:6	Loss:45.717	translation_Loss:30.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.326                                                   	MRR:15.98	Hits@10:28.98	Best:16.08
2024-12-27 17:53:12,766: Snapshot:3	Epoch:7	Loss:91.813	translation_Loss:91.776	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.98	Hits@10:28.98	Best:16.08
2024-12-27 17:53:20,336: End of token training: 3 Epoch: 8 Loss:91.718 MRR:15.98 Best Results: 16.08
2024-12-27 17:53:20,337: Snapshot:3	Epoch:8	Loss:91.718	translation_Loss:91.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.98	Hits@10:28.98	Best:16.08
2024-12-27 17:53:20,615: => loading checkpoint './checkpoint/FACTfact_0.01_512_1000/3model_best.tar'
2024-12-27 17:53:33,654: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1593 | 0.2789 | 0.3281 |  0.3887 |
|     1      | 0.2019 | 0.1295 | 0.237  | 0.2773 |  0.3329 |
|     2      | 0.184  | 0.1129 | 0.213  | 0.2588 |  0.3158 |
|     3      | 0.1617 | 0.0922 | 0.1871 | 0.2333 |  0.2929 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:53:59,895: Snapshot:4	Epoch:0	Loss:37.67	translation_Loss:24.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.258                                                   	MRR:14.99	Hits@10:30.1	Best:14.99
2024-12-27 17:54:07,726: Snapshot:4	Epoch:1	Loss:35.52	translation_Loss:22.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.153                                                   	MRR:15.07	Hits@10:30.11	Best:15.07
2024-12-27 17:54:15,634: Snapshot:4	Epoch:2	Loss:35.35	translation_Loss:22.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.198                                                   	MRR:15.11	Hits@10:30.14	Best:15.11
2024-12-27 17:54:23,465: Snapshot:4	Epoch:3	Loss:35.267	translation_Loss:22.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.208                                                   	MRR:15.0	Hits@10:30.12	Best:15.11
2024-12-27 17:54:31,347: Snapshot:4	Epoch:4	Loss:35.225	translation_Loss:21.997	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.228                                                   	MRR:15.04	Hits@10:29.92	Best:15.11
2024-12-27 17:54:39,183: Snapshot:4	Epoch:5	Loss:35.225	translation_Loss:21.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.239                                                   	MRR:14.95	Hits@10:30.0	Best:15.11
2024-12-27 17:54:46,974: Snapshot:4	Epoch:6	Loss:35.193	translation_Loss:21.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.261                                                   	MRR:15.0	Hits@10:30.11	Best:15.11
2024-12-27 17:54:54,846: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 15.11
2024-12-27 17:54:54,846: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:35.247 MRR:15.06 Best Results: 15.11
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:54:54,846: Snapshot:4	Epoch:7	Loss:35.247	translation_Loss:21.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.277                                                   	MRR:15.06	Hits@10:30.25	Best:15.11
2024-12-27 17:55:02,910: Snapshot:4	Epoch:8	Loss:81.849	translation_Loss:81.812	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.06	Hits@10:30.25	Best:15.11
2024-12-27 17:55:10,485: End of token training: 4 Epoch: 9 Loss:81.86 MRR:15.06 Best Results: 15.11
2024-12-27 17:55:10,485: Snapshot:4	Epoch:9	Loss:81.86	translation_Loss:81.86	multi_layer_Loss:-0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.06	Hits@10:30.25	Best:15.11
2024-12-27 17:55:10,759: => loading checkpoint './checkpoint/FACTfact_0.01_512_1000/4model_best.tar'
2024-12-27 17:55:27,167: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2364 | 0.1546 | 0.2728 | 0.3254 |  0.3868 |
|     1      | 0.1982 | 0.1255 | 0.2314 | 0.2739 |  0.3307 |
|     2      | 0.1817 | 0.1097 | 0.2104 | 0.2577 |  0.317  |
|     3      | 0.1624 | 0.0911 | 0.1871 | 0.2347 |  0.2986 |
|     4      | 0.1509 | 0.0758 | 0.1697 |  0.22  |  0.302  |
+------------+--------+--------+--------+--------+---------+
2024-12-27 17:55:27,187: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2401 | 0.1584 | 0.2796 | 0.3273 |  0.3883 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1619 | 0.281  | 0.3295 |  0.3891 |
|     1      | 0.2015 | 0.1302 | 0.235  | 0.2761 |  0.3299 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1599 | 0.2801 | 0.328  |  0.3886 |
|     1      | 0.2018 | 0.1285 | 0.2378 | 0.2771 |  0.331  |
|     2      | 0.1823 | 0.1119 | 0.2116 | 0.2553 |  0.3118 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1593 | 0.2789 | 0.3281 |  0.3887 |
|     1      | 0.2019 | 0.1295 | 0.237  | 0.2773 |  0.3329 |
|     2      | 0.184  | 0.1129 | 0.213  | 0.2588 |  0.3158 |
|     3      | 0.1617 | 0.0922 | 0.1871 | 0.2333 |  0.2929 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2364 | 0.1546 | 0.2728 | 0.3254 |  0.3868 |
|     1      | 0.1982 | 0.1255 | 0.2314 | 0.2739 |  0.3307 |
|     2      | 0.1817 | 0.1097 | 0.2104 | 0.2577 |  0.317  |
|     3      | 0.1624 | 0.0911 | 0.1871 | 0.2347 |  0.2986 |
|     4      | 0.1509 | 0.0758 | 0.1697 |  0.22  |  0.302  |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 17:55:27,187: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  88.5839102268219  |    0.24   |    0.158     |     0.28     |     0.388     |
|    1     | 104.41269183158875 |   0.222   |    0.146     |    0.258     |     0.359     |
|    2     | 123.06859493255615 |   0.208   |    0.133     |    0.243     |     0.344     |
|    3     | 84.79760813713074  |   0.197   |    0.123     |    0.229     |     0.333     |
|    4     | 93.34915804862976  |   0.186   |    0.111     |    0.214     |     0.327     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 17:55:27,187: Sum_Training_Time:494.2119631767273
2024-12-27 17:55:27,187: Every_Training_Time:[88.5839102268219, 104.41269183158875, 123.06859493255615, 84.79760813713074, 93.34915804862976]
2024-12-27 17:55:27,187: Forward transfer: 0.1505 Backward transfer: -0.0017250000000000112
2024-12-27 17:56:06,110: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227175531/FACTfact_0.01_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 17:56:16,242: Snapshot:0	Epoch:0	Loss:86.417	translation_Loss:86.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.4	Hits@10:28.42	Best:14.4
2024-12-27 17:56:22,838: Snapshot:0	Epoch:1	Loss:47.554	translation_Loss:47.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.41	Hits@10:36.06	Best:21.41
2024-12-27 17:56:29,431: Snapshot:0	Epoch:2	Loss:23.613	translation_Loss:23.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:38.32	Best:24.0
2024-12-27 17:56:36,011: Snapshot:0	Epoch:3	Loss:11.737	translation_Loss:11.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:39.13	Best:24.57
2024-12-27 17:56:42,660: Snapshot:0	Epoch:4	Loss:6.407	translation_Loss:6.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:39.52	Best:24.8
2024-12-27 17:56:49,288: Snapshot:0	Epoch:5	Loss:4.206	translation_Loss:4.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:39.48	Best:24.8
2024-12-27 17:56:55,865: Snapshot:0	Epoch:6	Loss:3.232	translation_Loss:3.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:39.44	Best:24.8
2024-12-27 17:57:02,502: Snapshot:0	Epoch:7	Loss:2.676	translation_Loss:2.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:39.27	Best:24.8
2024-12-27 17:57:09,579: Snapshot:0	Epoch:8	Loss:2.338	translation_Loss:2.338	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:39.33	Best:24.8
2024-12-27 17:57:16,144: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 24.8
2024-12-27 17:57:16,144: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:2.12 MRR:24.49 Best Results: 24.8
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:57:16,145: Snapshot:0	Epoch:9	Loss:2.12	translation_Loss:2.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:39.39	Best:24.8
2024-12-27 17:57:23,416: Snapshot:0	Epoch:10	Loss:73.325	translation_Loss:73.288	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:39.39	Best:24.8
2024-12-27 17:57:30,147: End of token training: 0 Epoch: 11 Loss:73.244 MRR:24.49 Best Results: 24.8
2024-12-27 17:57:30,147: Snapshot:0	Epoch:11	Loss:73.244	translation_Loss:73.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.49	Hits@10:39.39	Best:24.8
2024-12-27 17:57:30,418: => loading checkpoint './checkpoint/FACTfact_0.01_512_5000/0model_best.tar'
2024-12-27 17:57:33,257: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1608 | 0.2772 | 0.3253 |  0.3867 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:57:58,873: Snapshot:1	Epoch:0	Loss:55.121	translation_Loss:40.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:14.795                                                   	MRR:19.35	Hits@10:31.93	Best:19.35
2024-12-27 17:58:06,398: Snapshot:1	Epoch:1	Loss:44.194	translation_Loss:32.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.691                                                   	MRR:19.85	Hits@10:32.63	Best:19.85
2024-12-27 17:58:13,919: Snapshot:1	Epoch:2	Loss:41.809	translation_Loss:30.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.561                                                   	MRR:19.97	Hits@10:32.63	Best:19.97
2024-12-27 17:58:21,471: Snapshot:1	Epoch:3	Loss:41.08	translation_Loss:29.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.518                                                   	MRR:20.01	Hits@10:32.82	Best:20.01
2024-12-27 17:58:28,957: Snapshot:1	Epoch:4	Loss:40.744	translation_Loss:29.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.532                                                   	MRR:20.03	Hits@10:32.79	Best:20.03
2024-12-27 17:58:36,410: Snapshot:1	Epoch:5	Loss:40.579	translation_Loss:29.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.526                                                   	MRR:19.94	Hits@10:32.84	Best:20.03
2024-12-27 17:58:43,858: Snapshot:1	Epoch:6	Loss:40.404	translation_Loss:28.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.511                                                   	MRR:19.99	Hits@10:32.87	Best:20.03
2024-12-27 17:58:51,412: Snapshot:1	Epoch:7	Loss:40.369	translation_Loss:28.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.536                                                   	MRR:19.99	Hits@10:32.66	Best:20.03
2024-12-27 17:58:58,876: Snapshot:1	Epoch:8	Loss:40.315	translation_Loss:28.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.55                                                   	MRR:20.02	Hits@10:32.81	Best:20.03
2024-12-27 17:59:06,351: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 20.03
2024-12-27 17:59:06,351: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:40.16 MRR:20.01 Best Results: 20.03
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 17:59:06,352: Snapshot:1	Epoch:9	Loss:40.16	translation_Loss:28.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.518                                                   	MRR:20.01	Hits@10:32.81	Best:20.03
2024-12-27 17:59:13,620: Snapshot:1	Epoch:10	Loss:90.349	translation_Loss:90.313	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.01	Hits@10:32.81	Best:20.03
2024-12-27 17:59:20,955: End of token training: 1 Epoch: 11 Loss:90.363 MRR:20.01 Best Results: 20.03
2024-12-27 17:59:20,956: Snapshot:1	Epoch:11	Loss:90.363	translation_Loss:90.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.01	Hits@10:32.81	Best:20.03
2024-12-27 17:59:21,300: => loading checkpoint './checkpoint/FACTfact_0.01_512_5000/1model_best.tar'
2024-12-27 17:59:27,973: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1615 | 0.2782 | 0.3253 |  0.3854 |
|     1      | 0.2002 | 0.129  | 0.2345 | 0.2748 |  0.327  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 17:59:54,025: Snapshot:2	Epoch:0	Loss:52.231	translation_Loss:36.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.74                                                   	MRR:17.79	Hits@10:30.36	Best:17.79
2024-12-27 18:00:01,740: Snapshot:2	Epoch:1	Loss:46.707	translation_Loss:33.485	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.222                                                   	MRR:17.89	Hits@10:30.53	Best:17.89
2024-12-27 18:00:09,558: Snapshot:2	Epoch:2	Loss:46.158	translation_Loss:32.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.256                                                   	MRR:17.89	Hits@10:30.56	Best:17.89
2024-12-27 18:00:17,232: Snapshot:2	Epoch:3	Loss:45.982	translation_Loss:32.727	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.255                                                   	MRR:17.88	Hits@10:30.56	Best:17.89
2024-12-27 18:00:25,024: Snapshot:2	Epoch:4	Loss:45.856	translation_Loss:32.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.268                                                   	MRR:17.9	Hits@10:30.57	Best:17.9
2024-12-27 18:00:32,769: Snapshot:2	Epoch:5	Loss:45.964	translation_Loss:32.643	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.321                                                   	MRR:17.98	Hits@10:30.66	Best:17.98
2024-12-27 18:00:40,464: Snapshot:2	Epoch:6	Loss:45.829	translation_Loss:32.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.329                                                   	MRR:17.98	Hits@10:30.61	Best:17.98
2024-12-27 18:00:48,211: Snapshot:2	Epoch:7	Loss:45.826	translation_Loss:32.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.327                                                   	MRR:17.97	Hits@10:30.59	Best:17.98
2024-12-27 18:00:55,971: Snapshot:2	Epoch:8	Loss:45.764	translation_Loss:32.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.324                                                   	MRR:17.93	Hits@10:30.51	Best:17.98
2024-12-27 18:01:03,755: Snapshot:2	Epoch:9	Loss:45.846	translation_Loss:32.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.352                                                   	MRR:17.93	Hits@10:30.52	Best:17.98
2024-12-27 18:01:11,505: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 17.98
2024-12-27 18:01:11,506: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:45.83 MRR:17.9 Best Results: 17.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:01:11,506: Snapshot:2	Epoch:10	Loss:45.83	translation_Loss:32.49	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.34                                                   	MRR:17.9	Hits@10:30.52	Best:17.98
2024-12-27 18:01:18,937: Snapshot:2	Epoch:11	Loss:92.997	translation_Loss:92.96	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.9	Hits@10:30.52	Best:17.98
2024-12-27 18:01:26,929: End of token training: 2 Epoch: 12 Loss:93.013 MRR:17.9 Best Results: 17.98
2024-12-27 18:01:26,929: Snapshot:2	Epoch:12	Loss:93.013	translation_Loss:93.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.9	Hits@10:30.52	Best:17.98
2024-12-27 18:01:27,211: => loading checkpoint './checkpoint/FACTfact_0.01_512_5000/2model_best.tar'
2024-12-27 18:01:36,985: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1615 | 0.278  | 0.3258 |  0.3868 |
|     1      | 0.2008 | 0.1295 | 0.2356 | 0.2757 |  0.3286 |
|     2      | 0.1782 | 0.1097 | 0.2064 | 0.2479 |  0.305  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:02:03,204: Snapshot:3	Epoch:0	Loss:50.184	translation_Loss:34.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.377                                                   	MRR:14.9	Hits@10:27.01	Best:14.9
2024-12-27 18:02:11,057: Snapshot:3	Epoch:1	Loss:46.495	translation_Loss:33.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.858                                                   	MRR:14.96	Hits@10:27.04	Best:14.96
2024-12-27 18:02:18,865: Snapshot:3	Epoch:2	Loss:46.398	translation_Loss:33.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.93                                                   	MRR:14.95	Hits@10:27.11	Best:14.96
2024-12-27 18:02:26,737: Snapshot:3	Epoch:3	Loss:46.374	translation_Loss:33.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.982                                                   	MRR:14.99	Hits@10:27.08	Best:14.99
2024-12-27 18:02:34,528: Snapshot:3	Epoch:4	Loss:46.383	translation_Loss:33.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.015                                                   	MRR:14.98	Hits@10:27.04	Best:14.99
2024-12-27 18:02:42,411: Snapshot:3	Epoch:5	Loss:46.526	translation_Loss:33.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.067                                                   	MRR:15.04	Hits@10:27.15	Best:15.04
2024-12-27 18:02:50,265: Snapshot:3	Epoch:6	Loss:46.423	translation_Loss:33.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.064                                                   	MRR:14.98	Hits@10:27.12	Best:15.04
2024-12-27 18:02:58,029: Snapshot:3	Epoch:7	Loss:46.456	translation_Loss:33.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.071                                                   	MRR:14.97	Hits@10:27.07	Best:15.04
2024-12-27 18:03:06,298: Snapshot:3	Epoch:8	Loss:46.452	translation_Loss:33.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.061                                                   	MRR:14.99	Hits@10:27.12	Best:15.04
2024-12-27 18:03:14,189: Snapshot:3	Epoch:9	Loss:46.435	translation_Loss:33.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.068                                                   	MRR:14.98	Hits@10:27.04	Best:15.04
2024-12-27 18:03:21,989: Early Stopping! Snapshot: 3 Epoch: 10 Best Results: 15.04
2024-12-27 18:03:21,990: Start to training tokens! Snapshot: 3 Epoch: 10 Loss:46.451 MRR:15.0 Best Results: 15.04
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:03:21,990: Snapshot:3	Epoch:10	Loss:46.451	translation_Loss:33.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.108                                                   	MRR:15.0	Hits@10:27.12	Best:15.04
2024-12-27 18:03:29,496: Snapshot:3	Epoch:11	Loss:93.146	translation_Loss:93.11	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.0	Hits@10:27.12	Best:15.04
2024-12-27 18:03:37,005: End of token training: 3 Epoch: 12 Loss:93.062 MRR:15.0 Best Results: 15.04
2024-12-27 18:03:37,005: Snapshot:3	Epoch:12	Loss:93.062	translation_Loss:93.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.0	Hits@10:27.12	Best:15.04
2024-12-27 18:03:37,287: => loading checkpoint './checkpoint/FACTfact_0.01_512_5000/3model_best.tar'
2024-12-27 18:03:50,363: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2413 | 0.1614 | 0.2786 | 0.3257 |  0.3859 |
|     1      | 0.2015 | 0.1304 | 0.2353 | 0.2751 |  0.3296 |
|     2      | 0.179  | 0.1096 | 0.2085 | 0.2505 |  0.3069 |
|     3      | 0.1518 | 0.0857 | 0.1756 | 0.2186 |  0.2762 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:04:16,639: Snapshot:4	Epoch:0	Loss:45.648	translation_Loss:32.034	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:13.614                                                   	MRR:12.28	Hits@10:23.64	Best:12.28
2024-12-27 18:04:24,514: Snapshot:4	Epoch:1	Loss:40.979	translation_Loss:30.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.9                                                   	MRR:12.29	Hits@10:23.7	Best:12.29
2024-12-27 18:04:32,349: Snapshot:4	Epoch:2	Loss:40.921	translation_Loss:29.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.996                                                   	MRR:12.28	Hits@10:23.71	Best:12.29
2024-12-27 18:04:40,252: Snapshot:4	Epoch:3	Loss:40.826	translation_Loss:29.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.029                                                   	MRR:12.36	Hits@10:23.85	Best:12.36
2024-12-27 18:04:48,118: Snapshot:4	Epoch:4	Loss:40.858	translation_Loss:29.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.056                                                   	MRR:12.35	Hits@10:23.77	Best:12.36
2024-12-27 18:04:55,979: Snapshot:4	Epoch:5	Loss:40.898	translation_Loss:29.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.081                                                   	MRR:12.34	Hits@10:23.74	Best:12.36
2024-12-27 18:05:03,790: Snapshot:4	Epoch:6	Loss:40.819	translation_Loss:29.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.099                                                   	MRR:12.33	Hits@10:23.78	Best:12.36
2024-12-27 18:05:11,668: Snapshot:4	Epoch:7	Loss:40.844	translation_Loss:29.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.1                                                   	MRR:12.36	Hits@10:23.8	Best:12.36
2024-12-27 18:05:19,520: Early Stopping! Snapshot: 4 Epoch: 8 Best Results: 12.36
2024-12-27 18:05:19,522: Start to training tokens! Snapshot: 4 Epoch: 8 Loss:40.799 MRR:12.3 Best Results: 12.36
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:05:19,522: Snapshot:4	Epoch:8	Loss:40.799	translation_Loss:29.709	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.09                                                   	MRR:12.3	Hits@10:23.75	Best:12.36
2024-12-27 18:05:27,114: Snapshot:4	Epoch:9	Loss:86.137	translation_Loss:86.101	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.3	Hits@10:23.75	Best:12.36
2024-12-27 18:05:34,689: End of token training: 4 Epoch: 10 Loss:86.061 MRR:12.3 Best Results: 12.36
2024-12-27 18:05:34,689: Snapshot:4	Epoch:10	Loss:86.061	translation_Loss:86.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:12.3	Hits@10:23.75	Best:12.36
2024-12-27 18:05:35,055: => loading checkpoint './checkpoint/FACTfact_0.01_512_5000/4model_best.tar'
2024-12-27 18:05:51,748: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2401 | 0.1598 | 0.2778 | 0.3251 |  0.3864 |
|     1      | 0.2005 | 0.1285 | 0.2343 | 0.2758 |  0.3303 |
|     2      | 0.1796 | 0.1099 | 0.2092 | 0.2515 |  0.3078 |
|     3      | 0.1541 | 0.0874 | 0.1774 | 0.2208 |  0.2798 |
|     4      | 0.1253 | 0.064  | 0.1375 | 0.1755 |  0.2385 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:05:51,750: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1608 | 0.2772 | 0.3253 |  0.3867 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1615 | 0.2782 | 0.3253 |  0.3854 |
|     1      | 0.2002 | 0.129  | 0.2345 | 0.2748 |  0.327  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1615 | 0.278  | 0.3258 |  0.3868 |
|     1      | 0.2008 | 0.1295 | 0.2356 | 0.2757 |  0.3286 |
|     2      | 0.1782 | 0.1097 | 0.2064 | 0.2479 |  0.305  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2413 | 0.1614 | 0.2786 | 0.3257 |  0.3859 |
|     1      | 0.2015 | 0.1304 | 0.2353 | 0.2751 |  0.3296 |
|     2      | 0.179  | 0.1096 | 0.2085 | 0.2505 |  0.3069 |
|     3      | 0.1518 | 0.0857 | 0.1756 | 0.2186 |  0.2762 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2401 | 0.1598 | 0.2778 | 0.3251 |  0.3864 |
|     1      | 0.2005 | 0.1285 | 0.2343 | 0.2758 |  0.3303 |
|     2      | 0.1796 | 0.1099 | 0.2092 | 0.2515 |  0.3078 |
|     3      | 0.1541 | 0.0874 | 0.1774 | 0.2208 |  0.2798 |
|     4      | 0.1253 | 0.064  | 0.1375 | 0.1755 |  0.2385 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:05:51,751: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 84.03635358810425  |    0.24   |    0.161     |    0.277     |     0.387     |
|    1     | 104.5305745601654  |   0.221   |    0.145     |    0.256     |     0.356     |
|    2     | 115.55538725852966 |   0.207   |    0.134     |     0.24     |      0.34     |
|    3     | 116.58411240577698 |   0.193   |    0.122     |    0.224     |     0.325     |
|    4     | 100.83649754524231 |    0.18   |     0.11     |    0.207     |     0.309     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:05:51,751: Sum_Training_Time:521.5429253578186
2024-12-27 18:05:51,751: Every_Training_Time:[84.03635358810425, 104.5305745601654, 115.55538725852966, 116.58411240577698, 100.83649754524231]
2024-12-27 18:05:51,751: Forward transfer: 0.145375 Backward transfer: 0.0009250000000000091
2024-12-27 18:06:30,881: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227180555/FACTfact_0.01_512_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_512_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_512_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:06:41,091: Snapshot:0	Epoch:0	Loss:86.417	translation_Loss:86.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.31	Hits@10:28.45	Best:14.31
2024-12-27 18:06:47,685: Snapshot:0	Epoch:1	Loss:47.543	translation_Loss:47.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.47	Hits@10:36.13	Best:21.47
2024-12-27 18:06:54,309: Snapshot:0	Epoch:2	Loss:23.628	translation_Loss:23.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:38.17	Best:23.88
2024-12-27 18:07:00,947: Snapshot:0	Epoch:3	Loss:11.744	translation_Loss:11.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:39.16	Best:24.57
2024-12-27 18:07:07,568: Snapshot:0	Epoch:4	Loss:6.412	translation_Loss:6.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:39.37	Best:24.63
2024-12-27 18:07:14,170: Snapshot:0	Epoch:5	Loss:4.189	translation_Loss:4.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:39.25	Best:24.63
2024-12-27 18:07:20,799: Snapshot:0	Epoch:6	Loss:3.222	translation_Loss:3.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.6	Hits@10:39.37	Best:24.63
2024-12-27 18:07:27,392: Snapshot:0	Epoch:7	Loss:2.679	translation_Loss:2.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:39.48	Best:24.63
2024-12-27 18:07:34,500: Snapshot:0	Epoch:8	Loss:2.337	translation_Loss:2.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:39.24	Best:24.63
2024-12-27 18:07:41,172: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 24.63
2024-12-27 18:07:41,172: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:2.131 MRR:24.51 Best Results: 24.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:07:41,173: Snapshot:0	Epoch:9	Loss:2.131	translation_Loss:2.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:39.33	Best:24.63
2024-12-27 18:07:48,528: Snapshot:0	Epoch:10	Loss:73.303	translation_Loss:73.267	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:39.33	Best:24.63
2024-12-27 18:07:55,352: End of token training: 0 Epoch: 11 Loss:73.243 MRR:24.51 Best Results: 24.63
2024-12-27 18:07:55,353: Snapshot:0	Epoch:11	Loss:73.243	translation_Loss:73.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.51	Hits@10:39.33	Best:24.63
2024-12-27 18:07:55,668: => loading checkpoint './checkpoint/FACTfact_0.01_512_10000/0model_best.tar'
2024-12-27 18:07:58,568: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2395 | 0.1593 | 0.2766 | 0.3243 |  0.3868 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:08:24,150: Snapshot:1	Epoch:0	Loss:57.503	translation_Loss:40.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:17.069                                                   	MRR:19.13	Hits@10:31.73	Best:19.13
2024-12-27 18:08:31,716: Snapshot:1	Epoch:1	Loss:42.723	translation_Loss:32.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.136                                                   	MRR:19.75	Hits@10:32.51	Best:19.75
2024-12-27 18:08:39,283: Snapshot:1	Epoch:2	Loss:40.454	translation_Loss:30.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.122                                                   	MRR:19.82	Hits@10:32.59	Best:19.82
2024-12-27 18:08:46,863: Snapshot:1	Epoch:3	Loss:39.801	translation_Loss:29.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.114                                                   	MRR:19.92	Hits@10:32.55	Best:19.92
2024-12-27 18:08:54,408: Snapshot:1	Epoch:4	Loss:39.512	translation_Loss:29.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.187                                                   	MRR:19.93	Hits@10:32.63	Best:19.93
2024-12-27 18:09:01,953: Snapshot:1	Epoch:5	Loss:39.384	translation_Loss:29.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.2                                                   	MRR:19.9	Hits@10:32.6	Best:19.93
2024-12-27 18:09:09,488: Snapshot:1	Epoch:6	Loss:39.203	translation_Loss:28.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.208                                                   	MRR:19.93	Hits@10:32.8	Best:19.93
2024-12-27 18:09:16,986: Snapshot:1	Epoch:7	Loss:39.197	translation_Loss:28.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.251                                                   	MRR:19.88	Hits@10:32.68	Best:19.93
2024-12-27 18:09:24,501: Snapshot:1	Epoch:8	Loss:39.185	translation_Loss:28.884	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.301                                                   	MRR:19.88	Hits@10:32.58	Best:19.93
2024-12-27 18:09:32,011: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 19.93
2024-12-27 18:09:32,011: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:39.051 MRR:19.88 Best Results: 19.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:09:32,011: Snapshot:1	Epoch:9	Loss:39.051	translation_Loss:28.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.281                                                   	MRR:19.88	Hits@10:32.54	Best:19.93
2024-12-27 18:09:39,303: Snapshot:1	Epoch:10	Loss:90.289	translation_Loss:90.252	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.88	Hits@10:32.54	Best:19.93
2024-12-27 18:09:46,634: End of token training: 1 Epoch: 11 Loss:90.309 MRR:19.88 Best Results: 19.93
2024-12-27 18:09:46,634: Snapshot:1	Epoch:11	Loss:90.309	translation_Loss:90.309	multi_layer_Loss:-0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.88	Hits@10:32.54	Best:19.93
2024-12-27 18:09:46,994: => loading checkpoint './checkpoint/FACTfact_0.01_512_10000/1model_best.tar'
2024-12-27 18:09:53,617: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2397 | 0.1594 | 0.2768 | 0.3254 |  0.387  |
|     1      | 0.1993 | 0.1279 | 0.2334 | 0.2741 |  0.3258 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:10:19,838: Snapshot:2	Epoch:0	Loss:54.272	translation_Loss:36.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:17.558                                                   	MRR:17.68	Hits@10:30.27	Best:17.68
2024-12-27 18:10:27,645: Snapshot:2	Epoch:1	Loss:45.32	translation_Loss:33.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.598                                                   	MRR:17.8	Hits@10:30.53	Best:17.8
2024-12-27 18:10:35,447: Snapshot:2	Epoch:2	Loss:44.812	translation_Loss:33.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.677                                                   	MRR:17.84	Hits@10:30.45	Best:17.84
2024-12-27 18:10:43,244: Snapshot:2	Epoch:3	Loss:44.688	translation_Loss:32.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.732                                                   	MRR:17.82	Hits@10:30.53	Best:17.84
2024-12-27 18:10:51,022: Snapshot:2	Epoch:4	Loss:44.591	translation_Loss:32.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.766                                                   	MRR:17.81	Hits@10:30.5	Best:17.84
2024-12-27 18:10:58,753: Snapshot:2	Epoch:5	Loss:44.714	translation_Loss:32.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.837                                                   	MRR:17.8	Hits@10:30.58	Best:17.84
2024-12-27 18:11:06,502: Snapshot:2	Epoch:6	Loss:44.663	translation_Loss:32.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.902                                                   	MRR:17.84	Hits@10:30.49	Best:17.84
2024-12-27 18:11:14,263: Snapshot:2	Epoch:7	Loss:44.66	translation_Loss:32.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.923                                                   	MRR:17.86	Hits@10:30.5	Best:17.86
2024-12-27 18:11:22,101: Snapshot:2	Epoch:8	Loss:44.591	translation_Loss:32.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.909                                                   	MRR:17.86	Hits@10:30.48	Best:17.86
2024-12-27 18:11:29,832: Snapshot:2	Epoch:9	Loss:44.678	translation_Loss:32.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.938                                                   	MRR:17.83	Hits@10:30.45	Best:17.86
2024-12-27 18:11:37,548: Snapshot:2	Epoch:10	Loss:44.669	translation_Loss:32.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.935                                                   	MRR:17.84	Hits@10:30.47	Best:17.86
2024-12-27 18:11:45,408: Snapshot:2	Epoch:11	Loss:44.597	translation_Loss:32.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.929                                                   	MRR:17.87	Hits@10:30.48	Best:17.87
2024-12-27 18:11:53,670: Snapshot:2	Epoch:12	Loss:44.546	translation_Loss:32.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.943                                                   	MRR:17.82	Hits@10:30.56	Best:17.87
2024-12-27 18:12:01,405: Snapshot:2	Epoch:13	Loss:44.559	translation_Loss:32.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.947                                                   	MRR:17.83	Hits@10:30.56	Best:17.87
2024-12-27 18:12:09,155: Snapshot:2	Epoch:14	Loss:44.585	translation_Loss:32.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.946                                                   	MRR:17.86	Hits@10:30.55	Best:17.87
2024-12-27 18:12:16,883: Snapshot:2	Epoch:15	Loss:44.613	translation_Loss:32.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.971                                                   	MRR:17.81	Hits@10:30.57	Best:17.87
2024-12-27 18:12:24,615: Early Stopping! Snapshot: 2 Epoch: 16 Best Results: 17.87
2024-12-27 18:12:24,615: Start to training tokens! Snapshot: 2 Epoch: 16 Loss:44.598 MRR:17.84 Best Results: 17.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:12:24,616: Snapshot:2	Epoch:16	Loss:44.598	translation_Loss:32.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.963                                                   	MRR:17.84	Hits@10:30.45	Best:17.87
2024-12-27 18:12:32,103: Snapshot:2	Epoch:17	Loss:93.143	translation_Loss:93.106	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.84	Hits@10:30.45	Best:17.87
2024-12-27 18:12:39,621: End of token training: 2 Epoch: 18 Loss:93.019 MRR:17.84 Best Results: 17.87
2024-12-27 18:12:39,622: Snapshot:2	Epoch:18	Loss:93.019	translation_Loss:93.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.84	Hits@10:30.45	Best:17.87
2024-12-27 18:12:39,900: => loading checkpoint './checkpoint/FACTfact_0.01_512_10000/2model_best.tar'
2024-12-27 18:12:49,245: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1605 | 0.2771 | 0.3249 |  0.3875 |
|     1      | 0.1995 | 0.1279 | 0.2338 | 0.2742 |  0.3263 |
|     2      | 0.1778 | 0.1091 | 0.2057 | 0.2486 |  0.3037 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:13:15,926: Snapshot:3	Epoch:0	Loss:52.328	translation_Loss:35.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:17.096                                                   	MRR:14.8	Hits@10:26.71	Best:14.8
2024-12-27 18:13:23,773: Snapshot:3	Epoch:1	Loss:45.302	translation_Loss:34.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.154                                                   	MRR:14.91	Hits@10:26.8	Best:14.91
2024-12-27 18:13:31,630: Snapshot:3	Epoch:2	Loss:45.191	translation_Loss:33.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.266                                                   	MRR:14.92	Hits@10:26.76	Best:14.92
2024-12-27 18:13:39,423: Snapshot:3	Epoch:3	Loss:45.303	translation_Loss:33.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.378                                                   	MRR:14.88	Hits@10:26.79	Best:14.92
2024-12-27 18:13:47,372: Snapshot:3	Epoch:4	Loss:45.287	translation_Loss:33.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.437                                                   	MRR:14.93	Hits@10:26.84	Best:14.93
2024-12-27 18:13:55,232: Snapshot:3	Epoch:5	Loss:45.378	translation_Loss:33.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.466                                                   	MRR:14.97	Hits@10:26.79	Best:14.97
2024-12-27 18:14:03,032: Snapshot:3	Epoch:6	Loss:45.342	translation_Loss:33.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.491                                                   	MRR:14.95	Hits@10:26.8	Best:14.97
2024-12-27 18:14:10,798: Snapshot:3	Epoch:7	Loss:45.355	translation_Loss:33.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.54                                                   	MRR:14.95	Hits@10:26.74	Best:14.97
2024-12-27 18:14:18,575: Snapshot:3	Epoch:8	Loss:45.399	translation_Loss:33.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.543                                                   	MRR:14.9	Hits@10:26.82	Best:14.97
2024-12-27 18:14:26,461: Snapshot:3	Epoch:9	Loss:45.456	translation_Loss:33.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.569                                                   	MRR:14.99	Hits@10:26.85	Best:14.99
2024-12-27 18:14:34,276: Snapshot:3	Epoch:10	Loss:45.356	translation_Loss:33.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.569                                                   	MRR:14.94	Hits@10:26.86	Best:14.99
2024-12-27 18:14:42,084: Snapshot:3	Epoch:11	Loss:45.465	translation_Loss:33.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.579                                                   	MRR:14.95	Hits@10:26.86	Best:14.99
2024-12-27 18:14:49,925: Snapshot:3	Epoch:12	Loss:45.548	translation_Loss:33.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.609                                                   	MRR:14.92	Hits@10:26.8	Best:14.99
2024-12-27 18:14:57,728: Snapshot:3	Epoch:13	Loss:45.442	translation_Loss:33.849	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.593                                                   	MRR:14.94	Hits@10:26.86	Best:14.99
2024-12-27 18:15:05,580: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 14.99
2024-12-27 18:15:05,581: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:45.492 MRR:14.95 Best Results: 14.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:15:05,581: Snapshot:3	Epoch:14	Loss:45.492	translation_Loss:33.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.594                                                   	MRR:14.95	Hits@10:26.82	Best:14.99
2024-12-27 18:15:13,106: Snapshot:3	Epoch:15	Loss:93.451	translation_Loss:93.415	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.95	Hits@10:26.82	Best:14.99
2024-12-27 18:15:20,636: End of token training: 3 Epoch: 16 Loss:93.426 MRR:14.95 Best Results: 14.99
2024-12-27 18:15:20,636: Snapshot:3	Epoch:16	Loss:93.426	translation_Loss:93.426	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:14.95	Hits@10:26.82	Best:14.99
2024-12-27 18:15:20,999: => loading checkpoint './checkpoint/FACTfact_0.01_512_10000/3model_best.tar'
2024-12-27 18:15:33,589: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2402 | 0.1603 | 0.2775 | 0.3249 |  0.3875 |
|     1      |  0.2   | 0.1282 | 0.2348 | 0.2748 |  0.328  |
|     2      | 0.1786 | 0.1098 | 0.2058 | 0.2491 |  0.3051 |
|     3      | 0.1493 | 0.0843 | 0.1704 | 0.214  |  0.2734 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:16:00,454: Snapshot:4	Epoch:0	Loss:48.857	translation_Loss:33.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:15.384                                                   	MRR:11.84	Hits@10:22.95	Best:11.84
2024-12-27 18:16:08,360: Snapshot:4	Epoch:1	Loss:40.896	translation_Loss:31.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.295                                                   	MRR:11.89	Hits@10:23.1	Best:11.89
2024-12-27 18:16:16,286: Snapshot:4	Epoch:2	Loss:40.836	translation_Loss:31.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.426                                                   	MRR:11.92	Hits@10:23.05	Best:11.92
2024-12-27 18:16:24,240: Snapshot:4	Epoch:3	Loss:40.81	translation_Loss:31.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.5                                                   	MRR:11.93	Hits@10:23.05	Best:11.93
2024-12-27 18:16:32,105: Snapshot:4	Epoch:4	Loss:40.775	translation_Loss:31.229	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.547                                                   	MRR:11.93	Hits@10:23.0	Best:11.93
2024-12-27 18:16:39,986: Snapshot:4	Epoch:5	Loss:40.885	translation_Loss:31.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.594                                                   	MRR:11.89	Hits@10:23.04	Best:11.93
2024-12-27 18:16:47,900: Snapshot:4	Epoch:6	Loss:40.843	translation_Loss:31.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.623                                                   	MRR:11.96	Hits@10:23.16	Best:11.96
2024-12-27 18:16:55,738: Snapshot:4	Epoch:7	Loss:40.93	translation_Loss:31.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.679                                                   	MRR:11.93	Hits@10:23.18	Best:11.96
2024-12-27 18:17:03,599: Snapshot:4	Epoch:8	Loss:40.86	translation_Loss:31.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.667                                                   	MRR:11.92	Hits@10:23.11	Best:11.96
2024-12-27 18:17:11,501: Snapshot:4	Epoch:9	Loss:40.896	translation_Loss:31.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.717                                                   	MRR:11.99	Hits@10:23.17	Best:11.99
2024-12-27 18:17:19,419: Snapshot:4	Epoch:10	Loss:40.989	translation_Loss:31.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.751                                                   	MRR:11.94	Hits@10:23.19	Best:11.99
2024-12-27 18:17:27,321: Snapshot:4	Epoch:11	Loss:40.942	translation_Loss:31.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.757                                                   	MRR:12.02	Hits@10:23.27	Best:12.02
2024-12-27 18:17:35,179: Snapshot:4	Epoch:12	Loss:40.903	translation_Loss:31.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.73                                                   	MRR:11.99	Hits@10:23.21	Best:12.02
2024-12-27 18:17:43,055: Snapshot:4	Epoch:13	Loss:40.931	translation_Loss:31.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.777                                                   	MRR:11.98	Hits@10:23.19	Best:12.02
2024-12-27 18:17:50,881: Snapshot:4	Epoch:14	Loss:40.916	translation_Loss:31.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.761                                                   	MRR:11.96	Hits@10:23.15	Best:12.02
2024-12-27 18:17:58,721: Snapshot:4	Epoch:15	Loss:40.924	translation_Loss:31.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.757                                                   	MRR:11.94	Hits@10:23.16	Best:12.02
2024-12-27 18:18:07,103: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 12.02
2024-12-27 18:18:07,104: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:40.952 MRR:11.99 Best Results: 12.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:18:07,104: Snapshot:4	Epoch:16	Loss:40.952	translation_Loss:31.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.802                                                   	MRR:11.99	Hits@10:23.17	Best:12.02
2024-12-27 18:18:14,793: Snapshot:4	Epoch:17	Loss:87.073	translation_Loss:87.037	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.99	Hits@10:23.17	Best:12.02
2024-12-27 18:18:22,351: End of token training: 4 Epoch: 18 Loss:87.039 MRR:11.99 Best Results: 12.02
2024-12-27 18:18:22,351: Snapshot:4	Epoch:18	Loss:87.039	translation_Loss:87.039	multi_layer_Loss:-0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:11.99	Hits@10:23.17	Best:12.02
2024-12-27 18:18:22,630: => loading checkpoint './checkpoint/FACTfact_0.01_512_10000/4model_best.tar'
2024-12-27 18:18:38,851: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1607 | 0.2767 | 0.3243 |  0.387  |
|     1      | 0.2003 | 0.1284 | 0.2351 | 0.2759 |  0.3279 |
|     2      | 0.1793 | 0.1107 | 0.2063 | 0.2502 |  0.3067 |
|     3      | 0.1502 | 0.0844 | 0.1728 | 0.2156 |  0.2753 |
|     4      | 0.121  | 0.0621 | 0.1312 | 0.1696 |  0.2326 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:18:38,853: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2395 | 0.1593 | 0.2766 | 0.3243 |  0.3868 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2397 | 0.1594 | 0.2768 | 0.3254 |  0.387  |
|     1      | 0.1993 | 0.1279 | 0.2334 | 0.2741 |  0.3258 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1605 | 0.2771 | 0.3249 |  0.3875 |
|     1      | 0.1995 | 0.1279 | 0.2338 | 0.2742 |  0.3263 |
|     2      | 0.1778 | 0.1091 | 0.2057 | 0.2486 |  0.3037 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2402 | 0.1603 | 0.2775 | 0.3249 |  0.3875 |
|     1      |  0.2   | 0.1282 | 0.2348 | 0.2748 |  0.328  |
|     2      | 0.1786 | 0.1098 | 0.2058 | 0.2491 |  0.3051 |
|     3      | 0.1493 | 0.0843 | 0.1704 | 0.214  |  0.2734 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2404 | 0.1607 | 0.2767 | 0.3243 |  0.387  |
|     1      | 0.2003 | 0.1284 | 0.2351 | 0.2759 |  0.3279 |
|     2      | 0.1793 | 0.1107 | 0.2063 | 0.2502 |  0.3067 |
|     3      | 0.1502 | 0.0844 | 0.1728 | 0.2156 |  0.2753 |
|     4      | 0.121  | 0.0621 | 0.1312 | 0.1696 |  0.2326 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:18:38,854: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  84.4712495803833  |   0.239   |    0.159     |    0.277     |     0.387     |
|    1     | 104.90147066116333 |    0.22   |    0.144     |    0.255     |     0.356     |
|    2     | 162.6181206703186  |   0.206   |    0.133     |    0.239     |     0.339     |
|    3     | 147.96679043769836 |   0.192   |    0.121     |    0.222     |     0.324     |
|    4     |  164.951406955719  |   0.178   |    0.109     |    0.204     |     0.306     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:18:38,854: Sum_Training_Time:664.9090383052826
2024-12-27 18:18:38,854: Every_Training_Time:[84.4712495803833, 104.90147066116333, 162.6181206703186, 147.96679043769836, 164.951406955719]
2024-12-27 18:18:38,854: Forward transfer: 0.14429999999999998 Backward transfer: 0.0010749999999999996
2024-12-27 18:19:18,421: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227181843/FACTfact_0.01_1024_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_1024_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_1024_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:19:28,435: Snapshot:0	Epoch:0	Loss:44.248	translation_Loss:44.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.42	Hits@10:26.99	Best:13.42
2024-12-27 18:19:34,889: Snapshot:0	Epoch:1	Loss:24.915	translation_Loss:24.915	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.33	Hits@10:35.5	Best:20.33
2024-12-27 18:19:41,386: Snapshot:0	Epoch:2	Loss:13.066	translation_Loss:13.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:38.35	Best:23.76
2024-12-27 18:19:47,865: Snapshot:0	Epoch:3	Loss:6.661	translation_Loss:6.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.55	Hits@10:39.37	Best:24.55
2024-12-27 18:19:54,855: Snapshot:0	Epoch:4	Loss:3.619	translation_Loss:3.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.83	Hits@10:39.81	Best:24.83
2024-12-27 18:20:01,254: Snapshot:0	Epoch:5	Loss:2.318	translation_Loss:2.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:39.94	Best:24.83
2024-12-27 18:20:07,804: Snapshot:0	Epoch:6	Loss:1.712	translation_Loss:1.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:39.92	Best:24.83
2024-12-27 18:20:14,224: Snapshot:0	Epoch:7	Loss:1.36	translation_Loss:1.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:39.8	Best:24.83
2024-12-27 18:20:20,665: Snapshot:0	Epoch:8	Loss:1.157	translation_Loss:1.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:39.78	Best:24.83
2024-12-27 18:20:27,111: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 24.83
2024-12-27 18:20:27,111: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:1.026 MRR:24.78 Best Results: 24.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:20:27,112: Snapshot:0	Epoch:9	Loss:1.026	translation_Loss:1.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:39.83	Best:24.83
2024-12-27 18:20:34,145: Snapshot:0	Epoch:10	Loss:36.958	translation_Loss:36.922	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:39.83	Best:24.83
2024-12-27 18:20:40,703: End of token training: 0 Epoch: 11 Loss:36.898 MRR:24.78 Best Results: 24.83
2024-12-27 18:20:40,704: Snapshot:0	Epoch:11	Loss:36.898	translation_Loss:36.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.78	Hits@10:39.83	Best:24.83
2024-12-27 18:20:41,007: => loading checkpoint './checkpoint/FACTfact_0.01_1024_1000/0model_best.tar'
2024-12-27 18:20:43,784: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2406 | 0.1588 | 0.2794 | 0.3286 |  0.389  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:21:09,068: Snapshot:1	Epoch:0	Loss:28.476	translation_Loss:21.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.163                                                   	MRR:19.8	Hits@10:32.67	Best:19.8
2024-12-27 18:21:16,248: Snapshot:1	Epoch:1	Loss:23.895	translation_Loss:17.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.679                                                   	MRR:20.33	Hits@10:33.34	Best:20.33
2024-12-27 18:21:23,546: Snapshot:1	Epoch:2	Loss:22.617	translation_Loss:16.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.591                                                   	MRR:20.47	Hits@10:33.61	Best:20.47
2024-12-27 18:21:30,712: Snapshot:1	Epoch:3	Loss:22.225	translation_Loss:15.67	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.555                                                   	MRR:20.45	Hits@10:33.53	Best:20.47
2024-12-27 18:21:37,828: Snapshot:1	Epoch:4	Loss:22.01	translation_Loss:15.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.53                                                   	MRR:20.44	Hits@10:33.54	Best:20.47
2024-12-27 18:21:45,081: Snapshot:1	Epoch:5	Loss:21.931	translation_Loss:15.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.531                                                   	MRR:20.51	Hits@10:33.56	Best:20.51
2024-12-27 18:21:52,257: Snapshot:1	Epoch:6	Loss:21.807	translation_Loss:15.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.513                                                   	MRR:20.45	Hits@10:33.47	Best:20.51
2024-12-27 18:21:59,397: Snapshot:1	Epoch:7	Loss:21.816	translation_Loss:15.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.535                                                   	MRR:20.46	Hits@10:33.63	Best:20.51
2024-12-27 18:22:06,571: Snapshot:1	Epoch:8	Loss:21.773	translation_Loss:15.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.536                                                   	MRR:20.51	Hits@10:33.56	Best:20.51
2024-12-27 18:22:13,737: Snapshot:1	Epoch:9	Loss:21.706	translation_Loss:15.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.51                                                   	MRR:20.43	Hits@10:33.64	Best:20.51
2024-12-27 18:22:20,897: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 20.51
2024-12-27 18:22:20,897: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:21.735 MRR:20.49 Best Results: 20.51
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:22:20,898: Snapshot:1	Epoch:10	Loss:21.735	translation_Loss:15.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.531                                                   	MRR:20.49	Hits@10:33.44	Best:20.51
2024-12-27 18:22:27,907: Snapshot:1	Epoch:11	Loss:45.54	translation_Loss:45.504	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.49	Hits@10:33.44	Best:20.51
2024-12-27 18:22:34,905: End of token training: 1 Epoch: 12 Loss:45.473 MRR:20.49 Best Results: 20.51
2024-12-27 18:22:34,905: Snapshot:1	Epoch:12	Loss:45.473	translation_Loss:45.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.49	Hits@10:33.44	Best:20.51
2024-12-27 18:22:35,262: => loading checkpoint './checkpoint/FACTfact_0.01_1024_1000/1model_best.tar'
2024-12-27 18:22:41,662: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.242  | 0.1605 | 0.2802 | 0.3297 |  0.3909 |
|     1      | 0.2035 | 0.1299 | 0.2382 | 0.281  |  0.3366 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:23:07,380: Snapshot:2	Epoch:0	Loss:26.187	translation_Loss:18.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.631                                                   	MRR:18.68	Hits@10:31.76	Best:18.68
2024-12-27 18:23:14,794: Snapshot:2	Epoch:1	Loss:24.186	translation_Loss:16.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.272                                                   	MRR:18.77	Hits@10:31.82	Best:18.77
2024-12-27 18:23:22,126: Snapshot:2	Epoch:2	Loss:23.904	translation_Loss:16.613	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.291                                                   	MRR:18.76	Hits@10:31.91	Best:18.77
2024-12-27 18:23:29,531: Snapshot:2	Epoch:3	Loss:23.778	translation_Loss:16.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.286                                                   	MRR:18.92	Hits@10:32.03	Best:18.92
2024-12-27 18:23:36,931: Snapshot:2	Epoch:4	Loss:23.824	translation_Loss:16.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.3                                                   	MRR:18.82	Hits@10:31.96	Best:18.92
2024-12-27 18:23:44,354: Snapshot:2	Epoch:5	Loss:23.744	translation_Loss:16.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.31                                                   	MRR:18.87	Hits@10:31.91	Best:18.92
2024-12-27 18:23:51,721: Snapshot:2	Epoch:6	Loss:23.755	translation_Loss:16.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.305                                                   	MRR:18.85	Hits@10:31.92	Best:18.92
2024-12-27 18:23:59,048: Snapshot:2	Epoch:7	Loss:23.719	translation_Loss:16.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.326                                                   	MRR:18.89	Hits@10:32.05	Best:18.92
2024-12-27 18:24:06,371: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 18.92
2024-12-27 18:24:06,372: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:23.709 MRR:18.88 Best Results: 18.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:24:06,372: Snapshot:2	Epoch:8	Loss:23.709	translation_Loss:16.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.309                                                   	MRR:18.88	Hits@10:32.06	Best:18.92
2024-12-27 18:24:13,543: Snapshot:2	Epoch:9	Loss:46.641	translation_Loss:46.604	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.88	Hits@10:32.06	Best:18.92
2024-12-27 18:24:20,723: End of token training: 2 Epoch: 10 Loss:46.598 MRR:18.88 Best Results: 18.92
2024-12-27 18:24:20,724: Snapshot:2	Epoch:10	Loss:46.598	translation_Loss:46.598	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.88	Hits@10:32.06	Best:18.92
2024-12-27 18:24:21,081: => loading checkpoint './checkpoint/FACTfact_0.01_1024_1000/2model_best.tar'
2024-12-27 18:24:30,374: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1591 | 0.2788 | 0.3292 |  0.3902 |
|     1      | 0.2047 | 0.1295 | 0.2414 | 0.285  |  0.3399 |
|     2      | 0.1861 | 0.1135 | 0.2177 | 0.2616 |  0.3193 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:24:55,400: Snapshot:3	Epoch:0	Loss:23.943	translation_Loss:16.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.547                                                   	MRR:16.37	Hits@10:30.21	Best:16.37
2024-12-27 18:25:02,861: Snapshot:3	Epoch:1	Loss:22.923	translation_Loss:15.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.141                                                   	MRR:16.41	Hits@10:30.23	Best:16.41
2024-12-27 18:25:10,806: Snapshot:3	Epoch:2	Loss:22.872	translation_Loss:15.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.18                                                   	MRR:16.55	Hits@10:30.19	Best:16.55
2024-12-27 18:25:18,225: Snapshot:3	Epoch:3	Loss:22.862	translation_Loss:15.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.214                                                   	MRR:16.44	Hits@10:30.34	Best:16.55
2024-12-27 18:25:25,651: Snapshot:3	Epoch:4	Loss:22.827	translation_Loss:15.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.208                                                   	MRR:16.48	Hits@10:30.23	Best:16.55
2024-12-27 18:25:33,065: Snapshot:3	Epoch:5	Loss:22.845	translation_Loss:15.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.217                                                   	MRR:16.44	Hits@10:30.21	Best:16.55
2024-12-27 18:25:40,489: Snapshot:3	Epoch:6	Loss:22.86	translation_Loss:15.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.239                                                   	MRR:16.47	Hits@10:30.28	Best:16.55
2024-12-27 18:25:47,978: Early Stopping! Snapshot: 3 Epoch: 7 Best Results: 16.55
2024-12-27 18:25:47,978: Start to training tokens! Snapshot: 3 Epoch: 7 Loss:22.83 MRR:16.52 Best Results: 16.55
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:25:47,979: Snapshot:3	Epoch:7	Loss:22.83	translation_Loss:15.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.219                                                   	MRR:16.52	Hits@10:30.31	Best:16.55
2024-12-27 18:25:55,212: Snapshot:3	Epoch:8	Loss:46.174	translation_Loss:46.137	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.52	Hits@10:30.31	Best:16.55
2024-12-27 18:26:02,447: End of token training: 3 Epoch: 9 Loss:46.138 MRR:16.52 Best Results: 16.55
2024-12-27 18:26:02,448: Snapshot:3	Epoch:9	Loss:46.138	translation_Loss:46.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:16.52	Hits@10:30.31	Best:16.55
2024-12-27 18:26:02,806: => loading checkpoint './checkpoint/FACTfact_0.01_1024_1000/3model_best.tar'
2024-12-27 18:26:15,609: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2389 | 0.1572 | 0.2766 | 0.3268 |  0.3895 |
|     1      | 0.2043 | 0.1291 | 0.2394 | 0.2845 |  0.3422 |
|     2      | 0.1889 | 0.1149 | 0.2206 | 0.2672 |  0.3277 |
|     3      | 0.167  | 0.0952 | 0.1923 | 0.2411 |  0.3038 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:26:41,295: Snapshot:4	Epoch:0	Loss:18.432	translation_Loss:11.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.836                                                   	MRR:17.12	Hits@10:33.92	Best:17.12
2024-12-27 18:26:48,819: Snapshot:4	Epoch:1	Loss:16.67	translation_Loss:10.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.286                                                   	MRR:17.29	Hits@10:34.1	Best:17.29
2024-12-27 18:26:56,281: Snapshot:4	Epoch:2	Loss:16.583	translation_Loss:10.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.306                                                   	MRR:17.28	Hits@10:34.21	Best:17.29
2024-12-27 18:27:03,716: Snapshot:4	Epoch:3	Loss:16.547	translation_Loss:10.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.322                                                   	MRR:17.23	Hits@10:34.22	Best:17.29
2024-12-27 18:27:11,176: Snapshot:4	Epoch:4	Loss:16.526	translation_Loss:10.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.328                                                   	MRR:17.2	Hits@10:34.26	Best:17.29
2024-12-27 18:27:18,643: Snapshot:4	Epoch:5	Loss:16.541	translation_Loss:10.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.339                                                   	MRR:17.22	Hits@10:34.12	Best:17.29
2024-12-27 18:27:26,184: Snapshot:4	Epoch:6	Loss:16.499	translation_Loss:10.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.325                                                   	MRR:17.33	Hits@10:34.34	Best:17.33
2024-12-27 18:27:33,621: Snapshot:4	Epoch:7	Loss:16.541	translation_Loss:10.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.345                                                   	MRR:17.18	Hits@10:34.15	Best:17.33
2024-12-27 18:27:41,135: Snapshot:4	Epoch:8	Loss:16.516	translation_Loss:10.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.339                                                   	MRR:17.16	Hits@10:33.99	Best:17.33
2024-12-27 18:27:48,628: Snapshot:4	Epoch:9	Loss:16.536	translation_Loss:10.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.352                                                   	MRR:17.17	Hits@10:34.16	Best:17.33
2024-12-27 18:27:56,106: Snapshot:4	Epoch:10	Loss:16.473	translation_Loss:10.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.317                                                   	MRR:17.32	Hits@10:34.33	Best:17.33
2024-12-27 18:28:04,091: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 17.33
2024-12-27 18:28:04,092: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:16.55 MRR:17.3 Best Results: 17.33
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:28:04,092: Snapshot:4	Epoch:11	Loss:16.55	translation_Loss:10.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.368                                                   	MRR:17.3	Hits@10:34.18	Best:17.33
2024-12-27 18:28:11,347: Snapshot:4	Epoch:12	Loss:40.425	translation_Loss:40.389	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.3	Hits@10:34.18	Best:17.33
2024-12-27 18:28:18,631: End of token training: 4 Epoch: 13 Loss:40.369 MRR:17.3 Best Results: 17.33
2024-12-27 18:28:18,632: Snapshot:4	Epoch:13	Loss:40.369	translation_Loss:40.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.3	Hits@10:34.18	Best:17.33
2024-12-27 18:28:18,905: => loading checkpoint './checkpoint/FACTfact_0.01_1024_1000/4model_best.tar'
2024-12-27 18:28:35,149: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2337 | 0.152  | 0.2705 | 0.321  |  0.3843 |
|     1      | 0.1999 | 0.1248 | 0.2347 | 0.2796 |  0.3387 |
|     2      | 0.1862 | 0.1121 | 0.2168 | 0.2635 |  0.326  |
|     3      | 0.1666 | 0.0918 | 0.1928 | 0.2444 |  0.3099 |
|     4      | 0.172  | 0.0878 | 0.1975 | 0.2553 |  0.3394 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:28:35,152: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2406 | 0.1588 | 0.2794 | 0.3286 |  0.389  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.242  | 0.1605 | 0.2802 | 0.3297 |  0.3909 |
|     1      | 0.2035 | 0.1299 | 0.2382 | 0.281  |  0.3366 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1591 | 0.2788 | 0.3292 |  0.3902 |
|     1      | 0.2047 | 0.1295 | 0.2414 | 0.285  |  0.3399 |
|     2      | 0.1861 | 0.1135 | 0.2177 | 0.2616 |  0.3193 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2389 | 0.1572 | 0.2766 | 0.3268 |  0.3895 |
|     1      | 0.2043 | 0.1291 | 0.2394 | 0.2845 |  0.3422 |
|     2      | 0.1889 | 0.1149 | 0.2206 | 0.2672 |  0.3277 |
|     3      | 0.167  | 0.0952 | 0.1923 | 0.2411 |  0.3038 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2337 | 0.152  | 0.2705 | 0.321  |  0.3843 |
|     1      | 0.1999 | 0.1248 | 0.2347 | 0.2796 |  0.3387 |
|     2      | 0.1862 | 0.1121 | 0.2168 | 0.2635 |  0.326  |
|     3      | 0.1666 | 0.0918 | 0.1928 | 0.2444 |  0.3099 |
|     4      | 0.172  | 0.0878 | 0.1975 | 0.2553 |  0.3394 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:28:35,152: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 82.28233575820923  |   0.241   |    0.159     |    0.279     |     0.389     |
|    1     | 107.97333788871765 |   0.223   |    0.145     |    0.259     |     0.364     |
|    2     |  95.7217903137207  |    0.21   |    0.134     |    0.246     |      0.35     |
|    3     | 89.02050518989563  |    0.2    |    0.124     |    0.232     |     0.341     |
|    4     | 119.57607841491699 |   0.192   |    0.114     |    0.222     |      0.34     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:28:35,153: Sum_Training_Time:494.5740475654602
2024-12-27 18:28:35,153: Every_Training_Time:[82.28233575820923, 107.97333788871765, 95.7217903137207, 89.02050518989563, 119.57607841491699]
2024-12-27 18:28:35,153: Forward transfer: 0.154825 Backward transfer: -0.002700000000000001
2024-12-27 18:29:14,018: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227182838/FACTfact_0.01_1024_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_1024_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_1024_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:29:23,982: Snapshot:0	Epoch:0	Loss:44.248	translation_Loss:44.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.42	Hits@10:26.98	Best:13.42
2024-12-27 18:29:30,379: Snapshot:0	Epoch:1	Loss:24.911	translation_Loss:24.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.33	Hits@10:35.49	Best:20.33
2024-12-27 18:29:36,795: Snapshot:0	Epoch:2	Loss:13.049	translation_Loss:13.049	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.62	Hits@10:38.23	Best:23.62
2024-12-27 18:29:43,244: Snapshot:0	Epoch:3	Loss:6.665	translation_Loss:6.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.55	Hits@10:39.42	Best:24.55
2024-12-27 18:29:50,193: Snapshot:0	Epoch:4	Loss:3.628	translation_Loss:3.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:39.74	Best:24.76
2024-12-27 18:29:56,620: Snapshot:0	Epoch:5	Loss:2.304	translation_Loss:2.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:39.92	Best:24.82
2024-12-27 18:30:03,036: Snapshot:0	Epoch:6	Loss:1.706	translation_Loss:1.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.93	Hits@10:40.01	Best:24.93
2024-12-27 18:30:09,534: Snapshot:0	Epoch:7	Loss:1.368	translation_Loss:1.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:39.83	Best:24.93
2024-12-27 18:30:15,974: Snapshot:0	Epoch:8	Loss:1.167	translation_Loss:1.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.67	Hits@10:39.66	Best:24.93
2024-12-27 18:30:22,353: Snapshot:0	Epoch:9	Loss:1.025	translation_Loss:1.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:39.65	Best:24.93
2024-12-27 18:30:28,770: Snapshot:0	Epoch:10	Loss:0.929	translation_Loss:0.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:39.62	Best:24.93
2024-12-27 18:30:35,151: Early Stopping! Snapshot: 0 Epoch: 11 Best Results: 24.93
2024-12-27 18:30:35,152: Start to training tokens! Snapshot: 0 Epoch: 11 Loss:0.836 MRR:24.67 Best Results: 24.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:30:35,152: Snapshot:0	Epoch:11	Loss:0.836	translation_Loss:0.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.67	Hits@10:39.71	Best:24.93
2024-12-27 18:30:42,233: Snapshot:0	Epoch:12	Loss:36.707	translation_Loss:36.67	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.67	Hits@10:39.71	Best:24.93
2024-12-27 18:30:48,779: End of token training: 0 Epoch: 13 Loss:36.737 MRR:24.67 Best Results: 24.93
2024-12-27 18:30:48,779: Snapshot:0	Epoch:13	Loss:36.737	translation_Loss:36.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.67	Hits@10:39.71	Best:24.93
2024-12-27 18:30:49,136: => loading checkpoint './checkpoint/FACTfact_0.01_1024_5000/0model_best.tar'
2024-12-27 18:30:51,900: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 |  0.16  | 0.2797 | 0.3312 |  0.392  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:31:17,029: Snapshot:1	Epoch:0	Loss:28.645	translation_Loss:19.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.649                                                   	MRR:19.4	Hits@10:32.08	Best:19.4
2024-12-27 18:31:24,222: Snapshot:1	Epoch:1	Loss:20.485	translation_Loss:15.805	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.68                                                   	MRR:19.93	Hits@10:32.96	Best:19.93
2024-12-27 18:31:31,426: Snapshot:1	Epoch:2	Loss:19.344	translation_Loss:14.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.644                                                   	MRR:20.06	Hits@10:33.09	Best:20.06
2024-12-27 18:31:38,705: Snapshot:1	Epoch:3	Loss:19.039	translation_Loss:14.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.665                                                   	MRR:20.07	Hits@10:33.13	Best:20.07
2024-12-27 18:31:46,006: Snapshot:1	Epoch:4	Loss:18.866	translation_Loss:14.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.652                                                   	MRR:20.13	Hits@10:33.06	Best:20.13
2024-12-27 18:31:53,248: Snapshot:1	Epoch:5	Loss:18.844	translation_Loss:14.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.669                                                   	MRR:20.16	Hits@10:33.18	Best:20.16
2024-12-27 18:32:00,378: Snapshot:1	Epoch:6	Loss:18.73	translation_Loss:14.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.689                                                   	MRR:20.1	Hits@10:33.14	Best:20.16
2024-12-27 18:32:07,521: Snapshot:1	Epoch:7	Loss:18.73	translation_Loss:14.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.706                                                   	MRR:20.15	Hits@10:33.18	Best:20.16
2024-12-27 18:32:14,628: Snapshot:1	Epoch:8	Loss:18.75	translation_Loss:14.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.696                                                   	MRR:20.1	Hits@10:33.09	Best:20.16
2024-12-27 18:32:21,778: Snapshot:1	Epoch:9	Loss:18.66	translation_Loss:13.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.713                                                   	MRR:20.1	Hits@10:33.16	Best:20.16
2024-12-27 18:32:28,928: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 20.16
2024-12-27 18:32:28,928: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:18.645 MRR:20.07 Best Results: 20.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:32:28,928: Snapshot:1	Epoch:10	Loss:18.645	translation_Loss:13.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.704                                                   	MRR:20.07	Hits@10:33.18	Best:20.16
2024-12-27 18:32:35,930: Snapshot:1	Epoch:11	Loss:45.303	translation_Loss:45.267	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.07	Hits@10:33.18	Best:20.16
2024-12-27 18:32:42,910: End of token training: 1 Epoch: 12 Loss:45.301 MRR:20.07 Best Results: 20.16
2024-12-27 18:32:42,910: Snapshot:1	Epoch:12	Loss:45.301	translation_Loss:45.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.07	Hits@10:33.18	Best:20.16
2024-12-27 18:32:43,268: => loading checkpoint './checkpoint/FACTfact_0.01_1024_5000/1model_best.tar'
2024-12-27 18:32:49,423: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2425 | 0.1604 | 0.2803 | 0.331  |  0.3923 |
|     1      | 0.2001 | 0.1274 | 0.2349 | 0.2761 |  0.3305 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:33:15,049: Snapshot:2	Epoch:0	Loss:26.791	translation_Loss:17.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.958                                                   	MRR:17.97	Hits@10:30.34	Best:17.97
2024-12-27 18:33:22,394: Snapshot:2	Epoch:1	Loss:21.671	translation_Loss:16.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.397                                                   	MRR:18.11	Hits@10:30.58	Best:18.11
2024-12-27 18:33:29,802: Snapshot:2	Epoch:2	Loss:21.352	translation_Loss:15.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.4                                                   	MRR:18.2	Hits@10:30.56	Best:18.2
2024-12-27 18:33:37,101: Snapshot:2	Epoch:3	Loss:21.296	translation_Loss:15.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.438                                                   	MRR:18.17	Hits@10:30.58	Best:18.2
2024-12-27 18:33:44,446: Snapshot:2	Epoch:4	Loss:21.292	translation_Loss:15.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.472                                                   	MRR:18.17	Hits@10:30.64	Best:18.2
2024-12-27 18:33:51,731: Snapshot:2	Epoch:5	Loss:21.319	translation_Loss:15.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.477                                                   	MRR:18.17	Hits@10:30.6	Best:18.2
2024-12-27 18:33:59,086: Snapshot:2	Epoch:6	Loss:21.332	translation_Loss:15.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.499                                                   	MRR:18.21	Hits@10:30.58	Best:18.21
2024-12-27 18:34:06,401: Snapshot:2	Epoch:7	Loss:21.338	translation_Loss:15.805	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.533                                                   	MRR:18.2	Hits@10:30.62	Best:18.21
2024-12-27 18:34:13,688: Snapshot:2	Epoch:8	Loss:21.326	translation_Loss:15.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.53                                                   	MRR:18.18	Hits@10:30.64	Best:18.21
2024-12-27 18:34:21,011: Snapshot:2	Epoch:9	Loss:21.315	translation_Loss:15.792	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.523                                                   	MRR:18.2	Hits@10:30.62	Best:18.21
2024-12-27 18:34:28,315: Snapshot:2	Epoch:10	Loss:21.275	translation_Loss:15.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.517                                                   	MRR:18.15	Hits@10:30.61	Best:18.21
2024-12-27 18:34:35,630: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 18.21
2024-12-27 18:34:35,630: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:21.328 MRR:18.18 Best Results: 18.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:34:35,631: Snapshot:2	Epoch:11	Loss:21.328	translation_Loss:15.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.546                                                   	MRR:18.18	Hits@10:30.66	Best:18.21
2024-12-27 18:34:42,797: Snapshot:2	Epoch:12	Loss:46.653	translation_Loss:46.616	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.18	Hits@10:30.66	Best:18.21
2024-12-27 18:34:49,991: End of token training: 2 Epoch: 13 Loss:46.655 MRR:18.18 Best Results: 18.21
2024-12-27 18:34:49,992: Snapshot:2	Epoch:13	Loss:46.655	translation_Loss:46.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.18	Hits@10:30.66	Best:18.21
2024-12-27 18:34:50,350: => loading checkpoint './checkpoint/FACTfact_0.01_1024_5000/2model_best.tar'
2024-12-27 18:34:59,289: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1626 | 0.2806 | 0.3311 |  0.3916 |
|     1      | 0.201  | 0.1284 | 0.2352 | 0.2773 |  0.3318 |
|     2      | 0.1796 | 0.1101 | 0.2094 | 0.2505 |  0.306  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:35:24,708: Snapshot:3	Epoch:0	Loss:25.6	translation_Loss:16.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.768                                                   	MRR:15.33	Hits@10:27.55	Best:15.33
2024-12-27 18:35:32,151: Snapshot:3	Epoch:1	Loss:21.485	translation_Loss:16.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.288                                                   	MRR:15.41	Hits@10:27.66	Best:15.41
2024-12-27 18:35:39,611: Snapshot:3	Epoch:2	Loss:21.472	translation_Loss:16.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.331                                                   	MRR:15.46	Hits@10:27.52	Best:15.46
2024-12-27 18:35:47,096: Snapshot:3	Epoch:3	Loss:21.534	translation_Loss:16.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.378                                                   	MRR:15.46	Hits@10:27.6	Best:15.46
2024-12-27 18:35:54,469: Snapshot:3	Epoch:4	Loss:21.528	translation_Loss:16.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.399                                                   	MRR:15.44	Hits@10:27.54	Best:15.46
2024-12-27 18:36:01,877: Snapshot:3	Epoch:5	Loss:21.574	translation_Loss:16.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.424                                                   	MRR:15.39	Hits@10:27.5	Best:15.46
2024-12-27 18:36:09,740: Snapshot:3	Epoch:6	Loss:21.562	translation_Loss:16.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.441                                                   	MRR:15.46	Hits@10:27.58	Best:15.46
2024-12-27 18:36:17,105: Early Stopping! Snapshot: 3 Epoch: 7 Best Results: 15.46
2024-12-27 18:36:17,105: Start to training tokens! Snapshot: 3 Epoch: 7 Loss:21.58 MRR:15.44 Best Results: 15.46
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:36:17,106: Snapshot:3	Epoch:7	Loss:21.58	translation_Loss:16.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.463                                                   	MRR:15.44	Hits@10:27.57	Best:15.46
2024-12-27 18:36:24,309: Snapshot:3	Epoch:8	Loss:46.722	translation_Loss:46.685	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.44	Hits@10:27.57	Best:15.46
2024-12-27 18:36:31,560: End of token training: 3 Epoch: 9 Loss:46.667 MRR:15.44 Best Results: 15.46
2024-12-27 18:36:31,560: Snapshot:3	Epoch:9	Loss:46.667	translation_Loss:46.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.44	Hits@10:27.57	Best:15.46
2024-12-27 18:36:31,835: => loading checkpoint './checkpoint/FACTfact_0.01_1024_5000/3model_best.tar'
2024-12-27 18:36:44,795: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2434 | 0.1621 | 0.2801 | 0.3318 |  0.3922 |
|     1      | 0.2014 | 0.1285 | 0.2356 | 0.2788 |  0.3335 |
|     2      | 0.1807 | 0.1103 | 0.2115 | 0.2535 |  0.3091 |
|     3      | 0.1542 | 0.0878 | 0.1769 | 0.2202 |  0.2799 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:37:10,800: Snapshot:4	Epoch:0	Loss:23.484	translation_Loss:15.261	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.224                                                   	MRR:12.83	Hits@10:25.57	Best:12.83
2024-12-27 18:37:18,287: Snapshot:4	Epoch:1	Loss:18.927	translation_Loss:14.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.691                                                   	MRR:13.02	Hits@10:25.66	Best:13.02
2024-12-27 18:37:25,861: Snapshot:4	Epoch:2	Loss:18.858	translation_Loss:14.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.736                                                   	MRR:13.04	Hits@10:25.68	Best:13.04
2024-12-27 18:37:33,287: Snapshot:4	Epoch:3	Loss:18.901	translation_Loss:14.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.781                                                   	MRR:12.98	Hits@10:25.54	Best:13.04
2024-12-27 18:37:40,762: Snapshot:4	Epoch:4	Loss:18.913	translation_Loss:14.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.803                                                   	MRR:12.94	Hits@10:25.57	Best:13.04
2024-12-27 18:37:48,280: Snapshot:4	Epoch:5	Loss:18.902	translation_Loss:14.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.824                                                   	MRR:12.93	Hits@10:25.5	Best:13.04
2024-12-27 18:37:55,703: Snapshot:4	Epoch:6	Loss:18.892	translation_Loss:14.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.829                                                   	MRR:12.97	Hits@10:25.64	Best:13.04
2024-12-27 18:38:03,122: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 13.04
2024-12-27 18:38:03,122: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:18.905 MRR:12.97 Best Results: 13.04
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:38:03,122: Snapshot:4	Epoch:7	Loss:18.905	translation_Loss:14.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.849                                                   	MRR:12.97	Hits@10:25.65	Best:13.04
2024-12-27 18:38:10,373: Snapshot:4	Epoch:8	Loss:42.889	translation_Loss:42.853	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.97	Hits@10:25.65	Best:13.04
2024-12-27 18:38:17,682: End of token training: 4 Epoch: 9 Loss:42.89 MRR:12.97 Best Results: 13.04
2024-12-27 18:38:17,682: Snapshot:4	Epoch:9	Loss:42.89	translation_Loss:42.89	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:12.97	Hits@10:25.65	Best:13.04
2024-12-27 18:38:18,028: => loading checkpoint './checkpoint/FACTfact_0.01_1024_5000/4model_best.tar'
2024-12-27 18:38:33,783: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1615 | 0.2792 | 0.3309 |  0.3924 |
|     1      | 0.201  | 0.1281 | 0.2347 | 0.2778 |  0.3335 |
|     2      | 0.1809 | 0.1102 | 0.2114 | 0.2541 |  0.3104 |
|     3      | 0.1556 | 0.0879 | 0.179  | 0.2233 |  0.283  |
|     4      | 0.132  | 0.0665 | 0.1474 | 0.1904 |  0.2573 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:38:33,786: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 |  0.16  | 0.2797 | 0.3312 |  0.392  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2425 | 0.1604 | 0.2803 | 0.331  |  0.3923 |
|     1      | 0.2001 | 0.1274 | 0.2349 | 0.2761 |  0.3305 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1626 | 0.2806 | 0.3311 |  0.3916 |
|     1      | 0.201  | 0.1284 | 0.2352 | 0.2773 |  0.3318 |
|     2      | 0.1796 | 0.1101 | 0.2094 | 0.2505 |  0.306  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2434 | 0.1621 | 0.2801 | 0.3318 |  0.3922 |
|     1      | 0.2014 | 0.1285 | 0.2356 | 0.2788 |  0.3335 |
|     2      | 0.1807 | 0.1103 | 0.2115 | 0.2535 |  0.3091 |
|     3      | 0.1542 | 0.0878 | 0.1769 | 0.2202 |  0.2799 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1615 | 0.2792 | 0.3309 |  0.3924 |
|     1      | 0.201  | 0.1281 | 0.2347 | 0.2778 |  0.3335 |
|     2      | 0.1809 | 0.1102 | 0.2114 | 0.2541 |  0.3104 |
|     3      | 0.1556 | 0.0879 | 0.179  | 0.2233 |  0.283  |
|     4      | 0.132  | 0.0665 | 0.1474 | 0.1904 |  0.2573 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:38:33,786: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 94.76073026657104  |   0.242   |     0.16     |     0.28     |     0.392     |
|    1     | 107.88287401199341 |   0.221   |    0.144     |    0.258     |     0.361     |
|    2     | 117.22840762138367 |   0.208   |    0.134     |    0.242     |     0.343     |
|    3     | 88.90294313430786  |   0.195   |    0.122     |    0.226     |     0.329     |
|    4     | 89.38110566139221  |   0.182   |    0.111     |     0.21     |     0.315     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:38:33,786: Sum_Training_Time:498.1560606956482
2024-12-27 18:38:33,786: Every_Training_Time:[94.76073026657104, 107.88287401199341, 117.22840762138367, 88.90294313430786, 89.38110566139221]
2024-12-27 18:38:33,786: Forward transfer: 0.14695 Backward transfer: 0.0011000000000000038
2024-12-27 18:39:12,744: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227183837/FACTfact_0.01_1024_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_1024_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_1024_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:39:22,740: Snapshot:0	Epoch:0	Loss:44.248	translation_Loss:44.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.4	Hits@10:26.99	Best:13.4
2024-12-27 18:39:29,238: Snapshot:0	Epoch:1	Loss:24.913	translation_Loss:24.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.3	Hits@10:35.4	Best:20.3
2024-12-27 18:39:35,670: Snapshot:0	Epoch:2	Loss:13.046	translation_Loss:13.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.68	Hits@10:38.22	Best:23.68
2024-12-27 18:39:42,124: Snapshot:0	Epoch:3	Loss:6.669	translation_Loss:6.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.65	Hits@10:39.35	Best:24.65
2024-12-27 18:39:49,054: Snapshot:0	Epoch:4	Loss:3.622	translation_Loss:3.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:39.78	Best:24.91
2024-12-27 18:39:55,459: Snapshot:0	Epoch:5	Loss:2.307	translation_Loss:2.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:39.91	Best:24.91
2024-12-27 18:40:01,856: Snapshot:0	Epoch:6	Loss:1.698	translation_Loss:1.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:39.92	Best:24.91
2024-12-27 18:40:08,386: Snapshot:0	Epoch:7	Loss:1.37	translation_Loss:1.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:40.03	Best:24.91
2024-12-27 18:40:14,773: Snapshot:0	Epoch:8	Loss:1.171	translation_Loss:1.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:39.81	Best:24.91
2024-12-27 18:40:21,174: Early Stopping! Snapshot: 0 Epoch: 9 Best Results: 24.91
2024-12-27 18:40:21,174: Start to training tokens! Snapshot: 0 Epoch: 9 Loss:1.03 MRR:24.82 Best Results: 24.91
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:40:21,175: Snapshot:0	Epoch:9	Loss:1.03	translation_Loss:1.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:39.67	Best:24.91
2024-12-27 18:40:28,212: Snapshot:0	Epoch:10	Loss:36.985	translation_Loss:36.949	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:39.67	Best:24.91
2024-12-27 18:40:34,803: End of token training: 0 Epoch: 11 Loss:36.927 MRR:24.82 Best Results: 24.91
2024-12-27 18:40:34,803: Snapshot:0	Epoch:11	Loss:36.927	translation_Loss:36.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.82	Hits@10:39.67	Best:24.91
2024-12-27 18:40:35,118: => loading checkpoint './checkpoint/FACTfact_0.01_1024_10000/0model_best.tar'
2024-12-27 18:40:37,950: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1589 | 0.2816 | 0.3302 |  0.3909 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:41:03,300: Snapshot:1	Epoch:0	Loss:34.288	translation_Loss:21.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.547                                                   	MRR:19.28	Hits@10:32.15	Best:19.28
2024-12-27 18:41:10,557: Snapshot:1	Epoch:1	Loss:22.019	translation_Loss:17.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.341                                                   	MRR:19.93	Hits@10:32.81	Best:19.93
2024-12-27 18:41:17,735: Snapshot:1	Epoch:2	Loss:20.933	translation_Loss:16.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.431                                                   	MRR:20.08	Hits@10:32.95	Best:20.08
2024-12-27 18:41:24,901: Snapshot:1	Epoch:3	Loss:20.635	translation_Loss:16.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.47                                                   	MRR:20.06	Hits@10:33.04	Best:20.08
2024-12-27 18:41:32,082: Snapshot:1	Epoch:4	Loss:20.466	translation_Loss:15.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.502                                                   	MRR:20.1	Hits@10:33.09	Best:20.1
2024-12-27 18:41:39,217: Snapshot:1	Epoch:5	Loss:20.408	translation_Loss:15.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.527                                                   	MRR:20.09	Hits@10:33.05	Best:20.1
2024-12-27 18:41:46,481: Snapshot:1	Epoch:6	Loss:20.337	translation_Loss:15.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.548                                                   	MRR:20.1	Hits@10:33.04	Best:20.1
2024-12-27 18:41:53,609: Snapshot:1	Epoch:7	Loss:20.332	translation_Loss:15.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.562                                                   	MRR:20.08	Hits@10:33.14	Best:20.1
2024-12-27 18:42:00,733: Snapshot:1	Epoch:8	Loss:20.302	translation_Loss:15.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.577                                                   	MRR:20.08	Hits@10:33.13	Best:20.1
2024-12-27 18:42:07,853: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 20.1
2024-12-27 18:42:07,853: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:20.257 MRR:20.07 Best Results: 20.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:42:07,853: Snapshot:1	Epoch:9	Loss:20.257	translation_Loss:15.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.582                                                   	MRR:20.07	Hits@10:33.12	Best:20.1
2024-12-27 18:42:14,851: Snapshot:1	Epoch:10	Loss:45.772	translation_Loss:45.736	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.07	Hits@10:33.12	Best:20.1
2024-12-27 18:42:21,850: End of token training: 1 Epoch: 11 Loss:45.754 MRR:20.07 Best Results: 20.1
2024-12-27 18:42:21,851: Snapshot:1	Epoch:11	Loss:45.754	translation_Loss:45.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.07	Hits@10:33.12	Best:20.1
2024-12-27 18:42:22,198: => loading checkpoint './checkpoint/FACTfact_0.01_1024_10000/1model_best.tar'
2024-12-27 18:42:28,507: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2422 | 0.1598 | 0.2823 | 0.331  |  0.3913 |
|     1      |  0.2   | 0.1268 | 0.2346 | 0.2775 |  0.3322 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:42:53,622: Snapshot:2	Epoch:0	Loss:32.369	translation_Loss:19.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.553                                                   	MRR:17.83	Hits@10:30.39	Best:17.83
2024-12-27 18:43:01,028: Snapshot:2	Epoch:1	Loss:23.306	translation_Loss:18.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.054                                                   	MRR:17.99	Hits@10:30.58	Best:17.99
2024-12-27 18:43:08,769: Snapshot:2	Epoch:2	Loss:23.096	translation_Loss:17.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.147                                                   	MRR:17.99	Hits@10:30.59	Best:17.99
2024-12-27 18:43:16,133: Snapshot:2	Epoch:3	Loss:23.063	translation_Loss:17.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.199                                                   	MRR:18.06	Hits@10:30.62	Best:18.06
2024-12-27 18:43:23,456: Snapshot:2	Epoch:4	Loss:23.037	translation_Loss:17.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.241                                                   	MRR:18.01	Hits@10:30.63	Best:18.06
2024-12-27 18:43:30,802: Snapshot:2	Epoch:5	Loss:23.101	translation_Loss:17.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.282                                                   	MRR:18.02	Hits@10:30.52	Best:18.06
2024-12-27 18:43:38,206: Snapshot:2	Epoch:6	Loss:23.078	translation_Loss:17.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.311                                                   	MRR:18.07	Hits@10:30.59	Best:18.07
2024-12-27 18:43:45,674: Snapshot:2	Epoch:7	Loss:23.086	translation_Loss:17.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.324                                                   	MRR:18.08	Hits@10:30.55	Best:18.08
2024-12-27 18:43:53,025: Snapshot:2	Epoch:8	Loss:23.081	translation_Loss:17.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.341                                                   	MRR:18.06	Hits@10:30.61	Best:18.08
2024-12-27 18:44:00,354: Snapshot:2	Epoch:9	Loss:23.113	translation_Loss:17.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.357                                                   	MRR:18.07	Hits@10:30.56	Best:18.08
2024-12-27 18:44:07,660: Snapshot:2	Epoch:10	Loss:23.129	translation_Loss:17.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.371                                                   	MRR:18.04	Hits@10:30.6	Best:18.08
2024-12-27 18:44:14,959: Snapshot:2	Epoch:11	Loss:23.091	translation_Loss:17.725	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.366                                                   	MRR:18.07	Hits@10:30.56	Best:18.08
2024-12-27 18:44:22,294: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 18.08
2024-12-27 18:44:22,294: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:23.092 MRR:18.04 Best Results: 18.08
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:44:22,295: Snapshot:2	Epoch:12	Loss:23.092	translation_Loss:17.708	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.383                                                   	MRR:18.04	Hits@10:30.62	Best:18.08
2024-12-27 18:44:29,482: Snapshot:2	Epoch:13	Loss:47.17	translation_Loss:47.133	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.04	Hits@10:30.62	Best:18.08
2024-12-27 18:44:36,638: End of token training: 2 Epoch: 14 Loss:47.169 MRR:18.04 Best Results: 18.08
2024-12-27 18:44:36,638: Snapshot:2	Epoch:14	Loss:47.169	translation_Loss:47.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.04	Hits@10:30.62	Best:18.08
2024-12-27 18:44:36,996: => loading checkpoint './checkpoint/FACTfact_0.01_1024_10000/2model_best.tar'
2024-12-27 18:44:46,181: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2422 | 0.1599 | 0.2825 | 0.3305 |  0.3915 |
|     1      | 0.2005 | 0.127  | 0.236  | 0.2782 |  0.3318 |
|     2      | 0.1796 | 0.1096 | 0.2094 | 0.2524 |  0.3099 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:45:11,803: Snapshot:3	Epoch:0	Loss:31.402	translation_Loss:19.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.307                                                   	MRR:14.99	Hits@10:27.17	Best:14.99
2024-12-27 18:45:19,242: Snapshot:3	Epoch:1	Loss:23.428	translation_Loss:18.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.925                                                   	MRR:15.05	Hits@10:27.21	Best:15.05
2024-12-27 18:45:26,822: Snapshot:3	Epoch:2	Loss:23.428	translation_Loss:18.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.028                                                   	MRR:15.1	Hits@10:27.27	Best:15.1
2024-12-27 18:45:34,210: Snapshot:3	Epoch:3	Loss:23.493	translation_Loss:18.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.114                                                   	MRR:15.06	Hits@10:27.25	Best:15.1
2024-12-27 18:45:42,120: Snapshot:3	Epoch:4	Loss:23.561	translation_Loss:18.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.162                                                   	MRR:15.11	Hits@10:27.2	Best:15.11
2024-12-27 18:45:49,582: Snapshot:3	Epoch:5	Loss:23.603	translation_Loss:18.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.195                                                   	MRR:15.09	Hits@10:27.23	Best:15.11
2024-12-27 18:45:56,993: Snapshot:3	Epoch:6	Loss:23.627	translation_Loss:18.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.227                                                   	MRR:15.08	Hits@10:27.24	Best:15.11
2024-12-27 18:46:04,357: Snapshot:3	Epoch:7	Loss:23.605	translation_Loss:18.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.235                                                   	MRR:15.09	Hits@10:27.23	Best:15.11
2024-12-27 18:46:11,791: Snapshot:3	Epoch:8	Loss:23.657	translation_Loss:18.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.253                                                   	MRR:15.12	Hits@10:27.27	Best:15.12
2024-12-27 18:46:19,274: Snapshot:3	Epoch:9	Loss:23.62	translation_Loss:18.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.26                                                   	MRR:15.13	Hits@10:27.24	Best:15.13
2024-12-27 18:46:26,732: Snapshot:3	Epoch:10	Loss:23.663	translation_Loss:18.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.277                                                   	MRR:15.09	Hits@10:27.29	Best:15.13
2024-12-27 18:46:34,120: Snapshot:3	Epoch:11	Loss:23.664	translation_Loss:18.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.282                                                   	MRR:15.07	Hits@10:27.28	Best:15.13
2024-12-27 18:46:41,507: Snapshot:3	Epoch:12	Loss:23.677	translation_Loss:18.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.299                                                   	MRR:15.1	Hits@10:27.19	Best:15.13
2024-12-27 18:46:48,937: Snapshot:3	Epoch:13	Loss:23.691	translation_Loss:18.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.307                                                   	MRR:15.07	Hits@10:27.23	Best:15.13
2024-12-27 18:46:56,318: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 15.13
2024-12-27 18:46:56,318: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:23.656 MRR:15.05 Best Results: 15.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:46:56,318: Snapshot:3	Epoch:14	Loss:23.656	translation_Loss:18.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.314                                                   	MRR:15.05	Hits@10:27.19	Best:15.13
2024-12-27 18:47:03,550: Snapshot:3	Epoch:15	Loss:47.393	translation_Loss:47.357	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.05	Hits@10:27.19	Best:15.13
2024-12-27 18:47:10,777: End of token training: 3 Epoch: 16 Loss:47.375 MRR:15.05 Best Results: 15.13
2024-12-27 18:47:10,777: Snapshot:3	Epoch:16	Loss:47.375	translation_Loss:47.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.05	Hits@10:27.19	Best:15.13
2024-12-27 18:47:11,135: => loading checkpoint './checkpoint/FACTfact_0.01_1024_10000/3model_best.tar'
2024-12-27 18:47:24,126: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2427 | 0.1608 | 0.2833 | 0.3312 |  0.3906 |
|     1      | 0.2009 | 0.1277 | 0.2357 | 0.2782 |  0.3331 |
|     2      | 0.1804 | 0.1098 | 0.2113 | 0.2544 |  0.3103 |
|     3      | 0.1527 | 0.0867 | 0.1757 | 0.2195 |  0.2775 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:47:49,789: Snapshot:4	Epoch:0	Loss:29.729	translation_Loss:18.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.627                                                   	MRR:12.22	Hits@10:24.34	Best:12.22
2024-12-27 18:47:57,279: Snapshot:4	Epoch:1	Loss:21.358	translation_Loss:17.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.245                                                   	MRR:12.29	Hits@10:24.29	Best:12.29
2024-12-27 18:48:04,805: Snapshot:4	Epoch:2	Loss:21.333	translation_Loss:17.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.326                                                   	MRR:12.31	Hits@10:24.29	Best:12.31
2024-12-27 18:48:12,680: Snapshot:4	Epoch:3	Loss:21.347	translation_Loss:16.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.398                                                   	MRR:12.38	Hits@10:24.35	Best:12.38
2024-12-27 18:48:20,176: Snapshot:4	Epoch:4	Loss:21.394	translation_Loss:16.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.446                                                   	MRR:12.37	Hits@10:24.31	Best:12.38
2024-12-27 18:48:27,625: Snapshot:4	Epoch:5	Loss:21.452	translation_Loss:16.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.486                                                   	MRR:12.34	Hits@10:24.47	Best:12.38
2024-12-27 18:48:35,081: Snapshot:4	Epoch:6	Loss:21.459	translation_Loss:16.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.512                                                   	MRR:12.31	Hits@10:24.35	Best:12.38
2024-12-27 18:48:42,559: Snapshot:4	Epoch:7	Loss:21.453	translation_Loss:16.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.526                                                   	MRR:12.35	Hits@10:24.32	Best:12.38
2024-12-27 18:48:50,095: Snapshot:4	Epoch:8	Loss:21.468	translation_Loss:16.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.539                                                   	MRR:12.39	Hits@10:24.31	Best:12.39
2024-12-27 18:48:57,534: Snapshot:4	Epoch:9	Loss:21.467	translation_Loss:16.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.562                                                   	MRR:12.38	Hits@10:24.24	Best:12.39
2024-12-27 18:49:04,954: Snapshot:4	Epoch:10	Loss:21.473	translation_Loss:16.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.569                                                   	MRR:12.36	Hits@10:24.42	Best:12.39
2024-12-27 18:49:12,404: Snapshot:4	Epoch:11	Loss:21.495	translation_Loss:16.923	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.573                                                   	MRR:12.28	Hits@10:24.36	Best:12.39
2024-12-27 18:49:19,865: Snapshot:4	Epoch:12	Loss:21.487	translation_Loss:16.896	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.591                                                   	MRR:12.38	Hits@10:24.34	Best:12.39
2024-12-27 18:49:27,820: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 12.39
2024-12-27 18:49:27,820: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:21.476 MRR:12.33 Best Results: 12.39
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:49:27,821: Snapshot:4	Epoch:13	Loss:21.476	translation_Loss:16.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.591                                                   	MRR:12.33	Hits@10:24.3	Best:12.39
2024-12-27 18:49:35,176: Snapshot:4	Epoch:14	Loss:44.11	translation_Loss:44.073	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.33	Hits@10:24.3	Best:12.39
2024-12-27 18:49:42,510: End of token training: 4 Epoch: 15 Loss:44.065 MRR:12.33 Best Results: 12.39
2024-12-27 18:49:42,510: Snapshot:4	Epoch:15	Loss:44.065	translation_Loss:44.065	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:12.33	Hits@10:24.3	Best:12.39
2024-12-27 18:49:42,792: => loading checkpoint './checkpoint/FACTfact_0.01_1024_10000/4model_best.tar'
2024-12-27 18:49:59,169: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2419 |  0.16  | 0.2821 | 0.331  |  0.3906 |
|     1      | 0.2005 | 0.1268 | 0.2361 | 0.279  |  0.3334 |
|     2      | 0.1808 | 0.1097 | 0.212  | 0.2555 |  0.3127 |
|     3      | 0.1541 | 0.0871 | 0.1782 | 0.2212 |  0.2808 |
|     4      | 0.1261 | 0.0649 | 0.1378 | 0.181  |  0.2441 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 18:49:59,171: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1589 | 0.2816 | 0.3302 |  0.3909 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2422 | 0.1598 | 0.2823 | 0.331  |  0.3913 |
|     1      |  0.2   | 0.1268 | 0.2346 | 0.2775 |  0.3322 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2422 | 0.1599 | 0.2825 | 0.3305 |  0.3915 |
|     1      | 0.2005 | 0.127  | 0.236  | 0.2782 |  0.3318 |
|     2      | 0.1796 | 0.1096 | 0.2094 | 0.2524 |  0.3099 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2427 | 0.1608 | 0.2833 | 0.3312 |  0.3906 |
|     1      | 0.2009 | 0.1277 | 0.2357 | 0.2782 |  0.3331 |
|     2      | 0.1804 | 0.1098 | 0.2113 | 0.2544 |  0.3103 |
|     3      | 0.1527 | 0.0867 | 0.1757 | 0.2195 |  0.2775 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2419 |  0.16  | 0.2821 | 0.331  |  0.3906 |
|     1      | 0.2005 | 0.1268 | 0.2361 | 0.279  |  0.3334 |
|     2      | 0.1808 | 0.1097 | 0.212  | 0.2555 |  0.3127 |
|     3      | 0.1541 | 0.0871 | 0.1782 | 0.2212 |  0.2808 |
|     4      | 0.1261 | 0.0649 | 0.1378 | 0.181  |  0.2441 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 18:49:59,172: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 82.05827927589417  |   0.241   |    0.159     |    0.282     |     0.391     |
|    1     | 100.76642513275146 |   0.221   |    0.143     |    0.258     |     0.362     |
|    2     | 124.74418878555298 |   0.207   |    0.132     |    0.243     |     0.344     |
|    3     | 140.89329838752747 |   0.194   |    0.121     |    0.227     |     0.328     |
|    4     |  134.582377910614  |   0.181   |     0.11     |    0.209     |     0.312     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 18:49:59,172: Sum_Training_Time:583.0445694923401
2024-12-27 18:49:59,172: Every_Training_Time:[82.05827927589417, 100.76642513275146, 124.74418878555298, 140.89329838752747, 134.582377910614]
2024-12-27 18:49:59,172: Forward transfer: 0.14645 Backward transfer: 0.0009499999999999925
2024-12-27 18:50:40,792: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227185005/FACTfact_0.01_2048_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_2048_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_2048_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 18:50:50,689: Snapshot:0	Epoch:0	Loss:22.916	translation_Loss:22.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.96	Hits@10:24.79	Best:11.96
2024-12-27 18:50:57,087: Snapshot:0	Epoch:1	Loss:13.133	translation_Loss:13.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.43	Hits@10:34.38	Best:18.43
2024-12-27 18:51:03,832: Snapshot:0	Epoch:2	Loss:7.277	translation_Loss:7.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.57	Hits@10:37.73	Best:22.57
2024-12-27 18:51:10,163: Snapshot:0	Epoch:3	Loss:3.979	translation_Loss:3.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:39.22	Best:24.33
2024-12-27 18:51:16,503: Snapshot:0	Epoch:4	Loss:2.204	translation_Loss:2.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.79	Hits@10:39.58	Best:24.79
2024-12-27 18:51:22,829: Snapshot:0	Epoch:5	Loss:1.358	translation_Loss:1.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.02	Hits@10:39.88	Best:25.02
2024-12-27 18:51:29,186: Snapshot:0	Epoch:6	Loss:0.956	translation_Loss:0.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.01	Hits@10:39.98	Best:25.02
2024-12-27 18:51:36,035: Snapshot:0	Epoch:7	Loss:0.731	translation_Loss:0.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.06	Hits@10:39.94	Best:25.06
2024-12-27 18:51:42,366: Snapshot:0	Epoch:8	Loss:0.593	translation_Loss:0.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.05	Hits@10:40.14	Best:25.06
2024-12-27 18:51:48,704: Snapshot:0	Epoch:9	Loss:0.514	translation_Loss:0.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:40.06	Best:25.06
2024-12-27 18:51:55,023: Snapshot:0	Epoch:10	Loss:0.46	translation_Loss:0.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:40.15	Best:25.06
2024-12-27 18:52:01,337: Snapshot:0	Epoch:11	Loss:0.408	translation_Loss:0.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:40.01	Best:25.06
2024-12-27 18:52:07,645: Early Stopping! Snapshot: 0 Epoch: 12 Best Results: 25.06
2024-12-27 18:52:07,646: Start to training tokens! Snapshot: 0 Epoch: 12 Loss:0.375 MRR:24.8 Best Results: 25.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:52:07,646: Snapshot:0	Epoch:12	Loss:0.375	translation_Loss:0.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:39.81	Best:25.06
2024-12-27 18:52:15,043: Snapshot:0	Epoch:13	Loss:18.748	translation_Loss:18.712	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:39.81	Best:25.06
2024-12-27 18:52:21,423: End of token training: 0 Epoch: 14 Loss:18.701 MRR:24.8 Best Results: 25.06
2024-12-27 18:52:21,423: Snapshot:0	Epoch:14	Loss:18.701	translation_Loss:18.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.8	Hits@10:39.81	Best:25.06
2024-12-27 18:52:21,754: => loading checkpoint './checkpoint/FACTfact_0.01_2048_1000/0model_best.tar'
2024-12-27 18:52:24,554: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1613 | 0.2827 | 0.331  |  0.3929 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:52:49,441: Snapshot:1	Epoch:0	Loss:13.896	translation_Loss:10.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.619                                                   	MRR:19.58	Hits@10:32.55	Best:19.58
2024-12-27 18:52:56,453: Snapshot:1	Epoch:1	Loss:10.825	translation_Loss:7.972	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.853                                                   	MRR:20.31	Hits@10:33.33	Best:20.31
2024-12-27 18:53:03,416: Snapshot:1	Epoch:2	Loss:10.137	translation_Loss:7.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.759                                                   	MRR:20.46	Hits@10:33.44	Best:20.46
2024-12-27 18:53:10,419: Snapshot:1	Epoch:3	Loss:9.927	translation_Loss:7.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.727                                                   	MRR:20.5	Hits@10:33.51	Best:20.5
2024-12-27 18:53:17,490: Snapshot:1	Epoch:4	Loss:9.842	translation_Loss:7.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.706                                                   	MRR:20.5	Hits@10:33.58	Best:20.5
2024-12-27 18:53:24,503: Snapshot:1	Epoch:5	Loss:9.826	translation_Loss:7.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.717                                                   	MRR:20.54	Hits@10:33.57	Best:20.54
2024-12-27 18:53:31,504: Snapshot:1	Epoch:6	Loss:9.81	translation_Loss:7.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.718                                                   	MRR:20.57	Hits@10:33.62	Best:20.57
2024-12-27 18:53:38,472: Snapshot:1	Epoch:7	Loss:9.78	translation_Loss:7.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.719                                                   	MRR:20.53	Hits@10:33.59	Best:20.57
2024-12-27 18:53:45,480: Snapshot:1	Epoch:8	Loss:9.778	translation_Loss:7.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.721                                                   	MRR:20.55	Hits@10:33.57	Best:20.57
2024-12-27 18:53:52,408: Snapshot:1	Epoch:9	Loss:9.75	translation_Loss:7.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.715                                                   	MRR:20.51	Hits@10:33.6	Best:20.57
2024-12-27 18:53:59,419: Snapshot:1	Epoch:10	Loss:9.749	translation_Loss:7.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.717                                                   	MRR:20.56	Hits@10:33.5	Best:20.57
2024-12-27 18:54:06,345: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 20.57
2024-12-27 18:54:06,346: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:9.742 MRR:20.54 Best Results: 20.57
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:54:06,346: Snapshot:1	Epoch:11	Loss:9.742	translation_Loss:7.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.716                                                   	MRR:20.54	Hits@10:33.52	Best:20.57
2024-12-27 18:54:13,669: Snapshot:1	Epoch:12	Loss:22.953	translation_Loss:22.918	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.54	Hits@10:33.52	Best:20.57
2024-12-27 18:54:20,498: End of token training: 1 Epoch: 13 Loss:22.921 MRR:20.54 Best Results: 20.57
2024-12-27 18:54:20,498: Snapshot:1	Epoch:13	Loss:22.921	translation_Loss:22.921	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.54	Hits@10:33.52	Best:20.57
2024-12-27 18:54:20,779: => loading checkpoint './checkpoint/FACTfact_0.01_2048_1000/1model_best.tar'
2024-12-27 18:54:26,706: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2454 | 0.1631 | 0.2836 | 0.3336 |  0.3963 |
|     1      | 0.2061 | 0.1328 | 0.2412 | 0.284  |  0.3375 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:54:51,803: Snapshot:2	Epoch:0	Loss:12.632	translation_Loss:8.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.848                                                   	MRR:18.71	Hits@10:32.05	Best:18.71
2024-12-27 18:54:59,093: Snapshot:2	Epoch:1	Loss:10.915	translation_Loss:7.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.239                                                   	MRR:18.93	Hits@10:32.25	Best:18.93
2024-12-27 18:55:06,403: Snapshot:2	Epoch:2	Loss:10.769	translation_Loss:7.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.233                                                   	MRR:19.03	Hits@10:32.36	Best:19.03
2024-12-27 18:55:13,571: Snapshot:2	Epoch:3	Loss:10.731	translation_Loss:7.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.229                                                   	MRR:18.93	Hits@10:32.21	Best:19.03
2024-12-27 18:55:20,698: Snapshot:2	Epoch:4	Loss:10.715	translation_Loss:7.485	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.23                                                   	MRR:18.99	Hits@10:32.4	Best:19.03
2024-12-27 18:55:27,833: Snapshot:2	Epoch:5	Loss:10.731	translation_Loss:7.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.232                                                   	MRR:19.09	Hits@10:32.33	Best:19.09
2024-12-27 18:55:34,968: Snapshot:2	Epoch:6	Loss:10.72	translation_Loss:7.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.236                                                   	MRR:19.05	Hits@10:32.39	Best:19.09
2024-12-27 18:55:42,150: Snapshot:2	Epoch:7	Loss:10.699	translation_Loss:7.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.234                                                   	MRR:19.05	Hits@10:32.39	Best:19.09
2024-12-27 18:55:49,344: Snapshot:2	Epoch:8	Loss:10.709	translation_Loss:7.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.235                                                   	MRR:19.11	Hits@10:32.43	Best:19.11
2024-12-27 18:55:56,585: Snapshot:2	Epoch:9	Loss:10.705	translation_Loss:7.469	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.237                                                   	MRR:19.13	Hits@10:32.39	Best:19.13
2024-12-27 18:56:03,724: Snapshot:2	Epoch:10	Loss:10.71	translation_Loss:7.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.235                                                   	MRR:19.12	Hits@10:32.45	Best:19.13
2024-12-27 18:56:11,406: Snapshot:2	Epoch:11	Loss:10.704	translation_Loss:7.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.242                                                   	MRR:19.13	Hits@10:32.33	Best:19.13
2024-12-27 18:56:18,542: Snapshot:2	Epoch:12	Loss:10.698	translation_Loss:7.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.237                                                   	MRR:19.09	Hits@10:32.34	Best:19.13
2024-12-27 18:56:25,714: Snapshot:2	Epoch:13	Loss:10.7	translation_Loss:7.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.239                                                   	MRR:19.11	Hits@10:32.27	Best:19.13
2024-12-27 18:56:32,843: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 19.13
2024-12-27 18:56:32,844: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:10.694 MRR:19.07 Best Results: 19.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 18:56:32,844: Snapshot:2	Epoch:14	Loss:10.694	translation_Loss:7.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.236                                                   	MRR:19.07	Hits@10:32.32	Best:19.13
2024-12-27 18:56:39,827: Snapshot:2	Epoch:15	Loss:23.508	translation_Loss:23.471	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.07	Hits@10:32.32	Best:19.13
2024-12-27 18:56:47,244: End of token training: 2 Epoch: 16 Loss:23.495 MRR:19.07 Best Results: 19.13
2024-12-27 18:56:47,245: Snapshot:2	Epoch:16	Loss:23.495	translation_Loss:23.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.07	Hits@10:32.32	Best:19.13
2024-12-27 18:56:47,526: => loading checkpoint './checkpoint/FACTfact_0.01_2048_1000/2model_best.tar'
2024-12-27 18:56:56,664: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2434 | 0.1595 | 0.2829 | 0.3334 |  0.397  |
|     1      | 0.2076 | 0.1323 | 0.2438 | 0.2886 |  0.343  |
|     2      | 0.1898 | 0.1171 | 0.2217 | 0.2668 |  0.3234 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 18:57:22,038: Snapshot:3	Epoch:0	Loss:11.18	translation_Loss:7.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.789                                                   	MRR:16.84	Hits@10:30.75	Best:16.84
2024-12-27 18:57:29,320: Snapshot:3	Epoch:1	Loss:10.101	translation_Loss:6.879	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.222                                                   	MRR:16.93	Hits@10:30.94	Best:16.93
2024-12-27 18:57:36,546: Snapshot:3	Epoch:2	Loss:10.138	translation_Loss:6.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.235                                                   	MRR:16.94	Hits@10:30.92	Best:16.94
2024-12-27 18:57:43,915: Snapshot:3	Epoch:3	Loss:10.147	translation_Loss:6.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.239                                                   	MRR:16.96	Hits@10:31.0	Best:16.96
2024-12-27 18:57:51,133: Snapshot:3	Epoch:4	Loss:10.162	translation_Loss:6.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.25                                                   	MRR:16.99	Hits@10:30.92	Best:16.99
2024-12-27 18:57:58,479: Snapshot:3	Epoch:5	Loss:10.154	translation_Loss:6.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.247                                                   	MRR:16.99	Hits@10:31.02	Best:16.99
2024-12-27 18:58:05,631: Snapshot:3	Epoch:6	Loss:10.157	translation_Loss:6.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.247                                                   	MRR:16.97	Hits@10:30.89	Best:16.99
2024-12-27 18:58:12,782: Snapshot:3	Epoch:7	Loss:10.16	translation_Loss:6.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.253                                                   	MRR:16.99	Hits@10:30.87	Best:16.99
2024-12-27 18:58:20,020: Snapshot:3	Epoch:8	Loss:10.163	translation_Loss:6.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.255                                                   	MRR:17.01	Hits@10:30.95	Best:17.01
2024-12-27 18:58:27,204: Snapshot:3	Epoch:9	Loss:10.177	translation_Loss:6.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.263                                                   	MRR:16.95	Hits@10:30.9	Best:17.01
2024-12-27 18:58:34,376: Snapshot:3	Epoch:10	Loss:10.177	translation_Loss:6.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.256                                                   	MRR:16.96	Hits@10:30.94	Best:17.01
2024-12-27 18:58:41,600: Snapshot:3	Epoch:11	Loss:10.159	translation_Loss:6.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.26                                                   	MRR:16.91	Hits@10:30.81	Best:17.01
2024-12-27 18:58:48,779: Snapshot:3	Epoch:12	Loss:10.179	translation_Loss:6.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.259                                                   	MRR:17.0	Hits@10:30.88	Best:17.01
2024-12-27 18:58:56,008: Snapshot:3	Epoch:13	Loss:10.178	translation_Loss:6.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.262                                                   	MRR:17.04	Hits@10:30.8	Best:17.04
2024-12-27 18:59:03,664: Snapshot:3	Epoch:14	Loss:10.177	translation_Loss:6.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.259                                                   	MRR:16.94	Hits@10:30.92	Best:17.04
2024-12-27 18:59:10,823: Snapshot:3	Epoch:15	Loss:10.159	translation_Loss:6.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.254                                                   	MRR:16.96	Hits@10:30.82	Best:17.04
2024-12-27 18:59:17,990: Snapshot:3	Epoch:16	Loss:10.166	translation_Loss:6.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.26                                                   	MRR:17.04	Hits@10:31.01	Best:17.04
2024-12-27 18:59:25,155: Snapshot:3	Epoch:17	Loss:10.174	translation_Loss:6.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.253                                                   	MRR:17.03	Hits@10:30.91	Best:17.04
2024-12-27 18:59:32,404: Snapshot:3	Epoch:18	Loss:10.16	translation_Loss:6.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.252                                                   	MRR:17.1	Hits@10:31.0	Best:17.1
2024-12-27 18:59:39,705: Snapshot:3	Epoch:19	Loss:10.17	translation_Loss:6.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.251                                                   	MRR:16.98	Hits@10:30.96	Best:17.1
2024-12-27 18:59:47,417: Snapshot:3	Epoch:20	Loss:10.155	translation_Loss:6.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.255                                                   	MRR:17.05	Hits@10:30.91	Best:17.1
2024-12-27 18:59:54,612: Snapshot:3	Epoch:21	Loss:10.162	translation_Loss:6.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.252                                                   	MRR:16.98	Hits@10:30.98	Best:17.1
2024-12-27 19:00:01,803: Snapshot:3	Epoch:22	Loss:10.165	translation_Loss:6.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.261                                                   	MRR:17.0	Hits@10:30.96	Best:17.1
2024-12-27 19:00:09,190: Early Stopping! Snapshot: 3 Epoch: 23 Best Results: 17.1
2024-12-27 19:00:09,190: Start to training tokens! Snapshot: 3 Epoch: 23 Loss:10.172 MRR:16.99 Best Results: 17.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:00:09,191: Snapshot:3	Epoch:23	Loss:10.172	translation_Loss:6.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.265                                                   	MRR:16.99	Hits@10:30.89	Best:17.1
2024-12-27 19:00:16,246: Snapshot:3	Epoch:24	Loss:23.114	translation_Loss:23.078	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.99	Hits@10:30.89	Best:17.1
2024-12-27 19:00:23,731: End of token training: 3 Epoch: 25 Loss:23.079 MRR:16.99 Best Results: 17.1
2024-12-27 19:00:23,732: Snapshot:3	Epoch:25	Loss:23.079	translation_Loss:23.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:16.99	Hits@10:30.89	Best:17.1
2024-12-27 19:00:24,012: => loading checkpoint './checkpoint/FACTfact_0.01_2048_1000/3model_best.tar'
2024-12-27 19:00:37,086: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2425 | 0.1595 | 0.2811 | 0.3295 |  0.395  |
|     1      | 0.2082 | 0.1323 | 0.2459 | 0.2895 |  0.3468 |
|     2      | 0.1924 | 0.118  | 0.2247 | 0.272  |  0.3312 |
|     3      | 0.1722 | 0.0982 | 0.2002 | 0.2474 |  0.314  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:01:01,825: Snapshot:4	Epoch:0	Loss:8.246	translation_Loss:4.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.377                                                   	MRR:17.76	Hits@10:35.7	Best:17.76
2024-12-27 19:01:09,658: Snapshot:4	Epoch:1	Loss:6.922	translation_Loss:4.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.741                                                   	MRR:18.07	Hits@10:36.07	Best:18.07
2024-12-27 19:01:16,905: Snapshot:4	Epoch:2	Loss:6.896	translation_Loss:4.145	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.751                                                   	MRR:17.98	Hits@10:35.74	Best:18.07
2024-12-27 19:01:24,154: Snapshot:4	Epoch:3	Loss:6.892	translation_Loss:4.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.755                                                   	MRR:17.96	Hits@10:36.04	Best:18.07
2024-12-27 19:01:31,382: Snapshot:4	Epoch:4	Loss:6.892	translation_Loss:4.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.759                                                   	MRR:18.03	Hits@10:35.94	Best:18.07
2024-12-27 19:01:38,666: Snapshot:4	Epoch:5	Loss:6.891	translation_Loss:4.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.762                                                   	MRR:18.1	Hits@10:36.07	Best:18.1
2024-12-27 19:01:46,427: Snapshot:4	Epoch:6	Loss:6.887	translation_Loss:4.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.772                                                   	MRR:17.95	Hits@10:36.09	Best:18.1
2024-12-27 19:01:53,749: Snapshot:4	Epoch:7	Loss:6.888	translation_Loss:4.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.771                                                   	MRR:17.88	Hits@10:35.95	Best:18.1
2024-12-27 19:02:01,053: Snapshot:4	Epoch:8	Loss:6.883	translation_Loss:4.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.77                                                   	MRR:18.07	Hits@10:36.09	Best:18.1
2024-12-27 19:02:08,271: Snapshot:4	Epoch:9	Loss:6.88	translation_Loss:4.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.769                                                   	MRR:18.07	Hits@10:36.11	Best:18.1
2024-12-27 19:02:15,526: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 18.1
2024-12-27 19:02:15,527: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:6.877 MRR:17.99 Best Results: 18.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:02:15,527: Snapshot:4	Epoch:10	Loss:6.877	translation_Loss:4.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.763                                                   	MRR:17.99	Hits@10:35.99	Best:18.1
2024-12-27 19:02:22,648: Snapshot:4	Epoch:11	Loss:19.754	translation_Loss:19.717	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.99	Hits@10:35.99	Best:18.1
2024-12-27 19:02:30,392: End of token training: 4 Epoch: 12 Loss:19.723 MRR:17.99 Best Results: 18.1
2024-12-27 19:02:30,392: Snapshot:4	Epoch:12	Loss:19.723	translation_Loss:19.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.99	Hits@10:35.99	Best:18.1
2024-12-27 19:02:30,671: => loading checkpoint './checkpoint/FACTfact_0.01_2048_1000/4model_best.tar'
2024-12-27 19:02:47,019: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2374 | 0.1548 | 0.2748 | 0.3239 |  0.3912 |
|     1      | 0.2036 | 0.1273 | 0.2383 | 0.2855 |  0.3425 |
|     2      | 0.1895 | 0.116  | 0.2192 | 0.268  |  0.3287 |
|     3      | 0.1718 | 0.0957 | 0.1983 | 0.2498 |  0.3204 |
|     4      | 0.1825 | 0.0942 | 0.209  | 0.2723 |  0.3579 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:02:47,021: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.1613 | 0.2827 | 0.331  |  0.3929 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2454 | 0.1631 | 0.2836 | 0.3336 |  0.3963 |
|     1      | 0.2061 | 0.1328 | 0.2412 | 0.284  |  0.3375 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2434 | 0.1595 | 0.2829 | 0.3334 |  0.397  |
|     1      | 0.2076 | 0.1323 | 0.2438 | 0.2886 |  0.343  |
|     2      | 0.1898 | 0.1171 | 0.2217 | 0.2668 |  0.3234 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2425 | 0.1595 | 0.2811 | 0.3295 |  0.395  |
|     1      | 0.2082 | 0.1323 | 0.2459 | 0.2895 |  0.3468 |
|     2      | 0.1924 | 0.118  | 0.2247 | 0.272  |  0.3312 |
|     3      | 0.1722 | 0.0982 | 0.2002 | 0.2474 |  0.314  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2374 | 0.1548 | 0.2748 | 0.3239 |  0.3912 |
|     1      | 0.2036 | 0.1273 | 0.2383 | 0.2855 |  0.3425 |
|     2      | 0.1895 | 0.116  | 0.2192 | 0.268  |  0.3287 |
|     3      | 0.1718 | 0.0957 | 0.1983 | 0.2498 |  0.3204 |
|     4      | 0.1825 | 0.0942 | 0.209  | 0.2723 |  0.3579 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:02:47,022: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 100.63083529472351 |   0.244   |    0.161     |    0.283     |     0.393     |
|    1     | 112.80766606330872 |   0.226   |    0.148     |    0.262     |     0.367     |
|    2     | 137.20964121818542 |   0.214   |    0.136     |    0.249     |     0.354     |
|    3     | 203.68494129180908 |   0.204   |    0.127     |    0.238     |     0.347     |
|    4     | 109.85728764533997 |   0.197   |    0.118     |    0.228     |     0.348     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:02:47,022: Sum_Training_Time:664.1903715133667
2024-12-27 19:02:47,022: Every_Training_Time:[100.63083529472351, 112.80766606330872, 137.20964121818542, 203.68494129180908, 109.85728764533997]
2024-12-27 19:02:47,022: Forward transfer: 0.15755 Backward transfer: -0.002374999999999995
2024-12-27 19:03:26,239: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227190250/FACTfact_0.01_2048_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_2048_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_2048_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:03:36,251: Snapshot:0	Epoch:0	Loss:22.916	translation_Loss:22.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.96	Hits@10:24.8	Best:11.96
2024-12-27 19:03:42,758: Snapshot:0	Epoch:1	Loss:13.133	translation_Loss:13.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.44	Hits@10:34.42	Best:18.44
2024-12-27 19:03:49,617: Snapshot:0	Epoch:2	Loss:7.278	translation_Loss:7.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.56	Hits@10:37.75	Best:22.56
2024-12-27 19:03:56,087: Snapshot:0	Epoch:3	Loss:3.98	translation_Loss:3.98	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.39	Hits@10:39.19	Best:24.39
2024-12-27 19:04:02,541: Snapshot:0	Epoch:4	Loss:2.202	translation_Loss:2.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.83	Hits@10:39.74	Best:24.83
2024-12-27 19:04:09,001: Snapshot:0	Epoch:5	Loss:1.357	translation_Loss:1.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:40.12	Best:24.98
2024-12-27 19:04:15,424: Snapshot:0	Epoch:6	Loss:0.955	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:40.25	Best:24.98
2024-12-27 19:04:22,326: Snapshot:0	Epoch:7	Loss:0.726	translation_Loss:0.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:40.17	Best:24.98
2024-12-27 19:04:28,766: Snapshot:0	Epoch:8	Loss:0.597	translation_Loss:0.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.06	Hits@10:40.23	Best:25.06
2024-12-27 19:04:35,200: Snapshot:0	Epoch:9	Loss:0.515	translation_Loss:0.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:40.06	Best:25.06
2024-12-27 19:04:41,611: Snapshot:0	Epoch:10	Loss:0.46	translation_Loss:0.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:39.98	Best:25.06
2024-12-27 19:04:48,090: Snapshot:0	Epoch:11	Loss:0.406	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:40.09	Best:25.06
2024-12-27 19:04:54,533: Snapshot:0	Epoch:12	Loss:0.372	translation_Loss:0.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.83	Hits@10:39.81	Best:25.06
2024-12-27 19:05:01,462: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 25.06
2024-12-27 19:05:01,462: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.347 MRR:24.81 Best Results: 25.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:05:01,463: Snapshot:0	Epoch:13	Loss:0.347	translation_Loss:0.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.81	Hits@10:39.98	Best:25.06
2024-12-27 19:05:08,548: Snapshot:0	Epoch:14	Loss:18.732	translation_Loss:18.696	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.81	Hits@10:39.98	Best:25.06
2024-12-27 19:05:15,041: End of token training: 0 Epoch: 15 Loss:18.707 MRR:24.81 Best Results: 25.06
2024-12-27 19:05:15,041: Snapshot:0	Epoch:15	Loss:18.707	translation_Loss:18.707	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.81	Hits@10:39.98	Best:25.06
2024-12-27 19:05:15,351: => loading checkpoint './checkpoint/FACTfact_0.01_2048_5000/0model_best.tar'
2024-12-27 19:05:18,254: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2436 | 0.1609 | 0.2831 | 0.3302 |  0.3932 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:05:43,202: Snapshot:1	Epoch:0	Loss:16.711	translation_Loss:10.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.365                                                   	MRR:19.22	Hits@10:31.95	Best:19.22
2024-12-27 19:05:50,378: Snapshot:1	Epoch:1	Loss:10.179	translation_Loss:8.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.041                                                   	MRR:19.93	Hits@10:32.85	Best:19.93
2024-12-27 19:05:57,469: Snapshot:1	Epoch:2	Loss:9.496	translation_Loss:7.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.931                                                   	MRR:20.19	Hits@10:33.07	Best:20.19
2024-12-27 19:06:04,619: Snapshot:1	Epoch:3	Loss:9.333	translation_Loss:7.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.95                                                   	MRR:20.2	Hits@10:33.06	Best:20.2
2024-12-27 19:06:11,723: Snapshot:1	Epoch:4	Loss:9.275	translation_Loss:7.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.954                                                   	MRR:20.19	Hits@10:33.1	Best:20.2
2024-12-27 19:06:18,794: Snapshot:1	Epoch:5	Loss:9.265	translation_Loss:7.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.971                                                   	MRR:20.16	Hits@10:33.22	Best:20.2
2024-12-27 19:06:26,284: Snapshot:1	Epoch:6	Loss:9.259	translation_Loss:7.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.984                                                   	MRR:20.16	Hits@10:33.19	Best:20.2
2024-12-27 19:06:33,408: Snapshot:1	Epoch:7	Loss:9.239	translation_Loss:7.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.988                                                   	MRR:20.23	Hits@10:33.11	Best:20.23
2024-12-27 19:06:40,591: Snapshot:1	Epoch:8	Loss:9.225	translation_Loss:7.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.996                                                   	MRR:20.21	Hits@10:33.1	Best:20.23
2024-12-27 19:06:47,687: Snapshot:1	Epoch:9	Loss:9.215	translation_Loss:7.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.996                                                   	MRR:20.14	Hits@10:33.12	Best:20.23
2024-12-27 19:06:54,766: Snapshot:1	Epoch:10	Loss:9.22	translation_Loss:7.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.003                                                   	MRR:20.2	Hits@10:33.15	Best:20.23
2024-12-27 19:07:02,314: Snapshot:1	Epoch:11	Loss:9.211	translation_Loss:7.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.003                                                   	MRR:20.22	Hits@10:33.11	Best:20.23
2024-12-27 19:07:09,378: Early Stopping! Snapshot: 1 Epoch: 12 Best Results: 20.23
2024-12-27 19:07:09,379: Start to training tokens! Snapshot: 1 Epoch: 12 Loss:9.209 MRR:20.18 Best Results: 20.23
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:07:09,379: Snapshot:1	Epoch:12	Loss:9.209	translation_Loss:7.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.003                                                   	MRR:20.18	Hits@10:33.19	Best:20.23
2024-12-27 19:07:16,336: Snapshot:1	Epoch:13	Loss:23.059	translation_Loss:23.023	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.18	Hits@10:33.19	Best:20.23
2024-12-27 19:07:23,270: End of token training: 1 Epoch: 14 Loss:23.025 MRR:20.18 Best Results: 20.23
2024-12-27 19:07:23,271: Snapshot:1	Epoch:14	Loss:23.025	translation_Loss:23.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.18	Hits@10:33.19	Best:20.23
2024-12-27 19:07:23,551: => loading checkpoint './checkpoint/FACTfact_0.01_2048_5000/1model_best.tar'
2024-12-27 19:07:29,614: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2449 | 0.1619 | 0.2843 | 0.3322 |  0.3942 |
|     1      | 0.2017 | 0.1291 | 0.2361 | 0.2762 |  0.3328 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:07:55,556: Snapshot:2	Epoch:0	Loss:15.663	translation_Loss:9.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.443                                                   	MRR:18.06	Hits@10:30.78	Best:18.06
2024-12-27 19:08:02,831: Snapshot:2	Epoch:1	Loss:10.667	translation_Loss:8.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.354                                                   	MRR:18.3	Hits@10:31.04	Best:18.3
2024-12-27 19:08:10,072: Snapshot:2	Epoch:2	Loss:10.454	translation_Loss:8.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.268                                                   	MRR:18.25	Hits@10:31.08	Best:18.3
2024-12-27 19:08:17,326: Snapshot:2	Epoch:3	Loss:10.425	translation_Loss:8.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.276                                                   	MRR:18.26	Hits@10:31.1	Best:18.3
2024-12-27 19:08:24,615: Snapshot:2	Epoch:4	Loss:10.431	translation_Loss:8.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.292                                                   	MRR:18.32	Hits@10:31.15	Best:18.32
2024-12-27 19:08:31,843: Snapshot:2	Epoch:5	Loss:10.448	translation_Loss:8.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.31                                                   	MRR:18.28	Hits@10:31.1	Best:18.32
2024-12-27 19:08:39,074: Snapshot:2	Epoch:6	Loss:10.478	translation_Loss:8.159	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.319                                                   	MRR:18.32	Hits@10:31.11	Best:18.32
2024-12-27 19:08:46,410: Snapshot:2	Epoch:7	Loss:10.453	translation_Loss:8.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.319                                                   	MRR:18.36	Hits@10:31.12	Best:18.36
2024-12-27 19:08:53,697: Snapshot:2	Epoch:8	Loss:10.475	translation_Loss:8.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.327                                                   	MRR:18.39	Hits@10:31.16	Best:18.39
2024-12-27 19:09:00,923: Snapshot:2	Epoch:9	Loss:10.468	translation_Loss:8.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.336                                                   	MRR:18.37	Hits@10:31.04	Best:18.39
2024-12-27 19:09:08,139: Snapshot:2	Epoch:10	Loss:10.467	translation_Loss:8.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.34                                                   	MRR:18.36	Hits@10:31.1	Best:18.39
2024-12-27 19:09:15,377: Snapshot:2	Epoch:11	Loss:10.479	translation_Loss:8.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.345                                                   	MRR:18.35	Hits@10:31.1	Best:18.39
2024-12-27 19:09:22,632: Snapshot:2	Epoch:12	Loss:10.479	translation_Loss:8.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.341                                                   	MRR:18.35	Hits@10:31.06	Best:18.39
2024-12-27 19:09:30,388: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 18.39
2024-12-27 19:09:30,388: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:10.483 MRR:18.36 Best Results: 18.39
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:09:30,389: Snapshot:2	Epoch:13	Loss:10.483	translation_Loss:8.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.344                                                   	MRR:18.36	Hits@10:31.1	Best:18.39
2024-12-27 19:09:37,527: Snapshot:2	Epoch:14	Loss:23.769	translation_Loss:23.732	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.36	Hits@10:31.1	Best:18.39
2024-12-27 19:09:44,771: End of token training: 2 Epoch: 15 Loss:23.721 MRR:18.36 Best Results: 18.39
2024-12-27 19:09:44,772: Snapshot:2	Epoch:15	Loss:23.721	translation_Loss:23.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.36	Hits@10:31.1	Best:18.39
2024-12-27 19:09:45,049: => loading checkpoint './checkpoint/FACTfact_0.01_2048_5000/2model_best.tar'
2024-12-27 19:09:54,907: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1629 | 0.2847 | 0.3317 |  0.3947 |
|     1      | 0.2031 | 0.1301 | 0.2381 | 0.2786 |  0.3353 |
|     2      | 0.1829 | 0.1133 | 0.2125 | 0.2527 |  0.3122 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:10:20,129: Snapshot:3	Epoch:0	Loss:14.924	translation_Loss:8.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.348                                                   	MRR:15.47	Hits@10:28.22	Best:15.47
2024-12-27 19:10:27,490: Snapshot:3	Epoch:1	Loss:10.574	translation_Loss:8.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.38                                                   	MRR:15.65	Hits@10:28.42	Best:15.65
2024-12-27 19:10:35,250: Snapshot:3	Epoch:2	Loss:10.496	translation_Loss:8.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.293                                                   	MRR:15.64	Hits@10:28.42	Best:15.65
2024-12-27 19:10:42,569: Snapshot:3	Epoch:3	Loss:10.511	translation_Loss:8.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.305                                                   	MRR:15.59	Hits@10:28.37	Best:15.65
2024-12-27 19:10:49,981: Snapshot:3	Epoch:4	Loss:10.523	translation_Loss:8.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.315                                                   	MRR:15.61	Hits@10:28.4	Best:15.65
2024-12-27 19:10:57,350: Snapshot:3	Epoch:5	Loss:10.546	translation_Loss:8.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.336                                                   	MRR:15.64	Hits@10:28.42	Best:15.65
2024-12-27 19:11:04,681: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 15.65
2024-12-27 19:11:04,681: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:10.569 MRR:15.65 Best Results: 15.65
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:11:04,681: Snapshot:3	Epoch:6	Loss:10.569	translation_Loss:8.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.349                                                   	MRR:15.65	Hits@10:28.42	Best:15.65
2024-12-27 19:11:11,841: Snapshot:3	Epoch:7	Loss:23.701	translation_Loss:23.665	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.65	Hits@10:28.42	Best:15.65
2024-12-27 19:11:19,525: End of token training: 3 Epoch: 8 Loss:23.676 MRR:15.65 Best Results: 15.65
2024-12-27 19:11:19,525: Snapshot:3	Epoch:8	Loss:23.676	translation_Loss:23.676	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.65	Hits@10:28.42	Best:15.65
2024-12-27 19:11:19,798: => loading checkpoint './checkpoint/FACTfact_0.01_2048_5000/3model_best.tar'
2024-12-27 19:11:32,147: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2454 | 0.1622 | 0.2854 | 0.3326 |  0.3954 |
|     1      | 0.2041 | 0.131  | 0.2389 | 0.2806 |  0.3372 |
|     2      | 0.1845 | 0.1142 | 0.2146 | 0.2563 |  0.3147 |
|     3      | 0.1575 | 0.0895 | 0.1815 | 0.2238 |  0.2869 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:11:58,243: Snapshot:4	Epoch:0	Loss:13.673	translation_Loss:7.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.128                                                   	MRR:13.5	Hits@10:27.0	Best:13.5
2024-12-27 19:12:05,669: Snapshot:4	Epoch:1	Loss:9.21	translation_Loss:6.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.222                                                   	MRR:13.79	Hits@10:27.19	Best:13.79
2024-12-27 19:12:13,079: Snapshot:4	Epoch:2	Loss:9.06	translation_Loss:6.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.112                                                   	MRR:13.74	Hits@10:27.25	Best:13.79
2024-12-27 19:12:20,529: Snapshot:4	Epoch:3	Loss:9.062	translation_Loss:6.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.133                                                   	MRR:13.85	Hits@10:27.23	Best:13.85
2024-12-27 19:12:27,887: Snapshot:4	Epoch:4	Loss:9.054	translation_Loss:6.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.142                                                   	MRR:13.81	Hits@10:27.24	Best:13.85
2024-12-27 19:12:35,233: Snapshot:4	Epoch:5	Loss:9.074	translation_Loss:6.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.166                                                   	MRR:13.76	Hits@10:27.17	Best:13.85
2024-12-27 19:12:42,606: Snapshot:4	Epoch:6	Loss:9.089	translation_Loss:6.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.177                                                   	MRR:13.75	Hits@10:27.14	Best:13.85
2024-12-27 19:12:50,030: Snapshot:4	Epoch:7	Loss:9.078	translation_Loss:6.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.171                                                   	MRR:13.72	Hits@10:27.08	Best:13.85
2024-12-27 19:12:57,421: Early Stopping! Snapshot: 4 Epoch: 8 Best Results: 13.85
2024-12-27 19:12:57,422: Start to training tokens! Snapshot: 4 Epoch: 8 Loss:9.09 MRR:13.76 Best Results: 13.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:12:57,422: Snapshot:4	Epoch:8	Loss:9.09	translation_Loss:6.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.181                                                   	MRR:13.76	Hits@10:27.34	Best:13.85
2024-12-27 19:13:04,684: Snapshot:4	Epoch:9	Loss:21.456	translation_Loss:21.419	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.76	Hits@10:27.34	Best:13.85
2024-12-27 19:13:11,914: End of token training: 4 Epoch: 10 Loss:21.407 MRR:13.76 Best Results: 13.85
2024-12-27 19:13:11,914: Snapshot:4	Epoch:10	Loss:21.407	translation_Loss:21.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:13.76	Hits@10:27.34	Best:13.85
2024-12-27 19:13:12,276: => loading checkpoint './checkpoint/FACTfact_0.01_2048_5000/4model_best.tar'
2024-12-27 19:13:28,521: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2445 | 0.1615 | 0.2841 | 0.3319 |  0.3944 |
|     1      | 0.203  | 0.1296 | 0.2385 | 0.2799 |  0.3366 |
|     2      | 0.1842 | 0.1132 | 0.2154 | 0.2577 |  0.3162 |
|     3      | 0.1593 | 0.0897 | 0.1833 | 0.2299 |  0.2934 |
|     4      | 0.1398 | 0.0709 | 0.157  | 0.2022 |  0.2722 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:13:28,523: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2436 | 0.1609 | 0.2831 | 0.3302 |  0.3932 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2449 | 0.1619 | 0.2843 | 0.3322 |  0.3942 |
|     1      | 0.2017 | 0.1291 | 0.2361 | 0.2762 |  0.3328 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2453 | 0.1629 | 0.2847 | 0.3317 |  0.3947 |
|     1      | 0.2031 | 0.1301 | 0.2381 | 0.2786 |  0.3353 |
|     2      | 0.1829 | 0.1133 | 0.2125 | 0.2527 |  0.3122 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2454 | 0.1622 | 0.2854 | 0.3326 |  0.3954 |
|     1      | 0.2041 | 0.131  | 0.2389 | 0.2806 |  0.3372 |
|     2      | 0.1845 | 0.1142 | 0.2146 | 0.2563 |  0.3147 |
|     3      | 0.1575 | 0.0895 | 0.1815 | 0.2238 |  0.2869 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2445 | 0.1615 | 0.2841 | 0.3319 |  0.3944 |
|     1      | 0.203  | 0.1296 | 0.2385 | 0.2799 |  0.3366 |
|     2      | 0.1842 | 0.1132 | 0.2154 | 0.2577 |  0.3162 |
|     3      | 0.1593 | 0.0897 | 0.1833 | 0.2299 |  0.2934 |
|     4      | 0.1398 | 0.0709 | 0.157  | 0.2022 |  0.2722 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:13:28,524: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 108.80096101760864 |   0.244   |    0.161     |    0.283     |     0.393     |
|    1     | 121.88569664955139 |   0.223   |    0.145     |     0.26     |     0.364     |
|    2     | 131.82157850265503 |    0.21   |    0.135     |    0.245     |     0.347     |
|    3     |  81.2422034740448  |   0.198   |    0.124     |     0.23     |     0.334     |
|    4     |  96.0186595916748  |   0.186   |    0.113     |    0.216     |     0.323     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:13:28,524: Sum_Training_Time:539.7690992355347
2024-12-27 19:13:28,524: Every_Training_Time:[108.80096101760864, 121.88569664955139, 131.82157850265503, 81.2422034740448, 96.0186595916748]
2024-12-27 19:13:28,524: Forward transfer: 0.14902500000000002 Backward transfer: 0.0013249999999999998
2024-12-27 19:14:07,626: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.01, lifelong_name='double_tokened', log_path='./logs/20241227191332/FACTfact_0.01_2048_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.01_2048_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.01_2048_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:14:17,615: Snapshot:0	Epoch:0	Loss:22.916	translation_Loss:22.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.96	Hits@10:24.78	Best:11.96
2024-12-27 19:14:24,064: Snapshot:0	Epoch:1	Loss:13.132	translation_Loss:13.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.45	Hits@10:34.4	Best:18.45
2024-12-27 19:14:30,929: Snapshot:0	Epoch:2	Loss:7.281	translation_Loss:7.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.51	Hits@10:37.74	Best:22.51
2024-12-27 19:14:37,395: Snapshot:0	Epoch:3	Loss:3.979	translation_Loss:3.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:39.27	Best:24.31
2024-12-27 19:14:43,856: Snapshot:0	Epoch:4	Loss:2.199	translation_Loss:2.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:39.62	Best:24.72
2024-12-27 19:14:50,313: Snapshot:0	Epoch:5	Loss:1.359	translation_Loss:1.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.96	Hits@10:39.96	Best:24.96
2024-12-27 19:14:56,728: Snapshot:0	Epoch:6	Loss:0.955	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.93	Hits@10:39.83	Best:24.96
2024-12-27 19:15:03,683: Snapshot:0	Epoch:7	Loss:0.733	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:40.03	Best:24.99
2024-12-27 19:15:10,127: Snapshot:0	Epoch:8	Loss:0.595	translation_Loss:0.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:40.17	Best:24.99
2024-12-27 19:15:16,548: Snapshot:0	Epoch:9	Loss:0.517	translation_Loss:0.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:40.04	Best:24.99
2024-12-27 19:15:22,976: Snapshot:0	Epoch:10	Loss:0.46	translation_Loss:0.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:40.19	Best:25.0
2024-12-27 19:15:29,393: Snapshot:0	Epoch:11	Loss:0.407	translation_Loss:0.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:39.98	Best:25.0
2024-12-27 19:15:35,802: Snapshot:0	Epoch:12	Loss:0.376	translation_Loss:0.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:39.91	Best:25.0
2024-12-27 19:15:42,706: Snapshot:0	Epoch:13	Loss:0.347	translation_Loss:0.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:39.95	Best:25.0
2024-12-27 19:15:49,152: Snapshot:0	Epoch:14	Loss:0.327	translation_Loss:0.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:39.96	Best:25.0
2024-12-27 19:15:55,565: Early Stopping! Snapshot: 0 Epoch: 15 Best Results: 25.0
2024-12-27 19:15:55,565: Start to training tokens! Snapshot: 0 Epoch: 15 Loss:0.306 MRR:24.65 Best Results: 25.0
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:15:55,565: Snapshot:0	Epoch:15	Loss:0.306	translation_Loss:0.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.65	Hits@10:39.75	Best:25.0
2024-12-27 19:16:02,532: Snapshot:0	Epoch:16	Loss:18.658	translation_Loss:18.621	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.65	Hits@10:39.75	Best:25.0
2024-12-27 19:16:08,988: End of token training: 0 Epoch: 17 Loss:18.615 MRR:24.65 Best Results: 25.0
2024-12-27 19:16:08,989: Snapshot:0	Epoch:17	Loss:18.615	translation_Loss:18.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.65	Hits@10:39.75	Best:25.0
2024-12-27 19:16:09,299: => loading checkpoint './checkpoint/FACTfact_0.01_2048_10000/0model_best.tar'
2024-12-27 19:16:12,073: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1604 | 0.2813 | 0.331  |  0.3917 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:16:37,425: Snapshot:1	Epoch:0	Loss:20.43	translation_Loss:10.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.409                                                   	MRR:18.98	Hits@10:31.66	Best:18.98
2024-12-27 19:16:44,565: Snapshot:1	Epoch:1	Loss:9.443	translation_Loss:7.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.621                                                   	MRR:19.72	Hits@10:32.45	Best:19.72
2024-12-27 19:16:51,761: Snapshot:1	Epoch:2	Loss:8.719	translation_Loss:7.245	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.475                                                   	MRR:19.94	Hits@10:32.68	Best:19.94
2024-12-27 19:16:58,883: Snapshot:1	Epoch:3	Loss:8.606	translation_Loss:7.078	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.529                                                   	MRR:20.01	Hits@10:32.81	Best:20.01
2024-12-27 19:17:05,947: Snapshot:1	Epoch:4	Loss:8.571	translation_Loss:7.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.564                                                   	MRR:20.02	Hits@10:32.7	Best:20.02
2024-12-27 19:17:12,990: Snapshot:1	Epoch:5	Loss:8.582	translation_Loss:6.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.59                                                   	MRR:19.98	Hits@10:32.87	Best:20.02
2024-12-27 19:17:20,047: Snapshot:1	Epoch:6	Loss:8.573	translation_Loss:6.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.605                                                   	MRR:20.01	Hits@10:32.81	Best:20.02
2024-12-27 19:17:27,077: Snapshot:1	Epoch:7	Loss:8.564	translation_Loss:6.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.619                                                   	MRR:20.01	Hits@10:32.81	Best:20.02
2024-12-27 19:17:34,093: Snapshot:1	Epoch:8	Loss:8.571	translation_Loss:6.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.628                                                   	MRR:19.96	Hits@10:32.85	Best:20.02
2024-12-27 19:17:41,116: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 20.02
2024-12-27 19:17:41,116: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:8.55 MRR:19.99 Best Results: 20.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:17:41,117: Snapshot:1	Epoch:9	Loss:8.55	translation_Loss:6.914	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.635                                                   	MRR:19.99	Hits@10:32.81	Best:20.02
2024-12-27 19:17:48,073: Snapshot:1	Epoch:10	Loss:22.953	translation_Loss:22.917	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.99	Hits@10:32.81	Best:20.02
2024-12-27 19:17:54,980: End of token training: 1 Epoch: 11 Loss:22.934 MRR:19.99 Best Results: 20.02
2024-12-27 19:17:54,980: Snapshot:1	Epoch:11	Loss:22.934	translation_Loss:22.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.99	Hits@10:32.81	Best:20.02
2024-12-27 19:17:55,336: => loading checkpoint './checkpoint/FACTfact_0.01_2048_10000/1model_best.tar'
2024-12-27 19:18:01,733: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2431 | 0.1603 | 0.2825 | 0.3309 |  0.3923 |
|     1      | 0.2011 | 0.1283 | 0.2364 | 0.2762 |  0.3303 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:18:26,342: Snapshot:2	Epoch:0	Loss:19.258	translation_Loss:8.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.268                                                   	MRR:17.83	Hits@10:30.34	Best:17.83
2024-12-27 19:18:34,103: Snapshot:2	Epoch:1	Loss:10.132	translation_Loss:8.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.971                                                   	MRR:18.01	Hits@10:30.59	Best:18.01
2024-12-27 19:18:41,351: Snapshot:2	Epoch:2	Loss:9.848	translation_Loss:7.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.852                                                   	MRR:18.08	Hits@10:30.66	Best:18.08
2024-12-27 19:18:48,693: Snapshot:2	Epoch:3	Loss:9.879	translation_Loss:7.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.908                                                   	MRR:18.12	Hits@10:30.74	Best:18.12
2024-12-27 19:18:55,881: Snapshot:2	Epoch:4	Loss:9.906	translation_Loss:7.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.943                                                   	MRR:18.1	Hits@10:30.74	Best:18.12
2024-12-27 19:19:03,085: Snapshot:2	Epoch:5	Loss:9.921	translation_Loss:7.957	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.964                                                   	MRR:18.04	Hits@10:30.73	Best:18.12
2024-12-27 19:19:10,686: Snapshot:2	Epoch:6	Loss:9.931	translation_Loss:7.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.981                                                   	MRR:18.08	Hits@10:30.72	Best:18.12
2024-12-27 19:19:17,898: Snapshot:2	Epoch:7	Loss:9.931	translation_Loss:7.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.987                                                   	MRR:18.04	Hits@10:30.64	Best:18.12
2024-12-27 19:19:25,086: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 18.12
2024-12-27 19:19:25,086: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:9.937 MRR:18.11 Best Results: 18.12
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:19:25,087: Snapshot:2	Epoch:8	Loss:9.937	translation_Loss:7.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.003                                                   	MRR:18.11	Hits@10:30.65	Best:18.12
2024-12-27 19:19:32,171: Snapshot:2	Epoch:9	Loss:23.688	translation_Loss:23.651	multi_layer_Loss:0.037	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.11	Hits@10:30.65	Best:18.12
2024-12-27 19:19:39,259: End of token training: 2 Epoch: 10 Loss:23.67 MRR:18.11 Best Results: 18.12
2024-12-27 19:19:39,259: Snapshot:2	Epoch:10	Loss:23.67	translation_Loss:23.67	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.11	Hits@10:30.65	Best:18.12
2024-12-27 19:19:39,578: => loading checkpoint './checkpoint/FACTfact_0.01_2048_10000/2model_best.tar'
2024-12-27 19:19:49,347: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2436 | 0.1609 | 0.2833 | 0.3315 |  0.3932 |
|     1      | 0.2023 | 0.1297 | 0.2378 | 0.2783 |  0.3322 |
|     2      | 0.1792 | 0.1105 | 0.2077 | 0.2503 |  0.3043 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:20:14,318: Snapshot:3	Epoch:0	Loss:18.585	translation_Loss:8.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.078                                                   	MRR:15.2	Hits@10:27.47	Best:15.2
2024-12-27 19:20:21,690: Snapshot:3	Epoch:1	Loss:10.194	translation_Loss:8.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.996                                                   	MRR:15.3	Hits@10:27.67	Best:15.3
2024-12-27 19:20:29,425: Snapshot:3	Epoch:2	Loss:10.028	translation_Loss:8.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.864                                                   	MRR:15.34	Hits@10:27.71	Best:15.34
2024-12-27 19:20:36,731: Snapshot:3	Epoch:3	Loss:10.084	translation_Loss:8.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.916                                                   	MRR:15.33	Hits@10:27.73	Best:15.34
2024-12-27 19:20:44,032: Snapshot:3	Epoch:4	Loss:10.132	translation_Loss:8.182	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.95                                                   	MRR:15.34	Hits@10:27.73	Best:15.34
2024-12-27 19:20:51,407: Snapshot:3	Epoch:5	Loss:10.156	translation_Loss:8.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.974                                                   	MRR:15.36	Hits@10:27.72	Best:15.36
2024-12-27 19:20:58,664: Snapshot:3	Epoch:6	Loss:10.167	translation_Loss:8.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.991                                                   	MRR:15.35	Hits@10:27.65	Best:15.36
2024-12-27 19:21:06,463: Snapshot:3	Epoch:7	Loss:10.173	translation_Loss:8.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.996                                                   	MRR:15.35	Hits@10:27.68	Best:15.36
2024-12-27 19:21:13,706: Snapshot:3	Epoch:8	Loss:10.2	translation_Loss:8.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.01                                                   	MRR:15.35	Hits@10:27.66	Best:15.36
2024-12-27 19:21:20,998: Snapshot:3	Epoch:9	Loss:10.204	translation_Loss:8.182	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.021                                                   	MRR:15.35	Hits@10:27.66	Best:15.36
2024-12-27 19:21:28,326: Early Stopping! Snapshot: 3 Epoch: 10 Best Results: 15.36
2024-12-27 19:21:28,327: Start to training tokens! Snapshot: 3 Epoch: 10 Loss:10.195 MRR:15.33 Best Results: 15.36
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:21:28,327: Snapshot:3	Epoch:10	Loss:10.195	translation_Loss:8.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.026                                                   	MRR:15.33	Hits@10:27.65	Best:15.36
2024-12-27 19:21:35,481: Snapshot:3	Epoch:11	Loss:23.708	translation_Loss:23.671	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.33	Hits@10:27.65	Best:15.36
2024-12-27 19:21:42,636: End of token training: 3 Epoch: 12 Loss:23.651 MRR:15.33 Best Results: 15.36
2024-12-27 19:21:42,636: Snapshot:3	Epoch:12	Loss:23.651	translation_Loss:23.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:15.33	Hits@10:27.65	Best:15.36
2024-12-27 19:21:42,925: => loading checkpoint './checkpoint/FACTfact_0.01_2048_10000/3model_best.tar'
2024-12-27 19:21:55,714: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1615 | 0.2837 | 0.3317 |  0.3933 |
|     1      | 0.203  | 0.1305 | 0.238  | 0.2786 |  0.3321 |
|     2      | 0.1807 | 0.1112 | 0.2098 | 0.2519 |  0.3072 |
|     3      | 0.1536 | 0.0872 | 0.1759 | 0.2195 |  0.2804 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:22:21,499: Snapshot:4	Epoch:0	Loss:17.748	translation_Loss:7.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.795                                                   	MRR:12.36	Hits@10:24.45	Best:12.36
2024-12-27 19:22:28,899: Snapshot:4	Epoch:1	Loss:9.28	translation_Loss:7.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.883                                                   	MRR:12.56	Hits@10:24.69	Best:12.56
2024-12-27 19:22:36,331: Snapshot:4	Epoch:2	Loss:9.075	translation_Loss:7.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.726                                                   	MRR:12.59	Hits@10:24.82	Best:12.59
2024-12-27 19:22:43,672: Snapshot:4	Epoch:3	Loss:9.086	translation_Loss:7.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.753                                                   	MRR:12.54	Hits@10:24.81	Best:12.59
2024-12-27 19:22:51,107: Snapshot:4	Epoch:4	Loss:9.113	translation_Loss:7.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.771                                                   	MRR:12.65	Hits@10:24.7	Best:12.65
2024-12-27 19:22:58,431: Snapshot:4	Epoch:5	Loss:9.13	translation_Loss:7.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.797                                                   	MRR:12.6	Hits@10:24.75	Best:12.65
2024-12-27 19:23:05,789: Snapshot:4	Epoch:6	Loss:9.142	translation_Loss:7.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.811                                                   	MRR:12.61	Hits@10:24.76	Best:12.65
2024-12-27 19:23:13,142: Snapshot:4	Epoch:7	Loss:9.152	translation_Loss:7.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.82                                                   	MRR:12.68	Hits@10:24.75	Best:12.68
2024-12-27 19:23:20,519: Snapshot:4	Epoch:8	Loss:9.176	translation_Loss:7.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.84                                                   	MRR:12.59	Hits@10:24.82	Best:12.68
2024-12-27 19:23:27,889: Snapshot:4	Epoch:9	Loss:9.17	translation_Loss:7.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.837                                                   	MRR:12.64	Hits@10:24.78	Best:12.68
2024-12-27 19:23:35,249: Snapshot:4	Epoch:10	Loss:9.185	translation_Loss:7.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.849                                                   	MRR:12.63	Hits@10:24.76	Best:12.68
2024-12-27 19:23:42,636: Snapshot:4	Epoch:11	Loss:9.191	translation_Loss:7.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.855                                                   	MRR:12.62	Hits@10:24.81	Best:12.68
2024-12-27 19:23:49,993: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 12.68
2024-12-27 19:23:49,993: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:9.185 MRR:12.64 Best Results: 12.68
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:23:49,994: Snapshot:4	Epoch:12	Loss:9.185	translation_Loss:7.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.863                                                   	MRR:12.64	Hits@10:24.69	Best:12.68
2024-12-27 19:23:57,184: Snapshot:4	Epoch:13	Loss:21.73	translation_Loss:21.694	multi_layer_Loss:0.036	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.64	Hits@10:24.69	Best:12.68
2024-12-27 19:24:04,917: End of token training: 4 Epoch: 14 Loss:21.707 MRR:12.64 Best Results: 12.68
2024-12-27 19:24:04,917: Snapshot:4	Epoch:14	Loss:21.707	translation_Loss:21.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:12.64	Hits@10:24.69	Best:12.68
2024-12-27 19:24:05,223: => loading checkpoint './checkpoint/FACTfact_0.01_2048_10000/4model_best.tar'
2024-12-27 19:24:21,541: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.161  | 0.2834 | 0.3318 |  0.3924 |
|     1      | 0.2029 | 0.1303 | 0.2384 | 0.2789 |  0.3326 |
|     2      | 0.1815 | 0.112  | 0.2102 | 0.2524 |  0.3079 |
|     3      | 0.1552 | 0.0875 | 0.1789 | 0.2228 |  0.2846 |
|     4      | 0.1271 | 0.0637 | 0.1399 | 0.1821 |  0.2486 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:24:21,543: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1604 | 0.2813 | 0.331  |  0.3917 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2431 | 0.1603 | 0.2825 | 0.3309 |  0.3923 |
|     1      | 0.2011 | 0.1283 | 0.2364 | 0.2762 |  0.3303 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2436 | 0.1609 | 0.2833 | 0.3315 |  0.3932 |
|     1      | 0.2023 | 0.1297 | 0.2378 | 0.2783 |  0.3322 |
|     2      | 0.1792 | 0.1105 | 0.2077 | 0.2503 |  0.3043 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2441 | 0.1615 | 0.2837 | 0.3317 |  0.3933 |
|     1      | 0.203  | 0.1305 | 0.238  | 0.2786 |  0.3321 |
|     2      | 0.1807 | 0.1112 | 0.2098 | 0.2519 |  0.3072 |
|     3      | 0.1536 | 0.0872 | 0.1759 | 0.2195 |  0.2804 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.161  | 0.2834 | 0.3318 |  0.3924 |
|     1      | 0.2029 | 0.1303 | 0.2384 | 0.2789 |  0.3326 |
|     2      | 0.1815 | 0.112  | 0.2102 | 0.2524 |  0.3079 |
|     3      | 0.1552 | 0.0875 | 0.1789 | 0.2228 |  0.2846 |
|     4      | 0.1271 | 0.0637 | 0.1399 | 0.1821 |  0.2486 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:24:21,544: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 121.36158061027527 |   0.243   |     0.16     |    0.281     |     0.392     |
|    1     | 99.48976182937622  |   0.222   |    0.144     |    0.259     |     0.361     |
|    2     | 94.17924928665161  |   0.208   |    0.134     |    0.243     |     0.343     |
|    3     | 109.86879181861877 |   0.195   |    0.123     |    0.227     |     0.328     |
|    4     | 125.4815924167633  |   0.182   |    0.111     |     0.21     |     0.313     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:24:21,544: Sum_Training_Time:550.3809759616852
2024-12-27 19:24:21,544: Every_Training_Time:[121.36158061027527, 99.48976182937622, 94.17924928665161, 109.86879181861877, 125.4815924167633]
2024-12-27 19:24:21,544: Forward transfer: 0.1462 Backward transfer: 0.0017249999999999974
2024-12-27 19:25:00,952: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227192425/FACTfact_0.001_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:25:11,239: Snapshot:0	Epoch:0	Loss:92.886	translation_Loss:92.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.48	Hits@10:24.19	Best:11.48
2024-12-27 19:25:17,926: Snapshot:0	Epoch:1	Loss:56.061	translation_Loss:56.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.45	Hits@10:33.69	Best:17.45
2024-12-27 19:25:24,617: Snapshot:0	Epoch:2	Loss:32.54	translation_Loss:32.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.75	Hits@10:37.76	Best:21.75
2024-12-27 19:25:31,347: Snapshot:0	Epoch:3	Loss:18.208	translation_Loss:18.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.82	Hits@10:39.26	Best:23.82
2024-12-27 19:25:38,049: Snapshot:0	Epoch:4	Loss:10.205	translation_Loss:10.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:39.83	Best:24.74
2024-12-27 19:25:45,322: Snapshot:0	Epoch:5	Loss:6.327	translation_Loss:6.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.17	Best:24.88
2024-12-27 19:25:52,555: Snapshot:0	Epoch:6	Loss:4.357	translation_Loss:4.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:40.17	Best:25.07
2024-12-27 19:25:59,833: Snapshot:0	Epoch:7	Loss:3.294	translation_Loss:3.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.1	Hits@10:40.16	Best:25.1
2024-12-27 19:26:07,471: Snapshot:0	Epoch:8	Loss:2.663	translation_Loss:2.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.94	Hits@10:40.27	Best:25.1
2024-12-27 19:26:14,641: Snapshot:0	Epoch:9	Loss:2.255	translation_Loss:2.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.94	Hits@10:40.26	Best:25.1
2024-12-27 19:26:21,810: Snapshot:0	Epoch:10	Loss:2.055	translation_Loss:2.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.02	Best:25.1
2024-12-27 19:26:29,003: Snapshot:0	Epoch:11	Loss:1.868	translation_Loss:1.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.92	Hits@10:40.01	Best:25.1
2024-12-27 19:26:36,181: Early Stopping! Snapshot: 0 Epoch: 12 Best Results: 25.1
2024-12-27 19:26:36,181: Start to training tokens! Snapshot: 0 Epoch: 12 Loss:1.739 MRR:24.72 Best Results: 25.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:26:36,182: Snapshot:0	Epoch:12	Loss:1.739	translation_Loss:1.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:39.79	Best:25.1
2024-12-27 19:26:44,095: Snapshot:0	Epoch:13	Loss:73.228	translation_Loss:73.072	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:39.79	Best:25.1
2024-12-27 19:26:51,580: End of token training: 0 Epoch: 14 Loss:73.013 MRR:24.72 Best Results: 25.1
2024-12-27 19:26:51,580: Snapshot:0	Epoch:14	Loss:73.013	translation_Loss:73.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.72	Hits@10:39.79	Best:25.1
2024-12-27 19:26:51,928: => loading checkpoint './checkpoint/FACTfact_0.001_512_1000/0model_best.tar'
2024-12-27 19:26:54,756: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1575 | 0.2798 | 0.3298 |  0.3942 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:27:21,145: Snapshot:1	Epoch:0	Loss:42.699	translation_Loss:36.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.944                                                   	MRR:21.2	Hits@10:35.48	Best:21.2
2024-12-27 19:27:28,792: Snapshot:1	Epoch:1	Loss:30.958	translation_Loss:20.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.188                                                   	MRR:22.13	Hits@10:36.76	Best:22.13
2024-12-27 19:27:36,414: Snapshot:1	Epoch:2	Loss:27.732	translation_Loss:16.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.254                                                   	MRR:22.32	Hits@10:37.04	Best:22.32
2024-12-27 19:27:44,120: Snapshot:1	Epoch:3	Loss:26.721	translation_Loss:15.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.601                                                   	MRR:22.53	Hits@10:37.05	Best:22.53
2024-12-27 19:27:51,711: Snapshot:1	Epoch:4	Loss:26.476	translation_Loss:14.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.796                                                   	MRR:22.44	Hits@10:37.05	Best:22.53
2024-12-27 19:27:59,263: Snapshot:1	Epoch:5	Loss:26.397	translation_Loss:14.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.906                                                   	MRR:22.4	Hits@10:37.19	Best:22.53
2024-12-27 19:28:06,820: Snapshot:1	Epoch:6	Loss:26.373	translation_Loss:14.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.986                                                   	MRR:22.29	Hits@10:36.96	Best:22.53
2024-12-27 19:28:14,382: Snapshot:1	Epoch:7	Loss:26.288	translation_Loss:14.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.037                                                   	MRR:22.48	Hits@10:37.17	Best:22.53
2024-12-27 19:28:21,998: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 22.53
2024-12-27 19:28:21,998: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:26.239 MRR:22.48 Best Results: 22.53
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:28:21,999: Snapshot:1	Epoch:8	Loss:26.239	translation_Loss:14.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:12.079                                                   	MRR:22.48	Hits@10:37.11	Best:22.53
2024-12-27 19:28:29,423: Snapshot:1	Epoch:9	Loss:81.935	translation_Loss:81.784	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.48	Hits@10:37.11	Best:22.53
2024-12-27 19:28:36,818: End of token training: 1 Epoch: 10 Loss:81.692 MRR:22.48 Best Results: 22.53
2024-12-27 19:28:36,818: Snapshot:1	Epoch:10	Loss:81.692	translation_Loss:81.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.48	Hits@10:37.11	Best:22.53
2024-12-27 19:28:37,170: => loading checkpoint './checkpoint/FACTfact_0.001_512_1000/1model_best.tar'
2024-12-27 19:28:42,690: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1624 | 0.2904 | 0.3437 |  0.4118 |
|     1      | 0.2254 | 0.1448 | 0.2634 | 0.3121 |  0.3732 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:29:09,041: Snapshot:2	Epoch:0	Loss:27.404	translation_Loss:22.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.274                                                   	MRR:21.32	Hits@10:37.1	Best:21.32
2024-12-27 19:29:16,891: Snapshot:2	Epoch:1	Loss:19.953	translation_Loss:11.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.396                                                   	MRR:21.59	Hits@10:37.5	Best:21.59
2024-12-27 19:29:24,799: Snapshot:2	Epoch:2	Loss:19.093	translation_Loss:9.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.273                                                   	MRR:21.6	Hits@10:37.4	Best:21.6
2024-12-27 19:29:32,672: Snapshot:2	Epoch:3	Loss:18.934	translation_Loss:9.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.654                                                   	MRR:21.61	Hits@10:37.36	Best:21.61
2024-12-27 19:29:40,512: Snapshot:2	Epoch:4	Loss:18.964	translation_Loss:9.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.881                                                   	MRR:21.69	Hits@10:37.46	Best:21.69
2024-12-27 19:29:48,872: Snapshot:2	Epoch:5	Loss:19.111	translation_Loss:9.071	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.04                                                   	MRR:21.55	Hits@10:37.52	Best:21.69
2024-12-27 19:29:56,649: Snapshot:2	Epoch:6	Loss:19.194	translation_Loss:9.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.159                                                   	MRR:21.68	Hits@10:37.19	Best:21.69
2024-12-27 19:30:04,531: Snapshot:2	Epoch:7	Loss:19.159	translation_Loss:8.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.195                                                   	MRR:21.56	Hits@10:37.52	Best:21.69
2024-12-27 19:30:12,436: Snapshot:2	Epoch:8	Loss:19.277	translation_Loss:9.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.259                                                   	MRR:21.5	Hits@10:37.46	Best:21.69
2024-12-27 19:30:20,243: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 21.69
2024-12-27 19:30:20,243: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:19.23 MRR:21.55 Best Results: 21.69
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:30:20,244: Snapshot:2	Epoch:9	Loss:19.23	translation_Loss:8.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.29                                                   	MRR:21.55	Hits@10:37.21	Best:21.69
2024-12-27 19:30:27,804: Snapshot:2	Epoch:10	Loss:79.724	translation_Loss:79.565	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.55	Hits@10:37.21	Best:21.69
2024-12-27 19:30:35,359: End of token training: 2 Epoch: 11 Loss:79.633 MRR:21.55 Best Results: 21.69
2024-12-27 19:30:35,360: Snapshot:2	Epoch:11	Loss:79.633	translation_Loss:79.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.55	Hits@10:37.21	Best:21.69
2024-12-27 19:30:35,707: => loading checkpoint './checkpoint/FACTfact_0.001_512_1000/2model_best.tar'
2024-12-27 19:30:45,237: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2387 | 0.1543 | 0.2727 | 0.3277 |  0.4008 |
|     1      | 0.2259 | 0.1451 | 0.2597 | 0.3102 |  0.382  |
|     2      | 0.2189 | 0.1371 | 0.2522 | 0.3051 |  0.3751 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:31:12,095: Snapshot:3	Epoch:0	Loss:14.387	translation_Loss:10.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.877                                                   	MRR:19.91	Hits@10:37.68	Best:19.91
2024-12-27 19:31:19,981: Snapshot:3	Epoch:1	Loss:10.653	translation_Loss:5.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.335                                                   	MRR:19.98	Hits@10:37.74	Best:19.98
2024-12-27 19:31:27,956: Snapshot:3	Epoch:2	Loss:10.476	translation_Loss:4.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.765                                                   	MRR:19.72	Hits@10:37.65	Best:19.98
2024-12-27 19:31:35,809: Snapshot:3	Epoch:3	Loss:10.574	translation_Loss:4.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.011                                                   	MRR:19.81	Hits@10:37.61	Best:19.98
2024-12-27 19:31:43,689: Snapshot:3	Epoch:4	Loss:10.837	translation_Loss:4.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.231                                                   	MRR:19.91	Hits@10:37.58	Best:19.98
2024-12-27 19:31:51,593: Snapshot:3	Epoch:5	Loss:10.915	translation_Loss:4.584	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.331                                                   	MRR:19.68	Hits@10:37.64	Best:19.98
2024-12-27 19:31:59,422: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 19.98
2024-12-27 19:31:59,423: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:10.903 MRR:19.79 Best Results: 19.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:31:59,423: Snapshot:3	Epoch:6	Loss:10.903	translation_Loss:4.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.358                                                   	MRR:19.79	Hits@10:37.55	Best:19.98
2024-12-27 19:32:07,007: Snapshot:3	Epoch:7	Loss:75.93	translation_Loss:75.78	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.79	Hits@10:37.55	Best:19.98
2024-12-27 19:32:14,587: End of token training: 3 Epoch: 8 Loss:75.65 MRR:19.79 Best Results: 19.98
2024-12-27 19:32:14,588: Snapshot:3	Epoch:8	Loss:75.65	translation_Loss:75.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.79	Hits@10:37.55	Best:19.98
2024-12-27 19:32:14,938: => loading checkpoint './checkpoint/FACTfact_0.001_512_1000/3model_best.tar'
2024-12-27 19:32:28,153: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2249 | 0.1443 | 0.2544 | 0.3073 |  0.3795 |
|     1      | 0.2113 | 0.1321 | 0.2408 | 0.2947 |  0.3668 |
|     2      | 0.2072 | 0.1243 | 0.2378 | 0.2929 |  0.3706 |
|     3      |  0.2   | 0.1097 |  0.23  | 0.295  |  0.3797 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:32:55,083: Snapshot:4	Epoch:0	Loss:6.783	translation_Loss:4.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.024                                                   	MRR:21.21	Hits@10:44.51	Best:21.21
2024-12-27 19:33:03,108: Snapshot:4	Epoch:1	Loss:3.895	translation_Loss:1.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.228                                                   	MRR:21.27	Hits@10:44.28	Best:21.27
2024-12-27 19:33:10,995: Snapshot:4	Epoch:2	Loss:3.727	translation_Loss:1.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.355                                                   	MRR:21.11	Hits@10:44.24	Best:21.27
2024-12-27 19:33:18,990: Snapshot:4	Epoch:3	Loss:3.758	translation_Loss:1.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.48                                                   	MRR:21.33	Hits@10:43.98	Best:21.33
2024-12-27 19:33:26,935: Snapshot:4	Epoch:4	Loss:3.78	translation_Loss:1.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.52                                                   	MRR:21.31	Hits@10:44.49	Best:21.33
2024-12-27 19:33:34,849: Snapshot:4	Epoch:5	Loss:3.793	translation_Loss:1.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.556                                                   	MRR:21.31	Hits@10:44.67	Best:21.33
2024-12-27 19:33:42,782: Snapshot:4	Epoch:6	Loss:3.85	translation_Loss:1.239	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.611                                                   	MRR:21.14	Hits@10:44.22	Best:21.33
2024-12-27 19:33:50,720: Snapshot:4	Epoch:7	Loss:3.833	translation_Loss:1.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.592                                                   	MRR:21.19	Hits@10:43.88	Best:21.33
2024-12-27 19:33:58,695: Snapshot:4	Epoch:8	Loss:3.866	translation_Loss:1.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.636                                                   	MRR:21.6	Hits@10:44.0	Best:21.6
2024-12-27 19:34:06,597: Snapshot:4	Epoch:9	Loss:3.871	translation_Loss:1.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.639                                                   	MRR:21.14	Hits@10:43.91	Best:21.6
2024-12-27 19:34:14,510: Snapshot:4	Epoch:10	Loss:3.919	translation_Loss:1.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.667                                                   	MRR:21.08	Hits@10:43.77	Best:21.6
2024-12-27 19:34:22,460: Snapshot:4	Epoch:11	Loss:3.912	translation_Loss:1.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.665                                                   	MRR:21.84	Hits@10:44.84	Best:21.84
2024-12-27 19:34:30,412: Snapshot:4	Epoch:12	Loss:3.876	translation_Loss:1.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.642                                                   	MRR:21.21	Hits@10:44.0	Best:21.84
2024-12-27 19:34:38,325: Snapshot:4	Epoch:13	Loss:3.947	translation_Loss:1.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.697                                                   	MRR:21.32	Hits@10:44.18	Best:21.84
2024-12-27 19:34:46,260: Snapshot:4	Epoch:14	Loss:3.898	translation_Loss:1.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.67                                                   	MRR:21.06	Hits@10:43.9	Best:21.84
2024-12-27 19:34:54,173: Snapshot:4	Epoch:15	Loss:3.916	translation_Loss:1.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.693                                                   	MRR:20.84	Hits@10:43.99	Best:21.84
2024-12-27 19:35:02,157: Early Stopping! Snapshot: 4 Epoch: 16 Best Results: 21.84
2024-12-27 19:35:02,157: Start to training tokens! Snapshot: 4 Epoch: 16 Loss:3.888 MRR:21.11 Best Results: 21.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:35:02,158: Snapshot:4	Epoch:16	Loss:3.888	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.652                                                   	MRR:21.11	Hits@10:44.09	Best:21.84
2024-12-27 19:35:09,879: Snapshot:4	Epoch:17	Loss:62.846	translation_Loss:62.695	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.11	Hits@10:44.09	Best:21.84
2024-12-27 19:35:17,550: End of token training: 4 Epoch: 18 Loss:62.614 MRR:21.11 Best Results: 21.84
2024-12-27 19:35:17,550: Snapshot:4	Epoch:18	Loss:62.614	translation_Loss:62.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.11	Hits@10:44.09	Best:21.84
2024-12-27 19:35:17,900: => loading checkpoint './checkpoint/FACTfact_0.001_512_1000/4model_best.tar'
2024-12-27 19:35:34,677: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.207  | 0.128  | 0.2356 | 0.2839 |  0.3586 |
|     1      | 0.1933 | 0.117  |  0.22  | 0.2698 |  0.3405 |
|     2      | 0.1906 | 0.1125 | 0.215  | 0.2693 |  0.3447 |
|     3      | 0.181  | 0.0954 | 0.2058 | 0.2659 |  0.3547 |
|     4      | 0.2143 | 0.1027 | 0.2514 | 0.329  |  0.4422 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:35:34,679: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1575 | 0.2798 | 0.3298 |  0.3942 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1624 | 0.2904 | 0.3437 |  0.4118 |
|     1      | 0.2254 | 0.1448 | 0.2634 | 0.3121 |  0.3732 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2387 | 0.1543 | 0.2727 | 0.3277 |  0.4008 |
|     1      | 0.2259 | 0.1451 | 0.2597 | 0.3102 |  0.382  |
|     2      | 0.2189 | 0.1371 | 0.2522 | 0.3051 |  0.3751 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2249 | 0.1443 | 0.2544 | 0.3073 |  0.3795 |
|     1      | 0.2113 | 0.1321 | 0.2408 | 0.2947 |  0.3668 |
|     2      | 0.2072 | 0.1243 | 0.2378 | 0.2929 |  0.3706 |
|     3      |  0.2   | 0.1097 |  0.23  | 0.295  |  0.3797 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.207  | 0.128  | 0.2356 | 0.2839 |  0.3586 |
|     1      | 0.1933 | 0.117  |  0.22  | 0.2698 |  0.3405 |
|     2      | 0.1906 | 0.1125 | 0.215  | 0.2693 |  0.3447 |
|     3      | 0.181  | 0.0954 | 0.2058 | 0.2659 |  0.3547 |
|     4      | 0.2143 | 0.1027 | 0.2514 | 0.329  |  0.4422 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:35:34,680: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 110.62778544425964 |   0.241   |    0.158     |     0.28     |     0.394     |
|    1     | 98.91680240631104  |   0.238   |    0.154     |    0.277     |     0.392     |
|    2     | 108.9839391708374  |   0.228   |    0.145     |    0.262     |     0.386     |
|    3     | 85.94712257385254  |   0.211   |    0.128     |    0.241     |     0.374     |
|    4     | 165.95495676994324 |   0.197   |    0.111     |    0.226     |     0.368     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:35:34,680: Sum_Training_Time:570.4306063652039
2024-12-27 19:35:34,680: Every_Training_Time:[110.62778544425964, 98.91680240631104, 108.9839391708374, 85.94712257385254, 165.95495676994324]
2024-12-27 19:35:34,680: Forward transfer: 0.177725 Backward transfer: -0.02827500000000001
2024-12-27 19:36:13,641: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227193538/FACTfact_0.001_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:36:23,797: Snapshot:0	Epoch:0	Loss:92.886	translation_Loss:92.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.48	Hits@10:24.19	Best:11.48
2024-12-27 19:36:30,464: Snapshot:0	Epoch:1	Loss:56.059	translation_Loss:56.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.45	Hits@10:33.7	Best:17.45
2024-12-27 19:36:37,070: Snapshot:0	Epoch:2	Loss:32.538	translation_Loss:32.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.88	Hits@10:37.84	Best:21.88
2024-12-27 19:36:43,706: Snapshot:0	Epoch:3	Loss:18.196	translation_Loss:18.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.83	Hits@10:39.2	Best:23.83
2024-12-27 19:36:50,378: Snapshot:0	Epoch:4	Loss:10.204	translation_Loss:10.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.61	Hits@10:39.78	Best:24.61
2024-12-27 19:36:57,015: Snapshot:0	Epoch:5	Loss:6.325	translation_Loss:6.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.15	Best:24.88
2024-12-27 19:37:03,642: Snapshot:0	Epoch:6	Loss:4.366	translation_Loss:4.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.08	Hits@10:40.27	Best:25.08
2024-12-27 19:37:10,247: Snapshot:0	Epoch:7	Loss:3.306	translation_Loss:3.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.03	Hits@10:40.11	Best:25.08
2024-12-27 19:37:17,369: Snapshot:0	Epoch:8	Loss:2.66	translation_Loss:2.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:40.23	Best:25.08
2024-12-27 19:37:23,962: Snapshot:0	Epoch:9	Loss:2.284	translation_Loss:2.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:40.06	Best:25.08
2024-12-27 19:37:30,571: Snapshot:0	Epoch:10	Loss:2.056	translation_Loss:2.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:40.18	Best:25.08
2024-12-27 19:37:37,199: Early Stopping! Snapshot: 0 Epoch: 11 Best Results: 25.08
2024-12-27 19:37:37,199: Start to training tokens! Snapshot: 0 Epoch: 11 Loss:1.845 MRR:24.88 Best Results: 25.08
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:37:37,200: Snapshot:0	Epoch:11	Loss:1.845	translation_Loss:1.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.02	Best:25.08
2024-12-27 19:37:44,611: Snapshot:0	Epoch:12	Loss:73.12	translation_Loss:72.963	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.02	Best:25.08
2024-12-27 19:37:51,448: End of token training: 0 Epoch: 13 Loss:73.092 MRR:24.88 Best Results: 25.08
2024-12-27 19:37:51,448: Snapshot:0	Epoch:13	Loss:73.092	translation_Loss:73.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.88	Hits@10:40.02	Best:25.08
2024-12-27 19:37:51,805: => loading checkpoint './checkpoint/FACTfact_0.001_512_5000/0model_best.tar'
2024-12-27 19:37:54,709: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.1591 | 0.2818 | 0.3303 |  0.3915 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:38:20,222: Snapshot:1	Epoch:0	Loss:49.057	translation_Loss:40.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.779                                                   	MRR:20.23	Hits@10:33.74	Best:20.23
2024-12-27 19:38:27,757: Snapshot:1	Epoch:1	Loss:41.869	translation_Loss:31.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.481                                                   	MRR:20.96	Hits@10:34.52	Best:20.96
2024-12-27 19:38:35,315: Snapshot:1	Epoch:2	Loss:39.041	translation_Loss:28.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.502                                                   	MRR:21.23	Hits@10:34.88	Best:21.23
2024-12-27 19:38:43,277: Snapshot:1	Epoch:3	Loss:37.947	translation_Loss:27.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.437                                                   	MRR:21.26	Hits@10:34.92	Best:21.26
2024-12-27 19:38:50,790: Snapshot:1	Epoch:4	Loss:37.485	translation_Loss:27.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.393                                                   	MRR:21.11	Hits@10:34.84	Best:21.26
2024-12-27 19:38:58,295: Snapshot:1	Epoch:5	Loss:37.34	translation_Loss:26.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.369                                                   	MRR:21.19	Hits@10:34.77	Best:21.26
2024-12-27 19:39:05,812: Snapshot:1	Epoch:6	Loss:37.11	translation_Loss:26.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.415                                                   	MRR:21.13	Hits@10:34.81	Best:21.26
2024-12-27 19:39:13,333: Snapshot:1	Epoch:7	Loss:37.08	translation_Loss:26.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.445                                                   	MRR:21.07	Hits@10:34.93	Best:21.26
2024-12-27 19:39:20,891: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 21.26
2024-12-27 19:39:20,892: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:37.115 MRR:21.17 Best Results: 21.26
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:39:20,892: Snapshot:1	Epoch:8	Loss:37.115	translation_Loss:26.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.432                                                   	MRR:21.17	Hits@10:34.91	Best:21.26
2024-12-27 19:39:28,219: Snapshot:1	Epoch:9	Loss:87.799	translation_Loss:87.647	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.17	Hits@10:34.91	Best:21.26
2024-12-27 19:39:35,510: End of token training: 1 Epoch: 10 Loss:87.612 MRR:21.17 Best Results: 21.26
2024-12-27 19:39:35,511: Snapshot:1	Epoch:10	Loss:87.612	translation_Loss:87.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.17	Hits@10:34.91	Best:21.26
2024-12-27 19:39:35,870: => loading checkpoint './checkpoint/FACTfact_0.001_512_5000/1model_best.tar'
2024-12-27 19:39:42,474: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2452 | 0.1609 | 0.2865 | 0.3348 |  0.3966 |
|     1      | 0.2087 | 0.1321 | 0.2449 | 0.288  |  0.3476 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:40:09,245: Snapshot:2	Epoch:0	Loss:40.422	translation_Loss:31.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.309                                                   	MRR:19.78	Hits@10:34.34	Best:19.78
2024-12-27 19:40:17,081: Snapshot:2	Epoch:1	Loss:37.883	translation_Loss:26.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.56                                                   	MRR:19.88	Hits@10:34.36	Best:19.88
2024-12-27 19:40:24,887: Snapshot:2	Epoch:2	Loss:37.084	translation_Loss:25.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.732                                                   	MRR:19.94	Hits@10:34.46	Best:19.94
2024-12-27 19:40:32,693: Snapshot:2	Epoch:3	Loss:36.837	translation_Loss:25.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.79                                                   	MRR:19.93	Hits@10:34.43	Best:19.94
2024-12-27 19:40:40,418: Snapshot:2	Epoch:4	Loss:36.828	translation_Loss:25.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.816                                                   	MRR:19.93	Hits@10:34.43	Best:19.94
2024-12-27 19:40:48,188: Snapshot:2	Epoch:5	Loss:36.85	translation_Loss:24.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.871                                                   	MRR:20.0	Hits@10:34.44	Best:20.0
2024-12-27 19:40:55,965: Snapshot:2	Epoch:6	Loss:36.773	translation_Loss:24.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.89                                                   	MRR:19.91	Hits@10:34.45	Best:20.0
2024-12-27 19:41:03,755: Snapshot:2	Epoch:7	Loss:36.748	translation_Loss:24.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.889                                                   	MRR:20.03	Hits@10:34.4	Best:20.03
2024-12-27 19:41:11,505: Snapshot:2	Epoch:8	Loss:36.755	translation_Loss:24.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.918                                                   	MRR:19.88	Hits@10:34.48	Best:20.03
2024-12-27 19:41:19,213: Snapshot:2	Epoch:9	Loss:36.733	translation_Loss:24.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.911                                                   	MRR:19.91	Hits@10:34.38	Best:20.03
2024-12-27 19:41:26,994: Snapshot:2	Epoch:10	Loss:36.748	translation_Loss:24.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.938                                                   	MRR:19.92	Hits@10:34.43	Best:20.03
2024-12-27 19:41:34,753: Snapshot:2	Epoch:11	Loss:36.699	translation_Loss:24.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.92                                                   	MRR:19.91	Hits@10:34.37	Best:20.03
2024-12-27 19:41:42,502: Early Stopping! Snapshot: 2 Epoch: 12 Best Results: 20.03
2024-12-27 19:41:42,502: Start to training tokens! Snapshot: 2 Epoch: 12 Loss:36.76 MRR:19.98 Best Results: 20.03
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:41:42,503: Snapshot:2	Epoch:12	Loss:36.76	translation_Loss:24.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.941                                                   	MRR:19.98	Hits@10:34.51	Best:20.03
2024-12-27 19:41:49,998: Snapshot:2	Epoch:13	Loss:87.731	translation_Loss:87.572	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.98	Hits@10:34.51	Best:20.03
2024-12-27 19:41:57,458: End of token training: 2 Epoch: 14 Loss:87.594 MRR:19.98 Best Results: 20.03
2024-12-27 19:41:57,458: Snapshot:2	Epoch:14	Loss:87.594	translation_Loss:87.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.98	Hits@10:34.51	Best:20.03
2024-12-27 19:41:57,820: => loading checkpoint './checkpoint/FACTfact_0.001_512_5000/2model_best.tar'
2024-12-27 19:42:06,789: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1575 | 0.2832 | 0.3342 |  0.4004 |
|     1      | 0.2118 | 0.1325 | 0.2484 | 0.297  |  0.359  |
|     2      | 0.2005 | 0.1229 | 0.2344 | 0.2817 |  0.3448 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:42:33,331: Snapshot:3	Epoch:0	Loss:29.84	translation_Loss:21.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.466                                                   	MRR:18.75	Hits@10:34.89	Best:18.75
2024-12-27 19:42:41,214: Snapshot:3	Epoch:1	Loss:28.535	translation_Loss:17.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.583                                                   	MRR:18.74	Hits@10:35.02	Best:18.75
2024-12-27 19:42:49,060: Snapshot:3	Epoch:2	Loss:28.508	translation_Loss:17.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.811                                                   	MRR:18.9	Hits@10:35.17	Best:18.9
2024-12-27 19:42:56,863: Snapshot:3	Epoch:3	Loss:28.496	translation_Loss:17.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.889                                                   	MRR:18.84	Hits@10:34.9	Best:18.9
2024-12-27 19:43:04,663: Snapshot:3	Epoch:4	Loss:28.516	translation_Loss:17.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.91                                                   	MRR:18.89	Hits@10:35.09	Best:18.9
2024-12-27 19:43:12,443: Snapshot:3	Epoch:5	Loss:28.519	translation_Loss:17.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.941                                                   	MRR:18.87	Hits@10:35.14	Best:18.9
2024-12-27 19:43:20,194: Snapshot:3	Epoch:6	Loss:28.528	translation_Loss:17.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.958                                                   	MRR:18.83	Hits@10:34.95	Best:18.9
2024-12-27 19:43:28,496: Early Stopping! Snapshot: 3 Epoch: 7 Best Results: 18.9
2024-12-27 19:43:28,496: Start to training tokens! Snapshot: 3 Epoch: 7 Loss:28.637 MRR:18.89 Best Results: 18.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:43:28,497: Snapshot:3	Epoch:7	Loss:28.637	translation_Loss:17.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.048                                                   	MRR:18.89	Hits@10:35.1	Best:18.9
2024-12-27 19:43:36,060: Snapshot:3	Epoch:8	Loss:84.172	translation_Loss:84.022	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.89	Hits@10:35.1	Best:18.9
2024-12-27 19:43:43,605: End of token training: 3 Epoch: 9 Loss:83.983 MRR:18.89 Best Results: 18.9
2024-12-27 19:43:43,605: Snapshot:3	Epoch:9	Loss:83.983	translation_Loss:83.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.89	Hits@10:35.1	Best:18.9
2024-12-27 19:43:43,882: => loading checkpoint './checkpoint/FACTfact_0.001_512_5000/3model_best.tar'
2024-12-27 19:43:56,645: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2361 | 0.1518 | 0.273  | 0.3246 |  0.3911 |
|     1      | 0.2101 | 0.1316 | 0.2441 | 0.2955 |  0.359  |
|     2      | 0.2014 | 0.1214 | 0.2337 | 0.2852 |  0.3564 |
|     3      | 0.1893 | 0.1065 | 0.2189 | 0.2758 |  0.3526 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:44:22,863: Snapshot:4	Epoch:0	Loss:15.65	translation_Loss:10.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.506                                                   	MRR:20.85	Hits@10:43.08	Best:20.85
2024-12-27 19:44:30,710: Snapshot:4	Epoch:1	Loss:12.736	translation_Loss:6.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.435                                                   	MRR:20.71	Hits@10:43.16	Best:20.85
2024-12-27 19:44:38,534: Snapshot:4	Epoch:2	Loss:12.466	translation_Loss:5.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.503                                                   	MRR:20.65	Hits@10:43.11	Best:20.85
2024-12-27 19:44:46,409: Snapshot:4	Epoch:3	Loss:12.464	translation_Loss:5.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.565                                                   	MRR:20.9	Hits@10:43.38	Best:20.9
2024-12-27 19:44:54,267: Snapshot:4	Epoch:4	Loss:12.471	translation_Loss:5.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.625                                                   	MRR:21.04	Hits@10:43.41	Best:21.04
2024-12-27 19:45:02,201: Snapshot:4	Epoch:5	Loss:12.508	translation_Loss:5.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.649                                                   	MRR:20.74	Hits@10:43.0	Best:21.04
2024-12-27 19:45:10,145: Snapshot:4	Epoch:6	Loss:12.52	translation_Loss:5.852	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.668                                                   	MRR:20.86	Hits@10:43.3	Best:21.04
2024-12-27 19:45:17,961: Snapshot:4	Epoch:7	Loss:12.468	translation_Loss:5.803	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.665                                                   	MRR:20.6	Hits@10:42.71	Best:21.04
2024-12-27 19:45:25,796: Snapshot:4	Epoch:8	Loss:12.417	translation_Loss:5.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.631                                                   	MRR:20.45	Hits@10:42.84	Best:21.04
2024-12-27 19:45:34,151: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 21.04
2024-12-27 19:45:34,151: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:12.537 MRR:20.57 Best Results: 21.04
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:45:34,151: Snapshot:4	Epoch:9	Loss:12.537	translation_Loss:5.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.7                                                   	MRR:20.57	Hits@10:42.84	Best:21.04
2024-12-27 19:45:41,770: Snapshot:4	Epoch:10	Loss:69.363	translation_Loss:69.212	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.57	Hits@10:42.84	Best:21.04
2024-12-27 19:45:49,332: End of token training: 4 Epoch: 11 Loss:69.272 MRR:20.57 Best Results: 21.04
2024-12-27 19:45:49,333: Snapshot:4	Epoch:11	Loss:69.272	translation_Loss:69.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.57	Hits@10:42.84	Best:21.04
2024-12-27 19:45:49,608: => loading checkpoint './checkpoint/FACTfact_0.001_512_5000/4model_best.tar'
2024-12-27 19:46:06,546: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2278 | 0.1452 | 0.2641 | 0.3131 |  0.3805 |
|     1      | 0.202  | 0.1251 | 0.2345 | 0.2828 |  0.3492 |
|     2      | 0.1936 | 0.1156 | 0.2212 | 0.2728 |  0.346  |
|     3      | 0.1831 | 0.1004 | 0.2085 | 0.266  |  0.3509 |
|     4      | 0.2081 | 0.1012 | 0.2412 | 0.3219 |  0.4285 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:46:06,566: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2418 | 0.1591 | 0.2818 | 0.3303 |  0.3915 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2452 | 0.1609 | 0.2865 | 0.3348 |  0.3966 |
|     1      | 0.2087 | 0.1321 | 0.2449 | 0.288  |  0.3476 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1575 | 0.2832 | 0.3342 |  0.4004 |
|     1      | 0.2118 | 0.1325 | 0.2484 | 0.297  |  0.359  |
|     2      | 0.2005 | 0.1229 | 0.2344 | 0.2817 |  0.3448 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2361 | 0.1518 | 0.273  | 0.3246 |  0.3911 |
|     1      | 0.2101 | 0.1316 | 0.2441 | 0.2955 |  0.359  |
|     2      | 0.2014 | 0.1214 | 0.2337 | 0.2852 |  0.3564 |
|     3      | 0.1893 | 0.1065 | 0.2189 | 0.2758 |  0.3526 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2278 | 0.1452 | 0.2641 | 0.3131 |  0.3805 |
|     1      | 0.202  | 0.1251 | 0.2345 | 0.2828 |  0.3492 |
|     2      | 0.1936 | 0.1156 | 0.2212 | 0.2728 |  0.346  |
|     3      | 0.1831 | 0.1004 | 0.2085 | 0.266  |  0.3509 |
|     4      | 0.2081 | 0.1012 | 0.2412 | 0.3219 |  0.4285 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:46:06,566: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 97.80606079101562  |   0.242   |    0.159     |    0.282     |     0.392     |
|    1     | 97.63897776603699  |   0.227   |    0.146     |    0.266     |     0.372     |
|    2     | 131.62096977233887 |   0.218   |    0.138     |    0.255     |     0.368     |
|    3     | 93.10502791404724  |   0.209   |    0.128     |    0.242     |     0.365     |
|    4     | 109.19886326789856 |   0.203   |    0.117     |    0.234     |     0.371     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:46:06,566: Sum_Training_Time:529.3698995113373
2024-12-27 19:46:06,566: Every_Training_Time:[97.80606079101562, 97.63897776603699, 131.62096977233887, 93.10502791404724, 109.19886326789856]
2024-12-27 19:46:06,566: Forward transfer: 0.16804999999999998 Backward transfer: -0.008449999999999992
2024-12-27 19:46:45,571: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227194610/FACTfact_0.001_512_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_512_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_512_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:46:55,670: Snapshot:0	Epoch:0	Loss:92.885	translation_Loss:92.885	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.47	Hits@10:24.18	Best:11.47
2024-12-27 19:47:02,251: Snapshot:0	Epoch:1	Loss:56.064	translation_Loss:56.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.43	Hits@10:33.78	Best:17.43
2024-12-27 19:47:08,799: Snapshot:0	Epoch:2	Loss:32.559	translation_Loss:32.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.74	Hits@10:37.82	Best:21.74
2024-12-27 19:47:15,350: Snapshot:0	Epoch:3	Loss:18.184	translation_Loss:18.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:39.31	Best:23.89
2024-12-27 19:47:21,938: Snapshot:0	Epoch:4	Loss:10.212	translation_Loss:10.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.67	Hits@10:40.08	Best:24.67
2024-12-27 19:47:28,611: Snapshot:0	Epoch:5	Loss:6.309	translation_Loss:6.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.15	Best:24.78
2024-12-27 19:47:35,188: Snapshot:0	Epoch:6	Loss:4.354	translation_Loss:4.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:40.17	Best:25.04
2024-12-27 19:47:41,756: Snapshot:0	Epoch:7	Loss:3.297	translation_Loss:3.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.04	Hits@10:40.35	Best:25.04
2024-12-27 19:47:48,819: Snapshot:0	Epoch:8	Loss:2.658	translation_Loss:2.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:40.31	Best:25.04
2024-12-27 19:47:55,370: Snapshot:0	Epoch:9	Loss:2.291	translation_Loss:2.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.25	Best:25.04
2024-12-27 19:48:01,945: Snapshot:0	Epoch:10	Loss:2.052	translation_Loss:2.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.27	Best:25.04
2024-12-27 19:48:08,482: Early Stopping! Snapshot: 0 Epoch: 11 Best Results: 25.04
2024-12-27 19:48:08,483: Start to training tokens! Snapshot: 0 Epoch: 11 Loss:1.845 MRR:25.0 Best Results: 25.04
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:48:08,483: Snapshot:0	Epoch:11	Loss:1.845	translation_Loss:1.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:40.23	Best:25.04
2024-12-27 19:48:15,749: Snapshot:0	Epoch:12	Loss:73.178	translation_Loss:73.022	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:40.23	Best:25.04
2024-12-27 19:48:22,473: End of token training: 0 Epoch: 13 Loss:73.159 MRR:25.0 Best Results: 25.04
2024-12-27 19:48:22,473: Snapshot:0	Epoch:13	Loss:73.159	translation_Loss:73.159	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.0	Hits@10:40.23	Best:25.04
2024-12-27 19:48:22,814: => loading checkpoint './checkpoint/FACTfact_0.001_512_10000/0model_best.tar'
2024-12-27 19:48:25,578: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2413 | 0.1589 | 0.2784 | 0.3287 |  0.3926 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:48:51,018: Snapshot:1	Epoch:0	Loss:50.42	translation_Loss:41.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.046                                                   	MRR:19.95	Hits@10:33.1	Best:19.95
2024-12-27 19:48:58,586: Snapshot:1	Epoch:1	Loss:43.189	translation_Loss:33.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.074                                                   	MRR:20.54	Hits@10:33.87	Best:20.54
2024-12-27 19:49:06,143: Snapshot:1	Epoch:2	Loss:40.3	translation_Loss:30.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.052                                                   	MRR:20.85	Hits@10:34.24	Best:20.85
2024-12-27 19:49:14,082: Snapshot:1	Epoch:3	Loss:39.181	translation_Loss:29.212	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.969                                                   	MRR:20.81	Hits@10:34.3	Best:20.85
2024-12-27 19:49:21,515: Snapshot:1	Epoch:4	Loss:38.703	translation_Loss:28.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.935                                                   	MRR:20.82	Hits@10:34.28	Best:20.85
2024-12-27 19:49:29,013: Snapshot:1	Epoch:5	Loss:38.607	translation_Loss:28.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.94                                                   	MRR:20.9	Hits@10:34.29	Best:20.9
2024-12-27 19:49:36,503: Snapshot:1	Epoch:6	Loss:38.33	translation_Loss:28.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.948                                                   	MRR:20.85	Hits@10:34.2	Best:20.9
2024-12-27 19:49:44,047: Snapshot:1	Epoch:7	Loss:38.275	translation_Loss:28.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.956                                                   	MRR:20.81	Hits@10:34.19	Best:20.9
2024-12-27 19:49:51,572: Snapshot:1	Epoch:8	Loss:38.334	translation_Loss:28.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.987                                                   	MRR:20.83	Hits@10:34.3	Best:20.9
2024-12-27 19:49:59,051: Snapshot:1	Epoch:9	Loss:38.133	translation_Loss:28.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.954                                                   	MRR:20.84	Hits@10:34.31	Best:20.9
2024-12-27 19:50:06,622: Early Stopping! Snapshot: 1 Epoch: 10 Best Results: 20.9
2024-12-27 19:50:06,622: Start to training tokens! Snapshot: 1 Epoch: 10 Loss:38.105 MRR:20.81 Best Results: 20.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:50:06,623: Snapshot:1	Epoch:10	Loss:38.105	translation_Loss:28.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.988                                                   	MRR:20.81	Hits@10:34.23	Best:20.9
2024-12-27 19:50:13,945: Snapshot:1	Epoch:11	Loss:88.936	translation_Loss:88.785	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.81	Hits@10:34.23	Best:20.9
2024-12-27 19:50:21,205: End of token training: 1 Epoch: 12 Loss:88.832 MRR:20.81 Best Results: 20.9
2024-12-27 19:50:21,205: Snapshot:1	Epoch:12	Loss:88.832	translation_Loss:88.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.81	Hits@10:34.23	Best:20.9
2024-12-27 19:50:21,564: => loading checkpoint './checkpoint/FACTfact_0.001_512_10000/1model_best.tar'
2024-12-27 19:50:27,506: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.244  | 0.1607 | 0.2824 | 0.3334 |  0.394  |
|     1      | 0.2055 | 0.1294 | 0.2425 | 0.2852 |  0.3426 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:50:53,936: Snapshot:2	Epoch:0	Loss:43.71	translation_Loss:33.896	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.814                                                   	MRR:19.11	Hits@10:33.05	Best:19.11
2024-12-27 19:51:01,659: Snapshot:2	Epoch:1	Loss:41.391	translation_Loss:30.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.165                                                   	MRR:19.27	Hits@10:33.16	Best:19.27
2024-12-27 19:51:09,371: Snapshot:2	Epoch:2	Loss:40.495	translation_Loss:29.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.214                                                   	MRR:19.35	Hits@10:33.27	Best:19.35
2024-12-27 19:51:17,065: Snapshot:2	Epoch:3	Loss:40.214	translation_Loss:28.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.247                                                   	MRR:19.25	Hits@10:33.32	Best:19.35
2024-12-27 19:51:24,773: Snapshot:2	Epoch:4	Loss:40.144	translation_Loss:28.852	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.292                                                   	MRR:19.26	Hits@10:33.39	Best:19.35
2024-12-27 19:51:32,538: Snapshot:2	Epoch:5	Loss:40.16	translation_Loss:28.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.306                                                   	MRR:19.39	Hits@10:33.35	Best:19.39
2024-12-27 19:51:40,231: Snapshot:2	Epoch:6	Loss:40.171	translation_Loss:28.852	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.32                                                   	MRR:19.32	Hits@10:33.36	Best:19.39
2024-12-27 19:51:47,964: Snapshot:2	Epoch:7	Loss:40.138	translation_Loss:28.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.352                                                   	MRR:19.34	Hits@10:33.4	Best:19.39
2024-12-27 19:51:55,608: Snapshot:2	Epoch:8	Loss:40.077	translation_Loss:28.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.335                                                   	MRR:19.37	Hits@10:33.47	Best:19.39
2024-12-27 19:52:03,260: Snapshot:2	Epoch:9	Loss:40.075	translation_Loss:28.727	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.348                                                   	MRR:19.24	Hits@10:33.35	Best:19.39
2024-12-27 19:52:10,910: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 19.39
2024-12-27 19:52:10,911: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:40.013 MRR:19.32 Best Results: 19.39
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:52:10,911: Snapshot:2	Epoch:10	Loss:40.013	translation_Loss:28.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.341                                                   	MRR:19.32	Hits@10:33.25	Best:19.39
2024-12-27 19:52:18,365: Snapshot:2	Epoch:11	Loss:90.184	translation_Loss:90.025	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.32	Hits@10:33.25	Best:19.39
2024-12-27 19:52:25,835: End of token training: 2 Epoch: 12 Loss:90.043 MRR:19.32 Best Results: 19.39
2024-12-27 19:52:25,835: Snapshot:2	Epoch:12	Loss:90.043	translation_Loss:90.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.32	Hits@10:33.25	Best:19.39
2024-12-27 19:52:26,198: => loading checkpoint './checkpoint/FACTfact_0.001_512_10000/2model_best.tar'
2024-12-27 19:52:35,556: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2427 | 0.158  | 0.2825 | 0.3328 |  0.3977 |
|     1      | 0.2083 | 0.1297 | 0.2467 | 0.2933 |  0.3512 |
|     2      | 0.1928 | 0.1162 | 0.2273 | 0.2755 |  0.3332 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:53:02,398: Snapshot:3	Epoch:0	Loss:36.197	translation_Loss:26.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.575                                                   	MRR:17.86	Hits@10:32.93	Best:17.86
2024-12-27 19:53:10,174: Snapshot:3	Epoch:1	Loss:35.361	translation_Loss:24.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:10.906                                                   	MRR:17.88	Hits@10:33.05	Best:17.88
2024-12-27 19:53:17,973: Snapshot:3	Epoch:2	Loss:35.197	translation_Loss:24.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.04                                                   	MRR:17.84	Hits@10:33.02	Best:17.88
2024-12-27 19:53:25,783: Snapshot:3	Epoch:3	Loss:35.069	translation_Loss:24.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.045                                                   	MRR:17.89	Hits@10:33.02	Best:17.89
2024-12-27 19:53:33,579: Snapshot:3	Epoch:4	Loss:35.141	translation_Loss:24.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.112                                                   	MRR:17.82	Hits@10:32.99	Best:17.89
2024-12-27 19:53:41,352: Snapshot:3	Epoch:5	Loss:35.259	translation_Loss:24.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.163                                                   	MRR:17.78	Hits@10:33.05	Best:17.89
2024-12-27 19:53:49,082: Snapshot:3	Epoch:6	Loss:35.211	translation_Loss:24.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.173                                                   	MRR:17.81	Hits@10:33.04	Best:17.89
2024-12-27 19:53:56,831: Snapshot:3	Epoch:7	Loss:35.212	translation_Loss:24.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.154                                                   	MRR:17.88	Hits@10:33.02	Best:17.89
2024-12-27 19:54:04,592: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 17.89
2024-12-27 19:54:04,593: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:35.26 MRR:17.89 Best Results: 17.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:54:04,593: Snapshot:3	Epoch:8	Loss:35.26	translation_Loss:24.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:11.191                                                   	MRR:17.89	Hits@10:32.85	Best:17.89
2024-12-27 19:54:12,049: Snapshot:3	Epoch:9	Loss:88.052	translation_Loss:87.902	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.89	Hits@10:32.85	Best:17.89
2024-12-27 19:54:19,536: End of token training: 3 Epoch: 10 Loss:87.94 MRR:17.89 Best Results: 17.89
2024-12-27 19:54:19,536: Snapshot:3	Epoch:10	Loss:87.94	translation_Loss:87.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:17.89	Hits@10:32.85	Best:17.89
2024-12-27 19:54:19,901: => loading checkpoint './checkpoint/FACTfact_0.001_512_10000/3model_best.tar'
2024-12-27 19:54:31,978: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2391 | 0.1554 | 0.2776 | 0.3274 |  0.3945 |
|     1      | 0.2084 | 0.1307 | 0.2445 | 0.2914 |  0.3536 |
|     2      | 0.1964 | 0.1191 | 0.2293 | 0.2805 |  0.3426 |
|     3      | 0.1784 | 0.1011 | 0.2063 | 0.2601 |  0.3289 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 19:54:58,408: Snapshot:4	Epoch:0	Loss:23.284	translation_Loss:15.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.808                                                   	MRR:19.45	Hits@10:40.16	Best:19.45
2024-12-27 19:55:06,338: Snapshot:4	Epoch:1	Loss:20.444	translation_Loss:11.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.827                                                   	MRR:19.67	Hits@10:39.91	Best:19.67
2024-12-27 19:55:14,187: Snapshot:4	Epoch:2	Loss:20.185	translation_Loss:11.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.911                                                   	MRR:19.83	Hits@10:39.98	Best:19.83
2024-12-27 19:55:22,047: Snapshot:4	Epoch:3	Loss:20.116	translation_Loss:11.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.95                                                   	MRR:19.7	Hits@10:40.19	Best:19.83
2024-12-27 19:55:29,814: Snapshot:4	Epoch:4	Loss:20.101	translation_Loss:11.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.966                                                   	MRR:19.72	Hits@10:39.88	Best:19.83
2024-12-27 19:55:37,633: Snapshot:4	Epoch:5	Loss:20.108	translation_Loss:11.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.973                                                   	MRR:19.63	Hits@10:39.81	Best:19.83
2024-12-27 19:55:45,568: Snapshot:4	Epoch:6	Loss:20.166	translation_Loss:11.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.014                                                   	MRR:19.87	Hits@10:39.91	Best:19.87
2024-12-27 19:55:53,472: Snapshot:4	Epoch:7	Loss:20.041	translation_Loss:11.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.98                                                   	MRR:19.57	Hits@10:40.02	Best:19.87
2024-12-27 19:56:01,842: Snapshot:4	Epoch:8	Loss:20.072	translation_Loss:11.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.987                                                   	MRR:19.6	Hits@10:39.89	Best:19.87
2024-12-27 19:56:09,639: Snapshot:4	Epoch:9	Loss:19.999	translation_Loss:11.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.967                                                   	MRR:19.72	Hits@10:40.05	Best:19.87
2024-12-27 19:56:17,432: Snapshot:4	Epoch:10	Loss:20.062	translation_Loss:11.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.979                                                   	MRR:19.63	Hits@10:40.04	Best:19.87
2024-12-27 19:56:25,247: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 19.87
2024-12-27 19:56:25,247: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:20.096 MRR:19.74 Best Results: 19.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:56:25,248: Snapshot:4	Epoch:11	Loss:20.096	translation_Loss:11.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:9.03                                                   	MRR:19.74	Hits@10:40.05	Best:19.87
2024-12-27 19:56:32,754: Snapshot:4	Epoch:12	Loss:73.767	translation_Loss:73.615	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.74	Hits@10:40.05	Best:19.87
2024-12-27 19:56:40,284: End of token training: 4 Epoch: 13 Loss:73.626 MRR:19.74 Best Results: 19.87
2024-12-27 19:56:40,285: Snapshot:4	Epoch:13	Loss:73.626	translation_Loss:73.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.74	Hits@10:40.05	Best:19.87
2024-12-27 19:56:40,606: => loading checkpoint './checkpoint/FACTfact_0.001_512_10000/4model_best.tar'
2024-12-27 19:56:56,717: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.233  | 0.1502 | 0.2706 |  0.32  |  0.3879 |
|     1      | 0.202  | 0.1247 | 0.2364 | 0.2858 |  0.3452 |
|     2      | 0.1899 | 0.112  | 0.221  | 0.2717 |  0.3393 |
|     3      | 0.1774 | 0.0989 | 0.2019 | 0.2593 |  0.3363 |
|     4      | 0.1976 | 0.1005 | 0.2277 | 0.2985 |  0.3958 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 19:56:56,720: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2413 | 0.1589 | 0.2784 | 0.3287 |  0.3926 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.244  | 0.1607 | 0.2824 | 0.3334 |  0.394  |
|     1      | 0.2055 | 0.1294 | 0.2425 | 0.2852 |  0.3426 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2427 | 0.158  | 0.2825 | 0.3328 |  0.3977 |
|     1      | 0.2083 | 0.1297 | 0.2467 | 0.2933 |  0.3512 |
|     2      | 0.1928 | 0.1162 | 0.2273 | 0.2755 |  0.3332 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2391 | 0.1554 | 0.2776 | 0.3274 |  0.3945 |
|     1      | 0.2084 | 0.1307 | 0.2445 | 0.2914 |  0.3536 |
|     2      | 0.1964 | 0.1191 | 0.2293 | 0.2805 |  0.3426 |
|     3      | 0.1784 | 0.1011 | 0.2063 | 0.2601 |  0.3289 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.233  | 0.1502 | 0.2706 |  0.32  |  0.3879 |
|     1      | 0.202  | 0.1247 | 0.2364 | 0.2858 |  0.3452 |
|     2      | 0.1899 | 0.112  | 0.221  | 0.2717 |  0.3393 |
|     3      | 0.1774 | 0.0989 | 0.2019 | 0.2593 |  0.3363 |
|     4      | 0.1976 | 0.1005 | 0.2277 | 0.2985 |  0.3958 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 19:56:56,720: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 96.90150904655457  |   0.241   |    0.159     |    0.278     |     0.393     |
|    1     | 112.47071886062622 |   0.225   |    0.145     |    0.262     |     0.368     |
|    2     | 114.97010469436646 |   0.215   |    0.135     |    0.252     |     0.361     |
|    3     | 100.57983446121216 |   0.206   |    0.127     |    0.239     |     0.355     |
|    4     | 124.53803205490112 |    0.2    |    0.117     |    0.232     |     0.361     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 19:56:56,720: Sum_Training_Time:549.4601991176605
2024-12-27 19:56:56,720: Every_Training_Time:[96.90150904655457, 112.47071886062622, 114.97010469436646, 100.57983446121216, 124.53803205490112]
2024-12-27 19:56:56,720: Forward transfer: 0.16215000000000002 Backward transfer: -0.003924999999999984
2024-12-27 19:57:35,724: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227195700/FACTfact_0.001_1024_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_1024_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_1024_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 19:57:45,763: Snapshot:0	Epoch:0	Loss:47.826	translation_Loss:47.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.02	Hits@10:20.16	Best:9.02
2024-12-27 19:57:52,184: Snapshot:0	Epoch:1	Loss:30.672	translation_Loss:30.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.72	Hits@10:30.82	Best:14.72
2024-12-27 19:57:58,614: Snapshot:0	Epoch:2	Loss:19.46	translation_Loss:19.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.0	Hits@10:36.07	Best:19.0
2024-12-27 19:58:05,025: Snapshot:0	Epoch:3	Loss:11.991	translation_Loss:11.991	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.0	Hits@10:38.49	Best:22.0
2024-12-27 19:58:11,932: Snapshot:0	Epoch:4	Loss:7.189	translation_Loss:7.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.36	Best:23.63
2024-12-27 19:58:18,389: Snapshot:0	Epoch:5	Loss:4.447	translation_Loss:4.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:39.8	Best:24.41
2024-12-27 19:58:24,805: Snapshot:0	Epoch:6	Loss:2.929	translation_Loss:2.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:40.21	Best:24.76
2024-12-27 19:58:31,246: Snapshot:0	Epoch:7	Loss:2.076	translation_Loss:2.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.89	Hits@10:40.27	Best:24.89
2024-12-27 19:58:37,694: Snapshot:0	Epoch:8	Loss:1.567	translation_Loss:1.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.91	Hits@10:40.15	Best:24.91
2024-12-27 19:58:44,112: Snapshot:0	Epoch:9	Loss:1.272	translation_Loss:1.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.89	Hits@10:40.1	Best:24.91
2024-12-27 19:58:50,601: Snapshot:0	Epoch:10	Loss:1.087	translation_Loss:1.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:40.2	Best:24.91
2024-12-27 19:58:57,018: Snapshot:0	Epoch:11	Loss:0.931	translation_Loss:0.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:40.19	Best:24.97
2024-12-27 19:59:03,486: Snapshot:0	Epoch:12	Loss:0.848	translation_Loss:0.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:40.13	Best:24.99
2024-12-27 19:59:09,889: Snapshot:0	Epoch:13	Loss:0.777	translation_Loss:0.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.11	Best:24.99
2024-12-27 19:59:16,277: Snapshot:0	Epoch:14	Loss:0.731	translation_Loss:0.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.13	Best:24.99
2024-12-27 19:59:22,678: Snapshot:0	Epoch:15	Loss:0.672	translation_Loss:0.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:40.01	Best:24.99
2024-12-27 19:59:29,120: Snapshot:0	Epoch:16	Loss:0.627	translation_Loss:0.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.65	Hits@10:40.15	Best:24.99
2024-12-27 19:59:35,519: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 24.99
2024-12-27 19:59:35,519: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.593 MRR:24.66 Best Results: 24.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 19:59:35,520: Snapshot:0	Epoch:17	Loss:0.593	translation_Loss:0.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:40.15	Best:24.99
2024-12-27 19:59:42,577: Snapshot:0	Epoch:18	Loss:36.47	translation_Loss:36.313	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:40.15	Best:24.99
2024-12-27 19:59:49,082: End of token training: 0 Epoch: 19 Loss:36.353 MRR:24.66 Best Results: 24.99
2024-12-27 19:59:49,082: Snapshot:0	Epoch:19	Loss:36.353	translation_Loss:36.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.66	Hits@10:40.15	Best:24.99
2024-12-27 19:59:49,441: => loading checkpoint './checkpoint/FACTfact_0.001_1024_1000/0model_best.tar'
2024-12-27 19:59:52,183: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.1603 | 0.2836 | 0.3322 |  0.3945 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:00:17,393: Snapshot:1	Epoch:0	Loss:20.589	translation_Loss:18.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.154                                                   	MRR:21.17	Hits@10:35.32	Best:21.17
2024-12-27 20:00:24,659: Snapshot:1	Epoch:1	Loss:13.055	translation_Loss:9.034	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.021                                                   	MRR:22.41	Hits@10:36.88	Best:22.41
2024-12-27 20:00:31,889: Snapshot:1	Epoch:2	Loss:11.046	translation_Loss:6.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.639                                                   	MRR:22.75	Hits@10:37.47	Best:22.75
2024-12-27 20:00:39,063: Snapshot:1	Epoch:3	Loss:10.479	translation_Loss:5.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.912                                                   	MRR:22.73	Hits@10:37.54	Best:22.75
2024-12-27 20:00:46,259: Snapshot:1	Epoch:4	Loss:10.287	translation_Loss:5.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.03                                                   	MRR:22.78	Hits@10:37.61	Best:22.78
2024-12-27 20:00:53,425: Snapshot:1	Epoch:5	Loss:10.197	translation_Loss:5.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.111                                                   	MRR:22.85	Hits@10:37.52	Best:22.85
2024-12-27 20:01:00,745: Snapshot:1	Epoch:6	Loss:10.19	translation_Loss:5.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.167                                                   	MRR:22.86	Hits@10:37.5	Best:22.86
2024-12-27 20:01:07,953: Snapshot:1	Epoch:7	Loss:10.152	translation_Loss:4.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.205                                                   	MRR:22.8	Hits@10:37.6	Best:22.86
2024-12-27 20:01:15,076: Snapshot:1	Epoch:8	Loss:10.152	translation_Loss:4.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.224                                                   	MRR:22.84	Hits@10:37.6	Best:22.86
2024-12-27 20:01:22,282: Snapshot:1	Epoch:9	Loss:10.116	translation_Loss:4.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.243                                                   	MRR:22.93	Hits@10:37.68	Best:22.93
2024-12-27 20:01:29,467: Snapshot:1	Epoch:10	Loss:10.129	translation_Loss:4.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.27                                                   	MRR:22.93	Hits@10:37.43	Best:22.93
2024-12-27 20:01:37,154: Snapshot:1	Epoch:11	Loss:10.09	translation_Loss:4.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.273                                                   	MRR:22.67	Hits@10:37.48	Best:22.93
2024-12-27 20:01:44,449: Snapshot:1	Epoch:12	Loss:10.176	translation_Loss:4.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.301                                                   	MRR:22.84	Hits@10:37.69	Best:22.93
2024-12-27 20:01:51,678: Snapshot:1	Epoch:13	Loss:10.112	translation_Loss:4.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.315                                                   	MRR:22.78	Hits@10:37.66	Best:22.93
2024-12-27 20:01:58,823: Early Stopping! Snapshot: 1 Epoch: 14 Best Results: 22.93
2024-12-27 20:01:58,824: Start to training tokens! Snapshot: 1 Epoch: 14 Loss:10.109 MRR:22.8 Best Results: 22.93
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:01:58,824: Snapshot:1	Epoch:14	Loss:10.109	translation_Loss:4.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.31                                                   	MRR:22.8	Hits@10:37.55	Best:22.93
2024-12-27 20:02:05,805: Snapshot:1	Epoch:15	Loss:40.127	translation_Loss:39.976	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.8	Hits@10:37.55	Best:22.93
2024-12-27 20:02:12,787: End of token training: 1 Epoch: 16 Loss:40.06 MRR:22.8 Best Results: 22.93
2024-12-27 20:02:12,788: Snapshot:1	Epoch:16	Loss:40.06	translation_Loss:40.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.8	Hits@10:37.55	Best:22.93
2024-12-27 20:02:13,147: => loading checkpoint './checkpoint/FACTfact_0.001_1024_1000/1model_best.tar'
2024-12-27 20:02:19,498: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2531 | 0.1647 | 0.2951 | 0.3459 |  0.417  |
|     1      | 0.2307 | 0.1504 | 0.2678 | 0.3168 |  0.3789 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:02:44,906: Snapshot:2	Epoch:0	Loss:12.288	translation_Loss:10.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.865                                                   	MRR:21.71	Hits@10:37.51	Best:21.71
2024-12-27 20:02:52,285: Snapshot:2	Epoch:1	Loss:7.539	translation_Loss:4.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.057                                                   	MRR:22.19	Hits@10:38.25	Best:22.19
2024-12-27 20:02:59,698: Snapshot:2	Epoch:2	Loss:6.826	translation_Loss:3.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.358                                                   	MRR:22.22	Hits@10:38.41	Best:22.22
2024-12-27 20:03:07,054: Snapshot:2	Epoch:3	Loss:6.714	translation_Loss:3.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.549                                                   	MRR:22.1	Hits@10:38.24	Best:22.22
2024-12-27 20:03:14,346: Snapshot:2	Epoch:4	Loss:6.742	translation_Loss:3.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.652                                                   	MRR:22.21	Hits@10:38.32	Best:22.22
2024-12-27 20:03:21,684: Snapshot:2	Epoch:5	Loss:6.788	translation_Loss:3.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.742                                                   	MRR:22.23	Hits@10:38.33	Best:22.23
2024-12-27 20:03:29,058: Snapshot:2	Epoch:6	Loss:6.829	translation_Loss:3.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.801                                                   	MRR:22.28	Hits@10:38.1	Best:22.28
2024-12-27 20:03:36,420: Snapshot:2	Epoch:7	Loss:6.828	translation_Loss:2.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.845                                                   	MRR:22.22	Hits@10:38.24	Best:22.28
2024-12-27 20:03:43,768: Snapshot:2	Epoch:8	Loss:6.881	translation_Loss:2.998	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.884                                                   	MRR:22.24	Hits@10:38.28	Best:22.28
2024-12-27 20:03:51,080: Snapshot:2	Epoch:9	Loss:6.852	translation_Loss:2.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.875                                                   	MRR:22.2	Hits@10:38.1	Best:22.28
2024-12-27 20:03:58,400: Snapshot:2	Epoch:10	Loss:6.885	translation_Loss:2.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.909                                                   	MRR:22.12	Hits@10:38.26	Best:22.28
2024-12-27 20:04:05,707: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 22.28
2024-12-27 20:04:05,707: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:6.882 MRR:22.26 Best Results: 22.28
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:04:05,708: Snapshot:2	Epoch:11	Loss:6.882	translation_Loss:2.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.897                                                   	MRR:22.26	Hits@10:38.27	Best:22.28
2024-12-27 20:04:12,875: Snapshot:2	Epoch:12	Loss:39.279	translation_Loss:39.121	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.26	Hits@10:38.27	Best:22.28
2024-12-27 20:04:20,021: End of token training: 2 Epoch: 13 Loss:39.182 MRR:22.26 Best Results: 22.28
2024-12-27 20:04:20,021: Snapshot:2	Epoch:13	Loss:39.182	translation_Loss:39.182	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.26	Hits@10:38.27	Best:22.28
2024-12-27 20:04:20,379: => loading checkpoint './checkpoint/FACTfact_0.001_1024_1000/2model_best.tar'
2024-12-27 20:04:29,632: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1537 | 0.2797 | 0.3324 |  0.4055 |
|     1      | 0.2289 | 0.1464 | 0.2629 | 0.3172 |  0.3884 |
|     2      | 0.222  | 0.1378 | 0.2571 | 0.3099 |  0.3828 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:04:54,988: Snapshot:3	Epoch:0	Loss:6.241	translation_Loss:4.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.399                                                   	MRR:20.17	Hits@10:38.29	Best:20.17
2024-12-27 20:05:02,446: Snapshot:3	Epoch:1	Loss:3.787	translation_Loss:1.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.877                                                   	MRR:20.15	Hits@10:38.09	Best:20.17
2024-12-27 20:05:10,278: Snapshot:3	Epoch:2	Loss:3.584	translation_Loss:1.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.961                                                   	MRR:20.09	Hits@10:38.16	Best:20.17
2024-12-27 20:05:17,719: Snapshot:3	Epoch:3	Loss:3.645	translation_Loss:1.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.092                                                   	MRR:20.25	Hits@10:38.32	Best:20.25
2024-12-27 20:05:25,149: Snapshot:3	Epoch:4	Loss:3.705	translation_Loss:1.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.163                                                   	MRR:20.11	Hits@10:38.0	Best:20.25
2024-12-27 20:05:32,617: Snapshot:3	Epoch:5	Loss:3.73	translation_Loss:1.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.207                                                   	MRR:20.08	Hits@10:38.02	Best:20.25
2024-12-27 20:05:40,016: Snapshot:3	Epoch:6	Loss:3.791	translation_Loss:1.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.247                                                   	MRR:19.97	Hits@10:37.9	Best:20.25
2024-12-27 20:05:47,496: Snapshot:3	Epoch:7	Loss:3.788	translation_Loss:1.52	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.269                                                   	MRR:20.09	Hits@10:37.98	Best:20.25
2024-12-27 20:05:54,925: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 20.25
2024-12-27 20:05:54,925: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:3.866 MRR:20.2 Best Results: 20.25
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:05:54,925: Snapshot:3	Epoch:8	Loss:3.866	translation_Loss:1.551	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.314                                                   	MRR:20.2	Hits@10:37.82	Best:20.25
2024-12-27 20:06:02,217: Snapshot:3	Epoch:9	Loss:37.274	translation_Loss:37.123	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.2	Hits@10:37.82	Best:20.25
2024-12-27 20:06:09,479: End of token training: 3 Epoch: 10 Loss:37.184 MRR:20.2 Best Results: 20.25
2024-12-27 20:06:09,480: Snapshot:3	Epoch:10	Loss:37.184	translation_Loss:37.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.2	Hits@10:37.82	Best:20.25
2024-12-27 20:06:09,838: => loading checkpoint './checkpoint/FACTfact_0.001_1024_1000/3model_best.tar'
2024-12-27 20:06:22,584: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2248 | 0.1422 | 0.2574 | 0.3069 |  0.3826 |
|     1      | 0.2136 | 0.1343 | 0.2411 | 0.2967 |  0.3716 |
|     2      | 0.2081 | 0.1245 | 0.2364 | 0.2956 |  0.3751 |
|     3      | 0.2052 | 0.1143 | 0.2358 | 0.301  |  0.3867 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:06:48,080: Snapshot:4	Epoch:0	Loss:3.144	translation_Loss:2.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.78                                                   	MRR:21.63	Hits@10:45.03	Best:21.63
2024-12-27 20:06:55,549: Snapshot:4	Epoch:1	Loss:1.439	translation_Loss:0.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.783                                                   	MRR:21.73	Hits@10:45.42	Best:21.73
2024-12-27 20:07:03,029: Snapshot:4	Epoch:2	Loss:1.259	translation_Loss:0.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.779                                                   	MRR:21.61	Hits@10:45.24	Best:21.73
2024-12-27 20:07:10,850: Snapshot:4	Epoch:3	Loss:1.259	translation_Loss:0.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.814                                                   	MRR:21.63	Hits@10:44.83	Best:21.73
2024-12-27 20:07:18,325: Snapshot:4	Epoch:4	Loss:1.295	translation_Loss:0.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:21.81	Hits@10:45.18	Best:21.81
2024-12-27 20:07:25,855: Snapshot:4	Epoch:5	Loss:1.296	translation_Loss:0.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.862                                                   	MRR:21.92	Hits@10:45.75	Best:21.92
2024-12-27 20:07:33,338: Snapshot:4	Epoch:6	Loss:1.302	translation_Loss:0.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.884                                                   	MRR:21.69	Hits@10:44.92	Best:21.92
2024-12-27 20:07:40,794: Snapshot:4	Epoch:7	Loss:1.296	translation_Loss:0.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.88                                                   	MRR:21.46	Hits@10:45.18	Best:21.92
2024-12-27 20:07:48,339: Snapshot:4	Epoch:8	Loss:1.306	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.887                                                   	MRR:21.85	Hits@10:45.14	Best:21.92
2024-12-27 20:07:55,784: Snapshot:4	Epoch:9	Loss:1.321	translation_Loss:0.423	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.898                                                   	MRR:21.42	Hits@10:45.1	Best:21.92
2024-12-27 20:08:03,246: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 21.92
2024-12-27 20:08:03,247: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:1.327 MRR:21.5 Best Results: 21.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:08:03,247: Snapshot:4	Epoch:10	Loss:1.327	translation_Loss:0.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.909                                                   	MRR:21.5	Hits@10:45.31	Best:21.92
2024-12-27 20:08:10,516: Snapshot:4	Epoch:11	Loss:31.255	translation_Loss:31.104	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.5	Hits@10:45.31	Best:21.92
2024-12-27 20:08:17,805: End of token training: 4 Epoch: 12 Loss:31.133 MRR:21.5 Best Results: 21.92
2024-12-27 20:08:17,805: Snapshot:4	Epoch:12	Loss:31.133	translation_Loss:31.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.5	Hits@10:45.31	Best:21.92
2024-12-27 20:08:18,164: => loading checkpoint './checkpoint/FACTfact_0.001_1024_1000/4model_best.tar'
2024-12-27 20:08:34,467: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2092 | 0.1299 | 0.2366 | 0.2883 |  0.3604 |
|     1      | 0.1978 | 0.1221 | 0.2243 | 0.2739 |  0.3475 |
|     2      | 0.1911 | 0.1115 | 0.2155 | 0.2714 |  0.3525 |
|     3      | 0.1873 | 0.0983 | 0.2137 | 0.2757 |  0.3664 |
|     4      | 0.2143 | 0.1004 | 0.2505 | 0.3356 |  0.4516 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:08:34,469: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.1603 | 0.2836 | 0.3322 |  0.3945 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2531 | 0.1647 | 0.2951 | 0.3459 |  0.417  |
|     1      | 0.2307 | 0.1504 | 0.2678 | 0.3168 |  0.3789 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1537 | 0.2797 | 0.3324 |  0.4055 |
|     1      | 0.2289 | 0.1464 | 0.2629 | 0.3172 |  0.3884 |
|     2      | 0.222  | 0.1378 | 0.2571 | 0.3099 |  0.3828 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2248 | 0.1422 | 0.2574 | 0.3069 |  0.3826 |
|     1      | 0.2136 | 0.1343 | 0.2411 | 0.2967 |  0.3716 |
|     2      | 0.2081 | 0.1245 | 0.2364 | 0.2956 |  0.3751 |
|     3      | 0.2052 | 0.1143 | 0.2358 | 0.301  |  0.3867 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2092 | 0.1299 | 0.2366 | 0.2883 |  0.3604 |
|     1      | 0.1978 | 0.1221 | 0.2243 | 0.2739 |  0.3475 |
|     2      | 0.1911 | 0.1115 | 0.2155 | 0.2714 |  0.3525 |
|     3      | 0.1873 | 0.0983 | 0.2137 | 0.2757 |  0.3664 |
|     4      | 0.2143 | 0.1004 | 0.2505 | 0.3356 |  0.4516 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:08:34,470: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 133.35754346847534 |   0.244   |     0.16     |    0.284     |     0.395     |
|    1     |  137.167897939682  |   0.242   |    0.158     |    0.281     |     0.398     |
|    2     | 117.20900774002075 |   0.231   |    0.146     |    0.267     |     0.392     |
|    3     | 96.16143584251404  |   0.213   |    0.129     |    0.243     |     0.379     |
|    4     | 111.76977014541626 |    0.2    |    0.112     |    0.228     |     0.376     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:08:34,470: Sum_Training_Time:595.6656551361084
2024-12-27 20:08:34,470: Every_Training_Time:[133.35754346847534, 137.167897939682, 117.20900774002075, 96.16143584251404, 111.76977014541626]
2024-12-27 20:08:34,470: Forward transfer: 0.1779 Backward transfer: -0.029074999999999997
2024-12-27 20:09:13,427: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227200838/FACTfact_0.001_1024_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_1024_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_1024_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:09:23,344: Snapshot:0	Epoch:0	Loss:47.826	translation_Loss:47.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.02	Hits@10:20.16	Best:9.02
2024-12-27 20:09:29,731: Snapshot:0	Epoch:1	Loss:30.673	translation_Loss:30.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.71	Hits@10:30.78	Best:14.71
2024-12-27 20:09:36,114: Snapshot:0	Epoch:2	Loss:19.459	translation_Loss:19.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.03	Hits@10:36.05	Best:19.03
2024-12-27 20:09:42,508: Snapshot:0	Epoch:3	Loss:11.987	translation_Loss:11.987	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.01	Hits@10:38.52	Best:22.01
2024-12-27 20:09:49,412: Snapshot:0	Epoch:4	Loss:7.189	translation_Loss:7.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.46	Best:23.69
2024-12-27 20:09:55,829: Snapshot:0	Epoch:5	Loss:4.443	translation_Loss:4.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:40.04	Best:24.46
2024-12-27 20:10:02,264: Snapshot:0	Epoch:6	Loss:2.931	translation_Loss:2.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:40.42	Best:24.87
2024-12-27 20:10:08,776: Snapshot:0	Epoch:7	Loss:2.066	translation_Loss:2.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:40.41	Best:24.98
2024-12-27 20:10:15,143: Snapshot:0	Epoch:8	Loss:1.57	translation_Loss:1.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.05	Hits@10:40.37	Best:25.05
2024-12-27 20:10:21,518: Snapshot:0	Epoch:9	Loss:1.275	translation_Loss:1.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:40.33	Best:25.05
2024-12-27 20:10:27,889: Snapshot:0	Epoch:10	Loss:1.085	translation_Loss:1.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:40.52	Best:25.05
2024-12-27 20:10:34,256: Snapshot:0	Epoch:11	Loss:0.929	translation_Loss:0.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.03	Hits@10:40.41	Best:25.05
2024-12-27 20:10:40,626: Snapshot:0	Epoch:12	Loss:0.846	translation_Loss:0.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.94	Hits@10:40.28	Best:25.05
2024-12-27 20:10:47,033: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 25.05
2024-12-27 20:10:47,033: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.786 MRR:24.93 Best Results: 25.05
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:10:47,034: Snapshot:0	Epoch:13	Loss:0.786	translation_Loss:0.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.93	Hits@10:40.26	Best:25.05
2024-12-27 20:10:53,987: Snapshot:0	Epoch:14	Loss:36.551	translation_Loss:36.394	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.93	Hits@10:40.26	Best:25.05
2024-12-27 20:11:00,439: End of token training: 0 Epoch: 15 Loss:36.399 MRR:24.93 Best Results: 25.05
2024-12-27 20:11:00,439: Snapshot:0	Epoch:15	Loss:36.399	translation_Loss:36.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.93	Hits@10:40.26	Best:25.05
2024-12-27 20:11:00,799: => loading checkpoint './checkpoint/FACTfact_0.001_1024_5000/0model_best.tar'
2024-12-27 20:11:03,645: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2429 | 0.1593 | 0.2827 | 0.3329 |  0.3942 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:11:28,641: Snapshot:1	Epoch:0	Loss:24.38	translation_Loss:20.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.824                                                   	MRR:20.36	Hits@10:34.08	Best:20.36
2024-12-27 20:11:35,810: Snapshot:1	Epoch:1	Loss:20.45	translation_Loss:15.746	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.703                                                   	MRR:21.07	Hits@10:35.13	Best:21.07
2024-12-27 20:11:42,958: Snapshot:1	Epoch:2	Loss:18.799	translation_Loss:14.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.723                                                   	MRR:21.38	Hits@10:35.44	Best:21.38
2024-12-27 20:11:50,172: Snapshot:1	Epoch:3	Loss:18.119	translation_Loss:13.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.69                                                   	MRR:21.41	Hits@10:35.52	Best:21.41
2024-12-27 20:11:57,269: Snapshot:1	Epoch:4	Loss:17.883	translation_Loss:13.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.657                                                   	MRR:21.38	Hits@10:35.47	Best:21.41
2024-12-27 20:12:04,365: Snapshot:1	Epoch:5	Loss:17.777	translation_Loss:13.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.644                                                   	MRR:21.31	Hits@10:35.49	Best:21.41
2024-12-27 20:12:11,459: Snapshot:1	Epoch:6	Loss:17.733	translation_Loss:13.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.665                                                   	MRR:21.38	Hits@10:35.48	Best:21.41
2024-12-27 20:12:18,601: Snapshot:1	Epoch:7	Loss:17.666	translation_Loss:13.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.663                                                   	MRR:21.44	Hits@10:35.55	Best:21.44
2024-12-27 20:12:25,705: Snapshot:1	Epoch:8	Loss:17.613	translation_Loss:12.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.665                                                   	MRR:21.42	Hits@10:35.53	Best:21.44
2024-12-27 20:12:32,837: Snapshot:1	Epoch:9	Loss:17.598	translation_Loss:12.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.678                                                   	MRR:21.35	Hits@10:35.52	Best:21.44
2024-12-27 20:12:40,032: Snapshot:1	Epoch:10	Loss:17.579	translation_Loss:12.917	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.662                                                   	MRR:21.36	Hits@10:35.41	Best:21.44
2024-12-27 20:12:47,127: Snapshot:1	Epoch:11	Loss:17.562	translation_Loss:12.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.671                                                   	MRR:21.36	Hits@10:35.5	Best:21.44
2024-12-27 20:12:54,258: Early Stopping! Snapshot: 1 Epoch: 12 Best Results: 21.44
2024-12-27 20:12:54,258: Start to training tokens! Snapshot: 1 Epoch: 12 Loss:17.542 MRR:21.38 Best Results: 21.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:12:54,259: Snapshot:1	Epoch:12	Loss:17.542	translation_Loss:12.879	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.663                                                   	MRR:21.38	Hits@10:35.4	Best:21.44
2024-12-27 20:13:01,229: Snapshot:1	Epoch:13	Loss:43.27	translation_Loss:43.118	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.38	Hits@10:35.4	Best:21.44
2024-12-27 20:13:08,222: End of token training: 1 Epoch: 14 Loss:43.135 MRR:21.38 Best Results: 21.44
2024-12-27 20:13:08,223: Snapshot:1	Epoch:14	Loss:43.135	translation_Loss:43.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.38	Hits@10:35.4	Best:21.44
2024-12-27 20:13:08,585: => loading checkpoint './checkpoint/FACTfact_0.001_1024_5000/1model_best.tar'
2024-12-27 20:13:15,434: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2499 | 0.1638 | 0.2919 | 0.3418 |  0.4041 |
|     1      | 0.2141 | 0.1354 | 0.2543 | 0.2989 |  0.3568 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:13:40,373: Snapshot:2	Epoch:0	Loss:19.008	translation_Loss:15.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.857                                                   	MRR:20.38	Hits@10:35.11	Best:20.38
2024-12-27 20:13:47,843: Snapshot:2	Epoch:1	Loss:16.961	translation_Loss:11.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.983                                                   	MRR:20.58	Hits@10:35.39	Best:20.58
2024-12-27 20:13:55,195: Snapshot:2	Epoch:2	Loss:16.509	translation_Loss:11.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.081                                                   	MRR:20.6	Hits@10:35.53	Best:20.6
2024-12-27 20:14:02,504: Snapshot:2	Epoch:3	Loss:16.377	translation_Loss:11.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.108                                                   	MRR:20.58	Hits@10:35.44	Best:20.6
2024-12-27 20:14:09,797: Snapshot:2	Epoch:4	Loss:16.322	translation_Loss:11.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.13                                                   	MRR:20.57	Hits@10:35.5	Best:20.6
2024-12-27 20:14:17,069: Snapshot:2	Epoch:5	Loss:16.315	translation_Loss:11.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.139                                                   	MRR:20.56	Hits@10:35.47	Best:20.6
2024-12-27 20:14:24,384: Snapshot:2	Epoch:6	Loss:16.363	translation_Loss:11.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.173                                                   	MRR:20.61	Hits@10:35.46	Best:20.61
2024-12-27 20:14:31,685: Snapshot:2	Epoch:7	Loss:16.3	translation_Loss:11.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.153                                                   	MRR:20.51	Hits@10:35.45	Best:20.61
2024-12-27 20:14:38,972: Snapshot:2	Epoch:8	Loss:16.301	translation_Loss:11.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.162                                                   	MRR:20.56	Hits@10:35.47	Best:20.61
2024-12-27 20:14:46,837: Snapshot:2	Epoch:9	Loss:16.277	translation_Loss:11.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.181                                                   	MRR:20.64	Hits@10:35.53	Best:20.64
2024-12-27 20:14:54,132: Snapshot:2	Epoch:10	Loss:16.286	translation_Loss:11.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.187                                                   	MRR:20.63	Hits@10:35.53	Best:20.64
2024-12-27 20:15:01,396: Snapshot:2	Epoch:11	Loss:16.319	translation_Loss:11.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.182                                                   	MRR:20.6	Hits@10:35.52	Best:20.64
2024-12-27 20:15:08,760: Snapshot:2	Epoch:12	Loss:16.301	translation_Loss:11.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.192                                                   	MRR:20.56	Hits@10:35.42	Best:20.64
2024-12-27 20:15:16,007: Snapshot:2	Epoch:13	Loss:16.302	translation_Loss:11.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.214                                                   	MRR:20.51	Hits@10:35.44	Best:20.64
2024-12-27 20:15:23,277: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 20.64
2024-12-27 20:15:23,277: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:16.27 MRR:20.6 Best Results: 20.64
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:15:23,277: Snapshot:2	Epoch:14	Loss:16.27	translation_Loss:11.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.203                                                   	MRR:20.6	Hits@10:35.37	Best:20.64
2024-12-27 20:15:30,406: Snapshot:2	Epoch:15	Loss:43.025	translation_Loss:42.867	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.6	Hits@10:35.37	Best:20.64
2024-12-27 20:15:37,559: End of token training: 2 Epoch: 16 Loss:42.865 MRR:20.6 Best Results: 20.64
2024-12-27 20:15:37,559: Snapshot:2	Epoch:16	Loss:42.865	translation_Loss:42.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.6	Hits@10:35.37	Best:20.64
2024-12-27 20:15:37,919: => loading checkpoint './checkpoint/FACTfact_0.001_1024_5000/2model_best.tar'
2024-12-27 20:15:47,511: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1591 | 0.2855 | 0.3395 |  0.4065 |
|     1      | 0.2185 | 0.1381 | 0.2543 | 0.3051 |  0.3683 |
|     2      | 0.2059 | 0.1258 | 0.2391 |  0.29  |  0.3586 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:16:12,875: Snapshot:3	Epoch:0	Loss:13.077	translation_Loss:9.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.412                                                   	MRR:19.37	Hits@10:36.21	Best:19.37
2024-12-27 20:16:20,283: Snapshot:3	Epoch:1	Loss:11.729	translation_Loss:7.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.359                                                   	MRR:19.43	Hits@10:36.39	Best:19.43
2024-12-27 20:16:27,672: Snapshot:3	Epoch:2	Loss:11.617	translation_Loss:7.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.481                                                   	MRR:19.5	Hits@10:36.4	Best:19.5
2024-12-27 20:16:35,025: Snapshot:3	Epoch:3	Loss:11.641	translation_Loss:7.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.531                                                   	MRR:19.46	Hits@10:36.5	Best:19.5
2024-12-27 20:16:42,856: Snapshot:3	Epoch:4	Loss:11.667	translation_Loss:7.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.574                                                   	MRR:19.45	Hits@10:36.55	Best:19.5
2024-12-27 20:16:50,238: Snapshot:3	Epoch:5	Loss:11.666	translation_Loss:7.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.594                                                   	MRR:19.44	Hits@10:36.32	Best:19.5
2024-12-27 20:16:57,663: Snapshot:3	Epoch:6	Loss:11.667	translation_Loss:7.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.598                                                   	MRR:19.53	Hits@10:36.34	Best:19.53
2024-12-27 20:17:05,171: Snapshot:3	Epoch:7	Loss:11.693	translation_Loss:7.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.611                                                   	MRR:19.56	Hits@10:36.4	Best:19.56
2024-12-27 20:17:12,538: Snapshot:3	Epoch:8	Loss:11.702	translation_Loss:7.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.618                                                   	MRR:19.41	Hits@10:36.35	Best:19.56
2024-12-27 20:17:19,923: Snapshot:3	Epoch:9	Loss:11.711	translation_Loss:7.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.628                                                   	MRR:19.61	Hits@10:36.62	Best:19.61
2024-12-27 20:17:27,280: Snapshot:3	Epoch:10	Loss:11.712	translation_Loss:7.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.626                                                   	MRR:19.6	Hits@10:36.48	Best:19.61
2024-12-27 20:17:34,616: Snapshot:3	Epoch:11	Loss:11.698	translation_Loss:7.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.631                                                   	MRR:19.46	Hits@10:36.62	Best:19.61
2024-12-27 20:17:41,995: Snapshot:3	Epoch:12	Loss:11.705	translation_Loss:7.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.636                                                   	MRR:19.53	Hits@10:36.39	Best:19.61
2024-12-27 20:17:49,370: Snapshot:3	Epoch:13	Loss:11.724	translation_Loss:7.079	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.645                                                   	MRR:19.42	Hits@10:36.61	Best:19.61
2024-12-27 20:17:57,257: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 19.61
2024-12-27 20:17:57,257: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:11.73 MRR:19.46 Best Results: 19.61
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:17:57,258: Snapshot:3	Epoch:14	Loss:11.73	translation_Loss:7.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.647                                                   	MRR:19.46	Hits@10:36.46	Best:19.61
2024-12-27 20:18:04,446: Snapshot:3	Epoch:15	Loss:40.939	translation_Loss:40.788	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.46	Hits@10:36.46	Best:19.61
2024-12-27 20:18:11,623: End of token training: 3 Epoch: 16 Loss:40.806 MRR:19.46 Best Results: 19.61
2024-12-27 20:18:11,623: Snapshot:3	Epoch:16	Loss:40.806	translation_Loss:40.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.46	Hits@10:36.46	Best:19.61
2024-12-27 20:18:11,905: => loading checkpoint './checkpoint/FACTfact_0.001_1024_5000/3model_best.tar'
2024-12-27 20:18:25,065: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2377 | 0.1526 | 0.2753 | 0.328  |  0.3982 |
|     1      | 0.2139 | 0.1337 | 0.2473 | 0.2983 |  0.3701 |
|     2      | 0.2071 | 0.1249 | 0.2391 | 0.294  |  0.3672 |
|     3      | 0.195  | 0.1094 | 0.2243 | 0.2834 |  0.3667 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:18:50,391: Snapshot:4	Epoch:0	Loss:6.387	translation_Loss:4.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.046                                                   	MRR:21.15	Hits@10:44.32	Best:21.15
2024-12-27 20:18:57,840: Snapshot:4	Epoch:1	Loss:4.565	translation_Loss:2.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.226                                                   	MRR:21.34	Hits@10:44.65	Best:21.34
2024-12-27 20:19:05,270: Snapshot:4	Epoch:2	Loss:4.379	translation_Loss:2.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.271                                                   	MRR:21.16	Hits@10:44.23	Best:21.34
2024-12-27 20:19:12,661: Snapshot:4	Epoch:3	Loss:4.389	translation_Loss:2.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.291                                                   	MRR:21.29	Hits@10:44.54	Best:21.34
2024-12-27 20:19:20,044: Snapshot:4	Epoch:4	Loss:4.417	translation_Loss:2.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.332                                                   	MRR:21.25	Hits@10:44.31	Best:21.34
2024-12-27 20:19:27,881: Snapshot:4	Epoch:5	Loss:4.395	translation_Loss:2.064	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.33                                                   	MRR:21.36	Hits@10:44.52	Best:21.36
2024-12-27 20:19:35,352: Snapshot:4	Epoch:6	Loss:4.444	translation_Loss:2.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.352                                                   	MRR:21.44	Hits@10:44.43	Best:21.44
2024-12-27 20:19:42,783: Snapshot:4	Epoch:7	Loss:4.417	translation_Loss:2.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.348                                                   	MRR:21.26	Hits@10:44.11	Best:21.44
2024-12-27 20:19:50,243: Snapshot:4	Epoch:8	Loss:4.407	translation_Loss:2.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.366                                                   	MRR:21.4	Hits@10:44.22	Best:21.44
2024-12-27 20:19:57,634: Snapshot:4	Epoch:9	Loss:4.422	translation_Loss:2.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.359                                                   	MRR:21.3	Hits@10:44.14	Best:21.44
2024-12-27 20:20:05,163: Snapshot:4	Epoch:10	Loss:4.407	translation_Loss:2.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.36                                                   	MRR:21.37	Hits@10:44.53	Best:21.44
2024-12-27 20:20:12,646: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 21.44
2024-12-27 20:20:12,646: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:4.415 MRR:21.21 Best Results: 21.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:20:12,647: Snapshot:4	Epoch:11	Loss:4.415	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.353                                                   	MRR:21.21	Hits@10:44.35	Best:21.44
2024-12-27 20:20:19,884: Snapshot:4	Epoch:12	Loss:33.719	translation_Loss:33.568	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.21	Hits@10:44.35	Best:21.44
2024-12-27 20:20:27,080: End of token training: 4 Epoch: 13 Loss:33.554 MRR:21.21 Best Results: 21.44
2024-12-27 20:20:27,081: Snapshot:4	Epoch:13	Loss:33.554	translation_Loss:33.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.21	Hits@10:44.35	Best:21.44
2024-12-27 20:20:27,434: => loading checkpoint './checkpoint/FACTfact_0.001_1024_5000/4model_best.tar'
2024-12-27 20:20:44,707: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2272 | 0.1432 | 0.2621 | 0.3158 |  0.384  |
|     1      | 0.2035 | 0.1245 | 0.2341 | 0.2873 |  0.3555 |
|     2      | 0.1985 | 0.1187 | 0.2266 |  0.28  |  0.3564 |
|     3      | 0.1869 | 0.1006 | 0.2126 | 0.2738 |  0.3617 |
|     4      | 0.2127 | 0.1018 | 0.2469 | 0.3311 |  0.443  |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:20:44,709: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2429 | 0.1593 | 0.2827 | 0.3329 |  0.3942 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2499 | 0.1638 | 0.2919 | 0.3418 |  0.4041 |
|     1      | 0.2141 | 0.1354 | 0.2543 | 0.2989 |  0.3568 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2458 | 0.1591 | 0.2855 | 0.3395 |  0.4065 |
|     1      | 0.2185 | 0.1381 | 0.2543 | 0.3051 |  0.3683 |
|     2      | 0.2059 | 0.1258 | 0.2391 |  0.29  |  0.3586 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2377 | 0.1526 | 0.2753 | 0.328  |  0.3982 |
|     1      | 0.2139 | 0.1337 | 0.2473 | 0.2983 |  0.3701 |
|     2      | 0.2071 | 0.1249 | 0.2391 | 0.294  |  0.3672 |
|     3      | 0.195  | 0.1094 | 0.2243 | 0.2834 |  0.3667 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2272 | 0.1432 | 0.2621 | 0.3158 |  0.384  |
|     1      | 0.2035 | 0.1245 | 0.2341 | 0.2873 |  0.3555 |
|     2      | 0.1985 | 0.1187 | 0.2266 |  0.28  |  0.3564 |
|     3      | 0.1869 | 0.1006 | 0.2126 | 0.2738 |  0.3617 |
|     4      | 0.2127 | 0.1018 | 0.2469 | 0.3311 |  0.443  |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:20:44,710: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 107.01124668121338 |   0.243   |    0.159     |    0.283     |     0.394     |
|    1     | 121.46321773529053 |   0.232   |     0.15     |    0.273     |      0.38     |
|    2     | 138.76990938186646 |   0.223   |    0.141     |     0.26     |     0.378     |
|    3     | 140.41669011116028 |   0.213   |     0.13     |    0.246     |     0.376     |
|    4     | 118.53815531730652 |   0.206   |    0.118     |    0.236     |      0.38     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:20:44,710: Sum_Training_Time:626.1992192268372
2024-12-27 20:20:44,710: Every_Training_Time:[107.01124668121338, 121.46321773529053, 138.76990938186646, 140.41669011116028, 118.53815531730652]
2024-12-27 20:20:44,710: Forward transfer: 0.17085 Backward transfer: -0.010450000000000001
2024-12-27 20:21:24,131: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227202048/FACTfact_0.001_1024_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_1024_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_1024_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:21:34,155: Snapshot:0	Epoch:0	Loss:47.826	translation_Loss:47.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.02	Hits@10:20.16	Best:9.02
2024-12-27 20:21:40,634: Snapshot:0	Epoch:1	Loss:30.672	translation_Loss:30.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.71	Hits@10:30.8	Best:14.71
2024-12-27 20:21:47,170: Snapshot:0	Epoch:2	Loss:19.46	translation_Loss:19.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.01	Hits@10:36.04	Best:19.01
2024-12-27 20:21:53,630: Snapshot:0	Epoch:3	Loss:11.99	translation_Loss:11.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.03	Hits@10:38.56	Best:22.03
2024-12-27 20:22:00,674: Snapshot:0	Epoch:4	Loss:7.183	translation_Loss:7.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.68	Hits@10:39.56	Best:23.68
2024-12-27 20:22:07,161: Snapshot:0	Epoch:5	Loss:4.452	translation_Loss:4.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:39.99	Best:24.42
2024-12-27 20:22:13,627: Snapshot:0	Epoch:6	Loss:2.918	translation_Loss:2.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.79	Hits@10:40.25	Best:24.79
2024-12-27 20:22:20,083: Snapshot:0	Epoch:7	Loss:2.073	translation_Loss:2.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.93	Hits@10:40.39	Best:24.93
2024-12-27 20:22:26,551: Snapshot:0	Epoch:8	Loss:1.573	translation_Loss:1.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.03	Hits@10:40.37	Best:25.03
2024-12-27 20:22:33,038: Snapshot:0	Epoch:9	Loss:1.269	translation_Loss:1.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:40.4	Best:25.03
2024-12-27 20:22:39,511: Snapshot:0	Epoch:10	Loss:1.086	translation_Loss:1.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:40.46	Best:25.03
2024-12-27 20:22:45,981: Snapshot:0	Epoch:11	Loss:0.932	translation_Loss:0.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.03	Hits@10:40.3	Best:25.03
2024-12-27 20:22:52,445: Snapshot:0	Epoch:12	Loss:0.844	translation_Loss:0.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:40.2	Best:25.03
2024-12-27 20:22:58,885: Early Stopping! Snapshot: 0 Epoch: 13 Best Results: 25.03
2024-12-27 20:22:58,885: Start to training tokens! Snapshot: 0 Epoch: 13 Loss:0.768 MRR:24.85 Best Results: 25.03
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:22:58,886: Snapshot:0	Epoch:13	Loss:0.768	translation_Loss:0.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.85	Hits@10:40.26	Best:25.03
2024-12-27 20:23:05,983: Snapshot:0	Epoch:14	Loss:36.583	translation_Loss:36.426	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.85	Hits@10:40.26	Best:25.03
2024-12-27 20:23:12,528: End of token training: 0 Epoch: 15 Loss:36.434 MRR:24.85 Best Results: 25.03
2024-12-27 20:23:12,529: Snapshot:0	Epoch:15	Loss:36.434	translation_Loss:36.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.85	Hits@10:40.26	Best:25.03
2024-12-27 20:23:12,885: => loading checkpoint './checkpoint/FACTfact_0.001_1024_10000/0model_best.tar'
2024-12-27 20:23:15,653: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.159  | 0.285  | 0.3339 |  0.3957 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:23:40,776: Snapshot:1	Epoch:0	Loss:25.145	translation_Loss:21.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.03                                                   	MRR:19.81	Hits@10:33.3	Best:19.81
2024-12-27 20:23:48,053: Snapshot:1	Epoch:1	Loss:21.463	translation_Loss:16.987	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.476                                                   	MRR:20.55	Hits@10:34.22	Best:20.55
2024-12-27 20:23:55,283: Snapshot:1	Epoch:2	Loss:19.83	translation_Loss:15.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.481                                                   	MRR:20.87	Hits@10:34.55	Best:20.87
2024-12-27 20:24:02,532: Snapshot:1	Epoch:3	Loss:19.121	translation_Loss:14.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.429                                                   	MRR:20.95	Hits@10:34.56	Best:20.95
2024-12-27 20:24:09,694: Snapshot:1	Epoch:4	Loss:18.866	translation_Loss:14.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.398                                                   	MRR:20.93	Hits@10:34.75	Best:20.95
2024-12-27 20:24:16,862: Snapshot:1	Epoch:5	Loss:18.744	translation_Loss:14.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.389                                                   	MRR:20.93	Hits@10:34.82	Best:20.95
2024-12-27 20:24:24,055: Snapshot:1	Epoch:6	Loss:18.69	translation_Loss:14.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.401                                                   	MRR:20.98	Hits@10:34.76	Best:20.98
2024-12-27 20:24:31,241: Snapshot:1	Epoch:7	Loss:18.612	translation_Loss:14.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.397                                                   	MRR:20.96	Hits@10:34.79	Best:20.98
2024-12-27 20:24:38,396: Snapshot:1	Epoch:8	Loss:18.57	translation_Loss:14.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.41                                                   	MRR:20.97	Hits@10:34.71	Best:20.98
2024-12-27 20:24:45,573: Snapshot:1	Epoch:9	Loss:18.525	translation_Loss:14.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.398                                                   	MRR:20.97	Hits@10:34.82	Best:20.98
2024-12-27 20:24:52,765: Snapshot:1	Epoch:10	Loss:18.518	translation_Loss:14.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.393                                                   	MRR:20.96	Hits@10:34.87	Best:20.98
2024-12-27 20:24:59,979: Snapshot:1	Epoch:11	Loss:18.51	translation_Loss:14.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.408                                                   	MRR:20.99	Hits@10:34.7	Best:20.99
2024-12-27 20:25:07,349: Snapshot:1	Epoch:12	Loss:18.478	translation_Loss:14.077	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.401                                                   	MRR:21.0	Hits@10:34.86	Best:21.0
2024-12-27 20:25:14,600: Snapshot:1	Epoch:13	Loss:18.471	translation_Loss:14.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.398                                                   	MRR:20.95	Hits@10:34.76	Best:21.0
2024-12-27 20:25:21,768: Snapshot:1	Epoch:14	Loss:18.452	translation_Loss:14.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.406                                                   	MRR:21.0	Hits@10:34.83	Best:21.0
2024-12-27 20:25:28,922: Snapshot:1	Epoch:15	Loss:18.43	translation_Loss:14.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.406                                                   	MRR:20.92	Hits@10:34.76	Best:21.0
2024-12-27 20:25:36,679: Snapshot:1	Epoch:16	Loss:18.416	translation_Loss:14.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.396                                                   	MRR:20.92	Hits@10:34.74	Best:21.0
2024-12-27 20:25:43,910: Early Stopping! Snapshot: 1 Epoch: 17 Best Results: 21.0
2024-12-27 20:25:43,910: Start to training tokens! Snapshot: 1 Epoch: 17 Loss:18.438 MRR:20.94 Best Results: 21.0
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:25:43,911: Snapshot:1	Epoch:17	Loss:18.438	translation_Loss:14.036	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.402                                                   	MRR:20.94	Hits@10:34.74	Best:21.0
2024-12-27 20:25:51,021: Snapshot:1	Epoch:18	Loss:44.065	translation_Loss:43.914	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.94	Hits@10:34.74	Best:21.0
2024-12-27 20:25:58,069: End of token training: 1 Epoch: 19 Loss:43.912 MRR:20.94 Best Results: 21.0
2024-12-27 20:25:58,069: Snapshot:1	Epoch:19	Loss:43.912	translation_Loss:43.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.94	Hits@10:34.74	Best:21.0
2024-12-27 20:25:58,423: => loading checkpoint './checkpoint/FACTfact_0.001_1024_10000/1model_best.tar'
2024-12-27 20:26:04,427: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2474 | 0.1618 | 0.2889 | 0.338  |  0.4011 |
|     1      | 0.2086 | 0.1315 | 0.2466 | 0.2909 |  0.3458 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:26:30,033: Snapshot:2	Epoch:0	Loss:21.068	translation_Loss:16.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.235                                                   	MRR:19.53	Hits@10:33.47	Best:19.53
2024-12-27 20:26:37,446: Snapshot:2	Epoch:1	Loss:19.673	translation_Loss:14.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.783                                                   	MRR:19.71	Hits@10:33.92	Best:19.71
2024-12-27 20:26:44,856: Snapshot:2	Epoch:2	Loss:19.136	translation_Loss:14.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.81                                                   	MRR:19.77	Hits@10:34.01	Best:19.77
2024-12-27 20:26:52,272: Snapshot:2	Epoch:3	Loss:19.021	translation_Loss:14.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.82                                                   	MRR:19.8	Hits@10:33.95	Best:19.8
2024-12-27 20:26:59,697: Snapshot:2	Epoch:4	Loss:18.964	translation_Loss:14.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.831                                                   	MRR:19.73	Hits@10:33.92	Best:19.8
2024-12-27 20:27:07,073: Snapshot:2	Epoch:5	Loss:18.914	translation_Loss:14.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.84                                                   	MRR:19.76	Hits@10:33.97	Best:19.8
2024-12-27 20:27:14,414: Snapshot:2	Epoch:6	Loss:18.904	translation_Loss:14.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.85                                                   	MRR:19.7	Hits@10:34.06	Best:19.8
2024-12-27 20:27:21,764: Snapshot:2	Epoch:7	Loss:18.92	translation_Loss:14.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.865                                                   	MRR:19.75	Hits@10:33.98	Best:19.8
2024-12-27 20:27:29,120: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 19.8
2024-12-27 20:27:29,120: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:18.871 MRR:19.77 Best Results: 19.8
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:27:29,120: Snapshot:2	Epoch:8	Loss:18.871	translation_Loss:14.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.86                                                   	MRR:19.77	Hits@10:34.01	Best:19.8
2024-12-27 20:27:36,367: Snapshot:2	Epoch:9	Loss:44.561	translation_Loss:44.402	multi_layer_Loss:0.158	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:34.01	Best:19.8
2024-12-27 20:27:43,607: End of token training: 2 Epoch: 10 Loss:44.44 MRR:19.77 Best Results: 19.8
2024-12-27 20:27:43,607: Snapshot:2	Epoch:10	Loss:44.44	translation_Loss:44.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.77	Hits@10:34.01	Best:19.8
2024-12-27 20:27:43,966: => loading checkpoint './checkpoint/FACTfact_0.001_1024_10000/2model_best.tar'
2024-12-27 20:27:53,864: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2463 | 0.1602 | 0.2871 | 0.3374 |  0.4031 |
|     1      | 0.2123 | 0.1333 | 0.2495 | 0.2982 |  0.3583 |
|     2      | 0.1972 | 0.1199 | 0.2302 | 0.2796 |  0.3419 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:28:19,663: Snapshot:3	Epoch:0	Loss:16.794	translation_Loss:12.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.172                                                   	MRR:18.09	Hits@10:34.07	Best:18.09
2024-12-27 20:28:27,152: Snapshot:3	Epoch:1	Loss:16.066	translation_Loss:11.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.756                                                   	MRR:18.24	Hits@10:34.22	Best:18.24
2024-12-27 20:28:34,682: Snapshot:3	Epoch:2	Loss:15.971	translation_Loss:11.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.807                                                   	MRR:18.3	Hits@10:34.26	Best:18.3
2024-12-27 20:28:42,147: Snapshot:3	Epoch:3	Loss:15.973	translation_Loss:11.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.845                                                   	MRR:18.28	Hits@10:34.28	Best:18.3
2024-12-27 20:28:49,719: Snapshot:3	Epoch:4	Loss:15.918	translation_Loss:11.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.846                                                   	MRR:18.24	Hits@10:34.15	Best:18.3
2024-12-27 20:28:57,170: Snapshot:3	Epoch:5	Loss:15.962	translation_Loss:11.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.865                                                   	MRR:18.27	Hits@10:34.32	Best:18.3
2024-12-27 20:29:04,592: Snapshot:3	Epoch:6	Loss:15.96	translation_Loss:11.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.871                                                   	MRR:18.29	Hits@10:34.33	Best:18.3
2024-12-27 20:29:12,034: Early Stopping! Snapshot: 3 Epoch: 7 Best Results: 18.3
2024-12-27 20:29:12,034: Start to training tokens! Snapshot: 3 Epoch: 7 Loss:15.939 MRR:18.3 Best Results: 18.3
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:29:12,035: Snapshot:3	Epoch:7	Loss:15.939	translation_Loss:11.049	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.891                                                   	MRR:18.3	Hits@10:34.2	Best:18.3
2024-12-27 20:29:19,309: Snapshot:3	Epoch:8	Loss:43.166	translation_Loss:43.016	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.3	Hits@10:34.2	Best:18.3
2024-12-27 20:29:26,560: End of token training: 3 Epoch: 9 Loss:43.027 MRR:18.3 Best Results: 18.3
2024-12-27 20:29:26,560: Snapshot:3	Epoch:9	Loss:43.027	translation_Loss:43.027	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.3	Hits@10:34.2	Best:18.3
2024-12-27 20:29:26,920: => loading checkpoint './checkpoint/FACTfact_0.001_1024_10000/3model_best.tar'
2024-12-27 20:29:39,828: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1575 | 0.2813 | 0.3325 |  0.4004 |
|     1      | 0.2112 | 0.1318 | 0.2473 | 0.2956 |  0.3602 |
|     2      | 0.1994 |  0.12  | 0.2318 | 0.2858 |  0.3503 |
|     3      | 0.1843 | 0.1038 | 0.2142 | 0.268  |  0.3407 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:30:05,363: Snapshot:4	Epoch:0	Loss:10.097	translation_Loss:6.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.203                                                   	MRR:20.15	Hits@10:42.12	Best:20.15
2024-12-27 20:30:12,999: Snapshot:4	Epoch:1	Loss:8.241	translation_Loss:4.667	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.574                                                   	MRR:20.41	Hits@10:41.93	Best:20.41
2024-12-27 20:30:20,545: Snapshot:4	Epoch:2	Loss:8.038	translation_Loss:4.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.596                                                   	MRR:20.6	Hits@10:42.52	Best:20.6
2024-12-27 20:30:28,038: Snapshot:4	Epoch:3	Loss:8.023	translation_Loss:4.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.63                                                   	MRR:20.46	Hits@10:42.21	Best:20.6
2024-12-27 20:30:35,560: Snapshot:4	Epoch:4	Loss:7.976	translation_Loss:4.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.62                                                   	MRR:20.42	Hits@10:42.11	Best:20.6
2024-12-27 20:30:42,996: Snapshot:4	Epoch:5	Loss:7.987	translation_Loss:4.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.652                                                   	MRR:20.48	Hits@10:42.23	Best:20.6
2024-12-27 20:30:50,978: Snapshot:4	Epoch:6	Loss:7.989	translation_Loss:4.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.649                                                   	MRR:20.44	Hits@10:42.01	Best:20.6
2024-12-27 20:30:58,449: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 20.6
2024-12-27 20:30:58,449: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:7.987 MRR:20.48 Best Results: 20.6
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:30:58,449: Snapshot:4	Epoch:7	Loss:7.987	translation_Loss:4.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.653                                                   	MRR:20.48	Hits@10:42.28	Best:20.6
2024-12-27 20:31:05,768: Snapshot:4	Epoch:8	Loss:35.711	translation_Loss:35.56	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.48	Hits@10:42.28	Best:20.6
2024-12-27 20:31:13,056: End of token training: 4 Epoch: 9 Loss:35.583 MRR:20.48 Best Results: 20.6
2024-12-27 20:31:13,056: Snapshot:4	Epoch:9	Loss:35.583	translation_Loss:35.583	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.48	Hits@10:42.28	Best:20.6
2024-12-27 20:31:13,332: => loading checkpoint './checkpoint/FACTfact_0.001_1024_10000/4model_best.tar'
2024-12-27 20:31:29,883: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2357 | 0.1518 | 0.2724 | 0.3235 |  0.3914 |
|     1      | 0.2051 | 0.1268 | 0.2384 | 0.2881 |  0.3522 |
|     2      | 0.1942 | 0.1159 | 0.2246 | 0.2765 |  0.347  |
|     3      | 0.1806 | 0.098  | 0.2071 | 0.263  |  0.348  |
|     4      | 0.2064 | 0.1017 | 0.2401 | 0.3197 |  0.4231 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:31:29,886: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2438 | 0.159  | 0.285  | 0.3339 |  0.3957 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2474 | 0.1618 | 0.2889 | 0.338  |  0.4011 |
|     1      | 0.2086 | 0.1315 | 0.2466 | 0.2909 |  0.3458 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2463 | 0.1602 | 0.2871 | 0.3374 |  0.4031 |
|     1      | 0.2123 | 0.1333 | 0.2495 | 0.2982 |  0.3583 |
|     2      | 0.1972 | 0.1199 | 0.2302 | 0.2796 |  0.3419 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1575 | 0.2813 | 0.3325 |  0.4004 |
|     1      | 0.2112 | 0.1318 | 0.2473 | 0.2956 |  0.3602 |
|     2      | 0.1994 |  0.12  | 0.2318 | 0.2858 |  0.3503 |
|     3      | 0.1843 | 0.1038 | 0.2142 | 0.268  |  0.3407 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2357 | 0.1518 | 0.2724 | 0.3235 |  0.3914 |
|     1      | 0.2051 | 0.1268 | 0.2384 | 0.2881 |  0.3522 |
|     2      | 0.1942 | 0.1159 | 0.2246 | 0.2765 |  0.347  |
|     3      | 0.1806 | 0.098  | 0.2071 | 0.263  |  0.348  |
|     4      | 0.2064 | 0.1017 | 0.2401 | 0.3197 |  0.4231 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:31:29,886: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 108.39705038070679 |   0.244   |    0.159     |    0.285     |     0.396     |
|    1     | 159.2634778022766  |   0.228   |    0.147     |    0.268     |     0.373     |
|    2     | 95.82387971878052  |   0.219   |    0.138     |    0.256     |     0.368     |
|    3     | 89.60759496688843  |   0.209   |    0.128     |    0.244     |     0.363     |
|    4     | 89.70324563980103  |   0.204   |    0.119     |    0.237     |     0.372     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:31:29,886: Sum_Training_Time:542.7952485084534
2024-12-27 20:31:29,886: Every_Training_Time:[108.39705038070679, 159.2634778022766, 95.82387971878052, 89.60759496688843, 89.70324563980103]
2024-12-27 20:31:29,886: Forward transfer: 0.16495 Backward transfer: -0.004574999999999989
2024-12-27 20:32:09,135: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227203133/FACTfact_0.001_2048_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_2048_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_2048_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:32:19,173: Snapshot:0	Epoch:0	Loss:24.869	translation_Loss:24.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.15	Hits@10:15.62	Best:7.15
2024-12-27 20:32:25,602: Snapshot:0	Epoch:1	Loss:16.859	translation_Loss:16.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.6	Hits@10:25.86	Best:11.6
2024-12-27 20:32:32,473: Snapshot:0	Epoch:2	Loss:11.571	translation_Loss:11.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.35	Hits@10:32.19	Best:15.35
2024-12-27 20:32:38,914: Snapshot:0	Epoch:3	Loss:7.838	translation_Loss:7.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.53	Hits@10:35.97	Best:18.53
2024-12-27 20:32:45,349: Snapshot:0	Epoch:4	Loss:5.22	translation_Loss:5.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.03	Hits@10:38.0	Best:21.03
2024-12-27 20:32:51,827: Snapshot:0	Epoch:5	Loss:3.48	translation_Loss:3.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.65	Hits@10:39.17	Best:22.65
2024-12-27 20:32:58,339: Snapshot:0	Epoch:6	Loss:2.327	translation_Loss:2.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.68	Best:23.71
2024-12-27 20:33:05,273: Snapshot:0	Epoch:7	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.29	Hits@10:39.91	Best:24.29
2024-12-27 20:33:11,721: Snapshot:0	Epoch:8	Loss:1.154	translation_Loss:1.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:40.29	Best:24.5
2024-12-27 20:33:18,199: Snapshot:0	Epoch:9	Loss:0.879	translation_Loss:0.879	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.64	Hits@10:40.25	Best:24.64
2024-12-27 20:33:24,636: Snapshot:0	Epoch:10	Loss:0.704	translation_Loss:0.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.44	Best:24.78
2024-12-27 20:33:31,150: Snapshot:0	Epoch:11	Loss:0.579	translation_Loss:0.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.85	Hits@10:40.46	Best:24.85
2024-12-27 20:33:37,581: Snapshot:0	Epoch:12	Loss:0.496	translation_Loss:0.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.35	Best:24.85
2024-12-27 20:33:44,562: Snapshot:0	Epoch:13	Loss:0.441	translation_Loss:0.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.42	Best:24.88
2024-12-27 20:33:51,025: Snapshot:0	Epoch:14	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:40.49	Best:24.88
2024-12-27 20:33:57,431: Snapshot:0	Epoch:15	Loss:0.359	translation_Loss:0.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:40.32	Best:24.88
2024-12-27 20:34:03,866: Snapshot:0	Epoch:16	Loss:0.327	translation_Loss:0.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.92	Hits@10:40.5	Best:24.92
2024-12-27 20:34:10,318: Snapshot:0	Epoch:17	Loss:0.305	translation_Loss:0.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.8	Hits@10:40.54	Best:24.92
2024-12-27 20:34:17,203: Snapshot:0	Epoch:18	Loss:0.29	translation_Loss:0.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:40.3	Best:24.92
2024-12-27 20:34:23,605: Snapshot:0	Epoch:19	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:40.41	Best:24.92
2024-12-27 20:34:30,067: Snapshot:0	Epoch:20	Loss:0.254	translation_Loss:0.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.7	Hits@10:40.25	Best:24.92
2024-12-27 20:34:36,523: Early Stopping! Snapshot: 0 Epoch: 21 Best Results: 24.92
2024-12-27 20:34:36,523: Start to training tokens! Snapshot: 0 Epoch: 21 Loss:0.238 MRR:24.76 Best Results: 24.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:34:36,523: Snapshot:0	Epoch:21	Loss:0.238	translation_Loss:0.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:40.26	Best:24.92
2024-12-27 20:34:43,488: Snapshot:0	Epoch:22	Loss:18.426	translation_Loss:18.27	multi_layer_Loss:0.156	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:40.26	Best:24.92
2024-12-27 20:34:50,029: End of token training: 0 Epoch: 23 Loss:18.289 MRR:24.76 Best Results: 24.92
2024-12-27 20:34:50,029: Snapshot:0	Epoch:23	Loss:18.289	translation_Loss:18.288	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.76	Hits@10:40.26	Best:24.92
2024-12-27 20:34:50,317: => loading checkpoint './checkpoint/FACTfact_0.001_2048_1000/0model_best.tar'
2024-12-27 20:34:53,397: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2416 | 0.1572 | 0.2819 | 0.3321 |  0.3941 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:35:17,963: Snapshot:1	Epoch:0	Loss:10.725	translation_Loss:9.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.801                                                   	MRR:20.96	Hits@10:34.89	Best:20.96
2024-12-27 20:35:25,433: Snapshot:1	Epoch:1	Loss:6.083	translation_Loss:4.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.649                                                   	MRR:22.52	Hits@10:37.02	Best:22.52
2024-12-27 20:35:32,546: Snapshot:1	Epoch:2	Loss:4.706	translation_Loss:2.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.986                                                   	MRR:23.09	Hits@10:38.09	Best:23.09
2024-12-27 20:35:39,642: Snapshot:1	Epoch:3	Loss:4.263	translation_Loss:2.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.125                                                   	MRR:23.18	Hits@10:38.35	Best:23.18
2024-12-27 20:35:46,864: Snapshot:1	Epoch:4	Loss:4.142	translation_Loss:1.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.184                                                   	MRR:23.28	Hits@10:38.27	Best:23.28
2024-12-27 20:35:53,901: Snapshot:1	Epoch:5	Loss:4.079	translation_Loss:1.852	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.227                                                   	MRR:23.2	Hits@10:38.4	Best:23.28
2024-12-27 20:36:01,356: Snapshot:1	Epoch:6	Loss:4.055	translation_Loss:1.794	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.261                                                   	MRR:23.19	Hits@10:38.46	Best:23.28
2024-12-27 20:36:08,390: Snapshot:1	Epoch:7	Loss:4.05	translation_Loss:1.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.281                                                   	MRR:23.1	Hits@10:38.3	Best:23.28
2024-12-27 20:36:15,422: Snapshot:1	Epoch:8	Loss:4.044	translation_Loss:1.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.304                                                   	MRR:23.27	Hits@10:38.33	Best:23.28
2024-12-27 20:36:22,482: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 23.28
2024-12-27 20:36:22,483: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:4.02 MRR:23.16 Best Results: 23.28
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:36:22,483: Snapshot:1	Epoch:9	Loss:4.02	translation_Loss:1.707	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.313                                                   	MRR:23.16	Hits@10:38.35	Best:23.28
2024-12-27 20:36:29,437: Snapshot:1	Epoch:10	Loss:20.15	translation_Loss:20.0	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.16	Hits@10:38.35	Best:23.28
2024-12-27 20:36:36,384: End of token training: 1 Epoch: 11 Loss:20.021 MRR:23.16 Best Results: 23.28
2024-12-27 20:36:36,384: Snapshot:1	Epoch:11	Loss:20.021	translation_Loss:20.019	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.16	Hits@10:38.35	Best:23.28
2024-12-27 20:36:36,742: => loading checkpoint './checkpoint/FACTfact_0.001_2048_1000/1model_best.tar'
2024-12-27 20:36:43,676: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.253  | 0.1642 | 0.2924 | 0.3484 |  0.4202 |
|     1      | 0.2331 | 0.1504 | 0.2721 | 0.3218 |  0.3857 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:37:08,314: Snapshot:2	Epoch:0	Loss:6.32	translation_Loss:5.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.719                                                   	MRR:21.82	Hits@10:37.88	Best:21.82
2024-12-27 20:37:15,555: Snapshot:2	Epoch:1	Loss:3.232	translation_Loss:1.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.261                                                   	MRR:22.54	Hits@10:38.92	Best:22.54
2024-12-27 20:37:23,171: Snapshot:2	Epoch:2	Loss:2.621	translation_Loss:1.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.327                                                   	MRR:22.5	Hits@10:38.53	Best:22.54
2024-12-27 20:37:30,377: Snapshot:2	Epoch:3	Loss:2.542	translation_Loss:1.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.393                                                   	MRR:22.41	Hits@10:38.87	Best:22.54
2024-12-27 20:37:37,677: Snapshot:2	Epoch:4	Loss:2.505	translation_Loss:1.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.432                                                   	MRR:22.22	Hits@10:38.57	Best:22.54
2024-12-27 20:37:44,953: Snapshot:2	Epoch:5	Loss:2.511	translation_Loss:1.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.461                                                   	MRR:22.33	Hits@10:39.02	Best:22.54
2024-12-27 20:37:52,205: Early Stopping! Snapshot: 2 Epoch: 6 Best Results: 22.54
2024-12-27 20:37:52,206: Start to training tokens! Snapshot: 2 Epoch: 6 Loss:2.544 MRR:22.31 Best Results: 22.54
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:37:52,206: Snapshot:2	Epoch:6	Loss:2.544	translation_Loss:1.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.492                                                   	MRR:22.31	Hits@10:38.73	Best:22.54
2024-12-27 20:37:59,775: Snapshot:2	Epoch:7	Loss:19.807	translation_Loss:19.65	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.31	Hits@10:38.73	Best:22.54
2024-12-27 20:38:06,890: End of token training: 2 Epoch: 8 Loss:19.65 MRR:22.31 Best Results: 22.54
2024-12-27 20:38:06,890: Snapshot:2	Epoch:8	Loss:19.65	translation_Loss:19.649	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.31	Hits@10:38.73	Best:22.54
2024-12-27 20:38:07,162: => loading checkpoint './checkpoint/FACTfact_0.001_2048_1000/2model_best.tar'
2024-12-27 20:38:16,822: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1542 | 0.279  | 0.3339 |  0.4058 |
|     1      | 0.2309 | 0.1472 | 0.2671 | 0.3195 |  0.3905 |
|     2      | 0.2248 | 0.1403 | 0.2606 | 0.3133 |  0.3846 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:38:42,251: Snapshot:3	Epoch:0	Loss:3.336	translation_Loss:2.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.582                                                   	MRR:20.1	Hits@10:38.64	Best:20.1
2024-12-27 20:38:49,684: Snapshot:3	Epoch:1	Loss:1.67	translation_Loss:0.84	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.83                                                   	MRR:20.25	Hits@10:38.46	Best:20.25
2024-12-27 20:38:57,025: Snapshot:3	Epoch:2	Loss:1.404	translation_Loss:0.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.78                                                   	MRR:20.29	Hits@10:38.37	Best:20.29
2024-12-27 20:39:04,299: Snapshot:3	Epoch:3	Loss:1.398	translation_Loss:0.585	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.813                                                   	MRR:20.28	Hits@10:38.44	Best:20.29
2024-12-27 20:39:11,560: Snapshot:3	Epoch:4	Loss:1.416	translation_Loss:0.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.844                                                   	MRR:20.17	Hits@10:38.27	Best:20.29
2024-12-27 20:39:18,829: Snapshot:3	Epoch:5	Loss:1.439	translation_Loss:0.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.866                                                   	MRR:20.18	Hits@10:38.42	Best:20.29
2024-12-27 20:39:26,108: Snapshot:3	Epoch:6	Loss:1.455	translation_Loss:0.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.888                                                   	MRR:19.77	Hits@10:38.25	Best:20.29
2024-12-27 20:39:33,395: Early Stopping! Snapshot: 3 Epoch: 7 Best Results: 20.29
2024-12-27 20:39:33,396: Start to training tokens! Snapshot: 3 Epoch: 7 Loss:1.459 MRR:20.05 Best Results: 20.29
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:39:33,396: Snapshot:3	Epoch:7	Loss:1.459	translation_Loss:0.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.902                                                   	MRR:20.05	Hits@10:38.32	Best:20.29
2024-12-27 20:39:40,563: Snapshot:3	Epoch:8	Loss:19.014	translation_Loss:18.864	multi_layer_Loss:0.149	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.05	Hits@10:38.32	Best:20.29
2024-12-27 20:39:47,782: End of token training: 3 Epoch: 9 Loss:18.851 MRR:20.05 Best Results: 20.29
2024-12-27 20:39:47,782: Snapshot:3	Epoch:9	Loss:18.851	translation_Loss:18.85	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.05	Hits@10:38.32	Best:20.29
2024-12-27 20:39:48,144: => loading checkpoint './checkpoint/FACTfact_0.001_2048_1000/3model_best.tar'
2024-12-27 20:40:01,226: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2222 | 0.1399 | 0.2533 | 0.3045 |  0.378  |
|     1      | 0.2112 | 0.1318 | 0.2393 | 0.293  |  0.3678 |
|     2      | 0.2072 | 0.1228 | 0.2362 | 0.2942 |  0.376  |
|     3      | 0.2019 | 0.1112 | 0.2314 | 0.2967 |  0.3817 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:40:26,232: Snapshot:4	Epoch:0	Loss:1.71	translation_Loss:1.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.357                                                   	MRR:21.4	Hits@10:45.18	Best:21.4
2024-12-27 20:40:33,650: Snapshot:4	Epoch:1	Loss:0.676	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.361                                                   	MRR:21.52	Hits@10:45.07	Best:21.52
2024-12-27 20:40:41,529: Snapshot:4	Epoch:2	Loss:0.49	translation_Loss:0.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.292                                                   	MRR:21.73	Hits@10:44.99	Best:21.73
2024-12-27 20:40:48,976: Snapshot:4	Epoch:3	Loss:0.484	translation_Loss:0.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.304                                                   	MRR:21.89	Hits@10:45.21	Best:21.89
2024-12-27 20:40:56,318: Snapshot:4	Epoch:4	Loss:0.488	translation_Loss:0.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.32                                                   	MRR:21.45	Hits@10:44.94	Best:21.89
2024-12-27 20:41:03,678: Snapshot:4	Epoch:5	Loss:0.496	translation_Loss:0.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.33                                                   	MRR:21.57	Hits@10:44.93	Best:21.89
2024-12-27 20:41:11,054: Snapshot:4	Epoch:6	Loss:0.501	translation_Loss:0.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.337                                                   	MRR:21.74	Hits@10:45.03	Best:21.89
2024-12-27 20:41:18,894: Snapshot:4	Epoch:7	Loss:0.495	translation_Loss:0.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.339                                                   	MRR:21.88	Hits@10:45.09	Best:21.89
2024-12-27 20:41:26,298: Early Stopping! Snapshot: 4 Epoch: 8 Best Results: 21.89
2024-12-27 20:41:26,299: Start to training tokens! Snapshot: 4 Epoch: 8 Loss:0.51 MRR:21.78 Best Results: 21.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:41:26,299: Snapshot:4	Epoch:8	Loss:0.51	translation_Loss:0.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:21.78	Hits@10:44.96	Best:21.89
2024-12-27 20:41:33,604: Snapshot:4	Epoch:9	Loss:15.922	translation_Loss:15.772	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.78	Hits@10:44.96	Best:21.89
2024-12-27 20:41:40,848: End of token training: 4 Epoch: 10 Loss:15.787 MRR:21.78 Best Results: 21.89
2024-12-27 20:41:40,848: Snapshot:4	Epoch:10	Loss:15.787	translation_Loss:15.786	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.78	Hits@10:44.96	Best:21.89
2024-12-27 20:41:41,135: => loading checkpoint './checkpoint/FACTfact_0.001_2048_1000/4model_best.tar'
2024-12-27 20:41:57,796: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2054 | 0.1271 | 0.2317 | 0.2823 |  0.3555 |
|     1      | 0.1941 | 0.1189 | 0.2182 | 0.2681 |  0.3431 |
|     2      | 0.1861 | 0.1067 | 0.2108 | 0.2636 |  0.3463 |
|     3      | 0.1807 | 0.0944 | 0.2044 | 0.2652 |  0.3561 |
|     4      | 0.2177 | 0.1043 | 0.2525 | 0.3378 |  0.452  |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:41:57,798: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2416 | 0.1572 | 0.2819 | 0.3321 |  0.3941 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.253  | 0.1642 | 0.2924 | 0.3484 |  0.4202 |
|     1      | 0.2331 | 0.1504 | 0.2721 | 0.3218 |  0.3857 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1542 | 0.279  | 0.3339 |  0.4058 |
|     1      | 0.2309 | 0.1472 | 0.2671 | 0.3195 |  0.3905 |
|     2      | 0.2248 | 0.1403 | 0.2606 | 0.3133 |  0.3846 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2222 | 0.1399 | 0.2533 | 0.3045 |  0.378  |
|     1      | 0.2112 | 0.1318 | 0.2393 | 0.293  |  0.3678 |
|     2      | 0.2072 | 0.1228 | 0.2362 | 0.2942 |  0.376  |
|     3      | 0.2019 | 0.1112 | 0.2314 | 0.2967 |  0.3817 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2054 | 0.1271 | 0.2317 | 0.2823 |  0.3555 |
|     1      | 0.1941 | 0.1189 | 0.2182 | 0.2681 |  0.3431 |
|     2      | 0.1861 | 0.1067 | 0.2108 | 0.2636 |  0.3463 |
|     3      | 0.1807 | 0.0944 | 0.2044 | 0.2652 |  0.3561 |
|     4      | 0.2177 | 0.1043 | 0.2525 | 0.3378 |  0.452  |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:41:57,799: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 160.89323949813843 |   0.242   |    0.157     |    0.282     |     0.394     |
|    1     | 99.83990120887756  |   0.243   |    0.157     |    0.282     |     0.403     |
|    2     |  79.8918685913086  |   0.232   |    0.147     |    0.269     |     0.394     |
|    3     | 87.59896612167358  |   0.211   |    0.126     |     0.24     |     0.376     |
|    4     |  96.1285183429718  |   0.197   |     0.11     |    0.224     |     0.371     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:41:57,799: Sum_Training_Time:524.35249376297
2024-12-27 20:41:57,799: Every_Training_Time:[160.89323949813843, 99.83990120887756, 79.8918685913086, 87.59896612167358, 96.1285183429718]
2024-12-27 20:41:57,799: Forward transfer: 0.178075 Backward transfer: -0.033775000000000006
2024-12-27 20:42:36,730: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227204201/FACTfact_0.001_2048_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_2048_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_2048_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:42:46,746: Snapshot:0	Epoch:0	Loss:24.869	translation_Loss:24.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.15	Hits@10:15.62	Best:7.15
2024-12-27 20:42:53,206: Snapshot:0	Epoch:1	Loss:16.859	translation_Loss:16.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.6	Hits@10:25.86	Best:11.6
2024-12-27 20:43:00,070: Snapshot:0	Epoch:2	Loss:11.571	translation_Loss:11.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.35	Hits@10:32.2	Best:15.35
2024-12-27 20:43:06,542: Snapshot:0	Epoch:3	Loss:7.838	translation_Loss:7.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.51	Hits@10:35.95	Best:18.51
2024-12-27 20:43:12,993: Snapshot:0	Epoch:4	Loss:5.22	translation_Loss:5.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.06	Hits@10:37.96	Best:21.06
2024-12-27 20:43:19,461: Snapshot:0	Epoch:5	Loss:3.48	translation_Loss:3.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.7	Hits@10:39.21	Best:22.7
2024-12-27 20:43:25,908: Snapshot:0	Epoch:6	Loss:2.327	translation_Loss:2.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.72	Hits@10:39.69	Best:23.72
2024-12-27 20:43:32,845: Snapshot:0	Epoch:7	Loss:1.608	translation_Loss:1.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.32	Hits@10:40.01	Best:24.32
2024-12-27 20:43:39,306: Snapshot:0	Epoch:8	Loss:1.153	translation_Loss:1.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.54	Hits@10:40.17	Best:24.54
2024-12-27 20:43:45,840: Snapshot:0	Epoch:9	Loss:0.883	translation_Loss:0.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.64	Hits@10:40.26	Best:24.64
2024-12-27 20:43:52,315: Snapshot:0	Epoch:10	Loss:0.705	translation_Loss:0.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:40.34	Best:24.71
2024-12-27 20:43:58,763: Snapshot:0	Epoch:11	Loss:0.58	translation_Loss:0.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.27	Best:24.78
2024-12-27 20:44:05,229: Snapshot:0	Epoch:12	Loss:0.497	translation_Loss:0.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.79	Hits@10:40.21	Best:24.79
2024-12-27 20:44:12,132: Snapshot:0	Epoch:13	Loss:0.44	translation_Loss:0.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.79	Hits@10:40.44	Best:24.79
2024-12-27 20:44:18,579: Snapshot:0	Epoch:14	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.89	Hits@10:40.42	Best:24.89
2024-12-27 20:44:25,019: Snapshot:0	Epoch:15	Loss:0.362	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.79	Hits@10:40.16	Best:24.89
2024-12-27 20:44:31,416: Snapshot:0	Epoch:16	Loss:0.329	translation_Loss:0.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.23	Best:24.89
2024-12-27 20:44:37,874: Snapshot:0	Epoch:17	Loss:0.302	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.92	Hits@10:40.28	Best:24.92
2024-12-27 20:44:44,722: Snapshot:0	Epoch:18	Loss:0.288	translation_Loss:0.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.75	Hits@10:40.2	Best:24.92
2024-12-27 20:44:51,152: Snapshot:0	Epoch:19	Loss:0.269	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.73	Hits@10:40.2	Best:24.92
2024-12-27 20:44:57,636: Snapshot:0	Epoch:20	Loss:0.255	translation_Loss:0.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:40.22	Best:24.92
2024-12-27 20:45:04,119: Snapshot:0	Epoch:21	Loss:0.241	translation_Loss:0.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:40.25	Best:24.92
2024-12-27 20:45:10,560: Early Stopping! Snapshot: 0 Epoch: 22 Best Results: 24.92
2024-12-27 20:45:10,561: Start to training tokens! Snapshot: 0 Epoch: 22 Loss:0.23 MRR:24.64 Best Results: 24.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:45:10,561: Snapshot:0	Epoch:22	Loss:0.23	translation_Loss:0.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.64	Hits@10:40.2	Best:24.92
2024-12-27 20:45:17,533: Snapshot:0	Epoch:23	Loss:18.433	translation_Loss:18.278	multi_layer_Loss:0.156	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.64	Hits@10:40.2	Best:24.92
2024-12-27 20:45:24,523: End of token training: 0 Epoch: 24 Loss:18.261 MRR:24.64 Best Results: 24.92
2024-12-27 20:45:24,524: Snapshot:0	Epoch:24	Loss:18.261	translation_Loss:18.26	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.64	Hits@10:40.2	Best:24.92
2024-12-27 20:45:24,799: => loading checkpoint './checkpoint/FACTfact_0.001_2048_5000/0model_best.tar'
2024-12-27 20:45:27,537: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1586 | 0.2818 | 0.3318 |  0.396  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:45:52,081: Snapshot:1	Epoch:0	Loss:11.596	translation_Loss:10.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.594                                                   	MRR:20.24	Hits@10:34.01	Best:20.24
2024-12-27 20:45:59,198: Snapshot:1	Epoch:1	Loss:8.803	translation_Loss:6.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.029                                                   	MRR:21.14	Hits@10:35.15	Best:21.14
2024-12-27 20:46:06,719: Snapshot:1	Epoch:2	Loss:7.809	translation_Loss:5.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.974                                                   	MRR:21.44	Hits@10:35.71	Best:21.44
2024-12-27 20:46:13,817: Snapshot:1	Epoch:3	Loss:7.444	translation_Loss:5.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.966                                                   	MRR:21.65	Hits@10:35.79	Best:21.65
2024-12-27 20:46:20,867: Snapshot:1	Epoch:4	Loss:7.313	translation_Loss:5.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.947                                                   	MRR:21.58	Hits@10:35.93	Best:21.65
2024-12-27 20:46:27,919: Snapshot:1	Epoch:5	Loss:7.253	translation_Loss:5.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.946                                                   	MRR:21.62	Hits@10:35.85	Best:21.65
2024-12-27 20:46:35,012: Snapshot:1	Epoch:6	Loss:7.194	translation_Loss:5.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.932                                                   	MRR:21.64	Hits@10:35.88	Best:21.65
2024-12-27 20:46:42,051: Snapshot:1	Epoch:7	Loss:7.194	translation_Loss:5.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.936                                                   	MRR:21.58	Hits@10:35.9	Best:21.65
2024-12-27 20:46:49,632: Snapshot:1	Epoch:8	Loss:7.152	translation_Loss:5.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.933                                                   	MRR:21.68	Hits@10:35.81	Best:21.68
2024-12-27 20:46:56,678: Snapshot:1	Epoch:9	Loss:7.169	translation_Loss:5.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.934                                                   	MRR:21.64	Hits@10:35.76	Best:21.68
2024-12-27 20:47:03,717: Snapshot:1	Epoch:10	Loss:7.145	translation_Loss:5.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.94                                                   	MRR:21.59	Hits@10:35.87	Best:21.68
2024-12-27 20:47:10,759: Snapshot:1	Epoch:11	Loss:7.147	translation_Loss:5.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.939                                                   	MRR:21.67	Hits@10:35.89	Best:21.68
2024-12-27 20:47:17,806: Snapshot:1	Epoch:12	Loss:7.147	translation_Loss:5.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.943                                                   	MRR:21.64	Hits@10:35.86	Best:21.68
2024-12-27 20:47:25,351: Snapshot:1	Epoch:13	Loss:7.112	translation_Loss:5.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.935                                                   	MRR:21.69	Hits@10:35.88	Best:21.69
2024-12-27 20:47:32,455: Snapshot:1	Epoch:14	Loss:7.118	translation_Loss:5.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.935                                                   	MRR:21.68	Hits@10:35.93	Best:21.69
2024-12-27 20:47:39,523: Snapshot:1	Epoch:15	Loss:7.108	translation_Loss:5.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.935                                                   	MRR:21.54	Hits@10:35.9	Best:21.69
2024-12-27 20:47:46,649: Snapshot:1	Epoch:16	Loss:7.121	translation_Loss:5.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.935                                                   	MRR:21.63	Hits@10:35.85	Best:21.69
2024-12-27 20:47:53,707: Snapshot:1	Epoch:17	Loss:7.116	translation_Loss:5.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.937                                                   	MRR:21.61	Hits@10:35.91	Best:21.69
2024-12-27 20:48:00,805: Snapshot:1	Epoch:18	Loss:7.104	translation_Loss:5.163	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.941                                                   	MRR:21.7	Hits@10:35.86	Best:21.7
2024-12-27 20:48:08,371: Snapshot:1	Epoch:19	Loss:7.107	translation_Loss:5.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.94                                                   	MRR:21.57	Hits@10:35.97	Best:21.7
2024-12-27 20:48:15,404: Snapshot:1	Epoch:20	Loss:7.099	translation_Loss:5.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.937                                                   	MRR:21.59	Hits@10:35.77	Best:21.7
2024-12-27 20:48:22,466: Snapshot:1	Epoch:21	Loss:7.101	translation_Loss:5.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.939                                                   	MRR:21.64	Hits@10:35.89	Best:21.7
2024-12-27 20:48:29,499: Snapshot:1	Epoch:22	Loss:7.098	translation_Loss:5.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.933                                                   	MRR:21.64	Hits@10:35.87	Best:21.7
2024-12-27 20:48:36,603: Early Stopping! Snapshot: 1 Epoch: 23 Best Results: 21.7
2024-12-27 20:48:36,603: Start to training tokens! Snapshot: 1 Epoch: 23 Loss:7.106 MRR:21.59 Best Results: 21.7
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:48:36,604: Snapshot:1	Epoch:23	Loss:7.106	translation_Loss:5.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.941                                                   	MRR:21.59	Hits@10:35.88	Best:21.7
2024-12-27 20:48:43,932: Snapshot:1	Epoch:24	Loss:21.67	translation_Loss:21.52	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.59	Hits@10:35.88	Best:21.7
2024-12-27 20:48:50,924: End of token training: 1 Epoch: 25 Loss:21.525 MRR:21.59 Best Results: 21.7
2024-12-27 20:48:50,924: Snapshot:1	Epoch:25	Loss:21.525	translation_Loss:21.524	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.59	Hits@10:35.88	Best:21.7
2024-12-27 20:48:51,195: => loading checkpoint './checkpoint/FACTfact_0.001_2048_5000/1model_best.tar'
2024-12-27 20:48:57,791: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2512 | 0.1644 | 0.2925 | 0.344  |  0.4106 |
|     1      | 0.2151 | 0.1354 | 0.2533 | 0.3007 |  0.3605 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:49:23,313: Snapshot:2	Epoch:0	Loss:8.619	translation_Loss:7.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.538                                                   	MRR:20.42	Hits@10:35.57	Best:20.42
2024-12-27 20:49:30,647: Snapshot:2	Epoch:1	Loss:6.732	translation_Loss:4.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.051                                                   	MRR:20.74	Hits@10:35.84	Best:20.74
2024-12-27 20:49:37,948: Snapshot:2	Epoch:2	Loss:6.434	translation_Loss:4.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.031                                                   	MRR:20.9	Hits@10:36.07	Best:20.9
2024-12-27 20:49:45,334: Snapshot:2	Epoch:3	Loss:6.329	translation_Loss:4.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.053                                                   	MRR:20.89	Hits@10:36.25	Best:20.9
2024-12-27 20:49:52,668: Snapshot:2	Epoch:4	Loss:6.298	translation_Loss:4.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.054                                                   	MRR:20.82	Hits@10:36.04	Best:20.9
2024-12-27 20:49:59,948: Snapshot:2	Epoch:5	Loss:6.305	translation_Loss:4.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.069                                                   	MRR:20.89	Hits@10:36.26	Best:20.9
2024-12-27 20:50:07,382: Snapshot:2	Epoch:6	Loss:6.295	translation_Loss:4.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.069                                                   	MRR:20.91	Hits@10:36.23	Best:20.91
2024-12-27 20:50:14,640: Snapshot:2	Epoch:7	Loss:6.298	translation_Loss:4.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.081                                                   	MRR:20.88	Hits@10:36.11	Best:20.91
2024-12-27 20:50:21,895: Snapshot:2	Epoch:8	Loss:6.28	translation_Loss:4.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.066                                                   	MRR:20.8	Hits@10:36.24	Best:20.91
2024-12-27 20:50:29,137: Snapshot:2	Epoch:9	Loss:6.278	translation_Loss:4.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.083                                                   	MRR:20.77	Hits@10:36.22	Best:20.91
2024-12-27 20:50:36,416: Snapshot:2	Epoch:10	Loss:6.28	translation_Loss:4.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.08                                                   	MRR:20.76	Hits@10:36.13	Best:20.91
2024-12-27 20:50:43,703: Snapshot:2	Epoch:11	Loss:6.275	translation_Loss:4.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.084                                                   	MRR:20.94	Hits@10:36.29	Best:20.94
2024-12-27 20:50:51,001: Snapshot:2	Epoch:12	Loss:6.288	translation_Loss:4.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.084                                                   	MRR:20.89	Hits@10:36.08	Best:20.94
2024-12-27 20:50:58,253: Snapshot:2	Epoch:13	Loss:6.27	translation_Loss:4.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.086                                                   	MRR:20.84	Hits@10:36.14	Best:20.94
2024-12-27 20:51:06,039: Snapshot:2	Epoch:14	Loss:6.273	translation_Loss:4.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.09                                                   	MRR:20.86	Hits@10:36.21	Best:20.94
2024-12-27 20:51:13,272: Snapshot:2	Epoch:15	Loss:6.286	translation_Loss:4.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.086                                                   	MRR:20.84	Hits@10:36.05	Best:20.94
2024-12-27 20:51:20,557: Early Stopping! Snapshot: 2 Epoch: 16 Best Results: 20.94
2024-12-27 20:51:20,557: Start to training tokens! Snapshot: 2 Epoch: 16 Loss:6.275 MRR:20.93 Best Results: 20.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:51:20,557: Snapshot:2	Epoch:16	Loss:6.275	translation_Loss:4.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.094                                                   	MRR:20.93	Hits@10:36.15	Best:20.94
2024-12-27 20:51:27,755: Snapshot:2	Epoch:17	Loss:21.508	translation_Loss:21.351	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.93	Hits@10:36.15	Best:20.94
2024-12-27 20:51:34,902: End of token training: 2 Epoch: 18 Loss:21.351 MRR:20.93 Best Results: 20.94
2024-12-27 20:51:34,902: Snapshot:2	Epoch:18	Loss:21.351	translation_Loss:21.35	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.93	Hits@10:36.15	Best:20.94
2024-12-27 20:51:35,221: => loading checkpoint './checkpoint/FACTfact_0.001_2048_5000/2model_best.tar'
2024-12-27 20:51:44,970: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1619 |  0.29  | 0.3446 |  0.4133 |
|     1      | 0.2216 | 0.1398 | 0.2583 | 0.3099 |  0.3742 |
|     2      | 0.2105 | 0.1294 | 0.2458 | 0.2972 |  0.3626 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:52:10,704: Snapshot:3	Epoch:0	Loss:5.586	translation_Loss:4.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.328                                                   	MRR:19.52	Hits@10:36.71	Best:19.52
2024-12-27 20:52:18,038: Snapshot:3	Epoch:1	Loss:4.288	translation_Loss:2.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.696                                                   	MRR:19.61	Hits@10:37.03	Best:19.61
2024-12-27 20:52:25,405: Snapshot:3	Epoch:2	Loss:4.199	translation_Loss:2.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.682                                                   	MRR:19.7	Hits@10:36.92	Best:19.7
2024-12-27 20:52:32,820: Snapshot:3	Epoch:3	Loss:4.184	translation_Loss:2.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.718                                                   	MRR:19.79	Hits@10:37.02	Best:19.79
2024-12-27 20:52:40,188: Snapshot:3	Epoch:4	Loss:4.178	translation_Loss:2.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.722                                                   	MRR:19.75	Hits@10:37.11	Best:19.79
2024-12-27 20:52:47,496: Snapshot:3	Epoch:5	Loss:4.194	translation_Loss:2.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.74                                                   	MRR:19.66	Hits@10:36.77	Best:19.79
2024-12-27 20:52:54,799: Snapshot:3	Epoch:6	Loss:4.211	translation_Loss:2.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:19.77	Hits@10:37.0	Best:19.79
2024-12-27 20:53:02,103: Snapshot:3	Epoch:7	Loss:4.202	translation_Loss:2.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.756                                                   	MRR:19.63	Hits@10:37.0	Best:19.79
2024-12-27 20:53:09,395: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 19.79
2024-12-27 20:53:09,395: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:4.19 MRR:19.76 Best Results: 19.79
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:53:09,396: Snapshot:3	Epoch:8	Loss:4.19	translation_Loss:2.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.757                                                   	MRR:19.76	Hits@10:36.92	Best:19.79
2024-12-27 20:53:16,583: Snapshot:3	Epoch:9	Loss:20.485	translation_Loss:20.336	multi_layer_Loss:0.149	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.76	Hits@10:36.92	Best:19.79
2024-12-27 20:53:23,733: End of token training: 3 Epoch: 10 Loss:20.353 MRR:19.76 Best Results: 19.79
2024-12-27 20:53:23,734: Snapshot:3	Epoch:10	Loss:20.353	translation_Loss:20.352	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.76	Hits@10:36.92	Best:19.79
2024-12-27 20:53:24,092: => loading checkpoint './checkpoint/FACTfact_0.001_2048_5000/3model_best.tar'
2024-12-27 20:53:36,714: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2424 | 0.1564 | 0.2792 | 0.3319 |  0.4029 |
|     1      | 0.2186 | 0.1377 | 0.2522 | 0.3048 |  0.3723 |
|     2      | 0.2103 | 0.1272 | 0.2424 | 0.2955 |  0.371  |
|     3      | 0.1976 | 0.1107 | 0.2281 | 0.2873 |  0.3691 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 20:54:02,181: Snapshot:4	Epoch:0	Loss:2.871	translation_Loss:2.055	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.817                                                   	MRR:20.82	Hits@10:44.92	Best:20.82
2024-12-27 20:54:09,618: Snapshot:4	Epoch:1	Loss:1.648	translation_Loss:0.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.805                                                   	MRR:21.11	Hits@10:44.13	Best:21.11
2024-12-27 20:54:17,175: Snapshot:4	Epoch:2	Loss:1.516	translation_Loss:0.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.785                                                   	MRR:21.24	Hits@10:44.59	Best:21.24
2024-12-27 20:54:24,546: Snapshot:4	Epoch:3	Loss:1.488	translation_Loss:0.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.788                                                   	MRR:21.24	Hits@10:44.54	Best:21.24
2024-12-27 20:54:32,380: Snapshot:4	Epoch:4	Loss:1.486	translation_Loss:0.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.8                                                   	MRR:21.25	Hits@10:44.61	Best:21.25
2024-12-27 20:54:39,774: Snapshot:4	Epoch:5	Loss:1.503	translation_Loss:0.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.804                                                   	MRR:21.13	Hits@10:44.26	Best:21.25
2024-12-27 20:54:47,182: Snapshot:4	Epoch:6	Loss:1.501	translation_Loss:0.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.812                                                   	MRR:21.28	Hits@10:44.45	Best:21.28
2024-12-27 20:54:54,588: Snapshot:4	Epoch:7	Loss:1.498	translation_Loss:0.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.811                                                   	MRR:21.36	Hits@10:44.72	Best:21.36
2024-12-27 20:55:02,030: Snapshot:4	Epoch:8	Loss:1.501	translation_Loss:0.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.815                                                   	MRR:21.5	Hits@10:44.72	Best:21.5
2024-12-27 20:55:09,477: Snapshot:4	Epoch:9	Loss:1.495	translation_Loss:0.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.814                                                   	MRR:21.34	Hits@10:44.63	Best:21.5
2024-12-27 20:55:17,320: Snapshot:4	Epoch:10	Loss:1.495	translation_Loss:0.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.813                                                   	MRR:21.17	Hits@10:44.23	Best:21.5
2024-12-27 20:55:24,664: Snapshot:4	Epoch:11	Loss:1.498	translation_Loss:0.687	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.811                                                   	MRR:21.04	Hits@10:44.27	Best:21.5
2024-12-27 20:55:32,038: Snapshot:4	Epoch:12	Loss:1.499	translation_Loss:0.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.817                                                   	MRR:21.47	Hits@10:44.82	Best:21.5
2024-12-27 20:55:39,393: Early Stopping! Snapshot: 4 Epoch: 13 Best Results: 21.5
2024-12-27 20:55:39,393: Start to training tokens! Snapshot: 4 Epoch: 13 Loss:1.512 MRR:21.2 Best Results: 21.5
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:55:39,393: Snapshot:4	Epoch:13	Loss:1.512	translation_Loss:0.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.821                                                   	MRR:21.2	Hits@10:44.41	Best:21.5
2024-12-27 20:55:46,675: Snapshot:4	Epoch:14	Loss:16.936	translation_Loss:16.786	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.2	Hits@10:44.41	Best:21.5
2024-12-27 20:55:54,286: End of token training: 4 Epoch: 15 Loss:16.774 MRR:21.2 Best Results: 21.5
2024-12-27 20:55:54,287: Snapshot:4	Epoch:15	Loss:16.774	translation_Loss:16.773	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.2	Hits@10:44.41	Best:21.5
2024-12-27 20:55:54,566: => loading checkpoint './checkpoint/FACTfact_0.001_2048_5000/4model_best.tar'
2024-12-27 20:56:10,707: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2327 | 0.1493 | 0.2667 | 0.3185 |  0.3898 |
|     1      | 0.2079 | 0.1306 | 0.2366 | 0.2886 |  0.3603 |
|     2      | 0.2005 | 0.1206 | 0.2275 |  0.28  |  0.3595 |
|     3      | 0.1912 | 0.1052 | 0.2155 | 0.2775 |  0.3699 |
|     4      | 0.2147 | 0.1042 | 0.2475 | 0.3341 |  0.4448 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 20:56:10,709: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2426 | 0.1586 | 0.2818 | 0.3318 |  0.396  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2512 | 0.1644 | 0.2925 | 0.344  |  0.4106 |
|     1      | 0.2151 | 0.1354 | 0.2533 | 0.3007 |  0.3605 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1619 |  0.29  | 0.3446 |  0.4133 |
|     1      | 0.2216 | 0.1398 | 0.2583 | 0.3099 |  0.3742 |
|     2      | 0.2105 | 0.1294 | 0.2458 | 0.2972 |  0.3626 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2424 | 0.1564 | 0.2792 | 0.3319 |  0.4029 |
|     1      | 0.2186 | 0.1377 | 0.2522 | 0.3048 |  0.3723 |
|     2      | 0.2103 | 0.1272 | 0.2424 | 0.2955 |  0.371  |
|     3      | 0.1976 | 0.1107 | 0.2281 | 0.2873 |  0.3691 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2327 | 0.1493 | 0.2667 | 0.3185 |  0.3898 |
|     1      | 0.2079 | 0.1306 | 0.2366 | 0.2886 |  0.3603 |
|     2      | 0.2005 | 0.1206 | 0.2275 |  0.28  |  0.3595 |
|     3      | 0.1912 | 0.1052 | 0.2155 | 0.2775 |  0.3699 |
|     4      | 0.2147 | 0.1042 | 0.2475 | 0.3341 |  0.4448 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 20:56:10,710: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 167.7927803993225  |   0.243   |    0.159     |    0.282     |     0.396     |
|    1     | 200.2327516078949  |   0.233   |     0.15     |    0.273     |     0.386     |
|    2     | 153.75576257705688 |   0.227   |    0.144     |    0.265     |     0.383     |
|    3     | 95.09418272972107  |   0.217   |    0.133     |     0.25     |     0.379     |
|    4     | 134.14867615699768 |   0.209   |    0.122     |    0.239     |     0.385     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 20:56:10,710: Sum_Training_Time:751.024153470993
2024-12-27 20:56:10,710: Every_Training_Time:[167.7927803993225, 200.2327516078949, 153.75576257705688, 95.09418272972107, 134.14867615699768]
2024-12-27 20:56:10,710: Forward transfer: 0.17017500000000002 Backward transfer: -0.008375
2024-12-27 20:56:49,668: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241227205614/FACTfact_0.001_2048_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.001_2048_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.001_2048_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 20:56:59,594: Snapshot:0	Epoch:0	Loss:24.869	translation_Loss:24.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.15	Hits@10:15.62	Best:7.15
2024-12-27 20:57:05,979: Snapshot:0	Epoch:1	Loss:16.859	translation_Loss:16.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.6	Hits@10:25.86	Best:11.6
2024-12-27 20:57:12,736: Snapshot:0	Epoch:2	Loss:11.571	translation_Loss:11.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.35	Hits@10:32.18	Best:15.35
2024-12-27 20:57:19,105: Snapshot:0	Epoch:3	Loss:7.838	translation_Loss:7.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.53	Hits@10:35.9	Best:18.53
2024-12-27 20:57:25,437: Snapshot:0	Epoch:4	Loss:5.219	translation_Loss:5.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.98	Hits@10:38.04	Best:20.98
2024-12-27 20:57:31,820: Snapshot:0	Epoch:5	Loss:3.478	translation_Loss:3.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.66	Hits@10:39.11	Best:22.66
2024-12-27 20:57:38,168: Snapshot:0	Epoch:6	Loss:2.325	translation_Loss:2.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.77	Hits@10:39.66	Best:23.77
2024-12-27 20:57:45,074: Snapshot:0	Epoch:7	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.28	Hits@10:39.96	Best:24.28
2024-12-27 20:57:51,417: Snapshot:0	Epoch:8	Loss:1.152	translation_Loss:1.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.58	Hits@10:40.22	Best:24.58
2024-12-27 20:57:57,747: Snapshot:0	Epoch:9	Loss:0.883	translation_Loss:0.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:40.22	Best:24.74
2024-12-27 20:58:04,062: Snapshot:0	Epoch:10	Loss:0.704	translation_Loss:0.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.73	Hits@10:40.12	Best:24.74
2024-12-27 20:58:10,412: Snapshot:0	Epoch:11	Loss:0.577	translation_Loss:0.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.86	Hits@10:40.39	Best:24.86
2024-12-27 20:58:16,738: Snapshot:0	Epoch:12	Loss:0.495	translation_Loss:0.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.84	Hits@10:40.27	Best:24.86
2024-12-27 20:58:23,511: Snapshot:0	Epoch:13	Loss:0.438	translation_Loss:0.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.85	Hits@10:40.39	Best:24.86
2024-12-27 20:58:29,832: Snapshot:0	Epoch:14	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:40.37	Best:24.87
2024-12-27 20:58:36,179: Snapshot:0	Epoch:15	Loss:0.359	translation_Loss:0.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.21	Best:24.87
2024-12-27 20:58:42,486: Snapshot:0	Epoch:16	Loss:0.328	translation_Loss:0.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.87	Hits@10:40.2	Best:24.87
2024-12-27 20:58:48,855: Snapshot:0	Epoch:17	Loss:0.304	translation_Loss:0.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.94	Hits@10:40.44	Best:24.94
2024-12-27 20:58:55,640: Snapshot:0	Epoch:18	Loss:0.288	translation_Loss:0.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:40.27	Best:24.94
2024-12-27 20:59:01,942: Snapshot:0	Epoch:19	Loss:0.267	translation_Loss:0.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.9	Hits@10:40.27	Best:24.94
2024-12-27 20:59:08,280: Snapshot:0	Epoch:20	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.82	Hits@10:40.21	Best:24.94
2024-12-27 20:59:14,603: Snapshot:0	Epoch:21	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.26	Best:24.94
2024-12-27 20:59:20,938: Early Stopping! Snapshot: 0 Epoch: 22 Best Results: 24.94
2024-12-27 20:59:20,938: Start to training tokens! Snapshot: 0 Epoch: 22 Loss:0.224 MRR:24.72 Best Results: 24.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 20:59:20,939: Snapshot:0	Epoch:22	Loss:0.224	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:40.05	Best:24.94
2024-12-27 20:59:27,831: Snapshot:0	Epoch:23	Loss:18.451	translation_Loss:18.296	multi_layer_Loss:0.156	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.72	Hits@10:40.05	Best:24.94
2024-12-27 20:59:34,701: End of token training: 0 Epoch: 24 Loss:18.279 MRR:24.72 Best Results: 24.94
2024-12-27 20:59:34,702: Snapshot:0	Epoch:24	Loss:18.279	translation_Loss:18.278	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.72	Hits@10:40.05	Best:24.94
2024-12-27 20:59:34,995: => loading checkpoint './checkpoint/FACTfact_0.001_2048_10000/0model_best.tar'
2024-12-27 20:59:37,842: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2422 | 0.1576 | 0.2831 | 0.3336 |  0.3952 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:00:02,162: Snapshot:1	Epoch:0	Loss:11.986	translation_Loss:10.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.772                                                   	MRR:19.71	Hits@10:33.14	Best:19.71
2024-12-27 21:00:09,319: Snapshot:1	Epoch:1	Loss:9.952	translation_Loss:8.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:20.58	Hits@10:34.23	Best:20.58
2024-12-27 21:00:16,699: Snapshot:1	Epoch:2	Loss:8.83	translation_Loss:7.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.669                                                   	MRR:20.87	Hits@10:34.82	Best:20.87
2024-12-27 21:00:23,659: Snapshot:1	Epoch:3	Loss:8.45	translation_Loss:6.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.642                                                   	MRR:21.02	Hits@10:34.95	Best:21.02
2024-12-27 21:00:30,616: Snapshot:1	Epoch:4	Loss:8.306	translation_Loss:6.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.621                                                   	MRR:21.05	Hits@10:34.82	Best:21.05
2024-12-27 21:00:37,651: Snapshot:1	Epoch:5	Loss:8.238	translation_Loss:6.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.617                                                   	MRR:21.08	Hits@10:34.97	Best:21.08
2024-12-27 21:00:44,583: Snapshot:1	Epoch:6	Loss:8.182	translation_Loss:6.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.611                                                   	MRR:21.06	Hits@10:34.87	Best:21.08
2024-12-27 21:00:51,570: Snapshot:1	Epoch:7	Loss:8.175	translation_Loss:6.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.608                                                   	MRR:21.13	Hits@10:35.0	Best:21.13
2024-12-27 21:00:58,906: Snapshot:1	Epoch:8	Loss:8.132	translation_Loss:6.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.609                                                   	MRR:21.12	Hits@10:34.89	Best:21.13
2024-12-27 21:01:05,934: Snapshot:1	Epoch:9	Loss:8.152	translation_Loss:6.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.614                                                   	MRR:21.09	Hits@10:34.91	Best:21.13
2024-12-27 21:01:12,858: Snapshot:1	Epoch:10	Loss:8.128	translation_Loss:6.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.615                                                   	MRR:21.07	Hits@10:34.84	Best:21.13
2024-12-27 21:01:19,793: Snapshot:1	Epoch:11	Loss:8.128	translation_Loss:6.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.614                                                   	MRR:21.07	Hits@10:34.95	Best:21.13
2024-12-27 21:01:26,770: Early Stopping! Snapshot: 1 Epoch: 12 Best Results: 21.13
2024-12-27 21:01:26,770: Start to training tokens! Snapshot: 1 Epoch: 12 Loss:8.118 MRR:21.06 Best Results: 21.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:01:26,770: Snapshot:1	Epoch:12	Loss:8.118	translation_Loss:6.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.611                                                   	MRR:21.06	Hits@10:34.83	Best:21.13
2024-12-27 21:01:34,122: Snapshot:1	Epoch:13	Loss:22.208	translation_Loss:22.057	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.06	Hits@10:34.83	Best:21.13
2024-12-27 21:01:40,973: End of token training: 1 Epoch: 14 Loss:22.038 MRR:21.06 Best Results: 21.13
2024-12-27 21:01:40,973: Snapshot:1	Epoch:14	Loss:22.038	translation_Loss:22.037	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.06	Hits@10:34.83	Best:21.13
2024-12-27 21:01:41,261: => loading checkpoint './checkpoint/FACTfact_0.001_2048_10000/1model_best.tar'
2024-12-27 21:01:47,468: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2484 | 0.1624 | 0.2907 | 0.3398 |  0.4031 |
|     1      | 0.2086 | 0.1306 | 0.2466 | 0.293  |  0.3516 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:02:12,671: Snapshot:2	Epoch:0	Loss:9.665	translation_Loss:7.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.881                                                   	MRR:19.69	Hits@10:34.23	Best:19.69
2024-12-27 21:02:19,842: Snapshot:2	Epoch:1	Loss:8.526	translation_Loss:6.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.0                                                   	MRR:19.81	Hits@10:34.47	Best:19.81
2024-12-27 21:02:27,035: Snapshot:2	Epoch:2	Loss:8.041	translation_Loss:6.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.91                                                   	MRR:19.95	Hits@10:34.66	Best:19.95
2024-12-27 21:02:34,159: Snapshot:2	Epoch:3	Loss:7.966	translation_Loss:6.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.908                                                   	MRR:19.95	Hits@10:34.66	Best:19.95
2024-12-27 21:02:41,339: Snapshot:2	Epoch:4	Loss:7.922	translation_Loss:6.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.904                                                   	MRR:19.99	Hits@10:34.7	Best:19.99
2024-12-27 21:02:48,532: Snapshot:2	Epoch:5	Loss:7.923	translation_Loss:6.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.913                                                   	MRR:20.03	Hits@10:34.74	Best:20.03
2024-12-27 21:02:55,769: Snapshot:2	Epoch:6	Loss:7.913	translation_Loss:5.998	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.915                                                   	MRR:20.03	Hits@10:34.68	Best:20.03
2024-12-27 21:03:02,893: Snapshot:2	Epoch:7	Loss:7.915	translation_Loss:6.001	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.914                                                   	MRR:19.94	Hits@10:34.69	Best:20.03
2024-12-27 21:03:10,060: Snapshot:2	Epoch:8	Loss:7.881	translation_Loss:5.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.912                                                   	MRR:20.08	Hits@10:34.74	Best:20.08
2024-12-27 21:03:17,208: Snapshot:2	Epoch:9	Loss:7.906	translation_Loss:5.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.92                                                   	MRR:20.06	Hits@10:34.64	Best:20.08
2024-12-27 21:03:24,408: Snapshot:2	Epoch:10	Loss:7.9	translation_Loss:5.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.919                                                   	MRR:20.04	Hits@10:34.74	Best:20.08
2024-12-27 21:03:31,513: Snapshot:2	Epoch:11	Loss:7.903	translation_Loss:5.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.921                                                   	MRR:20.01	Hits@10:34.62	Best:20.08
2024-12-27 21:03:38,715: Snapshot:2	Epoch:12	Loss:7.893	translation_Loss:5.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.921                                                   	MRR:20.1	Hits@10:34.7	Best:20.1
2024-12-27 21:03:46,006: Snapshot:2	Epoch:13	Loss:7.891	translation_Loss:5.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.926                                                   	MRR:20.04	Hits@10:34.68	Best:20.1
2024-12-27 21:03:53,642: Snapshot:2	Epoch:14	Loss:7.898	translation_Loss:5.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.924                                                   	MRR:20.01	Hits@10:34.76	Best:20.1
2024-12-27 21:04:00,761: Snapshot:2	Epoch:15	Loss:7.893	translation_Loss:5.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.93                                                   	MRR:20.02	Hits@10:34.49	Best:20.1
2024-12-27 21:04:07,889: Snapshot:2	Epoch:16	Loss:7.89	translation_Loss:5.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.922                                                   	MRR:20.07	Hits@10:34.75	Best:20.1
2024-12-27 21:04:15,041: Early Stopping! Snapshot: 2 Epoch: 17 Best Results: 20.1
2024-12-27 21:04:15,041: Start to training tokens! Snapshot: 2 Epoch: 17 Loss:7.889 MRR:19.99 Best Results: 20.1
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:04:15,042: Snapshot:2	Epoch:17	Loss:7.889	translation_Loss:5.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.927                                                   	MRR:19.99	Hits@10:34.72	Best:20.1
2024-12-27 21:04:22,035: Snapshot:2	Epoch:18	Loss:22.284	translation_Loss:22.127	multi_layer_Loss:0.157	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.99	Hits@10:34.72	Best:20.1
2024-12-27 21:04:29,063: End of token training: 2 Epoch: 19 Loss:22.103 MRR:19.99 Best Results: 20.1
2024-12-27 21:04:29,063: Snapshot:2	Epoch:19	Loss:22.103	translation_Loss:22.102	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.99	Hits@10:34.72	Best:20.1
2024-12-27 21:04:29,415: => loading checkpoint './checkpoint/FACTfact_0.001_2048_10000/2model_best.tar'
2024-12-27 21:04:39,923: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2489 | 0.1626 | 0.2898 | 0.342  |  0.4075 |
|     1      | 0.2157 | 0.1363 | 0.2536 | 0.2996 |  0.3625 |
|     2      | 0.1998 | 0.1209 | 0.2347 | 0.2844 |  0.3449 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:05:04,516: Snapshot:3	Epoch:0	Loss:7.135	translation_Loss:5.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.758                                                   	MRR:18.66	Hits@10:34.89	Best:18.66
2024-12-27 21:05:11,786: Snapshot:3	Epoch:1	Loss:6.302	translation_Loss:4.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.922                                                   	MRR:18.92	Hits@10:35.18	Best:18.92
2024-12-27 21:05:19,385: Snapshot:3	Epoch:2	Loss:6.117	translation_Loss:4.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.86                                                   	MRR:18.88	Hits@10:35.11	Best:18.92
2024-12-27 21:05:26,526: Snapshot:3	Epoch:3	Loss:6.108	translation_Loss:4.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.87                                                   	MRR:18.91	Hits@10:35.32	Best:18.92
2024-12-27 21:05:33,785: Snapshot:3	Epoch:4	Loss:6.099	translation_Loss:4.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.88                                                   	MRR:18.99	Hits@10:35.39	Best:18.99
2024-12-27 21:05:41,006: Snapshot:3	Epoch:5	Loss:6.096	translation_Loss:4.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.877                                                   	MRR:18.96	Hits@10:35.26	Best:18.99
2024-12-27 21:05:48,275: Snapshot:3	Epoch:6	Loss:6.094	translation_Loss:4.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.88                                                   	MRR:19.01	Hits@10:35.22	Best:19.01
2024-12-27 21:05:56,025: Snapshot:3	Epoch:7	Loss:6.104	translation_Loss:4.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.887                                                   	MRR:19.0	Hits@10:35.21	Best:19.01
2024-12-27 21:06:03,225: Snapshot:3	Epoch:8	Loss:6.088	translation_Loss:4.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.884                                                   	MRR:18.9	Hits@10:35.19	Best:19.01
2024-12-27 21:06:10,456: Snapshot:3	Epoch:9	Loss:6.102	translation_Loss:4.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.891                                                   	MRR:19.07	Hits@10:35.17	Best:19.07
2024-12-27 21:06:17,627: Snapshot:3	Epoch:10	Loss:6.112	translation_Loss:4.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.891                                                   	MRR:18.96	Hits@10:35.26	Best:19.07
2024-12-27 21:06:24,785: Snapshot:3	Epoch:11	Loss:6.106	translation_Loss:4.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.899                                                   	MRR:18.93	Hits@10:35.14	Best:19.07
2024-12-27 21:06:31,930: Snapshot:3	Epoch:12	Loss:6.108	translation_Loss:4.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.893                                                   	MRR:18.93	Hits@10:35.17	Best:19.07
2024-12-27 21:06:39,526: Snapshot:3	Epoch:13	Loss:6.109	translation_Loss:4.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.896                                                   	MRR:18.9	Hits@10:35.2	Best:19.07
2024-12-27 21:06:46,714: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 19.07
2024-12-27 21:06:46,714: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:6.109 MRR:18.94 Best Results: 19.07
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:06:46,714: Snapshot:3	Epoch:14	Loss:6.109	translation_Loss:4.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.9                                                   	MRR:18.94	Hits@10:34.96	Best:19.07
2024-12-27 21:06:53,803: Snapshot:3	Epoch:15	Loss:21.361	translation_Loss:21.212	multi_layer_Loss:0.149	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:34.96	Best:19.07
2024-12-27 21:07:00,863: End of token training: 3 Epoch: 16 Loss:21.243 MRR:18.94 Best Results: 19.07
2024-12-27 21:07:00,863: Snapshot:3	Epoch:16	Loss:21.243	translation_Loss:21.242	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:18.94	Hits@10:34.96	Best:19.07
2024-12-27 21:07:01,248: => loading checkpoint './checkpoint/FACTfact_0.001_2048_10000/3model_best.tar'
2024-12-27 21:07:14,388: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2439 | 0.157  | 0.2834 | 0.3347 |  0.4038 |
|     1      | 0.2141 | 0.1337 | 0.2511 | 0.3004 |  0.3668 |
|     2      | 0.2042 | 0.1235 | 0.2367 | 0.2886 |  0.358  |
|     3      | 0.1902 | 0.1068 | 0.2214 | 0.2767 |  0.3519 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:07:39,241: Snapshot:4	Epoch:0	Loss:4.088	translation_Loss:2.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.243                                                   	MRR:20.17	Hits@10:43.27	Best:20.17
2024-12-27 21:07:46,630: Snapshot:4	Epoch:1	Loss:2.881	translation_Loss:1.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.246                                                   	MRR:20.48	Hits@10:43.1	Best:20.48
2024-12-27 21:07:54,411: Snapshot:4	Epoch:2	Loss:2.706	translation_Loss:1.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.217                                                   	MRR:20.52	Hits@10:42.65	Best:20.52
2024-12-27 21:08:01,664: Snapshot:4	Epoch:3	Loss:2.692	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.23                                                   	MRR:20.45	Hits@10:42.88	Best:20.52
2024-12-27 21:08:08,897: Snapshot:4	Epoch:4	Loss:2.671	translation_Loss:1.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.233                                                   	MRR:20.38	Hits@10:42.89	Best:20.52
2024-12-27 21:08:16,146: Snapshot:4	Epoch:5	Loss:2.675	translation_Loss:1.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.233                                                   	MRR:20.37	Hits@10:42.88	Best:20.52
2024-12-27 21:08:23,416: Snapshot:4	Epoch:6	Loss:2.671	translation_Loss:1.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.232                                                   	MRR:20.67	Hits@10:43.2	Best:20.67
2024-12-27 21:08:30,667: Snapshot:4	Epoch:7	Loss:2.672	translation_Loss:1.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.233                                                   	MRR:20.61	Hits@10:43.03	Best:20.67
2024-12-27 21:08:38,365: Snapshot:4	Epoch:8	Loss:2.672	translation_Loss:1.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.24                                                   	MRR:20.52	Hits@10:42.84	Best:20.67
2024-12-27 21:08:45,618: Snapshot:4	Epoch:9	Loss:2.673	translation_Loss:1.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.241                                                   	MRR:20.59	Hits@10:42.97	Best:20.67
2024-12-27 21:08:52,907: Snapshot:4	Epoch:10	Loss:2.677	translation_Loss:1.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.245                                                   	MRR:20.6	Hits@10:43.12	Best:20.67
2024-12-27 21:09:00,137: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 20.67
2024-12-27 21:09:00,138: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:2.677 MRR:20.55 Best Results: 20.67
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:09:00,138: Snapshot:4	Epoch:11	Loss:2.677	translation_Loss:1.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.243                                                   	MRR:20.55	Hits@10:43.11	Best:20.67
2024-12-27 21:09:07,270: Snapshot:4	Epoch:12	Loss:17.611	translation_Loss:17.461	multi_layer_Loss:0.15	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.55	Hits@10:43.11	Best:20.67
2024-12-27 21:09:14,922: End of token training: 4 Epoch: 13 Loss:17.456 MRR:20.55 Best Results: 20.67
2024-12-27 21:09:14,922: Snapshot:4	Epoch:13	Loss:17.456	translation_Loss:17.455	multi_layer_Loss:0.001	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.55	Hits@10:43.11	Best:20.67
2024-12-27 21:09:15,200: => loading checkpoint './checkpoint/FACTfact_0.001_2048_10000/4model_best.tar'
2024-12-27 21:09:31,411: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2362 | 0.1515 | 0.2733 | 0.3254 |  0.3947 |
|     1      | 0.2068 | 0.1281 | 0.2401 | 0.2903 |  0.357  |
|     2      | 0.1968 | 0.1178 | 0.2261 | 0.2778 |  0.3516 |
|     3      | 0.1851 | 0.1016 | 0.2102 | 0.2696 |  0.3554 |
|     4      | 0.208  | 0.099  | 0.2444 | 0.3255 |  0.4332 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 21:09:31,413: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2422 | 0.1576 | 0.2831 | 0.3336 |  0.3952 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2484 | 0.1624 | 0.2907 | 0.3398 |  0.4031 |
|     1      | 0.2086 | 0.1306 | 0.2466 | 0.293  |  0.3516 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2489 | 0.1626 | 0.2898 | 0.342  |  0.4075 |
|     1      | 0.2157 | 0.1363 | 0.2536 | 0.2996 |  0.3625 |
|     2      | 0.1998 | 0.1209 | 0.2347 | 0.2844 |  0.3449 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2439 | 0.157  | 0.2834 | 0.3347 |  0.4038 |
|     1      | 0.2141 | 0.1337 | 0.2511 | 0.3004 |  0.3668 |
|     2      | 0.2042 | 0.1235 | 0.2367 | 0.2886 |  0.358  |
|     3      | 0.1902 | 0.1068 | 0.2214 | 0.2767 |  0.3519 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2362 | 0.1515 | 0.2733 | 0.3254 |  0.3947 |
|     1      | 0.2068 | 0.1281 | 0.2401 | 0.2903 |  0.357  |
|     2      | 0.1968 | 0.1178 | 0.2261 | 0.2778 |  0.3516 |
|     3      | 0.1851 | 0.1016 | 0.2102 | 0.2696 |  0.3554 |
|     4      | 0.208  | 0.099  | 0.2444 | 0.3255 |  0.4332 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 21:09:31,414: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 165.03328394889832 |   0.242   |    0.158     |    0.283     |     0.395     |
|    1     | 119.96324729919434 |   0.229   |    0.146     |    0.269     |     0.377     |
|    2     | 158.25147581100464 |   0.221   |     0.14     |    0.259     |     0.372     |
|    3     | 137.54655408859253 |   0.213   |     0.13     |    0.248     |      0.37     |
|    4     | 117.06852984428406 |   0.207   |     0.12     |    0.239     |     0.378     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 21:09:31,414: Sum_Training_Time:697.8630909919739
2024-12-27 21:09:31,414: Every_Training_Time:[165.03328394889832, 119.96324729919434, 158.25147581100464, 137.54655408859253, 117.06852984428406]
2024-12-27 21:09:31,414: Forward transfer: 0.165 Backward transfer: -0.003975000000000006
2024-12-27 21:10:10,831: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227210935/FACTfact_0.0001_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 21:10:20,961: Snapshot:0	Epoch:0	Loss:102.979	translation_Loss:102.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.78	Hits@10:4.76	Best:2.78
2024-12-27 21:10:27,514: Snapshot:0	Epoch:1	Loss:93.519	translation_Loss:93.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:8.53	Best:4.54
2024-12-27 21:10:34,134: Snapshot:0	Epoch:2	Loss:84.912	translation_Loss:84.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.71	Hits@10:11.57	Best:5.71
2024-12-27 21:10:40,778: Snapshot:0	Epoch:3	Loss:77.12	translation_Loss:77.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.9	Hits@10:14.86	Best:6.9
2024-12-27 21:10:47,405: Snapshot:0	Epoch:4	Loss:69.774	translation_Loss:69.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.2	Hits@10:18.03	Best:8.2
2024-12-27 21:10:53,988: Snapshot:0	Epoch:5	Loss:62.903	translation_Loss:62.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.6	Hits@10:20.99	Best:9.6
2024-12-27 21:11:00,570: Snapshot:0	Epoch:6	Loss:56.536	translation_Loss:56.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.94	Hits@10:23.75	Best:10.94
2024-12-27 21:11:07,182: Snapshot:0	Epoch:7	Loss:50.671	translation_Loss:50.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.08	Hits@10:26.05	Best:12.08
2024-12-27 21:11:14,303: Snapshot:0	Epoch:8	Loss:45.219	translation_Loss:45.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.1	Hits@10:28.11	Best:13.1
2024-12-27 21:11:20,914: Snapshot:0	Epoch:9	Loss:40.269	translation_Loss:40.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.03	Hits@10:29.86	Best:14.03
2024-12-27 21:11:27,570: Snapshot:0	Epoch:10	Loss:35.787	translation_Loss:35.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.89	Hits@10:31.34	Best:14.89
2024-12-27 21:11:34,164: Snapshot:0	Epoch:11	Loss:31.68	translation_Loss:31.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.71	Hits@10:32.64	Best:15.71
2024-12-27 21:11:40,812: Snapshot:0	Epoch:12	Loss:28.084	translation_Loss:28.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.49	Hits@10:33.73	Best:16.49
2024-12-27 21:11:47,516: Snapshot:0	Epoch:13	Loss:24.798	translation_Loss:24.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.21	Hits@10:34.67	Best:17.21
2024-12-27 21:11:54,120: Snapshot:0	Epoch:14	Loss:21.946	translation_Loss:21.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.95	Hits@10:35.36	Best:17.95
2024-12-27 21:12:00,725: Snapshot:0	Epoch:15	Loss:19.371	translation_Loss:19.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.66	Hits@10:36.27	Best:18.66
2024-12-27 21:12:07,342: Snapshot:0	Epoch:16	Loss:17.059	translation_Loss:17.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.3	Hits@10:36.72	Best:19.3
2024-12-27 21:12:13,961: Snapshot:0	Epoch:17	Loss:15.009	translation_Loss:15.009	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.87	Hits@10:37.25	Best:19.87
2024-12-27 21:12:20,606: Snapshot:0	Epoch:18	Loss:13.195	translation_Loss:13.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.46	Hits@10:37.73	Best:20.46
2024-12-27 21:12:27,205: Snapshot:0	Epoch:19	Loss:11.561	translation_Loss:11.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.88	Hits@10:38.09	Best:20.88
2024-12-27 21:12:33,879: Snapshot:0	Epoch:20	Loss:10.128	translation_Loss:10.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.36	Hits@10:38.35	Best:21.36
2024-12-27 21:12:40,481: Snapshot:0	Epoch:21	Loss:8.908	translation_Loss:8.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.63	Hits@10:38.55	Best:21.63
2024-12-27 21:12:47,126: Snapshot:0	Epoch:22	Loss:7.842	translation_Loss:7.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.01	Hits@10:38.82	Best:22.01
2024-12-27 21:12:53,743: Snapshot:0	Epoch:23	Loss:6.88	translation_Loss:6.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.4	Hits@10:39.02	Best:22.4
2024-12-27 21:13:00,385: Snapshot:0	Epoch:24	Loss:6.073	translation_Loss:6.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:39.17	Best:22.67
2024-12-27 21:13:07,057: Snapshot:0	Epoch:25	Loss:5.411	translation_Loss:5.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.93	Hits@10:39.5	Best:22.93
2024-12-27 21:13:13,668: Snapshot:0	Epoch:26	Loss:4.758	translation_Loss:4.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.16	Hits@10:39.41	Best:23.16
2024-12-27 21:13:20,310: Snapshot:0	Epoch:27	Loss:4.278	translation_Loss:4.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.32	Hits@10:39.52	Best:23.32
2024-12-27 21:13:26,916: Snapshot:0	Epoch:28	Loss:3.816	translation_Loss:3.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:39.68	Best:23.51
2024-12-27 21:13:34,066: Snapshot:0	Epoch:29	Loss:3.408	translation_Loss:3.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.74	Best:23.65
2024-12-27 21:13:40,708: Snapshot:0	Epoch:30	Loss:3.096	translation_Loss:3.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:39.83	Best:23.74
2024-12-27 21:13:47,354: Snapshot:0	Epoch:31	Loss:2.838	translation_Loss:2.838	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.84	Hits@10:39.81	Best:23.84
2024-12-27 21:13:54,014: Snapshot:0	Epoch:32	Loss:2.571	translation_Loss:2.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:39.96	Best:23.94
2024-12-27 21:14:00,605: Snapshot:0	Epoch:33	Loss:2.343	translation_Loss:2.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:40.15	Best:24.06
2024-12-27 21:14:07,189: Snapshot:0	Epoch:34	Loss:2.138	translation_Loss:2.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:39.93	Best:24.06
2024-12-27 21:14:13,793: Snapshot:0	Epoch:35	Loss:2.01	translation_Loss:2.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:39.95	Best:24.14
2024-12-27 21:14:20,364: Snapshot:0	Epoch:36	Loss:1.841	translation_Loss:1.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.92	Best:24.14
2024-12-27 21:14:26,965: Snapshot:0	Epoch:37	Loss:1.733	translation_Loss:1.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.17	Hits@10:40.04	Best:24.17
2024-12-27 21:14:33,586: Snapshot:0	Epoch:38	Loss:1.6	translation_Loss:1.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:39.98	Best:24.21
2024-12-27 21:14:40,275: Snapshot:0	Epoch:39	Loss:1.522	translation_Loss:1.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:39.99	Best:24.22
2024-12-27 21:14:46,988: Snapshot:0	Epoch:40	Loss:1.435	translation_Loss:1.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:40.19	Best:24.23
2024-12-27 21:14:53,605: Snapshot:0	Epoch:41	Loss:1.36	translation_Loss:1.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.26	Hits@10:40.11	Best:24.26
2024-12-27 21:15:00,226: Snapshot:0	Epoch:42	Loss:1.277	translation_Loss:1.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:40.21	Best:24.33
2024-12-27 21:15:06,947: Snapshot:0	Epoch:43	Loss:1.231	translation_Loss:1.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:40.21	Best:24.36
2024-12-27 21:15:13,619: Snapshot:0	Epoch:44	Loss:1.176	translation_Loss:1.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.44	Hits@10:40.21	Best:24.44
2024-12-27 21:15:20,227: Snapshot:0	Epoch:45	Loss:1.126	translation_Loss:1.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:40.04	Best:24.44
2024-12-27 21:15:26,863: Snapshot:0	Epoch:46	Loss:1.076	translation_Loss:1.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.47	Hits@10:40.21	Best:24.47
2024-12-27 21:15:33,487: Snapshot:0	Epoch:47	Loss:1.026	translation_Loss:1.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:39.97	Best:24.48
2024-12-27 21:15:40,176: Snapshot:0	Epoch:48	Loss:0.997	translation_Loss:0.997	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.44	Hits@10:40.12	Best:24.48
2024-12-27 21:15:47,294: Snapshot:0	Epoch:49	Loss:0.96	translation_Loss:0.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:40.14	Best:24.48
2024-12-27 21:15:53,897: Snapshot:0	Epoch:50	Loss:0.93	translation_Loss:0.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.58	Hits@10:40.15	Best:24.58
2024-12-27 21:16:00,525: Snapshot:0	Epoch:51	Loss:0.891	translation_Loss:0.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.49	Hits@10:40.11	Best:24.58
2024-12-27 21:16:07,111: Snapshot:0	Epoch:52	Loss:0.873	translation_Loss:0.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:40.12	Best:24.58
2024-12-27 21:16:13,717: Snapshot:0	Epoch:53	Loss:0.834	translation_Loss:0.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.62	Hits@10:40.19	Best:24.62
2024-12-27 21:16:20,307: Snapshot:0	Epoch:54	Loss:0.814	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:40.14	Best:24.62
2024-12-27 21:16:26,884: Snapshot:0	Epoch:55	Loss:0.777	translation_Loss:0.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:40.07	Best:24.62
2024-12-27 21:16:33,462: Snapshot:0	Epoch:56	Loss:0.781	translation_Loss:0.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.6	Hits@10:40.17	Best:24.62
2024-12-27 21:16:40,076: Snapshot:0	Epoch:57	Loss:0.766	translation_Loss:0.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:40.17	Best:24.62
2024-12-27 21:16:46,689: Early Stopping! Snapshot: 0 Epoch: 58 Best Results: 24.62
2024-12-27 21:16:46,690: Start to training tokens! Snapshot: 0 Epoch: 58 Loss:0.76 MRR:24.53 Best Results: 24.62
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:16:46,690: Snapshot:0	Epoch:58	Loss:0.76	translation_Loss:0.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:40.21	Best:24.62
2024-12-27 21:16:53,941: Snapshot:0	Epoch:59	Loss:73.307	translation_Loss:72.058	multi_layer_Loss:1.248	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:40.21	Best:24.62
2024-12-27 21:17:00,718: End of token training: 0 Epoch: 60 Loss:72.109 MRR:24.53 Best Results: 24.62
2024-12-27 21:17:00,719: Snapshot:0	Epoch:60	Loss:72.109	translation_Loss:72.076	multi_layer_Loss:0.033	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.53	Hits@10:40.21	Best:24.62
2024-12-27 21:17:01,045: => loading checkpoint './checkpoint/FACTfact_0.0001_512_1000/0model_best.tar'
2024-12-27 21:17:04,178: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.239 | 0.154  | 0.2809 | 0.3269 |  0.3913 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:17:30,045: Snapshot:1	Epoch:0	Loss:44.449	translation_Loss:44.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.271                                                   	MRR:18.76	Hits@10:31.91	Best:18.76
2024-12-27 21:17:37,577: Snapshot:1	Epoch:1	Loss:34.611	translation_Loss:33.813	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.798                                                   	MRR:19.84	Hits@10:33.56	Best:19.84
2024-12-27 21:17:45,194: Snapshot:1	Epoch:2	Loss:27.299	translation_Loss:25.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.432                                                   	MRR:20.7	Hits@10:34.77	Best:20.7
2024-12-27 21:17:52,746: Snapshot:1	Epoch:3	Loss:21.788	translation_Loss:19.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.094                                                   	MRR:21.36	Hits@10:35.83	Best:21.36
2024-12-27 21:18:00,254: Snapshot:1	Epoch:4	Loss:17.588	translation_Loss:14.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.696                                                   	MRR:21.9	Hits@10:36.51	Best:21.9
2024-12-27 21:18:07,865: Snapshot:1	Epoch:5	Loss:14.49	translation_Loss:11.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.204                                                   	MRR:22.31	Hits@10:37.13	Best:22.31
2024-12-27 21:18:15,372: Snapshot:1	Epoch:6	Loss:12.339	translation_Loss:8.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.607                                                   	MRR:22.68	Hits@10:37.73	Best:22.68
2024-12-27 21:18:22,882: Snapshot:1	Epoch:7	Loss:10.896	translation_Loss:6.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.911                                                   	MRR:22.92	Hits@10:37.88	Best:22.92
2024-12-27 21:18:30,365: Snapshot:1	Epoch:8	Loss:9.972	translation_Loss:5.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.138                                                   	MRR:23.05	Hits@10:38.18	Best:23.05
2024-12-27 21:18:37,982: Snapshot:1	Epoch:9	Loss:9.341	translation_Loss:5.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.305                                                   	MRR:23.09	Hits@10:38.37	Best:23.09
2024-12-27 21:18:45,490: Snapshot:1	Epoch:10	Loss:8.892	translation_Loss:4.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.427                                                   	MRR:23.17	Hits@10:38.34	Best:23.17
2024-12-27 21:18:53,045: Snapshot:1	Epoch:11	Loss:8.627	translation_Loss:4.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.519                                                   	MRR:23.21	Hits@10:38.6	Best:23.21
2024-12-27 21:19:00,580: Snapshot:1	Epoch:12	Loss:8.454	translation_Loss:3.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.6                                                   	MRR:23.31	Hits@10:38.54	Best:23.31
2024-12-27 21:19:08,055: Snapshot:1	Epoch:13	Loss:8.286	translation_Loss:3.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.665                                                   	MRR:23.31	Hits@10:38.6	Best:23.31
2024-12-27 21:19:15,564: Snapshot:1	Epoch:14	Loss:8.118	translation_Loss:3.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.709                                                   	MRR:23.4	Hits@10:38.61	Best:23.4
2024-12-27 21:19:23,050: Snapshot:1	Epoch:15	Loss:8.016	translation_Loss:3.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.748                                                   	MRR:23.35	Hits@10:38.74	Best:23.4
2024-12-27 21:19:30,554: Snapshot:1	Epoch:16	Loss:7.946	translation_Loss:3.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.786                                                   	MRR:23.39	Hits@10:38.74	Best:23.4
2024-12-27 21:19:38,478: Snapshot:1	Epoch:17	Loss:7.887	translation_Loss:3.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.817                                                   	MRR:23.34	Hits@10:38.73	Best:23.4
2024-12-27 21:19:46,054: Snapshot:1	Epoch:18	Loss:7.832	translation_Loss:3.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.828                                                   	MRR:23.33	Hits@10:38.77	Best:23.4
2024-12-27 21:19:53,584: Early Stopping! Snapshot: 1 Epoch: 19 Best Results: 23.4
2024-12-27 21:19:53,585: Start to training tokens! Snapshot: 1 Epoch: 19 Loss:7.76 MRR:23.32 Best Results: 23.4
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:19:53,585: Snapshot:1	Epoch:19	Loss:7.76	translation_Loss:2.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.856                                                   	MRR:23.32	Hits@10:38.68	Best:23.4
2024-12-27 21:20:00,859: Snapshot:1	Epoch:20	Loss:79.272	translation_Loss:78.07	multi_layer_Loss:1.203	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.32	Hits@10:38.68	Best:23.4
2024-12-27 21:20:08,231: End of token training: 1 Epoch: 21 Loss:78.197 MRR:23.32 Best Results: 23.4
2024-12-27 21:20:08,231: Snapshot:1	Epoch:21	Loss:78.197	translation_Loss:78.171	multi_layer_Loss:0.026	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.32	Hits@10:38.68	Best:23.4
2024-12-27 21:20:08,502: => loading checkpoint './checkpoint/FACTfact_0.0001_512_1000/1model_best.tar'
2024-12-27 21:20:14,611: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2501 |  0.16  | 0.2923 | 0.348  |  0.4207 |
|     1      | 0.2348 | 0.151  | 0.2759 | 0.325  |  0.3874 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:20:40,277: Snapshot:2	Epoch:0	Loss:25.289	translation_Loss:25.027	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:21.18	Hits@10:36.78	Best:21.18
2024-12-27 21:20:48,123: Snapshot:2	Epoch:1	Loss:17.257	translation_Loss:16.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.682                                                   	MRR:21.74	Hits@10:37.84	Best:21.74
2024-12-27 21:20:55,862: Snapshot:2	Epoch:2	Loss:12.446	translation_Loss:11.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.076                                                   	MRR:22.07	Hits@10:38.24	Best:22.07
2024-12-27 21:21:03,565: Snapshot:2	Epoch:3	Loss:9.309	translation_Loss:7.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.417                                                   	MRR:22.44	Hits@10:38.67	Best:22.44
2024-12-27 21:21:11,256: Snapshot:2	Epoch:4	Loss:7.335	translation_Loss:5.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.691                                                   	MRR:22.51	Hits@10:38.75	Best:22.51
2024-12-27 21:21:18,924: Snapshot:2	Epoch:5	Loss:6.164	translation_Loss:4.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.898                                                   	MRR:22.51	Hits@10:38.77	Best:22.51
2024-12-27 21:21:26,654: Snapshot:2	Epoch:6	Loss:5.455	translation_Loss:3.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.047                                                   	MRR:22.63	Hits@10:38.92	Best:22.63
2024-12-27 21:21:34,824: Snapshot:2	Epoch:7	Loss:5.016	translation_Loss:2.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.158                                                   	MRR:22.5	Hits@10:39.08	Best:22.63
2024-12-27 21:21:42,520: Snapshot:2	Epoch:8	Loss:4.752	translation_Loss:2.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.237                                                   	MRR:22.51	Hits@10:38.87	Best:22.63
2024-12-27 21:21:50,162: Snapshot:2	Epoch:9	Loss:4.582	translation_Loss:2.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.295                                                   	MRR:22.53	Hits@10:38.93	Best:22.63
2024-12-27 21:21:57,805: Snapshot:2	Epoch:10	Loss:4.519	translation_Loss:2.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.345                                                   	MRR:22.46	Hits@10:38.9	Best:22.63
2024-12-27 21:22:05,493: Early Stopping! Snapshot: 2 Epoch: 11 Best Results: 22.63
2024-12-27 21:22:05,493: Start to training tokens! Snapshot: 2 Epoch: 11 Loss:4.434 MRR:22.57 Best Results: 22.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:22:05,493: Snapshot:2	Epoch:11	Loss:4.434	translation_Loss:2.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.393                                                   	MRR:22.57	Hits@10:38.82	Best:22.63
2024-12-27 21:22:12,887: Snapshot:2	Epoch:12	Loss:78.778	translation_Loss:77.522	multi_layer_Loss:1.257	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.57	Hits@10:38.82	Best:22.63
2024-12-27 21:22:20,337: End of token training: 2 Epoch: 13 Loss:77.519 MRR:22.57 Best Results: 22.63
2024-12-27 21:22:20,337: Snapshot:2	Epoch:13	Loss:77.519	translation_Loss:77.485	multi_layer_Loss:0.033	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.57	Hits@10:38.82	Best:22.63
2024-12-27 21:22:20,595: => loading checkpoint './checkpoint/FACTfact_0.0001_512_1000/2model_best.tar'
2024-12-27 21:22:29,936: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2369 | 0.1486 | 0.2754 | 0.3312 |  0.4042 |
|     1      | 0.2302 | 0.1452 | 0.2668 | 0.3206 |  0.3935 |
|     2      | 0.2275 | 0.141  | 0.2644 | 0.3194 |  0.3915 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:22:56,357: Snapshot:3	Epoch:0	Loss:12.484	translation_Loss:12.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:19.63	Hits@10:37.48	Best:19.63
2024-12-27 21:23:04,211: Snapshot:3	Epoch:1	Loss:7.353	translation_Loss:6.823	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.53                                                   	MRR:20.05	Hits@10:38.26	Best:20.05
2024-12-27 21:23:12,005: Snapshot:3	Epoch:2	Loss:5.029	translation_Loss:4.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.72                                                   	MRR:20.39	Hits@10:38.87	Best:20.39
2024-12-27 21:23:19,847: Snapshot:3	Epoch:3	Loss:3.768	translation_Loss:2.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.859                                                   	MRR:20.48	Hits@10:38.94	Best:20.48
2024-12-27 21:23:27,634: Snapshot:3	Epoch:4	Loss:3.041	translation_Loss:2.088	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.953                                                   	MRR:20.68	Hits@10:39.11	Best:20.68
2024-12-27 21:23:35,362: Snapshot:3	Epoch:5	Loss:2.693	translation_Loss:1.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.019                                                   	MRR:20.64	Hits@10:39.34	Best:20.68
2024-12-27 21:23:43,143: Snapshot:3	Epoch:6	Loss:2.482	translation_Loss:1.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.068                                                   	MRR:20.62	Hits@10:39.13	Best:20.68
2024-12-27 21:23:50,908: Snapshot:3	Epoch:7	Loss:2.364	translation_Loss:1.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.111                                                   	MRR:20.55	Hits@10:39.28	Best:20.68
2024-12-27 21:23:58,620: Snapshot:3	Epoch:8	Loss:2.293	translation_Loss:1.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.141                                                   	MRR:20.56	Hits@10:39.24	Best:20.68
2024-12-27 21:24:06,340: Early Stopping! Snapshot: 3 Epoch: 9 Best Results: 20.68
2024-12-27 21:24:06,340: Start to training tokens! Snapshot: 3 Epoch: 9 Loss:2.27 MRR:20.61 Best Results: 20.68
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:24:06,341: Snapshot:3	Epoch:9	Loss:2.27	translation_Loss:1.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.169                                                   	MRR:20.61	Hits@10:39.29	Best:20.68
2024-12-27 21:24:13,798: Snapshot:3	Epoch:10	Loss:75.941	translation_Loss:74.743	multi_layer_Loss:1.198	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.61	Hits@10:39.29	Best:20.68
2024-12-27 21:24:21,243: End of token training: 3 Epoch: 11 Loss:74.569 MRR:20.61 Best Results: 20.68
2024-12-27 21:24:21,244: Snapshot:3	Epoch:11	Loss:74.569	translation_Loss:74.543	multi_layer_Loss:0.027	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.61	Hits@10:39.29	Best:20.68
2024-12-27 21:24:21,559: => loading checkpoint './checkpoint/FACTfact_0.0001_512_1000/3model_best.tar'
2024-12-27 21:24:34,004: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2182 | 0.135  | 0.2512 | 0.3018 |  0.3773 |
|     1      | 0.2126 | 0.132  | 0.2417 | 0.295  |  0.3708 |
|     2      | 0.212  | 0.1251 | 0.2441 | 0.3033 |  0.3837 |
|     3      | 0.2055 | 0.1123 | 0.2382 | 0.3035 |  0.3927 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:25:00,372: Snapshot:4	Epoch:0	Loss:7.619	translation_Loss:7.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:20.38	Hits@10:44.73	Best:20.38
2024-12-27 21:25:08,313: Snapshot:4	Epoch:1	Loss:4.539	translation_Loss:4.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.374                                                   	MRR:21.34	Hits@10:46.01	Best:21.34
2024-12-27 21:25:16,163: Snapshot:4	Epoch:2	Loss:2.888	translation_Loss:2.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.44                                                   	MRR:21.98	Hits@10:46.96	Best:21.98
2024-12-27 21:25:24,024: Snapshot:4	Epoch:3	Loss:1.897	translation_Loss:1.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.454                                                   	MRR:22.23	Hits@10:47.41	Best:22.23
2024-12-27 21:25:31,866: Snapshot:4	Epoch:4	Loss:1.392	translation_Loss:0.952	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.44                                                   	MRR:22.54	Hits@10:47.42	Best:22.54
2024-12-27 21:25:39,772: Snapshot:4	Epoch:5	Loss:1.116	translation_Loss:0.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.423                                                   	MRR:22.57	Hits@10:47.41	Best:22.57
2024-12-27 21:25:47,728: Snapshot:4	Epoch:6	Loss:0.945	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.413                                                   	MRR:22.65	Hits@10:47.6	Best:22.65
2024-12-27 21:25:55,560: Snapshot:4	Epoch:7	Loss:0.831	translation_Loss:0.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.403                                                   	MRR:22.55	Hits@10:47.42	Best:22.65
2024-12-27 21:26:03,355: Snapshot:4	Epoch:8	Loss:0.805	translation_Loss:0.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:22.65	Hits@10:47.09	Best:22.65
2024-12-27 21:26:11,136: Snapshot:4	Epoch:9	Loss:0.744	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:22.62	Hits@10:47.33	Best:22.65
2024-12-27 21:26:18,982: Snapshot:4	Epoch:10	Loss:0.729	translation_Loss:0.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.402                                                   	MRR:22.62	Hits@10:47.91	Best:22.65
2024-12-27 21:26:26,879: Snapshot:4	Epoch:11	Loss:0.709	translation_Loss:0.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.404                                                   	MRR:22.7	Hits@10:47.72	Best:22.7
2024-12-27 21:26:34,739: Snapshot:4	Epoch:12	Loss:0.705	translation_Loss:0.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.408                                                   	MRR:22.94	Hits@10:47.63	Best:22.94
2024-12-27 21:26:43,099: Snapshot:4	Epoch:13	Loss:0.702	translation_Loss:0.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.412                                                   	MRR:22.93	Hits@10:47.65	Best:22.94
2024-12-27 21:26:50,946: Snapshot:4	Epoch:14	Loss:0.697	translation_Loss:0.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.42                                                   	MRR:22.93	Hits@10:47.98	Best:22.94
2024-12-27 21:26:58,753: Snapshot:4	Epoch:15	Loss:0.703	translation_Loss:0.281	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.422                                                   	MRR:22.82	Hits@10:47.58	Best:22.94
2024-12-27 21:27:06,549: Snapshot:4	Epoch:16	Loss:0.697	translation_Loss:0.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.425                                                   	MRR:22.89	Hits@10:47.84	Best:22.94
2024-12-27 21:27:14,329: Early Stopping! Snapshot: 4 Epoch: 17 Best Results: 22.94
2024-12-27 21:27:14,330: Start to training tokens! Snapshot: 4 Epoch: 17 Loss:0.7 MRR:22.81 Best Results: 22.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:27:14,330: Snapshot:4	Epoch:17	Loss:0.7	translation_Loss:0.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.426                                                   	MRR:22.81	Hits@10:47.71	Best:22.94
2024-12-27 21:27:21,860: Snapshot:4	Epoch:18	Loss:63.884	translation_Loss:62.688	multi_layer_Loss:1.196	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.81	Hits@10:47.71	Best:22.94
2024-12-27 21:27:29,395: End of token training: 4 Epoch: 19 Loss:62.764 MRR:22.81 Best Results: 22.94
2024-12-27 21:27:29,395: Snapshot:4	Epoch:19	Loss:62.764	translation_Loss:62.744	multi_layer_Loss:0.02	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.81	Hits@10:47.71	Best:22.94
2024-12-27 21:27:29,707: => loading checkpoint './checkpoint/FACTfact_0.0001_512_1000/4model_best.tar'
2024-12-27 21:27:46,074: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.198  | 0.1187 | 0.2264 | 0.2791 |  0.3494 |
|     1      | 0.1892 | 0.1114 | 0.216  | 0.2671 |  0.3397 |
|     2      | 0.185  | 0.1029 | 0.2117 | 0.2682 |  0.3466 |
|     3      | 0.1785 | 0.0894 | 0.2031 | 0.2671 |  0.3628 |
|     4      | 0.2297 | 0.1115 | 0.2665 | 0.3567 |  0.4747 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 21:27:46,077: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.239 | 0.154  | 0.2809 | 0.3269 |  0.3913 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2501 |  0.16  | 0.2923 | 0.348  |  0.4207 |
|     1      | 0.2348 | 0.151  | 0.2759 | 0.325  |  0.3874 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2369 | 0.1486 | 0.2754 | 0.3312 |  0.4042 |
|     1      | 0.2302 | 0.1452 | 0.2668 | 0.3206 |  0.3935 |
|     2      | 0.2275 | 0.141  | 0.2644 | 0.3194 |  0.3915 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2182 | 0.135  | 0.2512 | 0.3018 |  0.3773 |
|     1      | 0.2126 | 0.132  | 0.2417 | 0.295  |  0.3708 |
|     2      | 0.212  | 0.1251 | 0.2441 | 0.3033 |  0.3837 |
|     3      | 0.2055 | 0.1123 | 0.2382 | 0.3035 |  0.3927 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.198  | 0.1187 | 0.2264 | 0.2791 |  0.3494 |
|     1      | 0.1892 | 0.1114 | 0.216  | 0.2671 |  0.3397 |
|     2      | 0.185  | 0.1029 | 0.2117 | 0.2682 |  0.3466 |
|     3      | 0.1785 | 0.0894 | 0.2031 | 0.2671 |  0.3628 |
|     4      | 0.2297 | 0.1115 | 0.2665 | 0.3567 |  0.4747 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 21:27:46,077: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 409.8867349624634  |   0.239   |    0.154     |    0.281     |     0.391     |
|    1     | 180.95697784423828 |   0.242   |    0.155     |    0.284     |     0.404     |
|    2     | 122.40368294715881 |   0.232   |    0.145     |    0.269     |     0.396     |
|    3     | 107.95030283927917 |   0.212   |    0.126     |    0.244     |     0.381     |
|    4     | 171.63248085975647 |   0.196   |    0.107     |    0.225     |     0.375     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 21:27:46,077: Sum_Training_Time:992.8301794528961
2024-12-27 21:27:46,077: Every_Training_Time:[409.8867349624634, 180.95697784423828, 122.40368294715881, 107.95030283927917, 171.63248085975647]
2024-12-27 21:27:46,077: Forward transfer: 0.17705 Backward transfer: -0.039025
2024-12-27 21:28:25,174: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227212749/FACTfact_0.0001_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 21:28:35,340: Snapshot:0	Epoch:0	Loss:102.979	translation_Loss:102.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.78	Hits@10:4.76	Best:2.78
2024-12-27 21:28:41,971: Snapshot:0	Epoch:1	Loss:93.519	translation_Loss:93.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:8.53	Best:4.54
2024-12-27 21:28:48,647: Snapshot:0	Epoch:2	Loss:84.912	translation_Loss:84.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.71	Hits@10:11.57	Best:5.71
2024-12-27 21:28:55,278: Snapshot:0	Epoch:3	Loss:77.12	translation_Loss:77.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.9	Hits@10:14.86	Best:6.9
2024-12-27 21:29:01,889: Snapshot:0	Epoch:4	Loss:69.774	translation_Loss:69.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.2	Hits@10:18.03	Best:8.2
2024-12-27 21:29:08,543: Snapshot:0	Epoch:5	Loss:62.903	translation_Loss:62.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.61	Hits@10:20.98	Best:9.61
2024-12-27 21:29:15,212: Snapshot:0	Epoch:6	Loss:56.536	translation_Loss:56.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.95	Hits@10:23.74	Best:10.95
2024-12-27 21:29:21,864: Snapshot:0	Epoch:7	Loss:50.671	translation_Loss:50.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.08	Hits@10:26.04	Best:12.08
2024-12-27 21:29:29,034: Snapshot:0	Epoch:8	Loss:45.219	translation_Loss:45.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.09	Hits@10:28.08	Best:13.09
2024-12-27 21:29:35,719: Snapshot:0	Epoch:9	Loss:40.27	translation_Loss:40.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.04	Hits@10:29.86	Best:14.04
2024-12-27 21:29:42,398: Snapshot:0	Epoch:10	Loss:35.787	translation_Loss:35.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.9	Hits@10:31.32	Best:14.9
2024-12-27 21:29:49,082: Snapshot:0	Epoch:11	Loss:31.682	translation_Loss:31.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.7	Hits@10:32.67	Best:15.7
2024-12-27 21:29:55,728: Snapshot:0	Epoch:12	Loss:28.087	translation_Loss:28.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.47	Hits@10:33.72	Best:16.47
2024-12-27 21:30:02,418: Snapshot:0	Epoch:13	Loss:24.798	translation_Loss:24.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.24	Hits@10:34.64	Best:17.24
2024-12-27 21:30:09,184: Snapshot:0	Epoch:14	Loss:21.947	translation_Loss:21.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.96	Hits@10:35.45	Best:17.96
2024-12-27 21:30:15,868: Snapshot:0	Epoch:15	Loss:19.375	translation_Loss:19.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.65	Hits@10:36.2	Best:18.65
2024-12-27 21:30:22,513: Snapshot:0	Epoch:16	Loss:17.062	translation_Loss:17.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.31	Hits@10:36.71	Best:19.31
2024-12-27 21:30:29,174: Snapshot:0	Epoch:17	Loss:15.008	translation_Loss:15.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.88	Hits@10:37.22	Best:19.88
2024-12-27 21:30:35,877: Snapshot:0	Epoch:18	Loss:13.193	translation_Loss:13.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.48	Hits@10:37.74	Best:20.48
2024-12-27 21:30:42,504: Snapshot:0	Epoch:19	Loss:11.562	translation_Loss:11.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.89	Hits@10:38.08	Best:20.89
2024-12-27 21:30:49,166: Snapshot:0	Epoch:20	Loss:10.134	translation_Loss:10.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.32	Hits@10:38.34	Best:21.32
2024-12-27 21:30:55,834: Snapshot:0	Epoch:21	Loss:8.91	translation_Loss:8.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.68	Hits@10:38.55	Best:21.68
2024-12-27 21:31:02,467: Snapshot:0	Epoch:22	Loss:7.844	translation_Loss:7.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.0	Hits@10:38.73	Best:22.0
2024-12-27 21:31:09,133: Snapshot:0	Epoch:23	Loss:6.874	translation_Loss:6.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.37	Hits@10:38.99	Best:22.37
2024-12-27 21:31:15,843: Snapshot:0	Epoch:24	Loss:6.068	translation_Loss:6.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.68	Hits@10:39.15	Best:22.68
2024-12-27 21:31:22,505: Snapshot:0	Epoch:25	Loss:5.41	translation_Loss:5.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.91	Hits@10:39.25	Best:22.91
2024-12-27 21:31:29,181: Snapshot:0	Epoch:26	Loss:4.764	translation_Loss:4.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.15	Hits@10:39.33	Best:23.15
2024-12-27 21:31:35,847: Snapshot:0	Epoch:27	Loss:4.278	translation_Loss:4.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.32	Hits@10:39.49	Best:23.32
2024-12-27 21:31:42,498: Snapshot:0	Epoch:28	Loss:3.816	translation_Loss:3.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.53	Hits@10:39.67	Best:23.53
2024-12-27 21:31:49,664: Snapshot:0	Epoch:29	Loss:3.413	translation_Loss:3.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.78	Best:23.63
2024-12-27 21:31:56,327: Snapshot:0	Epoch:30	Loss:3.089	translation_Loss:3.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.72	Hits@10:39.78	Best:23.72
2024-12-27 21:32:03,037: Snapshot:0	Epoch:31	Loss:2.836	translation_Loss:2.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.85	Hits@10:39.88	Best:23.85
2024-12-27 21:32:09,702: Snapshot:0	Epoch:32	Loss:2.575	translation_Loss:2.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:39.92	Best:23.96
2024-12-27 21:32:16,354: Snapshot:0	Epoch:33	Loss:2.34	translation_Loss:2.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:39.99	Best:24.06
2024-12-27 21:32:22,966: Snapshot:0	Epoch:34	Loss:2.142	translation_Loss:2.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:39.92	Best:24.06
2024-12-27 21:32:29,584: Snapshot:0	Epoch:35	Loss:2.007	translation_Loss:2.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:39.93	Best:24.1
2024-12-27 21:32:36,233: Snapshot:0	Epoch:36	Loss:1.837	translation_Loss:1.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.88	Best:24.1
2024-12-27 21:32:42,894: Snapshot:0	Epoch:37	Loss:1.736	translation_Loss:1.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:40.05	Best:24.13
2024-12-27 21:32:49,626: Snapshot:0	Epoch:38	Loss:1.595	translation_Loss:1.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:39.95	Best:24.21
2024-12-27 21:32:56,271: Snapshot:0	Epoch:39	Loss:1.518	translation_Loss:1.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:40.06	Best:24.21
2024-12-27 21:33:02,977: Snapshot:0	Epoch:40	Loss:1.439	translation_Loss:1.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:40.04	Best:24.22
2024-12-27 21:33:09,625: Snapshot:0	Epoch:41	Loss:1.362	translation_Loss:1.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:40.16	Best:24.23
2024-12-27 21:33:16,307: Snapshot:0	Epoch:42	Loss:1.28	translation_Loss:1.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.35	Hits@10:40.27	Best:24.35
2024-12-27 21:33:22,942: Snapshot:0	Epoch:43	Loss:1.234	translation_Loss:1.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:40.36	Best:24.36
2024-12-27 21:33:29,584: Snapshot:0	Epoch:44	Loss:1.177	translation_Loss:1.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:40.25	Best:24.41
2024-12-27 21:33:36,223: Snapshot:0	Epoch:45	Loss:1.127	translation_Loss:1.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.41	Hits@10:40.29	Best:24.41
2024-12-27 21:33:42,921: Snapshot:0	Epoch:46	Loss:1.073	translation_Loss:1.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:40.15	Best:24.42
2024-12-27 21:33:49,544: Snapshot:0	Epoch:47	Loss:1.03	translation_Loss:1.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:40.18	Best:24.42
2024-12-27 21:33:56,166: Snapshot:0	Epoch:48	Loss:0.996	translation_Loss:0.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:40.07	Best:24.42
2024-12-27 21:34:03,278: Snapshot:0	Epoch:49	Loss:0.957	translation_Loss:0.957	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:40.07	Best:24.42
2024-12-27 21:34:09,914: Snapshot:0	Epoch:50	Loss:0.93	translation_Loss:0.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:40.11	Best:24.5
2024-12-27 21:34:16,660: Snapshot:0	Epoch:51	Loss:0.892	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.54	Hits@10:40.13	Best:24.54
2024-12-27 21:34:23,310: Snapshot:0	Epoch:52	Loss:0.87	translation_Loss:0.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:40.08	Best:24.54
2024-12-27 21:34:29,941: Snapshot:0	Epoch:53	Loss:0.83	translation_Loss:0.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.62	Hits@10:40.2	Best:24.62
2024-12-27 21:34:36,579: Snapshot:0	Epoch:54	Loss:0.812	translation_Loss:0.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.55	Hits@10:40.1	Best:24.62
2024-12-27 21:34:43,252: Snapshot:0	Epoch:55	Loss:0.779	translation_Loss:0.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.63	Hits@10:40.11	Best:24.63
2024-12-27 21:34:49,916: Snapshot:0	Epoch:56	Loss:0.783	translation_Loss:0.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:40.14	Best:24.63
2024-12-27 21:34:56,564: Snapshot:0	Epoch:57	Loss:0.769	translation_Loss:0.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.55	Hits@10:40.22	Best:24.63
2024-12-27 21:35:03,194: Snapshot:0	Epoch:58	Loss:0.753	translation_Loss:0.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.52	Hits@10:40.1	Best:24.63
2024-12-27 21:35:09,859: Snapshot:0	Epoch:59	Loss:0.711	translation_Loss:0.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:40.17	Best:24.63
2024-12-27 21:35:16,529: Early Stopping! Snapshot: 0 Epoch: 60 Best Results: 24.63
2024-12-27 21:35:16,530: Start to training tokens! Snapshot: 0 Epoch: 60 Loss:0.697 MRR:24.53 Best Results: 24.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:35:16,530: Snapshot:0	Epoch:60	Loss:0.697	translation_Loss:0.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:40.16	Best:24.63
2024-12-27 21:35:23,867: Snapshot:0	Epoch:61	Loss:73.235	translation_Loss:71.987	multi_layer_Loss:1.248	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.53	Hits@10:40.16	Best:24.63
2024-12-27 21:35:30,661: End of token training: 0 Epoch: 62 Loss:72.104 MRR:24.53 Best Results: 24.63
2024-12-27 21:35:30,661: Snapshot:0	Epoch:62	Loss:72.104	translation_Loss:72.071	multi_layer_Loss:0.033	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.53	Hits@10:40.16	Best:24.63
2024-12-27 21:35:30,912: => loading checkpoint './checkpoint/FACTfact_0.0001_512_5000/0model_best.tar'
2024-12-27 21:35:33,610: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2384 | 0.1533 | 0.2802 | 0.327  |  0.3918 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:35:59,465: Snapshot:1	Epoch:0	Loss:44.717	translation_Loss:44.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.703                                                   	MRR:18.59	Hits@10:31.63	Best:18.59
2024-12-27 21:36:07,006: Snapshot:1	Epoch:1	Loss:37.129	translation_Loss:35.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.084                                                   	MRR:19.47	Hits@10:32.92	Best:19.47
2024-12-27 21:36:14,606: Snapshot:1	Epoch:2	Loss:32.377	translation_Loss:28.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.588                                                   	MRR:20.04	Hits@10:33.98	Best:20.04
2024-12-27 21:36:22,171: Snapshot:1	Epoch:3	Loss:28.983	translation_Loss:24.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.84                                                   	MRR:20.67	Hits@10:34.75	Best:20.67
2024-12-27 21:36:29,792: Snapshot:1	Epoch:4	Loss:26.314	translation_Loss:20.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.799                                                   	MRR:21.04	Hits@10:35.25	Best:21.04
2024-12-27 21:36:37,430: Snapshot:1	Epoch:5	Loss:24.239	translation_Loss:17.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.496                                                   	MRR:21.37	Hits@10:35.7	Best:21.37
2024-12-27 21:36:45,058: Snapshot:1	Epoch:6	Loss:22.822	translation_Loss:15.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.995                                                   	MRR:21.64	Hits@10:36.06	Best:21.64
2024-12-27 21:36:53,053: Snapshot:1	Epoch:7	Loss:21.749	translation_Loss:14.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.357                                                   	MRR:21.81	Hits@10:36.38	Best:21.81
2024-12-27 21:37:00,592: Snapshot:1	Epoch:8	Loss:20.932	translation_Loss:13.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.606                                                   	MRR:21.91	Hits@10:36.63	Best:21.91
2024-12-27 21:37:08,226: Snapshot:1	Epoch:9	Loss:20.46	translation_Loss:12.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.764                                                   	MRR:22.05	Hits@10:36.74	Best:22.05
2024-12-27 21:37:15,816: Snapshot:1	Epoch:10	Loss:20.071	translation_Loss:12.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.903                                                   	MRR:22.1	Hits@10:36.95	Best:22.1
2024-12-27 21:37:23,370: Snapshot:1	Epoch:11	Loss:19.81	translation_Loss:11.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.995                                                   	MRR:22.15	Hits@10:36.9	Best:22.15
2024-12-27 21:37:31,002: Snapshot:1	Epoch:12	Loss:19.62	translation_Loss:11.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.057                                                   	MRR:22.21	Hits@10:37.05	Best:22.21
2024-12-27 21:37:38,601: Snapshot:1	Epoch:13	Loss:19.438	translation_Loss:11.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.111                                                   	MRR:22.24	Hits@10:37.05	Best:22.24
2024-12-27 21:37:46,235: Snapshot:1	Epoch:14	Loss:19.377	translation_Loss:11.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.153                                                   	MRR:22.24	Hits@10:37.12	Best:22.24
2024-12-27 21:37:53,831: Snapshot:1	Epoch:15	Loss:19.268	translation_Loss:11.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.187                                                   	MRR:22.26	Hits@10:37.03	Best:22.26
2024-12-27 21:38:01,403: Snapshot:1	Epoch:16	Loss:19.171	translation_Loss:10.957	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.215                                                   	MRR:22.29	Hits@10:37.07	Best:22.29
2024-12-27 21:38:09,035: Snapshot:1	Epoch:17	Loss:19.115	translation_Loss:10.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.239                                                   	MRR:22.31	Hits@10:37.08	Best:22.31
2024-12-27 21:38:16,570: Snapshot:1	Epoch:18	Loss:19.075	translation_Loss:10.805	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.271                                                   	MRR:22.3	Hits@10:37.23	Best:22.31
2024-12-27 21:38:24,113: Snapshot:1	Epoch:19	Loss:18.988	translation_Loss:10.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.273                                                   	MRR:22.36	Hits@10:37.23	Best:22.36
2024-12-27 21:38:31,642: Snapshot:1	Epoch:20	Loss:18.968	translation_Loss:10.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.284                                                   	MRR:22.38	Hits@10:37.17	Best:22.38
2024-12-27 21:38:39,178: Snapshot:1	Epoch:21	Loss:18.93	translation_Loss:10.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.29                                                   	MRR:22.36	Hits@10:37.29	Best:22.38
2024-12-27 21:38:46,738: Snapshot:1	Epoch:22	Loss:18.929	translation_Loss:10.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.31                                                   	MRR:22.36	Hits@10:37.23	Best:22.38
2024-12-27 21:38:54,310: Snapshot:1	Epoch:23	Loss:18.902	translation_Loss:10.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.316                                                   	MRR:22.39	Hits@10:37.24	Best:22.39
2024-12-27 21:39:01,879: Snapshot:1	Epoch:24	Loss:18.832	translation_Loss:10.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.329                                                   	MRR:22.42	Hits@10:37.28	Best:22.42
2024-12-27 21:39:09,429: Snapshot:1	Epoch:25	Loss:18.848	translation_Loss:10.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.331                                                   	MRR:22.37	Hits@10:37.26	Best:22.42
2024-12-27 21:39:16,984: Snapshot:1	Epoch:26	Loss:18.829	translation_Loss:10.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.338                                                   	MRR:22.44	Hits@10:37.19	Best:22.44
2024-12-27 21:39:24,985: Snapshot:1	Epoch:27	Loss:18.813	translation_Loss:10.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.33                                                   	MRR:22.36	Hits@10:37.12	Best:22.44
2024-12-27 21:39:32,531: Snapshot:1	Epoch:28	Loss:18.777	translation_Loss:10.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.341                                                   	MRR:22.35	Hits@10:37.22	Best:22.44
2024-12-27 21:39:40,089: Snapshot:1	Epoch:29	Loss:18.793	translation_Loss:10.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.351                                                   	MRR:22.38	Hits@10:37.34	Best:22.44
2024-12-27 21:39:47,706: Snapshot:1	Epoch:30	Loss:18.785	translation_Loss:10.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.348                                                   	MRR:22.37	Hits@10:37.16	Best:22.44
2024-12-27 21:39:55,247: Early Stopping! Snapshot: 1 Epoch: 31 Best Results: 22.44
2024-12-27 21:39:55,248: Start to training tokens! Snapshot: 1 Epoch: 31 Loss:18.746 MRR:22.4 Best Results: 22.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:39:55,248: Snapshot:1	Epoch:31	Loss:18.746	translation_Loss:10.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:8.346                                                   	MRR:22.4	Hits@10:37.2	Best:22.44
2024-12-27 21:40:02,550: Snapshot:1	Epoch:32	Loss:82.776	translation_Loss:81.573	multi_layer_Loss:1.203	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.4	Hits@10:37.2	Best:22.44
2024-12-27 21:40:09,982: End of token training: 1 Epoch: 33 Loss:81.652 MRR:22.4 Best Results: 22.44
2024-12-27 21:40:09,982: Snapshot:1	Epoch:33	Loss:81.652	translation_Loss:81.626	multi_layer_Loss:0.026	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.4	Hits@10:37.2	Best:22.44
2024-12-27 21:40:10,244: => loading checkpoint './checkpoint/FACTfact_0.0001_512_5000/1model_best.tar'
2024-12-27 21:40:16,148: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2527 | 0.1625 | 0.2942 | 0.3511 |  0.4185 |
|     1      | 0.2232 | 0.1404 | 0.263  | 0.3122 |  0.3757 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:40:42,714: Snapshot:2	Epoch:0	Loss:28.88	translation_Loss:28.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.669                                                   	MRR:20.37	Hits@10:35.39	Best:20.37
2024-12-27 21:40:50,540: Snapshot:2	Epoch:1	Loss:22.215	translation_Loss:20.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.828                                                   	MRR:21.07	Hits@10:36.55	Best:21.07
2024-12-27 21:40:58,348: Snapshot:2	Epoch:2	Loss:18.691	translation_Loss:15.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.016                                                   	MRR:21.41	Hits@10:37.25	Best:21.41
2024-12-27 21:41:06,108: Snapshot:2	Epoch:3	Loss:16.553	translation_Loss:12.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.951                                                   	MRR:21.67	Hits@10:37.65	Best:21.67
2024-12-27 21:41:13,876: Snapshot:2	Epoch:4	Loss:15.123	translation_Loss:10.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.625                                                   	MRR:21.8	Hits@10:37.9	Best:21.8
2024-12-27 21:41:21,702: Snapshot:2	Epoch:5	Loss:14.172	translation_Loss:9.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.105                                                   	MRR:22.08	Hits@10:38.05	Best:22.08
2024-12-27 21:41:29,448: Snapshot:2	Epoch:6	Loss:13.549	translation_Loss:8.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.439                                                   	MRR:22.03	Hits@10:38.23	Best:22.08
2024-12-27 21:41:37,223: Snapshot:2	Epoch:7	Loss:13.288	translation_Loss:7.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.673                                                   	MRR:22.15	Hits@10:38.32	Best:22.15
2024-12-27 21:41:45,081: Snapshot:2	Epoch:8	Loss:13.029	translation_Loss:7.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.85                                                   	MRR:22.26	Hits@10:38.29	Best:22.26
2024-12-27 21:41:52,811: Snapshot:2	Epoch:9	Loss:12.833	translation_Loss:6.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.968                                                   	MRR:22.23	Hits@10:38.43	Best:22.26
2024-12-27 21:42:00,509: Snapshot:2	Epoch:10	Loss:12.782	translation_Loss:6.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.051                                                   	MRR:22.18	Hits@10:38.37	Best:22.26
2024-12-27 21:42:08,318: Snapshot:2	Epoch:11	Loss:12.695	translation_Loss:6.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.121                                                   	MRR:22.29	Hits@10:38.58	Best:22.29
2024-12-27 21:42:16,053: Snapshot:2	Epoch:12	Loss:12.675	translation_Loss:6.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.184                                                   	MRR:22.24	Hits@10:38.44	Best:22.29
2024-12-27 21:42:23,752: Snapshot:2	Epoch:13	Loss:12.63	translation_Loss:6.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.242                                                   	MRR:22.18	Hits@10:38.49	Best:22.29
2024-12-27 21:42:31,441: Snapshot:2	Epoch:14	Loss:12.578	translation_Loss:6.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.269                                                   	MRR:22.28	Hits@10:38.46	Best:22.29
2024-12-27 21:42:39,187: Snapshot:2	Epoch:15	Loss:12.623	translation_Loss:6.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.322                                                   	MRR:22.27	Hits@10:38.49	Best:22.29
2024-12-27 21:42:46,905: Early Stopping! Snapshot: 2 Epoch: 16 Best Results: 22.29
2024-12-27 21:42:46,905: Start to training tokens! Snapshot: 2 Epoch: 16 Loss:12.548 MRR:22.28 Best Results: 22.29
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:42:46,905: Snapshot:2	Epoch:16	Loss:12.548	translation_Loss:6.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.342                                                   	MRR:22.28	Hits@10:38.54	Best:22.29
2024-12-27 21:42:54,355: Snapshot:2	Epoch:17	Loss:81.305	translation_Loss:80.048	multi_layer_Loss:1.257	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.28	Hits@10:38.54	Best:22.29
2024-12-27 21:43:01,804: End of token training: 2 Epoch: 18 Loss:80.144 MRR:22.28 Best Results: 22.29
2024-12-27 21:43:01,804: Snapshot:2	Epoch:18	Loss:80.144	translation_Loss:80.111	multi_layer_Loss:0.033	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.28	Hits@10:38.54	Best:22.29
2024-12-27 21:43:02,109: => loading checkpoint './checkpoint/FACTfact_0.0001_512_5000/2model_best.tar'
2024-12-27 21:43:10,908: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2496 | 0.1603 | 0.2889 | 0.3466 |  0.4179 |
|     1      | 0.231  | 0.146  | 0.2694 | 0.3214 |  0.3905 |
|     2      | 0.2215 | 0.1356 | 0.2585 | 0.3115 |  0.3847 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:43:37,171: Snapshot:3	Epoch:0	Loss:15.467	translation_Loss:14.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.608                                                   	MRR:19.59	Hits@10:37.17	Best:19.59
2024-12-27 21:43:45,156: Snapshot:3	Epoch:1	Loss:10.649	translation_Loss:9.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.382                                                   	MRR:20.04	Hits@10:38.27	Best:20.04
2024-12-27 21:43:53,142: Snapshot:3	Epoch:2	Loss:8.744	translation_Loss:6.75	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.994                                                   	MRR:20.23	Hits@10:38.61	Best:20.23
2024-12-27 21:44:00,993: Snapshot:3	Epoch:3	Loss:7.64	translation_Loss:5.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.431                                                   	MRR:20.46	Hits@10:38.86	Best:20.46
2024-12-27 21:44:08,881: Snapshot:3	Epoch:4	Loss:7.073	translation_Loss:4.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.714                                                   	MRR:20.5	Hits@10:38.91	Best:20.5
2024-12-27 21:44:16,821: Snapshot:3	Epoch:5	Loss:6.828	translation_Loss:3.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.915                                                   	MRR:20.54	Hits@10:38.97	Best:20.54
2024-12-27 21:44:24,660: Snapshot:3	Epoch:6	Loss:6.609	translation_Loss:3.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.044                                                   	MRR:20.56	Hits@10:39.0	Best:20.56
2024-12-27 21:44:32,520: Snapshot:3	Epoch:7	Loss:6.523	translation_Loss:3.39	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.133                                                   	MRR:20.66	Hits@10:39.04	Best:20.66
2024-12-27 21:44:40,415: Snapshot:3	Epoch:8	Loss:6.475	translation_Loss:3.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.181                                                   	MRR:20.54	Hits@10:38.88	Best:20.66
2024-12-27 21:44:48,315: Snapshot:3	Epoch:9	Loss:6.421	translation_Loss:3.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.24                                                   	MRR:20.74	Hits@10:39.07	Best:20.74
2024-12-27 21:44:56,254: Snapshot:3	Epoch:10	Loss:6.473	translation_Loss:3.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.281                                                   	MRR:20.63	Hits@10:39.01	Best:20.74
2024-12-27 21:45:04,063: Snapshot:3	Epoch:11	Loss:6.447	translation_Loss:3.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.318                                                   	MRR:20.59	Hits@10:39.02	Best:20.74
2024-12-27 21:45:11,881: Snapshot:3	Epoch:12	Loss:6.432	translation_Loss:3.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.339                                                   	MRR:20.55	Hits@10:38.9	Best:20.74
2024-12-27 21:45:20,119: Snapshot:3	Epoch:13	Loss:6.445	translation_Loss:3.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.361                                                   	MRR:20.59	Hits@10:39.02	Best:20.74
2024-12-27 21:45:27,946: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 20.74
2024-12-27 21:45:27,947: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:6.424 MRR:20.58 Best Results: 20.74
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:45:27,947: Snapshot:3	Epoch:14	Loss:6.424	translation_Loss:3.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.374                                                   	MRR:20.58	Hits@10:38.91	Best:20.74
2024-12-27 21:45:35,476: Snapshot:3	Epoch:15	Loss:77.948	translation_Loss:76.75	multi_layer_Loss:1.198	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.58	Hits@10:38.91	Best:20.74
2024-12-27 21:45:43,031: End of token training: 3 Epoch: 16 Loss:76.672 MRR:20.58 Best Results: 20.74
2024-12-27 21:45:43,032: Snapshot:3	Epoch:16	Loss:76.672	translation_Loss:76.646	multi_layer_Loss:0.027	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.58	Hits@10:38.91	Best:20.74
2024-12-27 21:45:43,333: => loading checkpoint './checkpoint/FACTfact_0.0001_512_5000/3model_best.tar'
2024-12-27 21:45:56,349: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2372 | 0.1509 | 0.2717 | 0.3268 |  0.4018 |
|     1      | 0.2204 | 0.1372 | 0.2531 | 0.3069 |  0.3816 |
|     2      | 0.2156 | 0.1275 | 0.2506 | 0.3102 |  0.3893 |
|     3      | 0.207  | 0.1134 | 0.2401 | 0.3079 |  0.3942 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:46:22,439: Snapshot:4	Epoch:0	Loss:8.917	translation_Loss:8.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.429                                                   	MRR:20.69	Hits@10:45.22	Best:20.69
2024-12-27 21:46:30,345: Snapshot:4	Epoch:1	Loss:5.669	translation_Loss:4.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.701                                                   	MRR:21.13	Hits@10:46.31	Best:21.13
2024-12-27 21:46:38,293: Snapshot:4	Epoch:2	Loss:4.121	translation_Loss:3.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.846                                                   	MRR:21.54	Hits@10:47.04	Best:21.54
2024-12-27 21:46:46,711: Snapshot:4	Epoch:3	Loss:3.054	translation_Loss:2.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.93                                                   	MRR:21.75	Hits@10:47.26	Best:21.75
2024-12-27 21:46:54,603: Snapshot:4	Epoch:4	Loss:2.499	translation_Loss:1.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.964                                                   	MRR:22.17	Hits@10:47.67	Best:22.17
2024-12-27 21:47:02,512: Snapshot:4	Epoch:5	Loss:2.198	translation_Loss:1.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.977                                                   	MRR:22.34	Hits@10:47.65	Best:22.34
2024-12-27 21:47:10,410: Snapshot:4	Epoch:6	Loss:2.035	translation_Loss:1.046	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.989                                                   	MRR:22.16	Hits@10:47.05	Best:22.34
2024-12-27 21:47:18,235: Snapshot:4	Epoch:7	Loss:1.974	translation_Loss:0.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.998                                                   	MRR:22.24	Hits@10:47.45	Best:22.34
2024-12-27 21:47:26,168: Snapshot:4	Epoch:8	Loss:1.913	translation_Loss:0.913	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.0                                                   	MRR:22.38	Hits@10:47.3	Best:22.38
2024-12-27 21:47:34,036: Snapshot:4	Epoch:9	Loss:1.897	translation_Loss:0.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.011                                                   	MRR:22.28	Hits@10:47.45	Best:22.38
2024-12-27 21:47:41,985: Snapshot:4	Epoch:10	Loss:1.861	translation_Loss:0.849	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.012                                                   	MRR:22.72	Hits@10:47.53	Best:22.72
2024-12-27 21:47:49,864: Snapshot:4	Epoch:11	Loss:1.832	translation_Loss:0.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.005                                                   	MRR:22.65	Hits@10:47.72	Best:22.72
2024-12-27 21:47:57,700: Snapshot:4	Epoch:12	Loss:1.836	translation_Loss:0.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.017                                                   	MRR:22.52	Hits@10:47.88	Best:22.72
2024-12-27 21:48:05,558: Snapshot:4	Epoch:13	Loss:1.811	translation_Loss:0.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.014                                                   	MRR:22.63	Hits@10:47.82	Best:22.72
2024-12-27 21:48:13,368: Snapshot:4	Epoch:14	Loss:1.837	translation_Loss:0.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.023                                                   	MRR:22.5	Hits@10:47.64	Best:22.72
2024-12-27 21:48:21,211: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 22.72
2024-12-27 21:48:21,212: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:1.848 MRR:22.58 Best Results: 22.72
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:48:21,212: Snapshot:4	Epoch:15	Loss:1.848	translation_Loss:0.818	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.029                                                   	MRR:22.58	Hits@10:47.78	Best:22.72
2024-12-27 21:48:28,787: Snapshot:4	Epoch:16	Loss:66.035	translation_Loss:64.839	multi_layer_Loss:1.196	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.58	Hits@10:47.78	Best:22.72
2024-12-27 21:48:36,416: End of token training: 4 Epoch: 17 Loss:64.831 MRR:22.58 Best Results: 22.72
2024-12-27 21:48:36,416: Snapshot:4	Epoch:17	Loss:64.831	translation_Loss:64.812	multi_layer_Loss:0.02	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.58	Hits@10:47.78	Best:22.72
2024-12-27 21:48:36,721: => loading checkpoint './checkpoint/FACTfact_0.0001_512_5000/4model_best.tar'
2024-12-27 21:48:52,851: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2239 | 0.1396 | 0.2569 | 0.3119 |  0.3851 |
|     1      | 0.2053 | 0.1235 | 0.2358 | 0.2892 |  0.3637 |
|     2      | 0.1999 | 0.1153 | 0.2292 | 0.2869 |  0.3694 |
|     3      | 0.1909 | 0.0979 | 0.2194 | 0.2879 |  0.3836 |
|     4      | 0.2255 | 0.1053 | 0.2666 | 0.356  |  0.4704 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 21:48:52,853: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2384 | 0.1533 | 0.2802 | 0.327  |  0.3918 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2527 | 0.1625 | 0.2942 | 0.3511 |  0.4185 |
|     1      | 0.2232 | 0.1404 | 0.263  | 0.3122 |  0.3757 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2496 | 0.1603 | 0.2889 | 0.3466 |  0.4179 |
|     1      | 0.231  | 0.146  | 0.2694 | 0.3214 |  0.3905 |
|     2      | 0.2215 | 0.1356 | 0.2585 | 0.3115 |  0.3847 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2372 | 0.1509 | 0.2717 | 0.3268 |  0.4018 |
|     1      | 0.2204 | 0.1372 | 0.2531 | 0.3069 |  0.3816 |
|     2      | 0.2156 | 0.1275 | 0.2506 | 0.3102 |  0.3893 |
|     3      | 0.207  | 0.1134 | 0.2401 | 0.3079 |  0.3942 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2239 | 0.1396 | 0.2569 | 0.3119 |  0.3851 |
|     1      | 0.2053 | 0.1235 | 0.2358 | 0.2892 |  0.3637 |
|     2      | 0.1999 | 0.1153 | 0.2292 | 0.2869 |  0.3694 |
|     3      | 0.1909 | 0.0979 | 0.2194 | 0.2879 |  0.3836 |
|     4      | 0.2255 | 0.1053 | 0.2666 | 0.356  |  0.4704 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 21:48:52,854: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 425.48718881607056 |   0.238   |    0.153     |     0.28     |     0.392     |
|    1     | 273.29465341567993 |   0.238   |    0.151     |    0.279     |     0.397     |
|    2     | 162.33315443992615 |   0.234   |    0.147     |    0.272     |     0.398     |
|    3     | 148.7925992012024  |    0.22   |    0.132     |    0.254     |     0.392     |
|    4     | 156.96240139007568 |   0.209   |    0.116     |    0.242     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 21:48:52,854: Sum_Training_Time:1166.8699972629547
2024-12-27 21:48:52,854: Every_Training_Time:[425.48718881607056, 273.29465341567993, 162.33315443992615, 148.7925992012024, 156.96240139007568]
2024-12-27 21:48:52,854: Forward transfer: 0.174575 Backward transfer: -0.017525000000000006
2024-12-27 21:49:32,110: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227214856/FACTfact_0.0001_512_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_512_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_512_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 21:49:42,312: Snapshot:0	Epoch:0	Loss:102.979	translation_Loss:102.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.78	Hits@10:4.76	Best:2.78
2024-12-27 21:49:48,946: Snapshot:0	Epoch:1	Loss:93.519	translation_Loss:93.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.54	Hits@10:8.53	Best:4.54
2024-12-27 21:49:55,608: Snapshot:0	Epoch:2	Loss:84.912	translation_Loss:84.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.71	Hits@10:11.57	Best:5.71
2024-12-27 21:50:02,229: Snapshot:0	Epoch:3	Loss:77.12	translation_Loss:77.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.9	Hits@10:14.85	Best:6.9
2024-12-27 21:50:08,965: Snapshot:0	Epoch:4	Loss:69.774	translation_Loss:69.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.2	Hits@10:18.03	Best:8.2
2024-12-27 21:50:15,684: Snapshot:0	Epoch:5	Loss:62.903	translation_Loss:62.903	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.61	Hits@10:21.0	Best:9.61
2024-12-27 21:50:22,285: Snapshot:0	Epoch:6	Loss:56.536	translation_Loss:56.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.94	Hits@10:23.74	Best:10.94
2024-12-27 21:50:28,944: Snapshot:0	Epoch:7	Loss:50.671	translation_Loss:50.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.08	Hits@10:26.04	Best:12.08
2024-12-27 21:50:36,165: Snapshot:0	Epoch:8	Loss:45.22	translation_Loss:45.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.08	Hits@10:28.13	Best:13.08
2024-12-27 21:50:42,788: Snapshot:0	Epoch:9	Loss:40.269	translation_Loss:40.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.03	Hits@10:29.88	Best:14.03
2024-12-27 21:50:49,515: Snapshot:0	Epoch:10	Loss:35.786	translation_Loss:35.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.89	Hits@10:31.4	Best:14.89
2024-12-27 21:50:56,162: Snapshot:0	Epoch:11	Loss:31.681	translation_Loss:31.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.72	Hits@10:32.64	Best:15.72
2024-12-27 21:51:02,826: Snapshot:0	Epoch:12	Loss:28.082	translation_Loss:28.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.49	Hits@10:33.69	Best:16.49
2024-12-27 21:51:09,475: Snapshot:0	Epoch:13	Loss:24.797	translation_Loss:24.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.25	Hits@10:34.68	Best:17.25
2024-12-27 21:51:16,145: Snapshot:0	Epoch:14	Loss:21.943	translation_Loss:21.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.95	Hits@10:35.46	Best:17.95
2024-12-27 21:51:22,784: Snapshot:0	Epoch:15	Loss:19.372	translation_Loss:19.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.68	Hits@10:36.26	Best:18.68
2024-12-27 21:51:29,423: Snapshot:0	Epoch:16	Loss:17.057	translation_Loss:17.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.31	Hits@10:36.73	Best:19.31
2024-12-27 21:51:36,064: Snapshot:0	Epoch:17	Loss:15.006	translation_Loss:15.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.9	Hits@10:37.31	Best:19.9
2024-12-27 21:51:42,715: Snapshot:0	Epoch:18	Loss:13.197	translation_Loss:13.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.43	Hits@10:37.75	Best:20.43
2024-12-27 21:51:49,396: Snapshot:0	Epoch:19	Loss:11.565	translation_Loss:11.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.91	Hits@10:38.09	Best:20.91
2024-12-27 21:51:56,080: Snapshot:0	Epoch:20	Loss:10.132	translation_Loss:10.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.31	Hits@10:38.34	Best:21.31
2024-12-27 21:52:02,735: Snapshot:0	Epoch:21	Loss:8.912	translation_Loss:8.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.67	Hits@10:38.62	Best:21.67
2024-12-27 21:52:09,404: Snapshot:0	Epoch:22	Loss:7.847	translation_Loss:7.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.0	Hits@10:38.77	Best:22.0
2024-12-27 21:52:16,084: Snapshot:0	Epoch:23	Loss:6.873	translation_Loss:6.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.39	Hits@10:38.95	Best:22.39
2024-12-27 21:52:22,811: Snapshot:0	Epoch:24	Loss:6.074	translation_Loss:6.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:39.09	Best:22.69
2024-12-27 21:52:29,453: Snapshot:0	Epoch:25	Loss:5.412	translation_Loss:5.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.92	Hits@10:39.3	Best:22.92
2024-12-27 21:52:36,129: Snapshot:0	Epoch:26	Loss:4.764	translation_Loss:4.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.08	Hits@10:39.34	Best:23.08
2024-12-27 21:52:42,748: Snapshot:0	Epoch:27	Loss:4.279	translation_Loss:4.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.34	Hits@10:39.55	Best:23.34
2024-12-27 21:52:49,481: Snapshot:0	Epoch:28	Loss:3.814	translation_Loss:3.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.54	Hits@10:39.65	Best:23.54
2024-12-27 21:52:56,684: Snapshot:0	Epoch:29	Loss:3.411	translation_Loss:3.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.79	Best:23.64
2024-12-27 21:53:03,335: Snapshot:0	Epoch:30	Loss:3.093	translation_Loss:3.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:39.83	Best:23.75
2024-12-27 21:53:10,035: Snapshot:0	Epoch:31	Loss:2.832	translation_Loss:2.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:39.92	Best:23.89
2024-12-27 21:53:16,703: Snapshot:0	Epoch:32	Loss:2.577	translation_Loss:2.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:39.96	Best:23.98
2024-12-27 21:53:23,317: Snapshot:0	Epoch:33	Loss:2.345	translation_Loss:2.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.03	Hits@10:39.99	Best:24.03
2024-12-27 21:53:29,965: Snapshot:0	Epoch:34	Loss:2.139	translation_Loss:2.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.04	Hits@10:40.02	Best:24.04
2024-12-27 21:53:36,622: Snapshot:0	Epoch:35	Loss:2.005	translation_Loss:2.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:40.03	Best:24.12
2024-12-27 21:53:43,297: Snapshot:0	Epoch:36	Loss:1.834	translation_Loss:1.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.04	Hits@10:40.05	Best:24.12
2024-12-27 21:53:50,002: Snapshot:0	Epoch:37	Loss:1.731	translation_Loss:1.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.15	Hits@10:40.1	Best:24.15
2024-12-27 21:53:56,632: Snapshot:0	Epoch:38	Loss:1.603	translation_Loss:1.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:40.15	Best:24.21
2024-12-27 21:54:03,271: Snapshot:0	Epoch:39	Loss:1.525	translation_Loss:1.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.25	Hits@10:40.12	Best:24.25
2024-12-27 21:54:09,885: Snapshot:0	Epoch:40	Loss:1.441	translation_Loss:1.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:40.09	Best:24.25
2024-12-27 21:54:16,507: Snapshot:0	Epoch:41	Loss:1.361	translation_Loss:1.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.28	Hits@10:40.12	Best:24.28
2024-12-27 21:54:23,148: Snapshot:0	Epoch:42	Loss:1.278	translation_Loss:1.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.35	Hits@10:40.21	Best:24.35
2024-12-27 21:54:29,767: Snapshot:0	Epoch:43	Loss:1.235	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:40.08	Best:24.35
2024-12-27 21:54:36,428: Snapshot:0	Epoch:44	Loss:1.177	translation_Loss:1.177	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.44	Hits@10:40.15	Best:24.44
2024-12-27 21:54:43,089: Snapshot:0	Epoch:45	Loss:1.127	translation_Loss:1.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:40.07	Best:24.46
2024-12-27 21:54:49,799: Snapshot:0	Epoch:46	Loss:1.069	translation_Loss:1.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:40.18	Best:24.48
2024-12-27 21:54:56,442: Snapshot:0	Epoch:47	Loss:1.03	translation_Loss:1.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.47	Hits@10:40.19	Best:24.48
2024-12-27 21:55:03,060: Snapshot:0	Epoch:48	Loss:1.0	translation_Loss:1.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.44	Hits@10:40.12	Best:24.48
2024-12-27 21:55:10,251: Snapshot:0	Epoch:49	Loss:0.954	translation_Loss:0.954	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:40.06	Best:24.48
2024-12-27 21:55:16,851: Snapshot:0	Epoch:50	Loss:0.928	translation_Loss:0.928	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:40.05	Best:24.48
2024-12-27 21:55:23,501: Snapshot:0	Epoch:51	Loss:0.894	translation_Loss:0.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.54	Hits@10:40.08	Best:24.54
2024-12-27 21:55:30,123: Snapshot:0	Epoch:52	Loss:0.875	translation_Loss:0.875	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:40.19	Best:24.54
2024-12-27 21:55:36,794: Snapshot:0	Epoch:53	Loss:0.832	translation_Loss:0.832	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:40.23	Best:24.54
2024-12-27 21:55:43,486: Snapshot:0	Epoch:54	Loss:0.815	translation_Loss:0.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.56	Hits@10:40.17	Best:24.56
2024-12-27 21:55:50,170: Snapshot:0	Epoch:55	Loss:0.778	translation_Loss:0.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.56	Hits@10:40.18	Best:24.56
2024-12-27 21:55:56,792: Snapshot:0	Epoch:56	Loss:0.786	translation_Loss:0.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.59	Hits@10:40.25	Best:24.59
2024-12-27 21:56:03,412: Snapshot:0	Epoch:57	Loss:0.767	translation_Loss:0.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.62	Hits@10:40.33	Best:24.62
2024-12-27 21:56:10,057: Snapshot:0	Epoch:58	Loss:0.753	translation_Loss:0.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.51	Hits@10:40.21	Best:24.62
2024-12-27 21:56:16,672: Snapshot:0	Epoch:59	Loss:0.712	translation_Loss:0.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.57	Hits@10:40.37	Best:24.62
2024-12-27 21:56:23,261: Snapshot:0	Epoch:60	Loss:0.7	translation_Loss:0.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.54	Hits@10:40.25	Best:24.62
2024-12-27 21:56:29,862: Snapshot:0	Epoch:61	Loss:0.674	translation_Loss:0.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:40.11	Best:24.62
2024-12-27 21:56:36,523: Early Stopping! Snapshot: 0 Epoch: 62 Best Results: 24.62
2024-12-27 21:56:36,523: Start to training tokens! Snapshot: 0 Epoch: 62 Loss:0.682 MRR:24.46 Best Results: 24.62
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 21:56:36,523: Snapshot:0	Epoch:62	Loss:0.682	translation_Loss:0.682	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:40.01	Best:24.62
2024-12-27 21:56:43,788: Snapshot:0	Epoch:63	Loss:73.273	translation_Loss:72.025	multi_layer_Loss:1.248	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.46	Hits@10:40.01	Best:24.62
2024-12-27 21:56:50,581: End of token training: 0 Epoch: 64 Loss:72.095 MRR:24.46 Best Results: 24.62
2024-12-27 21:56:50,582: Snapshot:0	Epoch:64	Loss:72.095	translation_Loss:72.062	multi_layer_Loss:0.033	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.46	Hits@10:40.01	Best:24.62
2024-12-27 21:56:50,937: => loading checkpoint './checkpoint/FACTfact_0.0001_512_10000/0model_best.tar'
2024-12-27 21:56:53,911: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2389 | 0.1542 | 0.2795 | 0.3281 |  0.3913 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 21:57:20,012: Snapshot:1	Epoch:0	Loss:44.806	translation_Loss:43.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.017                                                   	MRR:18.46	Hits@10:31.57	Best:18.46
2024-12-27 21:57:27,536: Snapshot:1	Epoch:1	Loss:38.866	translation_Loss:36.077	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.79                                                   	MRR:19.22	Hits@10:32.61	Best:19.22
2024-12-27 21:57:35,089: Snapshot:1	Epoch:2	Loss:35.558	translation_Loss:31.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.271                                                   	MRR:19.68	Hits@10:33.37	Best:19.68
2024-12-27 21:57:42,689: Snapshot:1	Epoch:3	Loss:32.87	translation_Loss:27.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.241                                                   	MRR:20.15	Hits@10:34.0	Best:20.15
2024-12-27 21:57:50,334: Snapshot:1	Epoch:4	Loss:30.676	translation_Loss:24.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.86                                                   	MRR:20.41	Hits@10:34.37	Best:20.41
2024-12-27 21:57:57,900: Snapshot:1	Epoch:5	Loss:28.883	translation_Loss:22.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.26                                                   	MRR:20.74	Hits@10:34.74	Best:20.74
2024-12-27 21:58:05,509: Snapshot:1	Epoch:6	Loss:27.587	translation_Loss:21.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.501                                                   	MRR:20.98	Hits@10:35.05	Best:20.98
2024-12-27 21:58:13,062: Snapshot:1	Epoch:7	Loss:26.485	translation_Loss:19.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.626                                                   	MRR:21.07	Hits@10:35.23	Best:21.07
2024-12-27 21:58:20,691: Snapshot:1	Epoch:8	Loss:25.811	translation_Loss:19.114	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.698                                                   	MRR:21.22	Hits@10:35.41	Best:21.22
2024-12-27 21:58:28,297: Snapshot:1	Epoch:9	Loss:25.291	translation_Loss:18.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.735                                                   	MRR:21.36	Hits@10:35.62	Best:21.36
2024-12-27 21:58:35,966: Snapshot:1	Epoch:10	Loss:24.852	translation_Loss:18.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.743                                                   	MRR:21.39	Hits@10:35.67	Best:21.39
2024-12-27 21:58:43,626: Snapshot:1	Epoch:11	Loss:24.639	translation_Loss:17.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.744                                                   	MRR:21.41	Hits@10:35.82	Best:21.41
2024-12-27 21:58:51,264: Snapshot:1	Epoch:12	Loss:24.379	translation_Loss:17.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.728                                                   	MRR:21.49	Hits@10:35.91	Best:21.49
2024-12-27 21:58:58,950: Snapshot:1	Epoch:13	Loss:24.238	translation_Loss:17.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.726                                                   	MRR:21.52	Hits@10:35.89	Best:21.52
2024-12-27 21:59:07,002: Snapshot:1	Epoch:14	Loss:24.181	translation_Loss:17.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.72                                                   	MRR:21.5	Hits@10:35.9	Best:21.52
2024-12-27 21:59:14,657: Snapshot:1	Epoch:15	Loss:24.077	translation_Loss:17.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.715                                                   	MRR:21.53	Hits@10:35.9	Best:21.53
2024-12-27 21:59:22,226: Snapshot:1	Epoch:16	Loss:23.991	translation_Loss:17.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.705                                                   	MRR:21.51	Hits@10:35.86	Best:21.53
2024-12-27 21:59:29,767: Snapshot:1	Epoch:17	Loss:23.897	translation_Loss:17.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.696                                                   	MRR:21.54	Hits@10:35.94	Best:21.54
2024-12-27 21:59:37,348: Snapshot:1	Epoch:18	Loss:23.887	translation_Loss:17.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.69                                                   	MRR:21.6	Hits@10:35.97	Best:21.6
2024-12-27 21:59:45,034: Snapshot:1	Epoch:19	Loss:23.788	translation_Loss:17.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.68                                                   	MRR:21.51	Hits@10:35.95	Best:21.6
2024-12-27 21:59:52,579: Snapshot:1	Epoch:20	Loss:23.717	translation_Loss:17.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.677                                                   	MRR:21.56	Hits@10:35.94	Best:21.6
2024-12-27 22:00:00,130: Snapshot:1	Epoch:21	Loss:23.675	translation_Loss:17.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.668                                                   	MRR:21.52	Hits@10:35.99	Best:21.6
2024-12-27 22:00:07,783: Snapshot:1	Epoch:22	Loss:23.664	translation_Loss:16.999	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.665                                                   	MRR:21.54	Hits@10:35.97	Best:21.6
2024-12-27 22:00:15,304: Early Stopping! Snapshot: 1 Epoch: 23 Best Results: 21.6
2024-12-27 22:00:15,304: Start to training tokens! Snapshot: 1 Epoch: 23 Loss:23.73 MRR:21.46 Best Results: 21.6
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:00:15,304: Snapshot:1	Epoch:23	Loss:23.73	translation_Loss:17.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.661                                                   	MRR:21.46	Hits@10:35.95	Best:21.6
2024-12-27 22:00:22,592: Snapshot:1	Epoch:24	Loss:85.954	translation_Loss:84.752	multi_layer_Loss:1.203	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.46	Hits@10:35.95	Best:21.6
2024-12-27 22:00:29,873: End of token training: 1 Epoch: 25 Loss:84.794 MRR:21.46 Best Results: 21.6
2024-12-27 22:00:29,873: Snapshot:1	Epoch:25	Loss:84.794	translation_Loss:84.768	multi_layer_Loss:0.026	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.46	Hits@10:35.95	Best:21.6
2024-12-27 22:00:30,234: => loading checkpoint './checkpoint/FACTfact_0.0001_512_10000/1model_best.tar'
2024-12-27 22:00:36,345: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2513 | 0.1643 | 0.2906 | 0.3445 |  0.4102 |
|     1      | 0.2151 | 0.1342 | 0.2542 | 0.3017 |  0.3634 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:01:02,761: Snapshot:2	Epoch:0	Loss:32.295	translation_Loss:31.308	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.987                                                   	MRR:19.62	Hits@10:34.37	Best:19.62
2024-12-27 22:01:10,603: Snapshot:2	Epoch:1	Loss:26.91	translation_Loss:24.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.692                                                   	MRR:20.16	Hits@10:35.24	Best:20.16
2024-12-27 22:01:18,470: Snapshot:2	Epoch:2	Loss:24.379	translation_Loss:20.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.162                                                   	MRR:20.49	Hits@10:35.73	Best:20.49
2024-12-27 22:01:26,304: Snapshot:2	Epoch:3	Loss:22.745	translation_Loss:17.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.176                                                   	MRR:20.78	Hits@10:36.09	Best:20.78
2024-12-27 22:01:34,098: Snapshot:2	Epoch:4	Loss:21.583	translation_Loss:15.749	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:5.834                                                   	MRR:20.94	Hits@10:36.25	Best:20.94
2024-12-27 22:01:41,934: Snapshot:2	Epoch:5	Loss:20.874	translation_Loss:14.61	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.264                                                   	MRR:21.15	Hits@10:36.55	Best:21.15
2024-12-27 22:01:49,879: Snapshot:2	Epoch:6	Loss:20.405	translation_Loss:13.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.539                                                   	MRR:21.21	Hits@10:36.58	Best:21.21
2024-12-27 22:01:57,674: Snapshot:2	Epoch:7	Loss:20.011	translation_Loss:13.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.706                                                   	MRR:21.22	Hits@10:36.68	Best:21.22
2024-12-27 22:02:05,415: Snapshot:2	Epoch:8	Loss:19.784	translation_Loss:12.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.833                                                   	MRR:21.21	Hits@10:36.85	Best:21.22
2024-12-27 22:02:13,211: Snapshot:2	Epoch:9	Loss:19.686	translation_Loss:12.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.918                                                   	MRR:21.24	Hits@10:36.85	Best:21.24
2024-12-27 22:02:21,068: Snapshot:2	Epoch:10	Loss:19.584	translation_Loss:12.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:6.985                                                   	MRR:21.27	Hits@10:36.93	Best:21.27
2024-12-27 22:02:28,861: Snapshot:2	Epoch:11	Loss:19.5	translation_Loss:12.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.037                                                   	MRR:21.32	Hits@10:37.0	Best:21.32
2024-12-27 22:02:36,611: Snapshot:2	Epoch:12	Loss:19.45	translation_Loss:12.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.055                                                   	MRR:21.3	Hits@10:36.95	Best:21.32
2024-12-27 22:02:44,917: Snapshot:2	Epoch:13	Loss:19.439	translation_Loss:12.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.069                                                   	MRR:21.34	Hits@10:36.88	Best:21.34
2024-12-27 22:02:52,662: Snapshot:2	Epoch:14	Loss:19.411	translation_Loss:12.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.091                                                   	MRR:21.34	Hits@10:36.95	Best:21.34
2024-12-27 22:03:00,451: Snapshot:2	Epoch:15	Loss:19.393	translation_Loss:12.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.115                                                   	MRR:21.36	Hits@10:37.03	Best:21.36
2024-12-27 22:03:08,260: Snapshot:2	Epoch:16	Loss:19.334	translation_Loss:12.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.123                                                   	MRR:21.34	Hits@10:37.03	Best:21.36
2024-12-27 22:03:16,062: Snapshot:2	Epoch:17	Loss:19.358	translation_Loss:12.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.128                                                   	MRR:21.26	Hits@10:37.04	Best:21.36
2024-12-27 22:03:23,764: Snapshot:2	Epoch:18	Loss:19.399	translation_Loss:12.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.133                                                   	MRR:21.28	Hits@10:37.0	Best:21.36
2024-12-27 22:03:31,499: Snapshot:2	Epoch:19	Loss:19.375	translation_Loss:12.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.157                                                   	MRR:21.3	Hits@10:36.98	Best:21.36
2024-12-27 22:03:39,290: Early Stopping! Snapshot: 2 Epoch: 20 Best Results: 21.36
2024-12-27 22:03:39,290: Start to training tokens! Snapshot: 2 Epoch: 20 Loss:19.372 MRR:21.32 Best Results: 21.36
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:03:39,291: Snapshot:2	Epoch:20	Loss:19.372	translation_Loss:12.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:7.167                                                   	MRR:21.32	Hits@10:36.94	Best:21.36
2024-12-27 22:03:46,867: Snapshot:2	Epoch:21	Loss:84.548	translation_Loss:83.292	multi_layer_Loss:1.257	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.32	Hits@10:36.94	Best:21.36
2024-12-27 22:03:54,359: End of token training: 2 Epoch: 22 Loss:83.253 MRR:21.32 Best Results: 21.36
2024-12-27 22:03:54,360: Snapshot:2	Epoch:22	Loss:83.253	translation_Loss:83.22	multi_layer_Loss:0.033	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.32	Hits@10:36.94	Best:21.36
2024-12-27 22:03:54,717: => loading checkpoint './checkpoint/FACTfact_0.0001_512_10000/2model_best.tar'
2024-12-27 22:04:04,207: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1628 | 0.2904 | 0.3451 |  0.4134 |
|     1      | 0.2237 | 0.1403 | 0.2618 | 0.3124 |  0.3821 |
|     2      | 0.2123 | 0.1281 | 0.2505 | 0.3011 |  0.3706 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:04:30,719: Snapshot:3	Epoch:0	Loss:19.668	translation_Loss:18.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.909                                                   	MRR:18.93	Hits@10:35.91	Best:18.93
2024-12-27 22:04:38,612: Snapshot:3	Epoch:1	Loss:15.038	translation_Loss:12.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.211                                                   	MRR:19.39	Hits@10:36.91	Best:19.39
2024-12-27 22:04:46,521: Snapshot:3	Epoch:2	Loss:13.296	translation_Loss:10.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.197                                                   	MRR:19.63	Hits@10:37.42	Best:19.63
2024-12-27 22:04:54,411: Snapshot:3	Epoch:3	Loss:12.416	translation_Loss:8.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.823                                                   	MRR:19.75	Hits@10:37.53	Best:19.75
2024-12-27 22:05:02,372: Snapshot:3	Epoch:4	Loss:11.905	translation_Loss:7.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.203                                                   	MRR:19.83	Hits@10:37.57	Best:19.83
2024-12-27 22:05:10,348: Snapshot:3	Epoch:5	Loss:11.668	translation_Loss:7.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.453                                                   	MRR:19.9	Hits@10:37.98	Best:19.9
2024-12-27 22:05:18,163: Snapshot:3	Epoch:6	Loss:11.529	translation_Loss:6.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.6                                                   	MRR:19.84	Hits@10:37.85	Best:19.9
2024-12-27 22:05:26,008: Snapshot:3	Epoch:7	Loss:11.391	translation_Loss:6.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.704                                                   	MRR:19.91	Hits@10:37.86	Best:19.91
2024-12-27 22:05:33,813: Snapshot:3	Epoch:8	Loss:11.348	translation_Loss:6.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.782                                                   	MRR:19.89	Hits@10:37.72	Best:19.91
2024-12-27 22:05:41,750: Snapshot:3	Epoch:9	Loss:11.366	translation_Loss:6.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.827                                                   	MRR:19.92	Hits@10:37.92	Best:19.92
2024-12-27 22:05:49,676: Snapshot:3	Epoch:10	Loss:11.315	translation_Loss:6.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.864                                                   	MRR:20.09	Hits@10:37.78	Best:20.09
2024-12-27 22:05:57,455: Snapshot:3	Epoch:11	Loss:11.326	translation_Loss:6.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.902                                                   	MRR:20.04	Hits@10:38.04	Best:20.09
2024-12-27 22:06:05,247: Snapshot:3	Epoch:12	Loss:11.29	translation_Loss:6.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.914                                                   	MRR:20.02	Hits@10:38.01	Best:20.09
2024-12-27 22:06:13,039: Snapshot:3	Epoch:13	Loss:11.313	translation_Loss:6.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.936                                                   	MRR:19.91	Hits@10:37.85	Best:20.09
2024-12-27 22:06:20,826: Snapshot:3	Epoch:14	Loss:11.37	translation_Loss:6.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.959                                                   	MRR:20.03	Hits@10:38.09	Best:20.09
2024-12-27 22:06:29,098: Early Stopping! Snapshot: 3 Epoch: 15 Best Results: 20.09
2024-12-27 22:06:29,099: Start to training tokens! Snapshot: 3 Epoch: 15 Loss:11.304 MRR:20.08 Best Results: 20.09
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:06:29,099: Snapshot:3	Epoch:15	Loss:11.304	translation_Loss:6.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.97                                                   	MRR:20.08	Hits@10:38.1	Best:20.09
2024-12-27 22:06:36,674: Snapshot:3	Epoch:16	Loss:80.515	translation_Loss:79.317	multi_layer_Loss:1.198	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.08	Hits@10:38.1	Best:20.09
2024-12-27 22:06:44,187: End of token training: 3 Epoch: 17 Loss:79.435 MRR:20.08 Best Results: 20.09
2024-12-27 22:06:44,188: Snapshot:3	Epoch:17	Loss:79.435	translation_Loss:79.409	multi_layer_Loss:0.027	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.08	Hits@10:38.1	Best:20.09
2024-12-27 22:06:44,472: => loading checkpoint './checkpoint/FACTfact_0.0001_512_10000/3model_best.tar'
2024-12-27 22:06:57,347: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1552 | 0.2785 | 0.332  |  0.4032 |
|     1      | 0.2199 | 0.1372 | 0.2539 | 0.3073 |  0.3778 |
|     2      | 0.2124 | 0.1265 | 0.2472 | 0.3035 |  0.3816 |
|     3      | 0.2013 | 0.1101 | 0.2358 | 0.2977 |  0.382  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:07:23,635: Snapshot:4	Epoch:0	Loss:11.036	translation_Loss:10.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.655                                                   	MRR:19.73	Hits@10:43.98	Best:19.73
2024-12-27 22:07:31,552: Snapshot:4	Epoch:1	Loss:7.344	translation_Loss:6.159	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.184                                                   	MRR:20.54	Hits@10:45.51	Best:20.54
2024-12-27 22:07:39,519: Snapshot:4	Epoch:2	Loss:5.735	translation_Loss:4.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.451                                                   	MRR:20.82	Hits@10:45.9	Best:20.82
2024-12-27 22:07:47,988: Snapshot:4	Epoch:3	Loss:4.685	translation_Loss:3.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.588                                                   	MRR:21.2	Hits@10:46.22	Best:21.2
2024-12-27 22:07:55,939: Snapshot:4	Epoch:4	Loss:4.078	translation_Loss:2.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.642                                                   	MRR:21.58	Hits@10:46.42	Best:21.58
2024-12-27 22:08:03,839: Snapshot:4	Epoch:5	Loss:3.778	translation_Loss:2.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.669                                                   	MRR:21.53	Hits@10:46.11	Best:21.58
2024-12-27 22:08:11,739: Snapshot:4	Epoch:6	Loss:3.621	translation_Loss:1.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.683                                                   	MRR:21.69	Hits@10:46.35	Best:21.69
2024-12-27 22:08:19,627: Snapshot:4	Epoch:7	Loss:3.515	translation_Loss:1.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.693                                                   	MRR:21.74	Hits@10:46.26	Best:21.74
2024-12-27 22:08:27,625: Snapshot:4	Epoch:8	Loss:3.468	translation_Loss:1.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.697                                                   	MRR:22.08	Hits@10:46.57	Best:22.08
2024-12-27 22:08:35,555: Snapshot:4	Epoch:9	Loss:3.439	translation_Loss:1.725	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.714                                                   	MRR:22.03	Hits@10:46.8	Best:22.08
2024-12-27 22:08:43,454: Snapshot:4	Epoch:10	Loss:3.435	translation_Loss:1.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.724                                                   	MRR:22.21	Hits@10:46.72	Best:22.21
2024-12-27 22:08:51,344: Snapshot:4	Epoch:11	Loss:3.378	translation_Loss:1.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.713                                                   	MRR:22.04	Hits@10:46.64	Best:22.21
2024-12-27 22:08:59,184: Snapshot:4	Epoch:12	Loss:3.413	translation_Loss:1.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.719                                                   	MRR:22.02	Hits@10:46.49	Best:22.21
2024-12-27 22:09:07,057: Snapshot:4	Epoch:13	Loss:3.339	translation_Loss:1.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.714                                                   	MRR:22.0	Hits@10:46.73	Best:22.21
2024-12-27 22:09:14,870: Snapshot:4	Epoch:14	Loss:3.381	translation_Loss:1.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.709                                                   	MRR:22.07	Hits@10:46.48	Best:22.21
2024-12-27 22:09:22,685: Early Stopping! Snapshot: 4 Epoch: 15 Best Results: 22.21
2024-12-27 22:09:22,685: Start to training tokens! Snapshot: 4 Epoch: 15 Loss:3.375 MRR:21.87 Best Results: 22.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:09:22,686: Snapshot:4	Epoch:15	Loss:3.375	translation_Loss:1.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.732                                                   	MRR:21.87	Hits@10:46.37	Best:22.21
2024-12-27 22:09:30,251: Snapshot:4	Epoch:16	Loss:68.026	translation_Loss:66.83	multi_layer_Loss:1.196	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.87	Hits@10:46.37	Best:22.21
2024-12-27 22:09:37,869: End of token training: 4 Epoch: 17 Loss:66.915 MRR:21.87 Best Results: 22.21
2024-12-27 22:09:37,869: Snapshot:4	Epoch:17	Loss:66.915	translation_Loss:66.895	multi_layer_Loss:0.02	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.87	Hits@10:46.37	Best:22.21
2024-12-27 22:09:38,231: => loading checkpoint './checkpoint/FACTfact_0.0001_512_10000/4model_best.tar'
2024-12-27 22:09:54,954: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.231  | 0.1463 | 0.2659 | 0.3181 |  0.389  |
|     1      | 0.2079 | 0.1274 | 0.2387 | 0.2917 |  0.3642 |
|     2      | 0.2009 | 0.1172 | 0.2321 | 0.2879 |  0.3673 |
|     3      | 0.1923 | 0.1017 | 0.2209 | 0.2867 |  0.378  |
|     4      | 0.2209 | 0.1037 | 0.2607 | 0.3471 |  0.4637 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 22:09:54,956: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2389 | 0.1542 | 0.2795 | 0.3281 |  0.3913 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2513 | 0.1643 | 0.2906 | 0.3445 |  0.4102 |
|     1      | 0.2151 | 0.1342 | 0.2542 | 0.3017 |  0.3634 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1628 | 0.2904 | 0.3451 |  0.4134 |
|     1      | 0.2237 | 0.1403 | 0.2618 | 0.3124 |  0.3821 |
|     2      | 0.2123 | 0.1281 | 0.2505 | 0.3011 |  0.3706 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1552 | 0.2785 | 0.332  |  0.4032 |
|     1      | 0.2199 | 0.1372 | 0.2539 | 0.3073 |  0.3778 |
|     2      | 0.2124 | 0.1265 | 0.2472 | 0.3035 |  0.3816 |
|     3      | 0.2013 | 0.1101 | 0.2358 | 0.2977 |  0.382  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.231  | 0.1463 | 0.2659 | 0.3181 |  0.389  |
|     1      | 0.2079 | 0.1274 | 0.2387 | 0.2917 |  0.3642 |
|     2      | 0.2009 | 0.1172 | 0.2321 | 0.2879 |  0.3673 |
|     3      | 0.1923 | 0.1017 | 0.2209 | 0.2867 |  0.378  |
|     4      | 0.2209 | 0.1037 | 0.2607 | 0.3471 |  0.4637 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 22:09:54,957: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 438.4711444377899  |   0.239   |    0.154     |     0.28     |     0.391     |
|    1     | 212.51201844215393 |   0.233   |    0.149     |    0.272     |     0.387     |
|    2     | 194.66311717033386 |   0.229   |    0.144     |    0.268     |     0.389     |
|    3     | 156.6046121120453  |   0.219   |    0.132     |    0.254     |     0.386     |
|    4     | 157.00139451026917 |   0.211   |    0.119     |    0.244     |     0.392     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 22:09:54,957: Sum_Training_Time:1159.2522866725922
2024-12-27 22:09:54,957: Every_Training_Time:[438.4711444377899, 212.51201844215393, 194.66311717033386, 156.6046121120453, 157.00139451026917]
2024-12-27 22:09:54,957: Forward transfer: 0.17007499999999998 Backward transfer: -0.008875000000000001
2024-12-27 22:10:34,084: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227220958/FACTfact_0.0001_1024_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_1024_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_1024_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 22:10:44,275: Snapshot:0	Epoch:0	Loss:51.848	translation_Loss:51.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-27 22:10:50,830: Snapshot:0	Epoch:1	Loss:48.341	translation_Loss:48.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.35	Hits@10:5.97	Best:3.35
2024-12-27 22:10:57,340: Snapshot:0	Epoch:2	Loss:45.084	translation_Loss:45.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.51	Hits@10:8.56	Best:4.51
2024-12-27 22:11:03,899: Snapshot:0	Epoch:3	Loss:42.069	translation_Loss:42.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.4	Hits@10:10.8	Best:5.4
2024-12-27 22:11:10,905: Snapshot:0	Epoch:4	Loss:39.208	translation_Loss:39.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.07	Best:6.23
2024-12-27 22:11:17,418: Snapshot:0	Epoch:5	Loss:36.473	translation_Loss:36.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.09	Hits@10:15.44	Best:7.09
2024-12-27 22:11:23,955: Snapshot:0	Epoch:6	Loss:33.877	translation_Loss:33.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.92	Hits@10:17.7	Best:7.92
2024-12-27 22:11:30,472: Snapshot:0	Epoch:7	Loss:31.404	translation_Loss:31.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.84	Hits@10:19.88	Best:8.84
2024-12-27 22:11:36,990: Snapshot:0	Epoch:8	Loss:29.038	translation_Loss:29.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.75	Hits@10:22.01	Best:9.75
2024-12-27 22:11:43,549: Snapshot:0	Epoch:9	Loss:26.816	translation_Loss:26.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.66	Hits@10:23.9	Best:10.66
2024-12-27 22:11:50,059: Snapshot:0	Epoch:10	Loss:24.706	translation_Loss:24.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.55	Hits@10:25.57	Best:11.55
2024-12-27 22:11:56,546: Snapshot:0	Epoch:11	Loss:22.703	translation_Loss:22.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.32	Hits@10:27.18	Best:12.32
2024-12-27 22:12:03,049: Snapshot:0	Epoch:12	Loss:20.861	translation_Loss:20.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.04	Hits@10:28.61	Best:13.04
2024-12-27 22:12:09,558: Snapshot:0	Epoch:13	Loss:19.111	translation_Loss:19.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.69	Hits@10:29.78	Best:13.69
2024-12-27 22:12:16,097: Snapshot:0	Epoch:14	Loss:17.523	translation_Loss:17.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.36	Hits@10:30.82	Best:14.36
2024-12-27 22:12:22,616: Snapshot:0	Epoch:15	Loss:16.056	translation_Loss:16.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.97	Hits@10:31.82	Best:14.97
2024-12-27 22:12:29,131: Snapshot:0	Epoch:16	Loss:14.671	translation_Loss:14.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.61	Hits@10:32.7	Best:15.61
2024-12-27 22:12:35,632: Snapshot:0	Epoch:17	Loss:13.412	translation_Loss:13.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.12	Hits@10:33.5	Best:16.12
2024-12-27 22:12:42,134: Snapshot:0	Epoch:18	Loss:12.256	translation_Loss:12.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.73	Hits@10:34.3	Best:16.73
2024-12-27 22:12:48,703: Snapshot:0	Epoch:19	Loss:11.17	translation_Loss:11.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.29	Hits@10:34.94	Best:17.29
2024-12-27 22:12:55,307: Snapshot:0	Epoch:20	Loss:10.171	translation_Loss:10.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.78	Hits@10:35.43	Best:17.78
2024-12-27 22:13:01,827: Snapshot:0	Epoch:21	Loss:9.288	translation_Loss:9.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.3	Hits@10:35.93	Best:18.3
2024-12-27 22:13:08,450: Snapshot:0	Epoch:22	Loss:8.473	translation_Loss:8.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.75	Hits@10:36.35	Best:18.75
2024-12-27 22:13:14,959: Snapshot:0	Epoch:23	Loss:7.692	translation_Loss:7.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.22	Hits@10:36.8	Best:19.22
2024-12-27 22:13:22,002: Snapshot:0	Epoch:24	Loss:6.988	translation_Loss:6.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.62	Hits@10:37.19	Best:19.62
2024-12-27 22:13:28,571: Snapshot:0	Epoch:25	Loss:6.371	translation_Loss:6.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.03	Hits@10:37.48	Best:20.03
2024-12-27 22:13:35,102: Snapshot:0	Epoch:26	Loss:5.775	translation_Loss:5.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.37	Hits@10:37.8	Best:20.37
2024-12-27 22:13:41,639: Snapshot:0	Epoch:27	Loss:5.25	translation_Loss:5.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.72	Hits@10:37.99	Best:20.72
2024-12-27 22:13:48,166: Snapshot:0	Epoch:28	Loss:4.766	translation_Loss:4.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.03	Hits@10:38.17	Best:21.03
2024-12-27 22:13:54,752: Snapshot:0	Epoch:29	Loss:4.306	translation_Loss:4.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.29	Hits@10:38.36	Best:21.29
2024-12-27 22:14:01,262: Snapshot:0	Epoch:30	Loss:3.938	translation_Loss:3.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.6	Hits@10:38.48	Best:21.6
2024-12-27 22:14:07,805: Snapshot:0	Epoch:31	Loss:3.587	translation_Loss:3.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.81	Hits@10:38.69	Best:21.81
2024-12-27 22:14:14,429: Snapshot:0	Epoch:32	Loss:3.272	translation_Loss:3.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.01	Hits@10:38.7	Best:22.01
2024-12-27 22:14:20,993: Snapshot:0	Epoch:33	Loss:2.974	translation_Loss:2.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.21	Hits@10:38.86	Best:22.21
2024-12-27 22:14:27,510: Snapshot:0	Epoch:34	Loss:2.714	translation_Loss:2.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.45	Hits@10:39.06	Best:22.45
2024-12-27 22:14:34,557: Snapshot:0	Epoch:35	Loss:2.496	translation_Loss:2.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.65	Hits@10:39.19	Best:22.65
2024-12-27 22:14:41,116: Snapshot:0	Epoch:36	Loss:2.283	translation_Loss:2.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.8	Hits@10:39.26	Best:22.8
2024-12-27 22:14:47,653: Snapshot:0	Epoch:37	Loss:2.099	translation_Loss:2.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.92	Hits@10:39.32	Best:22.92
2024-12-27 22:14:54,190: Snapshot:0	Epoch:38	Loss:1.922	translation_Loss:1.922	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.05	Hits@10:39.37	Best:23.05
2024-12-27 22:15:00,708: Snapshot:0	Epoch:39	Loss:1.787	translation_Loss:1.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.12	Hits@10:39.38	Best:23.12
2024-12-27 22:15:07,326: Snapshot:0	Epoch:40	Loss:1.651	translation_Loss:1.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.23	Hits@10:39.37	Best:23.23
2024-12-27 22:15:13,935: Snapshot:0	Epoch:41	Loss:1.537	translation_Loss:1.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.32	Hits@10:39.51	Best:23.32
2024-12-27 22:15:20,544: Snapshot:0	Epoch:42	Loss:1.42	translation_Loss:1.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:39.61	Best:23.43
2024-12-27 22:15:27,149: Snapshot:0	Epoch:43	Loss:1.323	translation_Loss:1.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.49	Hits@10:39.62	Best:23.49
2024-12-27 22:15:33,670: Snapshot:0	Epoch:44	Loss:1.238	translation_Loss:1.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.62	Hits@10:39.82	Best:23.62
2024-12-27 22:15:40,223: Snapshot:0	Epoch:45	Loss:1.164	translation_Loss:1.164	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.63	Hits@10:39.77	Best:23.63
2024-12-27 22:15:46,843: Snapshot:0	Epoch:46	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.8	Best:23.67
2024-12-27 22:15:53,380: Snapshot:0	Epoch:47	Loss:1.019	translation_Loss:1.019	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.88	Best:23.71
2024-12-27 22:15:59,914: Snapshot:0	Epoch:48	Loss:0.96	translation_Loss:0.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.94	Best:23.76
2024-12-27 22:16:06,431: Snapshot:0	Epoch:49	Loss:0.904	translation_Loss:0.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.79	Hits@10:39.93	Best:23.79
2024-12-27 22:16:12,964: Snapshot:0	Epoch:50	Loss:0.858	translation_Loss:0.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:39.97	Best:23.87
2024-12-27 22:16:19,523: Snapshot:0	Epoch:51	Loss:0.813	translation_Loss:0.813	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:40.03	Best:23.87
2024-12-27 22:16:26,099: Snapshot:0	Epoch:52	Loss:0.772	translation_Loss:0.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.93	Hits@10:39.85	Best:23.93
2024-12-27 22:16:32,593: Snapshot:0	Epoch:53	Loss:0.734	translation_Loss:0.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:39.9	Best:23.94
2024-12-27 22:16:39,628: Snapshot:0	Epoch:54	Loss:0.703	translation_Loss:0.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:40.09	Best:23.94
2024-12-27 22:16:46,186: Snapshot:0	Epoch:55	Loss:0.664	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:40.05	Best:23.99
2024-12-27 22:16:52,706: Snapshot:0	Epoch:56	Loss:0.646	translation_Loss:0.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:40.06	Best:23.99
2024-12-27 22:16:59,286: Snapshot:0	Epoch:57	Loss:0.618	translation_Loss:0.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.02	Hits@10:40.06	Best:24.02
2024-12-27 22:17:05,815: Snapshot:0	Epoch:58	Loss:0.597	translation_Loss:0.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.07	Hits@10:40.08	Best:24.07
2024-12-27 22:17:12,325: Snapshot:0	Epoch:59	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.07	Hits@10:40.11	Best:24.07
2024-12-27 22:17:18,860: Snapshot:0	Epoch:60	Loss:0.544	translation_Loss:0.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:40.11	Best:24.1
2024-12-27 22:17:25,373: Snapshot:0	Epoch:61	Loss:0.526	translation_Loss:0.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.07	Hits@10:40.09	Best:24.1
2024-12-27 22:17:31,915: Snapshot:0	Epoch:62	Loss:0.513	translation_Loss:0.513	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.07	Best:24.11
2024-12-27 22:17:38,471: Snapshot:0	Epoch:63	Loss:0.493	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.13	Best:24.14
2024-12-27 22:17:45,608: Snapshot:0	Epoch:64	Loss:0.468	translation_Loss:0.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.17	Hits@10:40.06	Best:24.17
2024-12-27 22:17:52,085: Snapshot:0	Epoch:65	Loss:0.456	translation_Loss:0.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.05	Best:24.17
2024-12-27 22:17:58,591: Snapshot:0	Epoch:66	Loss:0.45	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:40.04	Best:24.17
2024-12-27 22:18:05,108: Snapshot:0	Epoch:67	Loss:0.442	translation_Loss:0.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.1	Best:24.17
2024-12-27 22:18:11,633: Snapshot:0	Epoch:68	Loss:0.416	translation_Loss:0.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.27	Best:24.17
2024-12-27 22:18:18,146: Early Stopping! Snapshot: 0 Epoch: 69 Best Results: 24.17
2024-12-27 22:18:18,146: Start to training tokens! Snapshot: 0 Epoch: 69 Loss:0.406 MRR:24.16 Best Results: 24.17
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:18:18,147: Snapshot:0	Epoch:69	Loss:0.406	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.2	Best:24.17
2024-12-27 22:18:25,228: Snapshot:0	Epoch:70	Loss:36.953	translation_Loss:35.958	multi_layer_Loss:0.995	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.2	Best:24.17
2024-12-27 22:18:31,805: End of token training: 0 Epoch: 71 Loss:36.176 MRR:24.16 Best Results: 24.17
2024-12-27 22:18:31,805: Snapshot:0	Epoch:71	Loss:36.176	translation_Loss:35.922	multi_layer_Loss:0.254	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.16	Hits@10:40.2	Best:24.17
2024-12-27 22:18:32,158: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_1000/0model_best.tar'
2024-12-27 22:18:34,933: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2344 | 0.147  | 0.2792 | 0.3286 |  0.3912 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:19:00,481: Snapshot:1	Epoch:0	Loss:23.932	translation_Loss:23.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.085                                                   	MRR:18.27	Hits@10:31.27	Best:18.27
2024-12-27 22:19:07,756: Snapshot:1	Epoch:1	Loss:19.681	translation_Loss:19.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.263                                                   	MRR:19.12	Hits@10:32.58	Best:19.12
2024-12-27 22:19:15,124: Snapshot:1	Epoch:2	Loss:16.524	translation_Loss:16.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.479                                                   	MRR:19.85	Hits@10:33.8	Best:19.85
2024-12-27 22:19:22,397: Snapshot:1	Epoch:3	Loss:13.884	translation_Loss:13.163	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.72                                                   	MRR:20.43	Hits@10:34.74	Best:20.43
2024-12-27 22:19:29,801: Snapshot:1	Epoch:4	Loss:11.743	translation_Loss:10.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.968                                                   	MRR:20.97	Hits@10:35.49	Best:20.97
2024-12-27 22:19:37,109: Snapshot:1	Epoch:5	Loss:9.965	translation_Loss:8.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.207                                                   	MRR:21.45	Hits@10:36.17	Best:21.45
2024-12-27 22:19:44,449: Snapshot:1	Epoch:6	Loss:8.542	translation_Loss:7.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.424                                                   	MRR:21.85	Hits@10:36.69	Best:21.85
2024-12-27 22:19:51,788: Snapshot:1	Epoch:7	Loss:7.373	translation_Loss:5.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.612                                                   	MRR:22.14	Hits@10:37.05	Best:22.14
2024-12-27 22:19:59,099: Snapshot:1	Epoch:8	Loss:6.564	translation_Loss:4.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.771                                                   	MRR:22.42	Hits@10:37.5	Best:22.42
2024-12-27 22:20:06,490: Snapshot:1	Epoch:9	Loss:5.89	translation_Loss:3.987	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.903                                                   	MRR:22.64	Hits@10:37.73	Best:22.64
2024-12-27 22:20:13,842: Snapshot:1	Epoch:10	Loss:5.41	translation_Loss:3.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.009                                                   	MRR:22.75	Hits@10:38.07	Best:22.75
2024-12-27 22:20:21,232: Snapshot:1	Epoch:11	Loss:5.074	translation_Loss:2.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.092                                                   	MRR:22.87	Hits@10:38.17	Best:22.87
2024-12-27 22:20:28,592: Snapshot:1	Epoch:12	Loss:4.796	translation_Loss:2.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.159                                                   	MRR:22.96	Hits@10:38.36	Best:22.96
2024-12-27 22:20:35,892: Snapshot:1	Epoch:13	Loss:4.626	translation_Loss:2.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.215                                                   	MRR:23.0	Hits@10:38.46	Best:23.0
2024-12-27 22:20:43,220: Snapshot:1	Epoch:14	Loss:4.489	translation_Loss:2.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.259                                                   	MRR:23.1	Hits@10:38.52	Best:23.1
2024-12-27 22:20:50,545: Snapshot:1	Epoch:15	Loss:4.342	translation_Loss:2.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.295                                                   	MRR:23.07	Hits@10:38.52	Best:23.1
2024-12-27 22:20:57,748: Snapshot:1	Epoch:16	Loss:4.264	translation_Loss:1.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.325                                                   	MRR:23.09	Hits@10:38.56	Best:23.1
2024-12-27 22:21:05,075: Snapshot:1	Epoch:17	Loss:4.18	translation_Loss:1.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.349                                                   	MRR:23.14	Hits@10:38.6	Best:23.14
2024-12-27 22:21:12,320: Snapshot:1	Epoch:18	Loss:4.104	translation_Loss:1.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.371                                                   	MRR:23.13	Hits@10:38.69	Best:23.14
2024-12-27 22:21:19,607: Snapshot:1	Epoch:19	Loss:4.056	translation_Loss:1.67	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.387                                                   	MRR:23.12	Hits@10:38.75	Best:23.14
2024-12-27 22:21:26,912: Snapshot:1	Epoch:20	Loss:4.008	translation_Loss:1.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.402                                                   	MRR:23.15	Hits@10:38.69	Best:23.15
2024-12-27 22:21:34,203: Snapshot:1	Epoch:21	Loss:3.974	translation_Loss:1.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.416                                                   	MRR:23.16	Hits@10:38.78	Best:23.16
2024-12-27 22:21:41,519: Snapshot:1	Epoch:22	Loss:3.931	translation_Loss:1.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.424                                                   	MRR:23.14	Hits@10:38.87	Best:23.16
2024-12-27 22:21:48,735: Snapshot:1	Epoch:23	Loss:3.928	translation_Loss:1.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.435                                                   	MRR:23.12	Hits@10:38.8	Best:23.16
2024-12-27 22:21:56,013: Snapshot:1	Epoch:24	Loss:3.891	translation_Loss:1.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.446                                                   	MRR:23.11	Hits@10:38.76	Best:23.16
2024-12-27 22:22:03,222: Snapshot:1	Epoch:25	Loss:3.863	translation_Loss:1.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.452                                                   	MRR:23.13	Hits@10:38.84	Best:23.16
2024-12-27 22:22:10,457: Early Stopping! Snapshot: 1 Epoch: 26 Best Results: 23.16
2024-12-27 22:22:10,458: Start to training tokens! Snapshot: 1 Epoch: 26 Loss:3.838 MRR:23.15 Best Results: 23.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:22:10,458: Snapshot:1	Epoch:26	Loss:3.838	translation_Loss:1.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.457                                                   	MRR:23.15	Hits@10:38.8	Best:23.16
2024-12-27 22:22:17,564: Snapshot:1	Epoch:27	Loss:39.8	translation_Loss:38.827	multi_layer_Loss:0.973	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.15	Hits@10:38.8	Best:23.16
2024-12-27 22:22:24,683: End of token training: 1 Epoch: 28 Loss:39.031 MRR:23.15 Best Results: 23.16
2024-12-27 22:22:24,684: Snapshot:1	Epoch:28	Loss:39.031	translation_Loss:38.801	multi_layer_Loss:0.23	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.15	Hits@10:38.8	Best:23.16
2024-12-27 22:22:25,043: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_1000/1model_best.tar'
2024-12-27 22:22:30,961: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2451 | 0.1532 | 0.2869 | 0.3457 |  0.4181 |
|     1      | 0.2337 | 0.1489 | 0.2743 | 0.3233 |  0.3867 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:22:56,481: Snapshot:2	Epoch:0	Loss:13.708	translation_Loss:13.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.085                                                   	MRR:20.92	Hits@10:36.57	Best:20.92
2024-12-27 22:23:04,344: Snapshot:2	Epoch:1	Loss:9.989	translation_Loss:9.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:21.51	Hits@10:37.54	Best:21.51
2024-12-27 22:23:11,782: Snapshot:2	Epoch:2	Loss:7.675	translation_Loss:7.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:21.88	Hits@10:38.14	Best:21.88
2024-12-27 22:23:19,248: Snapshot:2	Epoch:3	Loss:6.066	translation_Loss:5.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.533                                                   	MRR:22.13	Hits@10:38.54	Best:22.13
2024-12-27 22:23:26,703: Snapshot:2	Epoch:4	Loss:4.881	translation_Loss:4.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.661                                                   	MRR:22.32	Hits@10:38.75	Best:22.32
2024-12-27 22:23:34,162: Snapshot:2	Epoch:5	Loss:3.989	translation_Loss:3.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.767                                                   	MRR:22.44	Hits@10:38.88	Best:22.44
2024-12-27 22:23:41,677: Snapshot:2	Epoch:6	Loss:3.391	translation_Loss:2.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.855                                                   	MRR:22.53	Hits@10:38.96	Best:22.53
2024-12-27 22:23:49,147: Snapshot:2	Epoch:7	Loss:3.01	translation_Loss:2.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.924                                                   	MRR:22.52	Hits@10:38.96	Best:22.53
2024-12-27 22:23:56,588: Snapshot:2	Epoch:8	Loss:2.712	translation_Loss:1.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.979                                                   	MRR:22.53	Hits@10:39.18	Best:22.53
2024-12-27 22:24:04,080: Snapshot:2	Epoch:9	Loss:2.54	translation_Loss:1.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.023                                                   	MRR:22.57	Hits@10:39.09	Best:22.57
2024-12-27 22:24:11,545: Snapshot:2	Epoch:10	Loss:2.391	translation_Loss:1.335	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.056                                                   	MRR:22.45	Hits@10:39.23	Best:22.57
2024-12-27 22:24:18,958: Snapshot:2	Epoch:11	Loss:2.299	translation_Loss:1.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.082                                                   	MRR:22.42	Hits@10:39.25	Best:22.57
2024-12-27 22:24:26,371: Snapshot:2	Epoch:12	Loss:2.246	translation_Loss:1.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.106                                                   	MRR:22.5	Hits@10:39.39	Best:22.57
2024-12-27 22:24:33,817: Snapshot:2	Epoch:13	Loss:2.182	translation_Loss:1.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.125                                                   	MRR:22.45	Hits@10:39.34	Best:22.57
2024-12-27 22:24:41,275: Early Stopping! Snapshot: 2 Epoch: 14 Best Results: 22.57
2024-12-27 22:24:41,275: Start to training tokens! Snapshot: 2 Epoch: 14 Loss:2.141 MRR:22.38 Best Results: 22.57
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:24:41,275: Snapshot:2	Epoch:14	Loss:2.141	translation_Loss:0.999	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.141                                                   	MRR:22.38	Hits@10:39.17	Best:22.57
2024-12-27 22:24:48,522: Snapshot:2	Epoch:15	Loss:39.545	translation_Loss:38.54	multi_layer_Loss:1.006	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.38	Hits@10:39.17	Best:22.57
2024-12-27 22:24:55,768: End of token training: 2 Epoch: 16 Loss:38.798 MRR:22.38 Best Results: 22.57
2024-12-27 22:24:55,769: Snapshot:2	Epoch:16	Loss:38.798	translation_Loss:38.547	multi_layer_Loss:0.251	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.38	Hits@10:39.17	Best:22.57
2024-12-27 22:24:56,128: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_1000/2model_best.tar'
2024-12-27 22:25:05,403: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2327 | 0.1452 | 0.2684 | 0.3229 |  0.4007 |
|     1      | 0.2285 | 0.1431 | 0.2643 | 0.319  |  0.3948 |
|     2      | 0.2267 | 0.1392 | 0.264  | 0.3184 |  0.3937 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:25:31,635: Snapshot:3	Epoch:0	Loss:6.753	translation_Loss:6.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.077                                                   	MRR:19.6	Hits@10:37.39	Best:19.6
2024-12-27 22:25:39,176: Snapshot:3	Epoch:1	Loss:4.293	translation_Loss:4.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:20.05	Hits@10:38.44	Best:20.05
2024-12-27 22:25:46,883: Snapshot:3	Epoch:2	Loss:3.101	translation_Loss:2.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.276                                                   	MRR:20.46	Hits@10:38.98	Best:20.46
2024-12-27 22:25:54,376: Snapshot:3	Epoch:3	Loss:2.359	translation_Loss:2.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.341                                                   	MRR:20.67	Hits@10:39.36	Best:20.67
2024-12-27 22:26:01,901: Snapshot:3	Epoch:4	Loss:1.894	translation_Loss:1.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:20.76	Hits@10:39.47	Best:20.76
2024-12-27 22:26:09,435: Snapshot:3	Epoch:5	Loss:1.578	translation_Loss:1.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.429                                                   	MRR:20.79	Hits@10:39.44	Best:20.79
2024-12-27 22:26:17,009: Snapshot:3	Epoch:6	Loss:1.384	translation_Loss:0.929	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.456                                                   	MRR:20.84	Hits@10:39.5	Best:20.84
2024-12-27 22:26:24,512: Snapshot:3	Epoch:7	Loss:1.259	translation_Loss:0.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.477                                                   	MRR:20.77	Hits@10:39.67	Best:20.84
2024-12-27 22:26:31,967: Snapshot:3	Epoch:8	Loss:1.184	translation_Loss:0.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.493                                                   	MRR:20.78	Hits@10:39.49	Best:20.84
2024-12-27 22:26:39,514: Snapshot:3	Epoch:9	Loss:1.13	translation_Loss:0.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.507                                                   	MRR:20.76	Hits@10:39.43	Best:20.84
2024-12-27 22:26:47,014: Snapshot:3	Epoch:10	Loss:1.084	translation_Loss:0.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.518                                                   	MRR:20.71	Hits@10:39.65	Best:20.84
2024-12-27 22:26:54,462: Early Stopping! Snapshot: 3 Epoch: 11 Best Results: 20.84
2024-12-27 22:26:54,462: Start to training tokens! Snapshot: 3 Epoch: 11 Loss:1.058 MRR:20.79 Best Results: 20.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:26:54,463: Snapshot:3	Epoch:11	Loss:1.058	translation_Loss:0.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.528                                                   	MRR:20.79	Hits@10:39.47	Best:20.84
2024-12-27 22:27:01,784: Snapshot:3	Epoch:12	Loss:38.054	translation_Loss:37.079	multi_layer_Loss:0.975	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.79	Hits@10:39.47	Best:20.84
2024-12-27 22:27:09,094: End of token training: 3 Epoch: 13 Loss:37.352 MRR:20.79 Best Results: 20.84
2024-12-27 22:27:09,094: Snapshot:3	Epoch:13	Loss:37.352	translation_Loss:37.129	multi_layer_Loss:0.223	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.79	Hits@10:39.47	Best:20.84
2024-12-27 22:27:09,451: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_1000/3model_best.tar'
2024-12-27 22:27:21,764: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2131 | 0.1306 | 0.2434 | 0.2977 |  0.3723 |
|     1      | 0.2085 | 0.1257 | 0.2389 | 0.2937 |  0.3689 |
|     2      | 0.2125 | 0.1258 | 0.244  | 0.305  |  0.3854 |
|     3      | 0.2098 | 0.1156 | 0.2443 | 0.3076 |  0.3955 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:27:47,288: Snapshot:4	Epoch:0	Loss:4.099	translation_Loss:4.036	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.063                                                   	MRR:20.27	Hits@10:44.12	Best:20.27
2024-12-27 22:27:54,937: Snapshot:4	Epoch:1	Loss:2.696	translation_Loss:2.554	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.142                                                   	MRR:21.15	Hits@10:45.88	Best:21.15
2024-12-27 22:28:02,998: Snapshot:4	Epoch:2	Loss:1.92	translation_Loss:1.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:21.61	Hits@10:46.79	Best:21.61
2024-12-27 22:28:10,609: Snapshot:4	Epoch:3	Loss:1.367	translation_Loss:1.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:21.96	Hits@10:47.13	Best:21.96
2024-12-27 22:28:18,234: Snapshot:4	Epoch:4	Loss:0.995	translation_Loss:0.789	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.206                                                   	MRR:22.11	Hits@10:47.11	Best:22.11
2024-12-27 22:28:25,875: Snapshot:4	Epoch:5	Loss:0.753	translation_Loss:0.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.205                                                   	MRR:22.43	Hits@10:47.72	Best:22.43
2024-12-27 22:28:33,564: Snapshot:4	Epoch:6	Loss:0.603	translation_Loss:0.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.198                                                   	MRR:22.65	Hits@10:47.85	Best:22.65
2024-12-27 22:28:41,199: Snapshot:4	Epoch:7	Loss:0.497	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:22.71	Hits@10:47.67	Best:22.71
2024-12-27 22:28:48,814: Snapshot:4	Epoch:8	Loss:0.443	translation_Loss:0.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:22.73	Hits@10:47.72	Best:22.73
2024-12-27 22:28:56,409: Snapshot:4	Epoch:9	Loss:0.401	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:23.08	Hits@10:48.09	Best:23.08
2024-12-27 22:29:03,938: Snapshot:4	Epoch:10	Loss:0.374	translation_Loss:0.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:23.07	Hits@10:47.94	Best:23.08
2024-12-27 22:29:11,607: Snapshot:4	Epoch:11	Loss:0.354	translation_Loss:0.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:23.14	Hits@10:48.15	Best:23.14
2024-12-27 22:29:19,247: Snapshot:4	Epoch:12	Loss:0.349	translation_Loss:0.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:23.16	Hits@10:48.24	Best:23.16
2024-12-27 22:29:26,754: Snapshot:4	Epoch:13	Loss:0.334	translation_Loss:0.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:23.07	Hits@10:47.94	Best:23.16
2024-12-27 22:29:34,307: Snapshot:4	Epoch:14	Loss:0.324	translation_Loss:0.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:23.08	Hits@10:47.89	Best:23.16
2024-12-27 22:29:41,898: Snapshot:4	Epoch:15	Loss:0.316	translation_Loss:0.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:23.1	Hits@10:47.88	Best:23.16
2024-12-27 22:29:49,435: Snapshot:4	Epoch:16	Loss:0.3	translation_Loss:0.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:22.9	Hits@10:48.04	Best:23.16
2024-12-27 22:29:56,957: Early Stopping! Snapshot: 4 Epoch: 17 Best Results: 23.16
2024-12-27 22:29:56,957: Start to training tokens! Snapshot: 4 Epoch: 17 Loss:0.303 MRR:22.84 Best Results: 23.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:29:56,958: Snapshot:4	Epoch:17	Loss:0.303	translation_Loss:0.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:22.84	Hits@10:47.94	Best:23.16
2024-12-27 22:30:04,369: Snapshot:4	Epoch:18	Loss:32.086	translation_Loss:31.106	multi_layer_Loss:0.98	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.84	Hits@10:47.94	Best:23.16
2024-12-27 22:30:11,850: End of token training: 4 Epoch: 19 Loss:31.322 MRR:22.84 Best Results: 23.16
2024-12-27 22:30:11,850: Snapshot:4	Epoch:19	Loss:31.322	translation_Loss:31.106	multi_layer_Loss:0.216	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.84	Hits@10:47.94	Best:23.16
2024-12-27 22:30:12,211: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_1000/4model_best.tar'
2024-12-27 22:30:28,279: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1932 | 0.1148 | 0.2204 | 0.2719 |  0.3453 |
|     1      | 0.1867 | 0.1076 | 0.2156 | 0.2656 |  0.3381 |
|     2      | 0.1859 | 0.1028 | 0.2124 | 0.2706 |  0.3505 |
|     3      | 0.1844 | 0.0938 | 0.211  | 0.2784 |  0.3699 |
|     4      | 0.2291 | 0.1079 | 0.2703 | 0.3583 |  0.4778 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 22:30:28,281: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2344 | 0.147  | 0.2792 | 0.3286 |  0.3912 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2451 | 0.1532 | 0.2869 | 0.3457 |  0.4181 |
|     1      | 0.2337 | 0.1489 | 0.2743 | 0.3233 |  0.3867 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2327 | 0.1452 | 0.2684 | 0.3229 |  0.4007 |
|     1      | 0.2285 | 0.1431 | 0.2643 | 0.319  |  0.3948 |
|     2      | 0.2267 | 0.1392 | 0.264  | 0.3184 |  0.3937 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2131 | 0.1306 | 0.2434 | 0.2977 |  0.3723 |
|     1      | 0.2085 | 0.1257 | 0.2389 | 0.2937 |  0.3689 |
|     2      | 0.2125 | 0.1258 | 0.244  | 0.305  |  0.3854 |
|     3      | 0.2098 | 0.1156 | 0.2443 | 0.3076 |  0.3955 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1932 | 0.1148 | 0.2204 | 0.2719 |  0.3453 |
|     1      | 0.1867 | 0.1076 | 0.2156 | 0.2656 |  0.3381 |
|     2      | 0.1859 | 0.1028 | 0.2124 | 0.2706 |  0.3505 |
|     3      | 0.1844 | 0.0938 | 0.211  | 0.2784 |  0.3699 |
|     4      | 0.2291 | 0.1079 | 0.2703 | 0.3583 |  0.4778 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 22:30:28,282: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 477.7208032608032  |   0.234   |    0.147     |    0.279     |     0.391     |
|    1     | 226.60408282279968 |   0.239   |    0.151     |    0.281     |     0.402     |
|    2     | 141.4968023300171  |   0.229   |    0.142     |    0.266     |     0.396     |
|    3     | 120.00016379356384 |   0.211   |    0.124     |    0.243     |     0.381     |
|    4     | 166.62234234809875 |   0.196   |    0.105     |    0.226     |     0.376     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 22:30:28,282: Sum_Training_Time:1132.4441945552826
2024-12-27 22:30:28,282: Every_Training_Time:[477.7208032608032, 226.60408282279968, 141.4968023300171, 120.00016379356384, 166.62234234809875]
2024-12-27 22:30:28,282: Forward transfer: 0.176225 Backward transfer: -0.03859999999999999
2024-12-27 22:31:07,484: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227223032/FACTfact_0.0001_1024_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_1024_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_1024_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 22:31:17,547: Snapshot:0	Epoch:0	Loss:51.848	translation_Loss:51.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-27 22:31:24,093: Snapshot:0	Epoch:1	Loss:48.341	translation_Loss:48.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.35	Hits@10:5.97	Best:3.35
2024-12-27 22:31:30,586: Snapshot:0	Epoch:2	Loss:45.084	translation_Loss:45.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.51	Hits@10:8.56	Best:4.51
2024-12-27 22:31:37,095: Snapshot:0	Epoch:3	Loss:42.069	translation_Loss:42.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.4	Hits@10:10.8	Best:5.4
2024-12-27 22:31:44,181: Snapshot:0	Epoch:4	Loss:39.208	translation_Loss:39.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.07	Best:6.23
2024-12-27 22:31:50,786: Snapshot:0	Epoch:5	Loss:36.473	translation_Loss:36.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.09	Hits@10:15.44	Best:7.09
2024-12-27 22:31:57,317: Snapshot:0	Epoch:6	Loss:33.877	translation_Loss:33.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.92	Hits@10:17.7	Best:7.92
2024-12-27 22:32:03,841: Snapshot:0	Epoch:7	Loss:31.404	translation_Loss:31.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.84	Hits@10:19.88	Best:8.84
2024-12-27 22:32:10,376: Snapshot:0	Epoch:8	Loss:29.038	translation_Loss:29.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.74	Hits@10:22.02	Best:9.74
2024-12-27 22:32:16,898: Snapshot:0	Epoch:9	Loss:26.816	translation_Loss:26.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.66	Hits@10:23.89	Best:10.66
2024-12-27 22:32:23,427: Snapshot:0	Epoch:10	Loss:24.706	translation_Loss:24.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.54	Hits@10:25.58	Best:11.54
2024-12-27 22:32:29,940: Snapshot:0	Epoch:11	Loss:22.703	translation_Loss:22.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.32	Hits@10:27.19	Best:12.32
2024-12-27 22:32:36,484: Snapshot:0	Epoch:12	Loss:20.861	translation_Loss:20.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.05	Hits@10:28.58	Best:13.05
2024-12-27 22:32:43,019: Snapshot:0	Epoch:13	Loss:19.111	translation_Loss:19.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.7	Hits@10:29.75	Best:13.7
2024-12-27 22:32:49,524: Snapshot:0	Epoch:14	Loss:17.523	translation_Loss:17.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.37	Hits@10:30.88	Best:14.37
2024-12-27 22:32:56,042: Snapshot:0	Epoch:15	Loss:16.057	translation_Loss:16.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.99	Hits@10:31.87	Best:14.99
2024-12-27 22:33:02,551: Snapshot:0	Epoch:16	Loss:14.671	translation_Loss:14.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.6	Hits@10:32.7	Best:15.6
2024-12-27 22:33:09,105: Snapshot:0	Epoch:17	Loss:13.413	translation_Loss:13.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.11	Hits@10:33.54	Best:16.11
2024-12-27 22:33:15,694: Snapshot:0	Epoch:18	Loss:12.257	translation_Loss:12.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.72	Hits@10:34.29	Best:16.72
2024-12-27 22:33:22,235: Snapshot:0	Epoch:19	Loss:11.17	translation_Loss:11.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.3	Hits@10:34.95	Best:17.3
2024-12-27 22:33:28,752: Snapshot:0	Epoch:20	Loss:10.172	translation_Loss:10.172	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.8	Hits@10:35.46	Best:17.8
2024-12-27 22:33:35,291: Snapshot:0	Epoch:21	Loss:9.288	translation_Loss:9.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.29	Hits@10:35.95	Best:18.29
2024-12-27 22:33:41,911: Snapshot:0	Epoch:22	Loss:8.473	translation_Loss:8.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.76	Hits@10:36.45	Best:18.76
2024-12-27 22:33:48,450: Snapshot:0	Epoch:23	Loss:7.693	translation_Loss:7.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.24	Hits@10:36.8	Best:19.24
2024-12-27 22:33:55,520: Snapshot:0	Epoch:24	Loss:6.988	translation_Loss:6.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.63	Hits@10:37.24	Best:19.63
2024-12-27 22:34:02,041: Snapshot:0	Epoch:25	Loss:6.369	translation_Loss:6.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.04	Hits@10:37.49	Best:20.04
2024-12-27 22:34:08,582: Snapshot:0	Epoch:26	Loss:5.775	translation_Loss:5.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.4	Hits@10:37.77	Best:20.4
2024-12-27 22:34:15,099: Snapshot:0	Epoch:27	Loss:5.25	translation_Loss:5.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.7	Hits@10:38.05	Best:20.7
2024-12-27 22:34:21,716: Snapshot:0	Epoch:28	Loss:4.767	translation_Loss:4.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.03	Hits@10:38.23	Best:21.03
2024-12-27 22:34:28,251: Snapshot:0	Epoch:29	Loss:4.307	translation_Loss:4.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.32	Hits@10:38.42	Best:21.32
2024-12-27 22:34:34,807: Snapshot:0	Epoch:30	Loss:3.938	translation_Loss:3.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.6	Hits@10:38.54	Best:21.6
2024-12-27 22:34:41,343: Snapshot:0	Epoch:31	Loss:3.589	translation_Loss:3.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.77	Hits@10:38.62	Best:21.77
2024-12-27 22:34:47,906: Snapshot:0	Epoch:32	Loss:3.273	translation_Loss:3.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.98	Hits@10:38.73	Best:21.98
2024-12-27 22:34:54,442: Snapshot:0	Epoch:33	Loss:2.974	translation_Loss:2.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.22	Hits@10:38.95	Best:22.22
2024-12-27 22:35:00,994: Snapshot:0	Epoch:34	Loss:2.714	translation_Loss:2.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.44	Hits@10:39.02	Best:22.44
2024-12-27 22:35:08,147: Snapshot:0	Epoch:35	Loss:2.494	translation_Loss:2.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.65	Hits@10:39.22	Best:22.65
2024-12-27 22:35:14,708: Snapshot:0	Epoch:36	Loss:2.283	translation_Loss:2.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.8	Hits@10:39.25	Best:22.8
2024-12-27 22:35:21,282: Snapshot:0	Epoch:37	Loss:2.098	translation_Loss:2.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.89	Hits@10:39.21	Best:22.89
2024-12-27 22:35:27,833: Snapshot:0	Epoch:38	Loss:1.925	translation_Loss:1.925	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.05	Hits@10:39.39	Best:23.05
2024-12-27 22:35:34,348: Snapshot:0	Epoch:39	Loss:1.786	translation_Loss:1.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.11	Hits@10:39.4	Best:23.11
2024-12-27 22:35:40,965: Snapshot:0	Epoch:40	Loss:1.653	translation_Loss:1.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:39.44	Best:23.22
2024-12-27 22:35:47,541: Snapshot:0	Epoch:41	Loss:1.536	translation_Loss:1.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.3	Hits@10:39.49	Best:23.3
2024-12-27 22:35:54,081: Snapshot:0	Epoch:42	Loss:1.42	translation_Loss:1.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.44	Hits@10:39.59	Best:23.44
2024-12-27 22:36:00,585: Snapshot:0	Epoch:43	Loss:1.323	translation_Loss:1.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.44	Hits@10:39.56	Best:23.44
2024-12-27 22:36:07,189: Snapshot:0	Epoch:44	Loss:1.235	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.55	Hits@10:39.73	Best:23.55
2024-12-27 22:36:13,758: Snapshot:0	Epoch:45	Loss:1.166	translation_Loss:1.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.74	Best:23.58
2024-12-27 22:36:20,291: Snapshot:0	Epoch:46	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.75	Best:23.67
2024-12-27 22:36:26,825: Snapshot:0	Epoch:47	Loss:1.018	translation_Loss:1.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.73	Hits@10:39.87	Best:23.73
2024-12-27 22:36:33,363: Snapshot:0	Epoch:48	Loss:0.962	translation_Loss:0.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.77	Hits@10:39.87	Best:23.77
2024-12-27 22:36:39,996: Snapshot:0	Epoch:49	Loss:0.905	translation_Loss:0.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.85	Hits@10:39.88	Best:23.85
2024-12-27 22:36:46,534: Snapshot:0	Epoch:50	Loss:0.858	translation_Loss:0.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:39.93	Best:23.88
2024-12-27 22:36:53,091: Snapshot:0	Epoch:51	Loss:0.812	translation_Loss:0.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.9	Hits@10:39.9	Best:23.9
2024-12-27 22:36:59,644: Snapshot:0	Epoch:52	Loss:0.772	translation_Loss:0.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:39.79	Best:23.9
2024-12-27 22:37:06,208: Snapshot:0	Epoch:53	Loss:0.734	translation_Loss:0.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:39.82	Best:23.94
2024-12-27 22:37:13,336: Snapshot:0	Epoch:54	Loss:0.702	translation_Loss:0.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:39.96	Best:23.95
2024-12-27 22:37:19,940: Snapshot:0	Epoch:55	Loss:0.664	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:39.96	Best:23.98
2024-12-27 22:37:26,445: Snapshot:0	Epoch:56	Loss:0.648	translation_Loss:0.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:40.04	Best:23.98
2024-12-27 22:37:32,997: Snapshot:0	Epoch:57	Loss:0.618	translation_Loss:0.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.04	Hits@10:40.1	Best:24.04
2024-12-27 22:37:39,494: Snapshot:0	Epoch:58	Loss:0.599	translation_Loss:0.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.04	Hits@10:40.01	Best:24.04
2024-12-27 22:37:46,071: Snapshot:0	Epoch:59	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.04	Hits@10:40.05	Best:24.04
2024-12-27 22:37:52,624: Snapshot:0	Epoch:60	Loss:0.544	translation_Loss:0.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.07	Best:24.14
2024-12-27 22:37:59,160: Snapshot:0	Epoch:61	Loss:0.526	translation_Loss:0.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.09	Best:24.14
2024-12-27 22:38:05,679: Snapshot:0	Epoch:62	Loss:0.512	translation_Loss:0.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.09	Hits@10:40.07	Best:24.14
2024-12-27 22:38:12,209: Snapshot:0	Epoch:63	Loss:0.497	translation_Loss:0.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:40.08	Best:24.14
2024-12-27 22:38:19,288: Snapshot:0	Epoch:64	Loss:0.469	translation_Loss:0.469	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:40.0	Best:24.14
2024-12-27 22:38:25,825: Early Stopping! Snapshot: 0 Epoch: 65 Best Results: 24.14
2024-12-27 22:38:25,826: Start to training tokens! Snapshot: 0 Epoch: 65 Loss:0.458 MRR:24.09 Best Results: 24.14
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:38:25,826: Snapshot:0	Epoch:65	Loss:0.458	translation_Loss:0.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.09	Hits@10:39.98	Best:24.14
2024-12-27 22:38:32,956: Snapshot:0	Epoch:66	Loss:36.926	translation_Loss:35.931	multi_layer_Loss:0.995	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.09	Hits@10:39.98	Best:24.14
2024-12-27 22:38:39,552: End of token training: 0 Epoch: 67 Loss:36.145 MRR:24.09 Best Results: 24.14
2024-12-27 22:38:39,553: Snapshot:0	Epoch:67	Loss:36.145	translation_Loss:35.892	multi_layer_Loss:0.254	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.09	Hits@10:39.98	Best:24.14
2024-12-27 22:38:39,811: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_5000/0model_best.tar'
2024-12-27 22:38:42,813: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2344 | 0.147  | 0.2778 | 0.3287 |  0.3912 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:39:07,751: Snapshot:1	Epoch:0	Loss:24.509	translation_Loss:24.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.23                                                   	MRR:18.11	Hits@10:31.16	Best:18.11
2024-12-27 22:39:14,988: Snapshot:1	Epoch:1	Loss:20.996	translation_Loss:20.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.69                                                   	MRR:18.79	Hits@10:32.35	Best:18.79
2024-12-27 22:39:22,289: Snapshot:1	Epoch:2	Loss:18.737	translation_Loss:17.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.281                                                   	MRR:19.28	Hits@10:33.27	Best:19.28
2024-12-27 22:39:29,549: Snapshot:1	Epoch:3	Loss:17.046	translation_Loss:15.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.861                                                   	MRR:19.77	Hits@10:33.95	Best:19.77
2024-12-27 22:39:36,868: Snapshot:1	Epoch:4	Loss:15.745	translation_Loss:13.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.371                                                   	MRR:20.17	Hits@10:34.53	Best:20.17
2024-12-27 22:39:44,250: Snapshot:1	Epoch:5	Loss:14.657	translation_Loss:11.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.802                                                   	MRR:20.51	Hits@10:34.97	Best:20.51
2024-12-27 22:39:52,082: Snapshot:1	Epoch:6	Loss:13.718	translation_Loss:10.558	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.16                                                   	MRR:20.75	Hits@10:35.34	Best:20.75
2024-12-27 22:39:59,348: Snapshot:1	Epoch:7	Loss:13.008	translation_Loss:9.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.451                                                   	MRR:20.99	Hits@10:35.71	Best:20.99
2024-12-27 22:40:06,754: Snapshot:1	Epoch:8	Loss:12.405	translation_Loss:8.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.684                                                   	MRR:21.26	Hits@10:36.07	Best:21.26
2024-12-27 22:40:14,102: Snapshot:1	Epoch:9	Loss:11.94	translation_Loss:8.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.868                                                   	MRR:21.42	Hits@10:36.38	Best:21.42
2024-12-27 22:40:21,414: Snapshot:1	Epoch:10	Loss:11.543	translation_Loss:7.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.015                                                   	MRR:21.52	Hits@10:36.65	Best:21.52
2024-12-27 22:40:28,701: Snapshot:1	Epoch:11	Loss:11.274	translation_Loss:7.143	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.131                                                   	MRR:21.65	Hits@10:36.68	Best:21.65
2024-12-27 22:40:36,076: Snapshot:1	Epoch:12	Loss:11.013	translation_Loss:6.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.223                                                   	MRR:21.77	Hits@10:36.92	Best:21.77
2024-12-27 22:40:43,388: Snapshot:1	Epoch:13	Loss:10.872	translation_Loss:6.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.293                                                   	MRR:21.86	Hits@10:37.06	Best:21.86
2024-12-27 22:40:50,830: Snapshot:1	Epoch:14	Loss:10.735	translation_Loss:6.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.348                                                   	MRR:21.92	Hits@10:37.14	Best:21.92
2024-12-27 22:40:58,115: Snapshot:1	Epoch:15	Loss:10.626	translation_Loss:6.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.393                                                   	MRR:22.01	Hits@10:37.3	Best:22.01
2024-12-27 22:41:05,911: Snapshot:1	Epoch:16	Loss:10.525	translation_Loss:6.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.43                                                   	MRR:22.02	Hits@10:37.38	Best:22.02
2024-12-27 22:41:13,198: Snapshot:1	Epoch:17	Loss:10.469	translation_Loss:6.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.463                                                   	MRR:22.09	Hits@10:37.41	Best:22.09
2024-12-27 22:41:20,468: Snapshot:1	Epoch:18	Loss:10.427	translation_Loss:5.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.491                                                   	MRR:22.09	Hits@10:37.4	Best:22.09
2024-12-27 22:41:27,777: Snapshot:1	Epoch:19	Loss:10.374	translation_Loss:5.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.516                                                   	MRR:22.08	Hits@10:37.44	Best:22.09
2024-12-27 22:41:35,005: Snapshot:1	Epoch:20	Loss:10.312	translation_Loss:5.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.536                                                   	MRR:22.09	Hits@10:37.47	Best:22.09
2024-12-27 22:41:42,392: Snapshot:1	Epoch:21	Loss:10.309	translation_Loss:5.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.554                                                   	MRR:22.12	Hits@10:37.42	Best:22.12
2024-12-27 22:41:49,725: Snapshot:1	Epoch:22	Loss:10.277	translation_Loss:5.707	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.57                                                   	MRR:22.12	Hits@10:37.47	Best:22.12
2024-12-27 22:41:56,973: Snapshot:1	Epoch:23	Loss:10.239	translation_Loss:5.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.588                                                   	MRR:22.13	Hits@10:37.48	Best:22.13
2024-12-27 22:42:04,246: Snapshot:1	Epoch:24	Loss:10.195	translation_Loss:5.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.599                                                   	MRR:22.16	Hits@10:37.48	Best:22.16
2024-12-27 22:42:11,565: Snapshot:1	Epoch:25	Loss:10.178	translation_Loss:5.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.611                                                   	MRR:22.11	Hits@10:37.42	Best:22.16
2024-12-27 22:42:18,810: Snapshot:1	Epoch:26	Loss:10.15	translation_Loss:5.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.619                                                   	MRR:22.08	Hits@10:37.48	Best:22.16
2024-12-27 22:42:26,011: Snapshot:1	Epoch:27	Loss:10.134	translation_Loss:5.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.634                                                   	MRR:22.12	Hits@10:37.45	Best:22.16
2024-12-27 22:42:33,232: Snapshot:1	Epoch:28	Loss:10.118	translation_Loss:5.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.641                                                   	MRR:22.12	Hits@10:37.35	Best:22.16
2024-12-27 22:42:40,522: Early Stopping! Snapshot: 1 Epoch: 29 Best Results: 22.16
2024-12-27 22:42:40,522: Start to training tokens! Snapshot: 1 Epoch: 29 Loss:10.132 MRR:22.16 Best Results: 22.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:42:40,523: Snapshot:1	Epoch:29	Loss:10.132	translation_Loss:5.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:4.648                                                   	MRR:22.16	Hits@10:37.48	Best:22.16
2024-12-27 22:42:47,647: Snapshot:1	Epoch:30	Loss:41.59	translation_Loss:40.618	multi_layer_Loss:0.973	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.16	Hits@10:37.48	Best:22.16
2024-12-27 22:42:54,748: End of token training: 1 Epoch: 31 Loss:40.852 MRR:22.16 Best Results: 22.16
2024-12-27 22:42:54,749: Snapshot:1	Epoch:31	Loss:40.852	translation_Loss:40.622	multi_layer_Loss:0.23	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.16	Hits@10:37.48	Best:22.16
2024-12-27 22:42:55,100: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_5000/1model_best.tar'
2024-12-27 22:43:01,562: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2484 | 0.1572 | 0.2915 | 0.3478 |  0.4167 |
|     1      | 0.2219 | 0.1367 | 0.2642 | 0.3148 |  0.3765 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:43:26,543: Snapshot:2	Epoch:0	Loss:16.213	translation_Loss:15.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.23                                                   	MRR:20.04	Hits@10:35.34	Best:20.04
2024-12-27 22:43:34,434: Snapshot:2	Epoch:1	Loss:12.908	translation_Loss:12.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.638                                                   	MRR:20.64	Hits@10:36.31	Best:20.64
2024-12-27 22:43:41,965: Snapshot:2	Epoch:2	Loss:11.012	translation_Loss:9.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.124                                                   	MRR:20.95	Hits@10:36.92	Best:20.95
2024-12-27 22:43:49,567: Snapshot:2	Epoch:3	Loss:9.751	translation_Loss:8.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.581                                                   	MRR:21.31	Hits@10:37.32	Best:21.31
2024-12-27 22:43:57,101: Snapshot:2	Epoch:4	Loss:8.89	translation_Loss:6.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.971                                                   	MRR:21.44	Hits@10:37.66	Best:21.44
2024-12-27 22:44:04,611: Snapshot:2	Epoch:5	Loss:8.203	translation_Loss:5.922	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.281                                                   	MRR:21.62	Hits@10:38.01	Best:21.62
2024-12-27 22:44:12,168: Snapshot:2	Epoch:6	Loss:7.721	translation_Loss:5.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.524                                                   	MRR:21.81	Hits@10:38.18	Best:21.81
2024-12-27 22:44:19,766: Snapshot:2	Epoch:7	Loss:7.382	translation_Loss:4.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.713                                                   	MRR:21.92	Hits@10:38.33	Best:21.92
2024-12-27 22:44:27,225: Snapshot:2	Epoch:8	Loss:7.161	translation_Loss:4.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.86                                                   	MRR:22.02	Hits@10:38.5	Best:22.02
2024-12-27 22:44:34,732: Snapshot:2	Epoch:9	Loss:6.965	translation_Loss:3.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.972                                                   	MRR:22.0	Hits@10:38.61	Best:22.02
2024-12-27 22:44:42,225: Snapshot:2	Epoch:10	Loss:6.846	translation_Loss:3.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.058                                                   	MRR:22.03	Hits@10:38.54	Best:22.03
2024-12-27 22:44:49,793: Snapshot:2	Epoch:11	Loss:6.763	translation_Loss:3.635	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.128                                                   	MRR:22.16	Hits@10:38.66	Best:22.16
2024-12-27 22:44:57,227: Snapshot:2	Epoch:12	Loss:6.713	translation_Loss:3.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.184                                                   	MRR:22.12	Hits@10:38.67	Best:22.16
2024-12-27 22:45:04,719: Snapshot:2	Epoch:13	Loss:6.649	translation_Loss:3.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.232                                                   	MRR:22.14	Hits@10:38.79	Best:22.16
2024-12-27 22:45:12,222: Snapshot:2	Epoch:14	Loss:6.619	translation_Loss:3.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.267                                                   	MRR:22.12	Hits@10:38.74	Best:22.16
2024-12-27 22:45:19,718: Snapshot:2	Epoch:15	Loss:6.564	translation_Loss:3.275	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.289                                                   	MRR:22.18	Hits@10:38.77	Best:22.18
2024-12-27 22:45:27,141: Snapshot:2	Epoch:16	Loss:6.567	translation_Loss:3.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.322                                                   	MRR:22.13	Hits@10:38.82	Best:22.18
2024-12-27 22:45:34,542: Snapshot:2	Epoch:17	Loss:6.535	translation_Loss:3.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.342                                                   	MRR:22.14	Hits@10:38.88	Best:22.18
2024-12-27 22:45:42,007: Snapshot:2	Epoch:18	Loss:6.551	translation_Loss:3.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.367                                                   	MRR:22.12	Hits@10:38.81	Best:22.18
2024-12-27 22:45:49,475: Snapshot:2	Epoch:19	Loss:6.518	translation_Loss:3.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.385                                                   	MRR:22.16	Hits@10:38.73	Best:22.18
2024-12-27 22:45:56,928: Snapshot:2	Epoch:20	Loss:6.512	translation_Loss:3.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.394                                                   	MRR:22.23	Hits@10:38.76	Best:22.23
2024-12-27 22:46:04,948: Snapshot:2	Epoch:21	Loss:6.522	translation_Loss:3.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.41                                                   	MRR:22.18	Hits@10:38.8	Best:22.23
2024-12-27 22:46:12,483: Snapshot:2	Epoch:22	Loss:6.497	translation_Loss:3.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.425                                                   	MRR:22.12	Hits@10:38.79	Best:22.23
2024-12-27 22:46:19,907: Snapshot:2	Epoch:23	Loss:6.476	translation_Loss:3.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.431                                                   	MRR:22.13	Hits@10:38.81	Best:22.23
2024-12-27 22:46:27,315: Snapshot:2	Epoch:24	Loss:6.491	translation_Loss:3.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.439                                                   	MRR:22.19	Hits@10:38.74	Best:22.23
2024-12-27 22:46:34,735: Early Stopping! Snapshot: 2 Epoch: 25 Best Results: 22.23
2024-12-27 22:46:34,735: Start to training tokens! Snapshot: 2 Epoch: 25 Loss:6.493 MRR:22.14 Best Results: 22.23
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:46:34,735: Snapshot:2	Epoch:25	Loss:6.493	translation_Loss:3.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.45                                                   	MRR:22.14	Hits@10:38.84	Best:22.23
2024-12-27 22:46:42,082: Snapshot:2	Epoch:26	Loss:40.62	translation_Loss:39.614	multi_layer_Loss:1.006	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.14	Hits@10:38.84	Best:22.23
2024-12-27 22:46:49,408: End of token training: 2 Epoch: 27 Loss:39.839 MRR:22.14 Best Results: 22.23
2024-12-27 22:46:49,408: Snapshot:2	Epoch:27	Loss:39.839	translation_Loss:39.589	multi_layer_Loss:0.251	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.14	Hits@10:38.84	Best:22.23
2024-12-27 22:46:49,759: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_5000/2model_best.tar'
2024-12-27 22:46:58,931: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2439 | 0.1544 | 0.2825 | 0.3405 |  0.4135 |
|     1      | 0.2271 | 0.1421 | 0.2624 | 0.3187 |  0.3938 |
|     2      | 0.2225 | 0.135  | 0.2607 | 0.3151 |  0.389  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:47:24,466: Snapshot:3	Epoch:0	Loss:8.602	translation_Loss:8.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.222                                                   	MRR:19.54	Hits@10:37.21	Best:19.54
2024-12-27 22:47:32,046: Snapshot:3	Epoch:1	Loss:6.093	translation_Loss:5.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.521                                                   	MRR:20.1	Hits@10:38.05	Best:20.1
2024-12-27 22:47:39,622: Snapshot:3	Epoch:2	Loss:5.009	translation_Loss:4.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.789                                                   	MRR:20.27	Hits@10:38.68	Best:20.27
2024-12-27 22:47:47,326: Snapshot:3	Epoch:3	Loss:4.343	translation_Loss:3.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.007                                                   	MRR:20.42	Hits@10:38.99	Best:20.42
2024-12-27 22:47:55,373: Snapshot:3	Epoch:4	Loss:3.905	translation_Loss:2.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.172                                                   	MRR:20.59	Hits@10:39.3	Best:20.59
2024-12-27 22:48:02,932: Snapshot:3	Epoch:5	Loss:3.641	translation_Loss:2.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.296                                                   	MRR:20.66	Hits@10:39.44	Best:20.66
2024-12-27 22:48:10,493: Snapshot:3	Epoch:6	Loss:3.482	translation_Loss:2.092	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.39                                                   	MRR:20.72	Hits@10:39.37	Best:20.72
2024-12-27 22:48:18,092: Snapshot:3	Epoch:7	Loss:3.353	translation_Loss:1.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.455                                                   	MRR:20.8	Hits@10:39.43	Best:20.8
2024-12-27 22:48:25,565: Snapshot:3	Epoch:8	Loss:3.279	translation_Loss:1.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.502                                                   	MRR:20.75	Hits@10:39.34	Best:20.8
2024-12-27 22:48:33,164: Snapshot:3	Epoch:9	Loss:3.258	translation_Loss:1.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.547                                                   	MRR:20.75	Hits@10:39.33	Best:20.8
2024-12-27 22:48:40,746: Snapshot:3	Epoch:10	Loss:3.22	translation_Loss:1.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.579                                                   	MRR:20.75	Hits@10:39.26	Best:20.8
2024-12-27 22:48:48,263: Snapshot:3	Epoch:11	Loss:3.19	translation_Loss:1.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.603                                                   	MRR:20.69	Hits@10:39.53	Best:20.8
2024-12-27 22:48:55,734: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 20.8
2024-12-27 22:48:55,734: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:3.193 MRR:20.8 Best Results: 20.8
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:48:55,735: Snapshot:3	Epoch:12	Loss:3.193	translation_Loss:1.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.622                                                   	MRR:20.8	Hits@10:39.69	Best:20.8
2024-12-27 22:49:03,067: Snapshot:3	Epoch:13	Loss:38.946	translation_Loss:37.971	multi_layer_Loss:0.975	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.8	Hits@10:39.69	Best:20.8
2024-12-27 22:49:10,895: End of token training: 3 Epoch: 14 Loss:38.161 MRR:20.8 Best Results: 20.8
2024-12-27 22:49:10,895: Snapshot:3	Epoch:14	Loss:38.161	translation_Loss:37.938	multi_layer_Loss:0.223	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.8	Hits@10:39.69	Best:20.8
2024-12-27 22:49:11,167: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_5000/3model_best.tar'
2024-12-27 22:49:23,829: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2315 | 0.1464 | 0.2647 | 0.3198 |  0.3948 |
|     1      | 0.2174 | 0.1333 | 0.2506 | 0.3038 |  0.3807 |
|     2      | 0.2155 | 0.1268 | 0.2497 | 0.3104 |  0.3932 |
|     3      | 0.2099 | 0.1153 | 0.2458 | 0.3111 |  0.397  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 22:49:49,421: Snapshot:4	Epoch:0	Loss:5.01	translation_Loss:4.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:20.14	Hits@10:44.61	Best:20.14
2024-12-27 22:49:57,037: Snapshot:4	Epoch:1	Loss:3.381	translation_Loss:3.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.293                                                   	MRR:21.09	Hits@10:46.89	Best:21.09
2024-12-27 22:50:04,685: Snapshot:4	Epoch:2	Loss:2.614	translation_Loss:2.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.362                                                   	MRR:21.35	Hits@10:47.11	Best:21.35
2024-12-27 22:50:12,408: Snapshot:4	Epoch:3	Loss:2.047	translation_Loss:1.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.415                                                   	MRR:21.6	Hits@10:47.5	Best:21.6
2024-12-27 22:50:20,033: Snapshot:4	Epoch:4	Loss:1.654	translation_Loss:1.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.451                                                   	MRR:21.67	Hits@10:47.44	Best:21.67
2024-12-27 22:50:27,592: Snapshot:4	Epoch:5	Loss:1.379	translation_Loss:0.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.473                                                   	MRR:22.14	Hits@10:47.71	Best:22.14
2024-12-27 22:50:35,235: Snapshot:4	Epoch:6	Loss:1.204	translation_Loss:0.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.485                                                   	MRR:22.43	Hits@10:47.97	Best:22.43
2024-12-27 22:50:43,358: Snapshot:4	Epoch:7	Loss:1.109	translation_Loss:0.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.49                                                   	MRR:22.51	Hits@10:48.27	Best:22.51
2024-12-27 22:50:51,019: Snapshot:4	Epoch:8	Loss:1.048	translation_Loss:0.551	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.497                                                   	MRR:22.55	Hits@10:48.23	Best:22.55
2024-12-27 22:50:58,636: Snapshot:4	Epoch:9	Loss:0.999	translation_Loss:0.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.499                                                   	MRR:22.63	Hits@10:48.07	Best:22.63
2024-12-27 22:51:06,364: Snapshot:4	Epoch:10	Loss:0.974	translation_Loss:0.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.5                                                   	MRR:22.65	Hits@10:48.14	Best:22.65
2024-12-27 22:51:13,910: Snapshot:4	Epoch:11	Loss:0.967	translation_Loss:0.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.502                                                   	MRR:22.55	Hits@10:47.92	Best:22.65
2024-12-27 22:51:21,495: Snapshot:4	Epoch:12	Loss:0.949	translation_Loss:0.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.506                                                   	MRR:22.88	Hits@10:48.47	Best:22.88
2024-12-27 22:51:29,112: Snapshot:4	Epoch:13	Loss:0.933	translation_Loss:0.423	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.51                                                   	MRR:22.99	Hits@10:48.66	Best:22.99
2024-12-27 22:51:36,707: Snapshot:4	Epoch:14	Loss:0.931	translation_Loss:0.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.51                                                   	MRR:23.04	Hits@10:48.88	Best:23.04
2024-12-27 22:51:44,329: Snapshot:4	Epoch:15	Loss:0.921	translation_Loss:0.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.51                                                   	MRR:22.89	Hits@10:48.48	Best:23.04
2024-12-27 22:51:51,853: Snapshot:4	Epoch:16	Loss:0.925	translation_Loss:0.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.512                                                   	MRR:22.97	Hits@10:48.37	Best:23.04
2024-12-27 22:51:59,380: Snapshot:4	Epoch:17	Loss:0.914	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.513                                                   	MRR:22.85	Hits@10:48.66	Best:23.04
2024-12-27 22:52:06,884: Snapshot:4	Epoch:18	Loss:0.913	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.512                                                   	MRR:22.91	Hits@10:48.71	Best:23.04
2024-12-27 22:52:14,429: Early Stopping! Snapshot: 4 Epoch: 19 Best Results: 23.04
2024-12-27 22:52:14,429: Start to training tokens! Snapshot: 4 Epoch: 19 Loss:0.922 MRR:22.71 Best Results: 23.04
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 22:52:14,429: Snapshot:4	Epoch:19	Loss:0.922	translation_Loss:0.409	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.513                                                   	MRR:22.71	Hits@10:48.52	Best:23.04
2024-12-27 22:52:21,794: Snapshot:4	Epoch:20	Loss:33.186	translation_Loss:32.206	multi_layer_Loss:0.98	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.71	Hits@10:48.52	Best:23.04
2024-12-27 22:52:29,155: End of token training: 4 Epoch: 21 Loss:32.415 MRR:22.71 Best Results: 23.04
2024-12-27 22:52:29,156: Snapshot:4	Epoch:21	Loss:32.415	translation_Loss:32.199	multi_layer_Loss:0.216	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.71	Hits@10:48.52	Best:23.04
2024-12-27 22:52:29,506: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_5000/4model_best.tar'
2024-12-27 22:52:46,031: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2147 | 0.1309 | 0.2465 | 0.3029 |  0.3769 |
|     1      | 0.2003 | 0.1198 | 0.2273 | 0.2831 |  0.3587 |
|     2      | 0.1983 | 0.113  | 0.2267 | 0.287  |  0.371  |
|     3      | 0.1927 | 0.0994 | 0.2235 | 0.2904 |  0.3825 |
|     4      | 0.2296 | 0.1081 | 0.2697 | 0.3605 |  0.4832 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 22:52:46,033: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2344 | 0.147  | 0.2778 | 0.3287 |  0.3912 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2484 | 0.1572 | 0.2915 | 0.3478 |  0.4167 |
|     1      | 0.2219 | 0.1367 | 0.2642 | 0.3148 |  0.3765 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2439 | 0.1544 | 0.2825 | 0.3405 |  0.4135 |
|     1      | 0.2271 | 0.1421 | 0.2624 | 0.3187 |  0.3938 |
|     2      | 0.2225 | 0.135  | 0.2607 | 0.3151 |  0.389  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2315 | 0.1464 | 0.2647 | 0.3198 |  0.3948 |
|     1      | 0.2174 | 0.1333 | 0.2506 | 0.3038 |  0.3807 |
|     2      | 0.2155 | 0.1268 | 0.2497 | 0.3104 |  0.3932 |
|     3      | 0.2099 | 0.1153 | 0.2458 | 0.3111 |  0.397  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2147 | 0.1309 | 0.2465 | 0.3029 |  0.3769 |
|     1      | 0.2003 | 0.1198 | 0.2273 | 0.2831 |  0.3587 |
|     2      | 0.1983 | 0.113  | 0.2267 | 0.287  |  0.371  |
|     3      | 0.1927 | 0.0994 | 0.2235 | 0.2904 |  0.3825 |
|     4      | 0.2296 | 0.1081 | 0.2697 | 0.3605 |  0.4832 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 22:52:46,033: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 452.06762862205505 |   0.234   |    0.147     |    0.278     |     0.391     |
|    1     | 248.7820439338684  |   0.235   |    0.147     |    0.278     |     0.397     |
|    2     | 224.84859490394592 |   0.231   |    0.144     |    0.269     |     0.399     |
|    3     | 128.28751945495605 |   0.219   |     0.13     |    0.253     |     0.391     |
|    4     | 181.83856081962585 |   0.207   |    0.114     |    0.239     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 22:52:46,033: Sum_Training_Time:1235.8243477344513
2024-12-27 22:52:46,033: Every_Training_Time:[452.06762862205505, 248.7820439338684, 224.84859490394592, 128.28751945495605, 181.83856081962585]
2024-12-27 22:52:46,033: Forward transfer: 0.17485 Backward transfer: -0.020674999999999992
2024-12-27 22:53:25,022: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227225249/FACTfact_0.0001_1024_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_1024_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_1024_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 22:53:35,049: Snapshot:0	Epoch:0	Loss:51.848	translation_Loss:51.848	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-27 22:53:41,610: Snapshot:0	Epoch:1	Loss:48.341	translation_Loss:48.341	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.35	Hits@10:5.97	Best:3.35
2024-12-27 22:53:48,145: Snapshot:0	Epoch:2	Loss:45.084	translation_Loss:45.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.51	Hits@10:8.56	Best:4.51
2024-12-27 22:53:54,659: Snapshot:0	Epoch:3	Loss:42.069	translation_Loss:42.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.4	Hits@10:10.8	Best:5.4
2024-12-27 22:54:01,656: Snapshot:0	Epoch:4	Loss:39.208	translation_Loss:39.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.07	Best:6.23
2024-12-27 22:54:08,179: Snapshot:0	Epoch:5	Loss:36.473	translation_Loss:36.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.09	Hits@10:15.45	Best:7.09
2024-12-27 22:54:14,707: Snapshot:0	Epoch:6	Loss:33.877	translation_Loss:33.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.92	Hits@10:17.71	Best:7.92
2024-12-27 22:54:21,218: Snapshot:0	Epoch:7	Loss:31.404	translation_Loss:31.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.84	Hits@10:19.87	Best:8.84
2024-12-27 22:54:27,740: Snapshot:0	Epoch:8	Loss:29.038	translation_Loss:29.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.75	Hits@10:22.0	Best:9.75
2024-12-27 22:54:34,263: Snapshot:0	Epoch:9	Loss:26.816	translation_Loss:26.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.65	Hits@10:23.89	Best:10.65
2024-12-27 22:54:40,824: Snapshot:0	Epoch:10	Loss:24.706	translation_Loss:24.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.54	Hits@10:25.57	Best:11.54
2024-12-27 22:54:47,375: Snapshot:0	Epoch:11	Loss:22.703	translation_Loss:22.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.19	Best:12.31
2024-12-27 22:54:53,882: Snapshot:0	Epoch:12	Loss:20.861	translation_Loss:20.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.05	Hits@10:28.56	Best:13.05
2024-12-27 22:55:00,424: Snapshot:0	Epoch:13	Loss:19.111	translation_Loss:19.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.69	Hits@10:29.77	Best:13.69
2024-12-27 22:55:07,022: Snapshot:0	Epoch:14	Loss:17.523	translation_Loss:17.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.36	Hits@10:30.84	Best:14.36
2024-12-27 22:55:13,568: Snapshot:0	Epoch:15	Loss:16.058	translation_Loss:16.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.98	Hits@10:31.85	Best:14.98
2024-12-27 22:55:20,107: Snapshot:0	Epoch:16	Loss:14.672	translation_Loss:14.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.6	Hits@10:32.72	Best:15.6
2024-12-27 22:55:26,605: Snapshot:0	Epoch:17	Loss:13.413	translation_Loss:13.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.11	Hits@10:33.54	Best:16.11
2024-12-27 22:55:33,151: Snapshot:0	Epoch:18	Loss:12.257	translation_Loss:12.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.7	Hits@10:34.33	Best:16.7
2024-12-27 22:55:39,708: Snapshot:0	Epoch:19	Loss:11.171	translation_Loss:11.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.27	Hits@10:34.98	Best:17.27
2024-12-27 22:55:46,325: Snapshot:0	Epoch:20	Loss:10.173	translation_Loss:10.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.8	Hits@10:35.43	Best:17.8
2024-12-27 22:55:52,829: Snapshot:0	Epoch:21	Loss:9.289	translation_Loss:9.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.29	Hits@10:35.91	Best:18.29
2024-12-27 22:55:59,353: Snapshot:0	Epoch:22	Loss:8.475	translation_Loss:8.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.75	Hits@10:36.43	Best:18.75
2024-12-27 22:56:05,895: Snapshot:0	Epoch:23	Loss:7.694	translation_Loss:7.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.21	Hits@10:36.87	Best:19.21
2024-12-27 22:56:12,996: Snapshot:0	Epoch:24	Loss:6.989	translation_Loss:6.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.64	Hits@10:37.27	Best:19.64
2024-12-27 22:56:19,551: Snapshot:0	Epoch:25	Loss:6.37	translation_Loss:6.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.04	Hits@10:37.52	Best:20.04
2024-12-27 22:56:26,062: Snapshot:0	Epoch:26	Loss:5.775	translation_Loss:5.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.38	Hits@10:37.84	Best:20.38
2024-12-27 22:56:32,588: Snapshot:0	Epoch:27	Loss:5.252	translation_Loss:5.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.69	Hits@10:38.0	Best:20.69
2024-12-27 22:56:39,166: Snapshot:0	Epoch:28	Loss:4.767	translation_Loss:4.767	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.99	Hits@10:38.16	Best:20.99
2024-12-27 22:56:45,668: Snapshot:0	Epoch:29	Loss:4.307	translation_Loss:4.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.31	Hits@10:38.42	Best:21.31
2024-12-27 22:56:52,183: Snapshot:0	Epoch:30	Loss:3.938	translation_Loss:3.938	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.55	Hits@10:38.54	Best:21.55
2024-12-27 22:56:58,738: Snapshot:0	Epoch:31	Loss:3.587	translation_Loss:3.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.8	Hits@10:38.71	Best:21.8
2024-12-27 22:57:05,256: Snapshot:0	Epoch:32	Loss:3.273	translation_Loss:3.273	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.03	Hits@10:38.84	Best:22.03
2024-12-27 22:57:11,783: Snapshot:0	Epoch:33	Loss:2.975	translation_Loss:2.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.2	Hits@10:38.96	Best:22.2
2024-12-27 22:57:18,327: Snapshot:0	Epoch:34	Loss:2.715	translation_Loss:2.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.43	Hits@10:39.14	Best:22.43
2024-12-27 22:57:25,389: Snapshot:0	Epoch:35	Loss:2.495	translation_Loss:2.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.61	Hits@10:39.24	Best:22.61
2024-12-27 22:57:31,908: Snapshot:0	Epoch:36	Loss:2.283	translation_Loss:2.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.78	Hits@10:39.39	Best:22.78
2024-12-27 22:57:38,431: Snapshot:0	Epoch:37	Loss:2.099	translation_Loss:2.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.88	Hits@10:39.35	Best:22.88
2024-12-27 22:57:45,054: Snapshot:0	Epoch:38	Loss:1.924	translation_Loss:1.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.03	Hits@10:39.47	Best:23.03
2024-12-27 22:57:51,554: Snapshot:0	Epoch:39	Loss:1.787	translation_Loss:1.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.12	Hits@10:39.51	Best:23.12
2024-12-27 22:57:58,069: Snapshot:0	Epoch:40	Loss:1.653	translation_Loss:1.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.21	Hits@10:39.45	Best:23.21
2024-12-27 22:58:04,631: Snapshot:0	Epoch:41	Loss:1.537	translation_Loss:1.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.28	Hits@10:39.5	Best:23.28
2024-12-27 22:58:11,218: Snapshot:0	Epoch:42	Loss:1.419	translation_Loss:1.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.43	Hits@10:39.66	Best:23.43
2024-12-27 22:58:17,798: Snapshot:0	Epoch:43	Loss:1.322	translation_Loss:1.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.47	Hits@10:39.7	Best:23.47
2024-12-27 22:58:24,395: Snapshot:0	Epoch:44	Loss:1.237	translation_Loss:1.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.61	Hits@10:39.7	Best:23.61
2024-12-27 22:58:30,925: Snapshot:0	Epoch:45	Loss:1.165	translation_Loss:1.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.79	Best:23.65
2024-12-27 22:58:37,479: Snapshot:0	Epoch:46	Loss:1.08	translation_Loss:1.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.65	Hits@10:39.79	Best:23.65
2024-12-27 22:58:43,990: Snapshot:0	Epoch:47	Loss:1.02	translation_Loss:1.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.86	Best:23.71
2024-12-27 22:58:50,544: Snapshot:0	Epoch:48	Loss:0.963	translation_Loss:0.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:39.89	Best:23.75
2024-12-27 22:58:57,113: Snapshot:0	Epoch:49	Loss:0.902	translation_Loss:0.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:39.85	Best:23.87
2024-12-27 22:59:03,605: Snapshot:0	Epoch:50	Loss:0.858	translation_Loss:0.858	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.93	Hits@10:39.83	Best:23.93
2024-12-27 22:59:10,174: Snapshot:0	Epoch:51	Loss:0.811	translation_Loss:0.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:39.83	Best:23.95
2024-12-27 22:59:16,709: Snapshot:0	Epoch:52	Loss:0.773	translation_Loss:0.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.93	Hits@10:39.81	Best:23.95
2024-12-27 22:59:23,263: Snapshot:0	Epoch:53	Loss:0.732	translation_Loss:0.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:39.91	Best:23.96
2024-12-27 22:59:30,283: Snapshot:0	Epoch:54	Loss:0.702	translation_Loss:0.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:40.04	Best:23.96
2024-12-27 22:59:36,791: Snapshot:0	Epoch:55	Loss:0.664	translation_Loss:0.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:40.08	Best:24.0
2024-12-27 22:59:43,374: Snapshot:0	Epoch:56	Loss:0.646	translation_Loss:0.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:40.09	Best:24.0
2024-12-27 22:59:49,925: Snapshot:0	Epoch:57	Loss:0.619	translation_Loss:0.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.03	Hits@10:40.07	Best:24.03
2024-12-27 22:59:56,422: Snapshot:0	Epoch:58	Loss:0.599	translation_Loss:0.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.04	Hits@10:40.04	Best:24.04
2024-12-27 23:00:02,931: Snapshot:0	Epoch:59	Loss:0.565	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.03	Hits@10:40.06	Best:24.04
2024-12-27 23:00:09,568: Snapshot:0	Epoch:60	Loss:0.543	translation_Loss:0.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:40.1	Best:24.12
2024-12-27 23:00:16,073: Snapshot:0	Epoch:61	Loss:0.527	translation_Loss:0.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.07	Hits@10:40.06	Best:24.12
2024-12-27 23:00:22,573: Snapshot:0	Epoch:62	Loss:0.512	translation_Loss:0.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.1	Best:24.16
2024-12-27 23:00:29,101: Snapshot:0	Epoch:63	Loss:0.496	translation_Loss:0.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.13	Best:24.16
2024-12-27 23:00:36,153: Snapshot:0	Epoch:64	Loss:0.468	translation_Loss:0.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.15	Hits@10:40.17	Best:24.16
2024-12-27 23:00:42,677: Snapshot:0	Epoch:65	Loss:0.457	translation_Loss:0.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:40.08	Best:24.16
2024-12-27 23:00:49,179: Snapshot:0	Epoch:66	Loss:0.452	translation_Loss:0.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.19	Hits@10:40.06	Best:24.19
2024-12-27 23:00:55,669: Snapshot:0	Epoch:67	Loss:0.443	translation_Loss:0.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:40.18	Best:24.19
2024-12-27 23:01:02,153: Snapshot:0	Epoch:68	Loss:0.416	translation_Loss:0.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.19	Hits@10:40.26	Best:24.19
2024-12-27 23:01:08,731: Snapshot:0	Epoch:69	Loss:0.407	translation_Loss:0.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.26	Hits@10:40.21	Best:24.26
2024-12-27 23:01:15,249: Snapshot:0	Epoch:70	Loss:0.404	translation_Loss:0.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.27	Hits@10:40.24	Best:24.27
2024-12-27 23:01:21,793: Snapshot:0	Epoch:71	Loss:0.389	translation_Loss:0.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.3	Hits@10:40.21	Best:24.3
2024-12-27 23:01:28,343: Snapshot:0	Epoch:72	Loss:0.374	translation_Loss:0.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.29	Hits@10:40.15	Best:24.3
2024-12-27 23:01:34,862: Snapshot:0	Epoch:73	Loss:0.372	translation_Loss:0.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.35	Hits@10:40.21	Best:24.35
2024-12-27 23:01:41,415: Snapshot:0	Epoch:74	Loss:0.359	translation_Loss:0.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:40.29	Best:24.36
2024-12-27 23:01:47,961: Snapshot:0	Epoch:75	Loss:0.354	translation_Loss:0.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:40.17	Best:24.36
2024-12-27 23:01:54,445: Snapshot:0	Epoch:76	Loss:0.345	translation_Loss:0.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:40.27	Best:24.36
2024-12-27 23:02:00,977: Snapshot:0	Epoch:77	Loss:0.343	translation_Loss:0.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.3	Hits@10:40.17	Best:24.36
2024-12-27 23:02:07,446: Snapshot:0	Epoch:78	Loss:0.322	translation_Loss:0.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:40.21	Best:24.36
2024-12-27 23:02:13,981: Snapshot:0	Epoch:79	Loss:0.323	translation_Loss:0.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:40.17	Best:24.37
2024-12-27 23:02:20,483: Snapshot:0	Epoch:80	Loss:0.321	translation_Loss:0.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.35	Hits@10:40.14	Best:24.37
2024-12-27 23:02:27,014: Snapshot:0	Epoch:81	Loss:0.314	translation_Loss:0.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:40.15	Best:24.38
2024-12-27 23:02:33,517: Snapshot:0	Epoch:82	Loss:0.311	translation_Loss:0.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.28	Hits@10:40.15	Best:24.38
2024-12-27 23:02:40,081: Snapshot:0	Epoch:83	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:40.15	Best:24.38
2024-12-27 23:02:47,104: Snapshot:0	Epoch:84	Loss:0.29	translation_Loss:0.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.31	Hits@10:40.14	Best:24.38
2024-12-27 23:02:53,614: Snapshot:0	Epoch:85	Loss:0.286	translation_Loss:0.286	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.34	Hits@10:40.21	Best:24.38
2024-12-27 23:03:00,142: Snapshot:0	Epoch:86	Loss:0.28	translation_Loss:0.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.39	Hits@10:40.19	Best:24.39
2024-12-27 23:03:06,649: Snapshot:0	Epoch:87	Loss:0.28	translation_Loss:0.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.43	Hits@10:40.23	Best:24.43
2024-12-27 23:03:13,201: Snapshot:0	Epoch:88	Loss:0.272	translation_Loss:0.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.48	Hits@10:40.27	Best:24.48
2024-12-27 23:03:19,732: Snapshot:0	Epoch:89	Loss:0.268	translation_Loss:0.268	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:40.17	Best:24.48
2024-12-27 23:03:26,220: Snapshot:0	Epoch:90	Loss:0.267	translation_Loss:0.267	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.4	Hits@10:40.13	Best:24.48
2024-12-27 23:03:32,769: Snapshot:0	Epoch:91	Loss:0.259	translation_Loss:0.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:40.26	Best:24.48
2024-12-27 23:03:39,290: Snapshot:0	Epoch:92	Loss:0.255	translation_Loss:0.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.37	Hits@10:40.13	Best:24.48
2024-12-27 23:03:45,841: Early Stopping! Snapshot: 0 Epoch: 93 Best Results: 24.48
2024-12-27 23:03:45,841: Start to training tokens! Snapshot: 0 Epoch: 93 Loss:0.25 MRR:24.39 Best Results: 24.48
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:03:45,842: Snapshot:0	Epoch:93	Loss:0.25	translation_Loss:0.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.39	Hits@10:40.02	Best:24.48
2024-12-27 23:03:53,438: Snapshot:0	Epoch:94	Loss:36.957	translation_Loss:35.962	multi_layer_Loss:0.995	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.39	Hits@10:40.02	Best:24.48
2024-12-27 23:04:00,038: End of token training: 0 Epoch: 95 Loss:36.17 MRR:24.39 Best Results: 24.48
2024-12-27 23:04:00,039: Snapshot:0	Epoch:95	Loss:36.17	translation_Loss:35.916	multi_layer_Loss:0.254	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.39	Hits@10:40.02	Best:24.48
2024-12-27 23:04:00,311: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_10000/0model_best.tar'
2024-12-27 23:04:03,367: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2374 | 0.1518 | 0.2805 | 0.3273 |   0.39  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:04:28,132: Snapshot:1	Epoch:0	Loss:22.783	translation_Loss:22.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:18.09	Hits@10:30.91	Best:18.09
2024-12-27 23:04:35,376: Snapshot:1	Epoch:1	Loss:19.928	translation_Loss:18.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.965                                                   	MRR:18.69	Hits@10:31.9	Best:18.69
2024-12-27 23:04:43,087: Snapshot:1	Epoch:2	Loss:18.316	translation_Loss:16.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.625                                                   	MRR:19.14	Hits@10:32.62	Best:19.14
2024-12-27 23:04:50,416: Snapshot:1	Epoch:3	Loss:17.128	translation_Loss:14.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.143                                                   	MRR:19.46	Hits@10:33.16	Best:19.46
2024-12-27 23:04:57,669: Snapshot:1	Epoch:4	Loss:16.168	translation_Loss:13.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.52                                                   	MRR:19.81	Hits@10:33.69	Best:19.81
2024-12-27 23:05:05,008: Snapshot:1	Epoch:5	Loss:15.282	translation_Loss:12.485	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.797                                                   	MRR:20.08	Hits@10:34.08	Best:20.08
2024-12-27 23:05:12,309: Snapshot:1	Epoch:6	Loss:14.549	translation_Loss:11.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.993                                                   	MRR:20.27	Hits@10:34.33	Best:20.27
2024-12-27 23:05:19,644: Snapshot:1	Epoch:7	Loss:13.915	translation_Loss:10.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.135                                                   	MRR:20.53	Hits@10:34.53	Best:20.53
2024-12-27 23:05:26,940: Snapshot:1	Epoch:8	Loss:13.371	translation_Loss:10.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.229                                                   	MRR:20.69	Hits@10:34.9	Best:20.69
2024-12-27 23:05:34,166: Snapshot:1	Epoch:9	Loss:12.952	translation_Loss:9.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.299                                                   	MRR:20.82	Hits@10:35.13	Best:20.82
2024-12-27 23:05:41,521: Snapshot:1	Epoch:10	Loss:12.633	translation_Loss:9.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.346                                                   	MRR:21.01	Hits@10:35.19	Best:21.01
2024-12-27 23:05:48,832: Snapshot:1	Epoch:11	Loss:12.349	translation_Loss:8.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.372                                                   	MRR:21.04	Hits@10:35.25	Best:21.04
2024-12-27 23:05:56,598: Snapshot:1	Epoch:12	Loss:12.183	translation_Loss:8.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.385                                                   	MRR:21.09	Hits@10:35.5	Best:21.09
2024-12-27 23:06:03,876: Snapshot:1	Epoch:13	Loss:12.025	translation_Loss:8.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.392                                                   	MRR:21.2	Hits@10:35.63	Best:21.2
2024-12-27 23:06:11,156: Snapshot:1	Epoch:14	Loss:11.905	translation_Loss:8.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.395                                                   	MRR:21.24	Hits@10:35.69	Best:21.24
2024-12-27 23:06:18,399: Snapshot:1	Epoch:15	Loss:11.794	translation_Loss:8.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.392                                                   	MRR:21.28	Hits@10:35.74	Best:21.28
2024-12-27 23:06:25,768: Snapshot:1	Epoch:16	Loss:11.719	translation_Loss:8.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.39                                                   	MRR:21.31	Hits@10:35.84	Best:21.31
2024-12-27 23:06:33,103: Snapshot:1	Epoch:17	Loss:11.679	translation_Loss:8.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.386                                                   	MRR:21.32	Hits@10:35.88	Best:21.32
2024-12-27 23:06:40,349: Snapshot:1	Epoch:18	Loss:11.625	translation_Loss:8.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.383                                                   	MRR:21.35	Hits@10:35.89	Best:21.35
2024-12-27 23:06:47,621: Snapshot:1	Epoch:19	Loss:11.581	translation_Loss:8.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.378                                                   	MRR:21.36	Hits@10:35.8	Best:21.36
2024-12-27 23:06:54,848: Snapshot:1	Epoch:20	Loss:11.545	translation_Loss:8.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.374                                                   	MRR:21.34	Hits@10:35.88	Best:21.36
2024-12-27 23:07:02,087: Snapshot:1	Epoch:21	Loss:11.494	translation_Loss:8.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.371                                                   	MRR:21.37	Hits@10:35.79	Best:21.37
2024-12-27 23:07:09,324: Snapshot:1	Epoch:22	Loss:11.462	translation_Loss:8.091	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.371                                                   	MRR:21.4	Hits@10:35.87	Best:21.4
2024-12-27 23:07:16,548: Snapshot:1	Epoch:23	Loss:11.48	translation_Loss:8.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.37                                                   	MRR:21.36	Hits@10:35.97	Best:21.4
2024-12-27 23:07:23,744: Snapshot:1	Epoch:24	Loss:11.43	translation_Loss:8.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.368                                                   	MRR:21.36	Hits@10:36.03	Best:21.4
2024-12-27 23:07:31,029: Snapshot:1	Epoch:25	Loss:11.406	translation_Loss:8.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.366                                                   	MRR:21.44	Hits@10:36.01	Best:21.44
2024-12-27 23:07:38,350: Snapshot:1	Epoch:26	Loss:11.392	translation_Loss:8.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.36                                                   	MRR:21.41	Hits@10:35.88	Best:21.44
2024-12-27 23:07:45,641: Snapshot:1	Epoch:27	Loss:11.4	translation_Loss:8.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.36                                                   	MRR:21.44	Hits@10:35.96	Best:21.44
2024-12-27 23:07:52,921: Snapshot:1	Epoch:28	Loss:11.338	translation_Loss:7.98	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.358                                                   	MRR:21.41	Hits@10:35.97	Best:21.44
2024-12-27 23:08:00,144: Snapshot:1	Epoch:29	Loss:11.36	translation_Loss:8.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.357                                                   	MRR:21.37	Hits@10:35.81	Best:21.44
2024-12-27 23:08:07,316: Early Stopping! Snapshot: 1 Epoch: 30 Best Results: 21.44
2024-12-27 23:08:07,317: Start to training tokens! Snapshot: 1 Epoch: 30 Loss:11.349 MRR:21.38 Best Results: 21.44
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:08:07,317: Snapshot:1	Epoch:30	Loss:11.349	translation_Loss:7.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.355                                                   	MRR:21.38	Hits@10:35.98	Best:21.44
2024-12-27 23:08:14,405: Snapshot:1	Epoch:31	Loss:43.344	translation_Loss:42.371	multi_layer_Loss:0.973	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.38	Hits@10:35.98	Best:21.44
2024-12-27 23:08:21,986: End of token training: 1 Epoch: 32 Loss:42.551 MRR:21.38 Best Results: 21.44
2024-12-27 23:08:21,986: Snapshot:1	Epoch:32	Loss:42.551	translation_Loss:42.321	multi_layer_Loss:0.23	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.38	Hits@10:35.98	Best:21.44
2024-12-27 23:08:22,257: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_10000/1model_best.tar'
2024-12-27 23:08:28,369: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1635 | 0.2927 | 0.3444 |  0.4096 |
|     1      | 0.2136 | 0.132  | 0.254  | 0.3016 |  0.362  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:08:53,525: Snapshot:2	Epoch:0	Loss:16.388	translation_Loss:16.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.326                                                   	MRR:19.36	Hits@10:33.86	Best:19.36
2024-12-27 23:09:00,965: Snapshot:2	Epoch:1	Loss:13.658	translation_Loss:12.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.916                                                   	MRR:19.85	Hits@10:34.87	Best:19.85
2024-12-27 23:09:08,898: Snapshot:2	Epoch:2	Loss:12.345	translation_Loss:10.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.548                                                   	MRR:20.17	Hits@10:35.38	Best:20.17
2024-12-27 23:09:16,407: Snapshot:2	Epoch:3	Loss:11.495	translation_Loss:9.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.056                                                   	MRR:20.44	Hits@10:35.8	Best:20.44
2024-12-27 23:09:23,886: Snapshot:2	Epoch:4	Loss:10.898	translation_Loss:8.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.438                                                   	MRR:20.64	Hits@10:36.17	Best:20.64
2024-12-27 23:09:31,318: Snapshot:2	Epoch:5	Loss:10.434	translation_Loss:7.717	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.718                                                   	MRR:20.8	Hits@10:36.46	Best:20.8
2024-12-27 23:09:38,813: Snapshot:2	Epoch:6	Loss:10.094	translation_Loss:7.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.921                                                   	MRR:20.94	Hits@10:36.52	Best:20.94
2024-12-27 23:09:46,355: Snapshot:2	Epoch:7	Loss:9.819	translation_Loss:6.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.075                                                   	MRR:21.05	Hits@10:36.69	Best:21.05
2024-12-27 23:09:53,890: Snapshot:2	Epoch:8	Loss:9.606	translation_Loss:6.429	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.177                                                   	MRR:21.11	Hits@10:36.74	Best:21.11
2024-12-27 23:10:01,402: Snapshot:2	Epoch:9	Loss:9.493	translation_Loss:6.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.259                                                   	MRR:21.15	Hits@10:36.92	Best:21.15
2024-12-27 23:10:09,057: Snapshot:2	Epoch:10	Loss:9.353	translation_Loss:6.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.315                                                   	MRR:21.17	Hits@10:37.01	Best:21.17
2024-12-27 23:10:16,542: Snapshot:2	Epoch:11	Loss:9.314	translation_Loss:5.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.356                                                   	MRR:21.26	Hits@10:37.01	Best:21.26
2024-12-27 23:10:24,441: Snapshot:2	Epoch:12	Loss:9.209	translation_Loss:5.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.384                                                   	MRR:21.24	Hits@10:36.98	Best:21.26
2024-12-27 23:10:31,869: Snapshot:2	Epoch:13	Loss:9.217	translation_Loss:5.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.411                                                   	MRR:21.25	Hits@10:37.05	Best:21.26
2024-12-27 23:10:39,290: Snapshot:2	Epoch:14	Loss:9.179	translation_Loss:5.747	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.432                                                   	MRR:21.23	Hits@10:37.06	Best:21.26
2024-12-27 23:10:46,780: Snapshot:2	Epoch:15	Loss:9.152	translation_Loss:5.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.45                                                   	MRR:21.29	Hits@10:37.05	Best:21.29
2024-12-27 23:10:54,195: Snapshot:2	Epoch:16	Loss:9.103	translation_Loss:5.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.462                                                   	MRR:21.26	Hits@10:37.06	Best:21.29
2024-12-27 23:11:01,633: Snapshot:2	Epoch:17	Loss:9.112	translation_Loss:5.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.471                                                   	MRR:21.28	Hits@10:37.15	Best:21.29
2024-12-27 23:11:09,012: Snapshot:2	Epoch:18	Loss:9.122	translation_Loss:5.643	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.479                                                   	MRR:21.29	Hits@10:37.12	Best:21.29
2024-12-27 23:11:16,483: Snapshot:2	Epoch:19	Loss:9.097	translation_Loss:5.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.49                                                   	MRR:21.32	Hits@10:37.11	Best:21.32
2024-12-27 23:11:23,940: Snapshot:2	Epoch:20	Loss:9.085	translation_Loss:5.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.495                                                   	MRR:21.28	Hits@10:37.03	Best:21.32
2024-12-27 23:11:31,362: Snapshot:2	Epoch:21	Loss:9.097	translation_Loss:5.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.5                                                   	MRR:21.28	Hits@10:36.99	Best:21.32
2024-12-27 23:11:38,811: Snapshot:2	Epoch:22	Loss:9.084	translation_Loss:5.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.504                                                   	MRR:21.34	Hits@10:37.09	Best:21.34
2024-12-27 23:11:46,295: Snapshot:2	Epoch:23	Loss:9.096	translation_Loss:5.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.51                                                   	MRR:21.3	Hits@10:37.04	Best:21.34
2024-12-27 23:11:53,714: Snapshot:2	Epoch:24	Loss:9.06	translation_Loss:5.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.515                                                   	MRR:21.31	Hits@10:37.15	Best:21.34
2024-12-27 23:12:01,106: Snapshot:2	Epoch:25	Loss:9.071	translation_Loss:5.551	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.52                                                   	MRR:21.26	Hits@10:37.05	Best:21.34
2024-12-27 23:12:08,491: Snapshot:2	Epoch:26	Loss:9.04	translation_Loss:5.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.515                                                   	MRR:21.26	Hits@10:37.05	Best:21.34
2024-12-27 23:12:15,926: Early Stopping! Snapshot: 2 Epoch: 27 Best Results: 21.34
2024-12-27 23:12:15,927: Start to training tokens! Snapshot: 2 Epoch: 27 Loss:9.04 MRR:21.29 Best Results: 21.34
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:12:15,927: Snapshot:2	Epoch:27	Loss:9.04	translation_Loss:5.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.514                                                   	MRR:21.29	Hits@10:37.1	Best:21.34
2024-12-27 23:12:23,132: Snapshot:2	Epoch:28	Loss:42.518	translation_Loss:41.512	multi_layer_Loss:1.006	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.29	Hits@10:37.1	Best:21.34
2024-12-27 23:12:30,357: End of token training: 2 Epoch: 29 Loss:41.778 MRR:21.29 Best Results: 21.34
2024-12-27 23:12:30,357: Snapshot:2	Epoch:29	Loss:41.778	translation_Loss:41.527	multi_layer_Loss:0.251	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.29	Hits@10:37.1	Best:21.34
2024-12-27 23:12:30,713: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_10000/2model_best.tar'
2024-12-27 23:12:40,697: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2505 | 0.162  | 0.2906 | 0.3449 |  0.4154 |
|     1      | 0.2228 | 0.1387 | 0.2611 | 0.3121 |  0.3815 |
|     2      | 0.213  | 0.129  | 0.2508 | 0.3015 |  0.3693 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:13:05,903: Snapshot:3	Epoch:0	Loss:9.891	translation_Loss:9.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.311                                                   	MRR:18.82	Hits@10:35.6	Best:18.82
2024-12-27 23:13:13,537: Snapshot:3	Epoch:1	Loss:7.612	translation_Loss:6.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.771                                                   	MRR:19.26	Hits@10:36.47	Best:19.26
2024-12-27 23:13:21,511: Snapshot:3	Epoch:2	Loss:6.685	translation_Loss:5.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.202                                                   	MRR:19.45	Hits@10:37.06	Best:19.45
2024-12-27 23:13:29,109: Snapshot:3	Epoch:3	Loss:6.174	translation_Loss:4.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.524                                                   	MRR:19.65	Hits@10:37.5	Best:19.65
2024-12-27 23:13:36,737: Snapshot:3	Epoch:4	Loss:5.819	translation_Loss:4.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.75                                                   	MRR:19.77	Hits@10:37.65	Best:19.77
2024-12-27 23:13:44,365: Snapshot:3	Epoch:5	Loss:5.596	translation_Loss:3.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.908                                                   	MRR:19.86	Hits@10:37.83	Best:19.86
2024-12-27 23:13:51,977: Snapshot:3	Epoch:6	Loss:5.429	translation_Loss:3.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.018                                                   	MRR:19.92	Hits@10:37.92	Best:19.92
2024-12-27 23:13:59,514: Snapshot:3	Epoch:7	Loss:5.329	translation_Loss:3.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.095                                                   	MRR:19.99	Hits@10:37.95	Best:19.99
2024-12-27 23:14:07,057: Snapshot:3	Epoch:8	Loss:5.258	translation_Loss:3.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.148                                                   	MRR:20.01	Hits@10:37.89	Best:20.01
2024-12-27 23:14:14,653: Snapshot:3	Epoch:9	Loss:5.254	translation_Loss:3.066	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.189                                                   	MRR:20.13	Hits@10:38.0	Best:20.13
2024-12-27 23:14:22,169: Snapshot:3	Epoch:10	Loss:5.202	translation_Loss:2.985	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.217                                                   	MRR:20.13	Hits@10:37.96	Best:20.13
2024-12-27 23:14:29,630: Snapshot:3	Epoch:11	Loss:5.204	translation_Loss:2.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.242                                                   	MRR:20.12	Hits@10:38.11	Best:20.13
2024-12-27 23:14:37,218: Snapshot:3	Epoch:12	Loss:5.178	translation_Loss:2.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.259                                                   	MRR:20.17	Hits@10:37.99	Best:20.17
2024-12-27 23:14:44,884: Snapshot:3	Epoch:13	Loss:5.141	translation_Loss:2.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.275                                                   	MRR:20.18	Hits@10:38.14	Best:20.18
2024-12-27 23:14:52,378: Snapshot:3	Epoch:14	Loss:5.16	translation_Loss:2.879	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.282                                                   	MRR:20.16	Hits@10:38.2	Best:20.18
2024-12-27 23:14:59,907: Snapshot:3	Epoch:15	Loss:5.132	translation_Loss:2.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.288                                                   	MRR:20.11	Hits@10:38.19	Best:20.18
2024-12-27 23:15:07,468: Snapshot:3	Epoch:16	Loss:5.159	translation_Loss:2.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.3                                                   	MRR:20.16	Hits@10:38.15	Best:20.18
2024-12-27 23:15:14,963: Snapshot:3	Epoch:17	Loss:5.166	translation_Loss:2.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.311                                                   	MRR:20.12	Hits@10:38.1	Best:20.18
2024-12-27 23:15:22,443: Early Stopping! Snapshot: 3 Epoch: 18 Best Results: 20.18
2024-12-27 23:15:22,444: Start to training tokens! Snapshot: 3 Epoch: 18 Loss:5.127 MRR:20.08 Best Results: 20.18
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:15:22,444: Snapshot:3	Epoch:18	Loss:5.127	translation_Loss:2.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.311                                                   	MRR:20.08	Hits@10:38.01	Best:20.18
2024-12-27 23:15:29,739: Snapshot:3	Epoch:19	Loss:40.603	translation_Loss:39.628	multi_layer_Loss:0.975	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.08	Hits@10:38.01	Best:20.18
2024-12-27 23:15:37,059: End of token training: 3 Epoch: 20 Loss:39.862 MRR:20.08 Best Results: 20.18
2024-12-27 23:15:37,060: Snapshot:3	Epoch:20	Loss:39.862	translation_Loss:39.639	multi_layer_Loss:0.223	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.08	Hits@10:38.01	Best:20.18
2024-12-27 23:15:37,415: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_10000/3model_best.tar'
2024-12-27 23:15:50,686: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2416 | 0.1546 | 0.2782 | 0.3324 |  0.405  |
|     1      | 0.2195 | 0.1363 | 0.2542 | 0.3055 |   0.38  |
|     2      | 0.2127 | 0.1259 | 0.2482 | 0.3054 |  0.381  |
|     3      | 0.202  | 0.1107 | 0.2352 | 0.2978 |  0.3827 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:16:16,018: Snapshot:4	Epoch:0	Loss:5.689	translation_Loss:5.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:19.49	Hits@10:43.22	Best:19.49
2024-12-27 23:16:23,590: Snapshot:4	Epoch:1	Loss:3.926	translation_Loss:3.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:20.33	Hits@10:45.3	Best:20.33
2024-12-27 23:16:31,197: Snapshot:4	Epoch:2	Loss:3.164	translation_Loss:2.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.575                                                   	MRR:20.87	Hits@10:45.94	Best:20.87
2024-12-27 23:16:38,905: Snapshot:4	Epoch:3	Loss:2.633	translation_Loss:1.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.655                                                   	MRR:21.07	Hits@10:46.23	Best:21.07
2024-12-27 23:16:46,533: Snapshot:4	Epoch:4	Loss:2.239	translation_Loss:1.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.703                                                   	MRR:21.4	Hits@10:46.51	Best:21.4
2024-12-27 23:16:54,169: Snapshot:4	Epoch:5	Loss:1.969	translation_Loss:1.239	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.73                                                   	MRR:21.62	Hits@10:46.61	Best:21.62
2024-12-27 23:17:01,777: Snapshot:4	Epoch:6	Loss:1.79	translation_Loss:1.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.744                                                   	MRR:21.73	Hits@10:46.53	Best:21.73
2024-12-27 23:17:09,384: Snapshot:4	Epoch:7	Loss:1.689	translation_Loss:0.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.748                                                   	MRR:21.87	Hits@10:46.51	Best:21.87
2024-12-27 23:17:17,007: Snapshot:4	Epoch:8	Loss:1.611	translation_Loss:0.857	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.754                                                   	MRR:21.96	Hits@10:46.87	Best:21.96
2024-12-27 23:17:25,081: Snapshot:4	Epoch:9	Loss:1.59	translation_Loss:0.837	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.753                                                   	MRR:21.93	Hits@10:46.97	Best:21.96
2024-12-27 23:17:32,614: Snapshot:4	Epoch:10	Loss:1.561	translation_Loss:0.804	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.757                                                   	MRR:21.98	Hits@10:46.73	Best:21.98
2024-12-27 23:17:40,192: Snapshot:4	Epoch:11	Loss:1.521	translation_Loss:0.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.756                                                   	MRR:22.23	Hits@10:47.01	Best:22.23
2024-12-27 23:17:47,832: Snapshot:4	Epoch:12	Loss:1.517	translation_Loss:0.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.753                                                   	MRR:22.04	Hits@10:47.12	Best:22.23
2024-12-27 23:17:55,429: Snapshot:4	Epoch:13	Loss:1.513	translation_Loss:0.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.759                                                   	MRR:22.17	Hits@10:47.17	Best:22.23
2024-12-27 23:18:02,952: Snapshot:4	Epoch:14	Loss:1.514	translation_Loss:0.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.756                                                   	MRR:22.14	Hits@10:47.07	Best:22.23
2024-12-27 23:18:10,516: Snapshot:4	Epoch:15	Loss:1.498	translation_Loss:0.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.759                                                   	MRR:22.28	Hits@10:46.65	Best:22.28
2024-12-27 23:18:18,079: Snapshot:4	Epoch:16	Loss:1.505	translation_Loss:0.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.762                                                   	MRR:22.17	Hits@10:46.76	Best:22.28
2024-12-27 23:18:25,624: Snapshot:4	Epoch:17	Loss:1.498	translation_Loss:0.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:22.33	Hits@10:46.85	Best:22.33
2024-12-27 23:18:33,241: Snapshot:4	Epoch:18	Loss:1.49	translation_Loss:0.729	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:22.33	Hits@10:46.92	Best:22.33
2024-12-27 23:18:41,305: Snapshot:4	Epoch:19	Loss:1.492	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:22.42	Hits@10:46.89	Best:22.42
2024-12-27 23:18:48,904: Snapshot:4	Epoch:20	Loss:1.489	translation_Loss:0.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.758                                                   	MRR:22.49	Hits@10:46.72	Best:22.49
2024-12-27 23:18:56,397: Snapshot:4	Epoch:21	Loss:1.489	translation_Loss:0.729	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.76                                                   	MRR:22.4	Hits@10:46.7	Best:22.49
2024-12-27 23:19:03,988: Snapshot:4	Epoch:22	Loss:1.471	translation_Loss:0.708	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.763                                                   	MRR:22.26	Hits@10:46.83	Best:22.49
2024-12-27 23:19:11,497: Snapshot:4	Epoch:23	Loss:1.48	translation_Loss:0.718	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.762                                                   	MRR:22.19	Hits@10:46.65	Best:22.49
2024-12-27 23:19:19,041: Snapshot:4	Epoch:24	Loss:1.491	translation_Loss:0.724	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.767                                                   	MRR:22.13	Hits@10:46.74	Best:22.49
2024-12-27 23:19:26,536: Early Stopping! Snapshot: 4 Epoch: 25 Best Results: 22.49
2024-12-27 23:19:26,536: Start to training tokens! Snapshot: 4 Epoch: 25 Loss:1.477 MRR:22.16 Best Results: 22.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:19:26,537: Snapshot:4	Epoch:25	Loss:1.477	translation_Loss:0.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.764                                                   	MRR:22.16	Hits@10:46.8	Best:22.49
2024-12-27 23:19:33,995: Snapshot:4	Epoch:26	Loss:34.253	translation_Loss:33.273	multi_layer_Loss:0.98	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.16	Hits@10:46.8	Best:22.49
2024-12-27 23:19:41,360: End of token training: 4 Epoch: 27 Loss:33.446 MRR:22.16 Best Results: 22.49
2024-12-27 23:19:41,360: Snapshot:4	Epoch:27	Loss:33.446	translation_Loss:33.23	multi_layer_Loss:0.216	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.16	Hits@10:46.8	Best:22.49
2024-12-27 23:19:41,717: => loading checkpoint './checkpoint/FACTfact_0.0001_1024_10000/4model_best.tar'
2024-12-27 23:19:57,845: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      |  0.23  | 0.1451 | 0.2634 | 0.3172 |  0.3896 |
|     1      | 0.2074 | 0.1268 | 0.2377 | 0.2918 |  0.3642 |
|     2      | 0.2001 | 0.1153 | 0.2307 | 0.2877 |  0.3693 |
|     3      | 0.1915 | 0.0999 | 0.2197 | 0.2878 |  0.381  |
|     4      | 0.2233 | 0.1062 | 0.2623 |  0.35  |  0.4653 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 23:19:57,847: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2374 | 0.1518 | 0.2805 | 0.3273 |   0.39  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2511 | 0.1635 | 0.2927 | 0.3444 |  0.4096 |
|     1      | 0.2136 | 0.132  | 0.254  | 0.3016 |  0.362  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2505 | 0.162  | 0.2906 | 0.3449 |  0.4154 |
|     1      | 0.2228 | 0.1387 | 0.2611 | 0.3121 |  0.3815 |
|     2      | 0.213  | 0.129  | 0.2508 | 0.3015 |  0.3693 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2416 | 0.1546 | 0.2782 | 0.3324 |  0.405  |
|     1      | 0.2195 | 0.1363 | 0.2542 | 0.3055 |   0.38  |
|     2      | 0.2127 | 0.1259 | 0.2482 | 0.3054 |  0.381  |
|     3      | 0.202  | 0.1107 | 0.2352 | 0.2978 |  0.3827 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      |  0.23  | 0.1451 | 0.2634 | 0.3172 |  0.3896 |
|     1      | 0.2074 | 0.1268 | 0.2377 | 0.2918 |  0.3642 |
|     2      | 0.2001 | 0.1153 | 0.2307 | 0.2877 |  0.3693 |
|     3      | 0.1915 | 0.0999 | 0.2197 | 0.2878 |  0.381  |
|     4      | 0.2233 | 0.1062 | 0.2623 |  0.35  |  0.4653 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 23:19:57,848: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 635.0163345336914  |   0.237   |    0.152     |    0.281     |      0.39     |
|    1     | 255.80448484420776 |   0.232   |    0.148     |    0.273     |     0.386     |
|    2     | 238.6394362449646  |   0.229   |    0.143     |    0.268     |     0.389     |
|    3     |  172.960186958313  |   0.219   |    0.132     |    0.254     |     0.387     |
|    4     | 227.19700407981873 |    0.21   |    0.119     |    0.243     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 23:19:57,848: Sum_Training_Time:1529.6174466609955
2024-12-27 23:19:57,848: Every_Training_Time:[635.0163345336914, 255.80448484420776, 238.6394362449646, 172.960186958313, 227.19700407981873]
2024-12-27 23:19:57,848: Forward transfer: 0.169125 Backward transfer: -0.009250000000000001
2024-12-27 23:20:38,358: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227232003/FACTfact_0.0001_2048_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_2048_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_2048_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 23:20:48,339: Snapshot:0	Epoch:0	Loss:26.399	translation_Loss:26.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.43	Best:1.42
2024-12-27 23:20:54,797: Snapshot:0	Epoch:1	Loss:25.069	translation_Loss:25.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.81	Hits@10:2.56	Best:1.81
2024-12-27 23:21:01,614: Snapshot:0	Epoch:2	Loss:23.874	translation_Loss:23.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.3	Best:2.98
2024-12-27 23:21:08,048: Snapshot:0	Epoch:3	Loss:22.731	translation_Loss:22.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.96	Hits@10:7.35	Best:3.96
2024-12-27 23:21:14,461: Snapshot:0	Epoch:4	Loss:21.623	translation_Loss:21.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.72	Hits@10:9.23	Best:4.72
2024-12-27 23:21:20,882: Snapshot:0	Epoch:5	Loss:20.546	translation_Loss:20.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.38	Hits@10:10.87	Best:5.38
2024-12-27 23:21:27,360: Snapshot:0	Epoch:6	Loss:19.518	translation_Loss:19.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.01	Hits@10:12.51	Best:6.01
2024-12-27 23:21:34,349: Snapshot:0	Epoch:7	Loss:18.524	translation_Loss:18.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.64	Hits@10:14.27	Best:6.64
2024-12-27 23:21:40,747: Snapshot:0	Epoch:8	Loss:17.557	translation_Loss:17.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.25	Hits@10:15.93	Best:7.25
2024-12-27 23:21:47,195: Snapshot:0	Epoch:9	Loss:16.633	translation_Loss:16.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.8	Hits@10:17.36	Best:7.8
2024-12-27 23:21:53,604: Snapshot:0	Epoch:10	Loss:15.731	translation_Loss:15.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.35	Hits@10:18.99	Best:8.35
2024-12-27 23:22:00,018: Snapshot:0	Epoch:11	Loss:14.856	translation_Loss:14.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.92	Hits@10:20.48	Best:8.92
2024-12-27 23:22:06,430: Snapshot:0	Epoch:12	Loss:14.025	translation_Loss:14.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.48	Hits@10:21.91	Best:9.48
2024-12-27 23:22:13,347: Snapshot:0	Epoch:13	Loss:13.222	translation_Loss:13.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.05	Hits@10:23.26	Best:10.05
2024-12-27 23:22:19,801: Snapshot:0	Epoch:14	Loss:12.461	translation_Loss:12.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.59	Hits@10:24.56	Best:10.59
2024-12-27 23:22:26,192: Snapshot:0	Epoch:15	Loss:11.742	translation_Loss:11.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.15	Hits@10:25.77	Best:11.15
2024-12-27 23:22:32,649: Snapshot:0	Epoch:16	Loss:11.041	translation_Loss:11.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.72	Hits@10:26.83	Best:11.72
2024-12-27 23:22:39,062: Snapshot:0	Epoch:17	Loss:10.379	translation_Loss:10.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.24	Hits@10:27.79	Best:12.24
2024-12-27 23:22:45,948: Snapshot:0	Epoch:18	Loss:9.757	translation_Loss:9.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.7	Hits@10:28.72	Best:12.7
2024-12-27 23:22:52,433: Snapshot:0	Epoch:19	Loss:9.158	translation_Loss:9.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.2	Hits@10:29.57	Best:13.2
2024-12-27 23:22:58,861: Snapshot:0	Epoch:20	Loss:8.589	translation_Loss:8.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.66	Hits@10:30.24	Best:13.66
2024-12-27 23:23:05,258: Snapshot:0	Epoch:21	Loss:8.072	translation_Loss:8.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.11	Hits@10:31.04	Best:14.11
2024-12-27 23:23:11,648: Snapshot:0	Epoch:22	Loss:7.58	translation_Loss:7.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.56	Hits@10:31.77	Best:14.56
2024-12-27 23:23:18,150: Snapshot:0	Epoch:23	Loss:7.1	translation_Loss:7.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.02	Hits@10:32.44	Best:15.02
2024-12-27 23:23:25,033: Snapshot:0	Epoch:24	Loss:6.658	translation_Loss:6.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.4	Hits@10:33.01	Best:15.4
2024-12-27 23:23:31,460: Snapshot:0	Epoch:25	Loss:6.25	translation_Loss:6.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.84	Hits@10:33.6	Best:15.84
2024-12-27 23:23:37,876: Snapshot:0	Epoch:26	Loss:5.851	translation_Loss:5.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.23	Hits@10:34.1	Best:16.23
2024-12-27 23:23:44,390: Snapshot:0	Epoch:27	Loss:5.48	translation_Loss:5.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.67	Hits@10:34.49	Best:16.67
2024-12-27 23:23:50,786: Snapshot:0	Epoch:28	Loss:5.133	translation_Loss:5.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.03	Hits@10:34.86	Best:17.03
2024-12-27 23:23:57,664: Snapshot:0	Epoch:29	Loss:4.788	translation_Loss:4.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.44	Hits@10:35.29	Best:17.44
2024-12-27 23:24:04,132: Snapshot:0	Epoch:30	Loss:4.494	translation_Loss:4.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.81	Hits@10:35.58	Best:17.81
2024-12-27 23:24:10,546: Snapshot:0	Epoch:31	Loss:4.201	translation_Loss:4.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.16	Hits@10:35.97	Best:18.16
2024-12-27 23:24:16,985: Snapshot:0	Epoch:32	Loss:3.932	translation_Loss:3.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.46	Hits@10:36.23	Best:18.46
2024-12-27 23:24:23,401: Snapshot:0	Epoch:33	Loss:3.657	translation_Loss:3.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.77	Hits@10:36.62	Best:18.77
2024-12-27 23:24:29,838: Snapshot:0	Epoch:34	Loss:3.418	translation_Loss:3.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.1	Hits@10:36.85	Best:19.1
2024-12-27 23:24:36,757: Snapshot:0	Epoch:35	Loss:3.194	translation_Loss:3.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.41	Hits@10:37.15	Best:19.41
2024-12-27 23:24:43,199: Snapshot:0	Epoch:36	Loss:2.978	translation_Loss:2.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.69	Hits@10:37.48	Best:19.69
2024-12-27 23:24:49,614: Snapshot:0	Epoch:37	Loss:2.785	translation_Loss:2.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.95	Hits@10:37.58	Best:19.95
2024-12-27 23:24:56,011: Snapshot:0	Epoch:38	Loss:2.59	translation_Loss:2.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.2	Hits@10:37.73	Best:20.2
2024-12-27 23:25:02,409: Snapshot:0	Epoch:39	Loss:2.42	translation_Loss:2.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.46	Hits@10:37.93	Best:20.46
2024-12-27 23:25:09,315: Snapshot:0	Epoch:40	Loss:2.264	translation_Loss:2.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.65	Hits@10:38.05	Best:20.65
2024-12-27 23:25:15,736: Snapshot:0	Epoch:41	Loss:2.112	translation_Loss:2.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.85	Hits@10:38.18	Best:20.85
2024-12-27 23:25:22,123: Snapshot:0	Epoch:42	Loss:1.97	translation_Loss:1.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.06	Hits@10:38.41	Best:21.06
2024-12-27 23:25:28,561: Snapshot:0	Epoch:43	Loss:1.836	translation_Loss:1.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.24	Hits@10:38.55	Best:21.24
2024-12-27 23:25:34,975: Snapshot:0	Epoch:44	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.42	Hits@10:38.68	Best:21.42
2024-12-27 23:25:41,442: Snapshot:0	Epoch:45	Loss:1.608	translation_Loss:1.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.54	Hits@10:38.83	Best:21.54
2024-12-27 23:25:48,407: Snapshot:0	Epoch:46	Loss:1.495	translation_Loss:1.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.68	Hits@10:38.9	Best:21.68
2024-12-27 23:25:54,818: Snapshot:0	Epoch:47	Loss:1.406	translation_Loss:1.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.81	Hits@10:38.99	Best:21.81
2024-12-27 23:26:01,237: Snapshot:0	Epoch:48	Loss:1.321	translation_Loss:1.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.0	Hits@10:39.04	Best:22.0
2024-12-27 23:26:07,644: Snapshot:0	Epoch:49	Loss:1.235	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.13	Hits@10:39.11	Best:22.13
2024-12-27 23:26:14,103: Snapshot:0	Epoch:50	Loss:1.157	translation_Loss:1.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.3	Hits@10:39.15	Best:22.3
2024-12-27 23:26:20,919: Snapshot:0	Epoch:51	Loss:1.086	translation_Loss:1.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.44	Hits@10:39.22	Best:22.44
2024-12-27 23:26:27,367: Snapshot:0	Epoch:52	Loss:1.024	translation_Loss:1.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.55	Hits@10:39.3	Best:22.55
2024-12-27 23:26:33,789: Snapshot:0	Epoch:53	Loss:0.964	translation_Loss:0.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.65	Hits@10:39.38	Best:22.65
2024-12-27 23:26:40,195: Snapshot:0	Epoch:54	Loss:0.91	translation_Loss:0.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.73	Hits@10:39.33	Best:22.73
2024-12-27 23:26:46,632: Snapshot:0	Epoch:55	Loss:0.856	translation_Loss:0.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.81	Hits@10:39.35	Best:22.81
2024-12-27 23:26:53,054: Snapshot:0	Epoch:56	Loss:0.815	translation_Loss:0.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.86	Hits@10:39.38	Best:22.86
2024-12-27 23:26:59,922: Snapshot:0	Epoch:57	Loss:0.77	translation_Loss:0.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.95	Hits@10:39.34	Best:22.95
2024-12-27 23:27:06,307: Snapshot:0	Epoch:58	Loss:0.733	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.01	Hits@10:39.4	Best:23.01
2024-12-27 23:27:12,690: Snapshot:0	Epoch:59	Loss:0.688	translation_Loss:0.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.09	Hits@10:39.53	Best:23.09
2024-12-27 23:27:19,098: Snapshot:0	Epoch:60	Loss:0.649	translation_Loss:0.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.13	Hits@10:39.51	Best:23.13
2024-12-27 23:27:25,596: Snapshot:0	Epoch:61	Loss:0.625	translation_Loss:0.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.15	Hits@10:39.5	Best:23.15
2024-12-27 23:27:32,446: Snapshot:0	Epoch:62	Loss:0.592	translation_Loss:0.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.2	Hits@10:39.49	Best:23.2
2024-12-27 23:27:38,915: Snapshot:0	Epoch:63	Loss:0.563	translation_Loss:0.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.23	Hits@10:39.54	Best:23.23
2024-12-27 23:27:45,413: Snapshot:0	Epoch:64	Loss:0.533	translation_Loss:0.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.26	Hits@10:39.58	Best:23.26
2024-12-27 23:27:51,824: Snapshot:0	Epoch:65	Loss:0.51	translation_Loss:0.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.31	Hits@10:39.55	Best:23.31
2024-12-27 23:27:58,267: Snapshot:0	Epoch:66	Loss:0.493	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.37	Hits@10:39.57	Best:23.37
2024-12-27 23:28:04,720: Snapshot:0	Epoch:67	Loss:0.478	translation_Loss:0.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.41	Hits@10:39.58	Best:23.41
2024-12-27 23:28:11,552: Snapshot:0	Epoch:68	Loss:0.447	translation_Loss:0.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.44	Hits@10:39.65	Best:23.44
2024-12-27 23:28:17,910: Snapshot:0	Epoch:69	Loss:0.427	translation_Loss:0.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.44	Hits@10:39.64	Best:23.44
2024-12-27 23:28:24,302: Snapshot:0	Epoch:70	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:39.69	Best:23.51
2024-12-27 23:28:30,712: Snapshot:0	Epoch:71	Loss:0.395	translation_Loss:0.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.55	Hits@10:39.79	Best:23.55
2024-12-27 23:28:37,108: Snapshot:0	Epoch:72	Loss:0.38	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.8	Best:23.58
2024-12-27 23:28:43,941: Snapshot:0	Epoch:73	Loss:0.368	translation_Loss:0.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.64	Hits@10:39.85	Best:23.64
2024-12-27 23:28:50,432: Snapshot:0	Epoch:74	Loss:0.353	translation_Loss:0.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.79	Best:23.69
2024-12-27 23:28:56,808: Snapshot:0	Epoch:75	Loss:0.345	translation_Loss:0.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.69	Hits@10:39.79	Best:23.69
2024-12-27 23:29:03,319: Snapshot:0	Epoch:76	Loss:0.332	translation_Loss:0.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.82	Best:23.71
2024-12-27 23:29:09,678: Snapshot:0	Epoch:77	Loss:0.324	translation_Loss:0.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.68	Hits@10:39.84	Best:23.71
2024-12-27 23:29:16,137: Snapshot:0	Epoch:78	Loss:0.307	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.72	Hits@10:39.85	Best:23.72
2024-12-27 23:29:22,926: Snapshot:0	Epoch:79	Loss:0.299	translation_Loss:0.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.72	Hits@10:39.86	Best:23.72
2024-12-27 23:29:29,321: Snapshot:0	Epoch:80	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:39.81	Best:23.74
2024-12-27 23:29:35,748: Snapshot:0	Epoch:81	Loss:0.284	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.81	Hits@10:39.84	Best:23.81
2024-12-27 23:29:42,215: Snapshot:0	Epoch:82	Loss:0.278	translation_Loss:0.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.86	Best:23.81
2024-12-27 23:29:48,644: Snapshot:0	Epoch:83	Loss:0.262	translation_Loss:0.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.89	Best:23.81
2024-12-27 23:29:55,500: Snapshot:0	Epoch:84	Loss:0.257	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.8	Hits@10:39.89	Best:23.81
2024-12-27 23:30:01,945: Snapshot:0	Epoch:85	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:39.95	Best:23.87
2024-12-27 23:30:08,459: Snapshot:0	Epoch:86	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:39.93	Best:23.88
2024-12-27 23:30:14,876: Snapshot:0	Epoch:87	Loss:0.242	translation_Loss:0.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:39.97	Best:23.89
2024-12-27 23:30:21,296: Snapshot:0	Epoch:88	Loss:0.232	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.91	Hits@10:39.93	Best:23.91
2024-12-27 23:30:27,758: Snapshot:0	Epoch:89	Loss:0.228	translation_Loss:0.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.92	Hits@10:39.92	Best:23.92
2024-12-27 23:30:34,630: Snapshot:0	Epoch:90	Loss:0.224	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:39.93	Best:23.96
2024-12-27 23:30:41,028: Snapshot:0	Epoch:91	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:39.95	Best:23.96
2024-12-27 23:30:47,432: Snapshot:0	Epoch:92	Loss:0.211	translation_Loss:0.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:39.94	Best:23.98
2024-12-27 23:30:53,828: Snapshot:0	Epoch:93	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.92	Hits@10:39.9	Best:23.98
2024-12-27 23:31:00,197: Snapshot:0	Epoch:94	Loss:0.202	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:39.94	Best:23.98
2024-12-27 23:31:06,949: Snapshot:0	Epoch:95	Loss:0.195	translation_Loss:0.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:40.04	Best:23.98
2024-12-27 23:31:13,338: Snapshot:0	Epoch:96	Loss:0.193	translation_Loss:0.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:40.02	Best:23.98
2024-12-27 23:31:19,704: Snapshot:0	Epoch:97	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:40.0	Best:23.99
2024-12-27 23:31:26,138: Snapshot:0	Epoch:98	Loss:0.184	translation_Loss:0.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:39.99	Best:24.0
2024-12-27 23:31:32,534: Snapshot:0	Epoch:99	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:40.09	Best:24.05
2024-12-27 23:31:38,902: Snapshot:0	Epoch:100	Loss:0.178	translation_Loss:0.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:39.97	Best:24.05
2024-12-27 23:31:45,810: Snapshot:0	Epoch:101	Loss:0.173	translation_Loss:0.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:40.0	Best:24.05
2024-12-27 23:31:52,244: Snapshot:0	Epoch:102	Loss:0.168	translation_Loss:0.168	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.06	Hits@10:39.93	Best:24.06
2024-12-27 23:31:58,641: Snapshot:0	Epoch:103	Loss:0.171	translation_Loss:0.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.98	Best:24.08
2024-12-27 23:32:05,057: Snapshot:0	Epoch:104	Loss:0.162	translation_Loss:0.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:39.97	Best:24.11
2024-12-27 23:32:11,419: Snapshot:0	Epoch:105	Loss:0.161	translation_Loss:0.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:40.02	Best:24.11
2024-12-27 23:32:18,198: Snapshot:0	Epoch:106	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.98	Best:24.11
2024-12-27 23:32:24,577: Snapshot:0	Epoch:107	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.06	Best:24.14
2024-12-27 23:32:30,971: Snapshot:0	Epoch:108	Loss:0.153	translation_Loss:0.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:39.98	Best:24.14
2024-12-27 23:32:37,353: Snapshot:0	Epoch:109	Loss:0.146	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.98	Best:24.14
2024-12-27 23:32:43,747: Snapshot:0	Epoch:110	Loss:0.144	translation_Loss:0.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:39.98	Best:24.14
2024-12-27 23:32:50,172: Snapshot:0	Epoch:111	Loss:0.149	translation_Loss:0.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:40.0	Best:24.14
2024-12-27 23:32:56,935: Snapshot:0	Epoch:112	Loss:0.144	translation_Loss:0.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:39.91	Best:24.16
2024-12-27 23:33:03,433: Snapshot:0	Epoch:113	Loss:0.139	translation_Loss:0.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:39.94	Best:24.18
2024-12-27 23:33:09,855: Snapshot:0	Epoch:114	Loss:0.14	translation_Loss:0.14	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.19	Hits@10:40.06	Best:24.19
2024-12-27 23:33:16,261: Snapshot:0	Epoch:115	Loss:0.132	translation_Loss:0.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:40.08	Best:24.21
2024-12-27 23:33:22,656: Snapshot:0	Epoch:116	Loss:0.137	translation_Loss:0.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:40.06	Best:24.22
2024-12-27 23:33:29,446: Snapshot:0	Epoch:117	Loss:0.132	translation_Loss:0.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.17	Hits@10:40.07	Best:24.22
2024-12-27 23:33:35,830: Snapshot:0	Epoch:118	Loss:0.132	translation_Loss:0.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.19	Hits@10:40.11	Best:24.22
2024-12-27 23:33:42,257: Snapshot:0	Epoch:119	Loss:0.133	translation_Loss:0.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.25	Hits@10:40.17	Best:24.25
2024-12-27 23:33:48,630: Snapshot:0	Epoch:120	Loss:0.124	translation_Loss:0.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:40.15	Best:24.25
2024-12-27 23:33:54,999: Snapshot:0	Epoch:121	Loss:0.124	translation_Loss:0.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:40.15	Best:24.25
2024-12-27 23:34:01,415: Snapshot:0	Epoch:122	Loss:0.12	translation_Loss:0.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:40.08	Best:24.25
2024-12-27 23:34:08,167: Snapshot:0	Epoch:123	Loss:0.126	translation_Loss:0.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.25	Hits@10:40.11	Best:24.25
2024-12-27 23:34:14,575: Snapshot:0	Epoch:124	Loss:0.121	translation_Loss:0.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.3	Hits@10:40.08	Best:24.3
2024-12-27 23:34:20,943: Snapshot:0	Epoch:125	Loss:0.115	translation_Loss:0.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:40.08	Best:24.3
2024-12-27 23:34:27,277: Snapshot:0	Epoch:126	Loss:0.112	translation_Loss:0.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:40.11	Best:24.3
2024-12-27 23:34:33,637: Snapshot:0	Epoch:127	Loss:0.115	translation_Loss:0.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:40.03	Best:24.3
2024-12-27 23:34:40,467: Snapshot:0	Epoch:128	Loss:0.112	translation_Loss:0.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:40.03	Best:24.3
2024-12-27 23:34:46,843: Early Stopping! Snapshot: 0 Epoch: 129 Best Results: 24.3
2024-12-27 23:34:46,843: Start to training tokens! Snapshot: 0 Epoch: 129 Loss:0.113 MRR:24.24 Best Results: 24.3
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:34:46,843: Snapshot:0	Epoch:129	Loss:0.113	translation_Loss:0.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:39.97	Best:24.3
2024-12-27 23:34:53,810: Snapshot:0	Epoch:130	Loss:18.769	translation_Loss:18.143	multi_layer_Loss:0.626	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:39.97	Best:24.3
2024-12-27 23:35:00,215: End of token training: 0 Epoch: 131 Loss:18.508 MRR:24.24 Best Results: 24.3
2024-12-27 23:35:00,215: Snapshot:0	Epoch:131	Loss:18.508	translation_Loss:18.133	multi_layer_Loss:0.375	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.24	Hits@10:39.97	Best:24.3
2024-12-27 23:35:00,415: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_1000/0model_best.tar'
2024-12-27 23:35:03,707: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2344 | 0.147  | 0.2788 | 0.3251 |  0.3887 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:35:28,330: Snapshot:1	Epoch:0	Loss:11.85	translation_Loss:11.825	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.025                                                   	MRR:17.79	Hits@10:30.64	Best:17.79
2024-12-27 23:35:35,412: Snapshot:1	Epoch:1	Loss:10.032	translation_Loss:9.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.085                                                   	MRR:18.58	Hits@10:31.8	Best:18.58
2024-12-27 23:35:42,551: Snapshot:1	Epoch:2	Loss:8.717	translation_Loss:8.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:19.22	Hits@10:32.7	Best:19.22
2024-12-27 23:35:49,619: Snapshot:1	Epoch:3	Loss:7.617	translation_Loss:7.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:19.71	Hits@10:33.47	Best:19.71
2024-12-27 23:35:57,086: Snapshot:1	Epoch:4	Loss:6.65	translation_Loss:6.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.314                                                   	MRR:20.14	Hits@10:34.2	Best:20.14
2024-12-27 23:36:04,248: Snapshot:1	Epoch:5	Loss:5.84	translation_Loss:5.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:20.6	Hits@10:34.84	Best:20.6
2024-12-27 23:36:11,295: Snapshot:1	Epoch:6	Loss:5.136	translation_Loss:4.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.481                                                   	MRR:21.05	Hits@10:35.46	Best:21.05
2024-12-27 23:36:18,359: Snapshot:1	Epoch:7	Loss:4.508	translation_Loss:3.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.56                                                   	MRR:21.46	Hits@10:35.86	Best:21.46
2024-12-27 23:36:25,478: Snapshot:1	Epoch:8	Loss:3.991	translation_Loss:3.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.634                                                   	MRR:21.73	Hits@10:36.23	Best:21.73
2024-12-27 23:36:32,901: Snapshot:1	Epoch:9	Loss:3.541	translation_Loss:2.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.699                                                   	MRR:21.92	Hits@10:36.6	Best:21.92
2024-12-27 23:36:39,972: Snapshot:1	Epoch:10	Loss:3.204	translation_Loss:2.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.758                                                   	MRR:22.1	Hits@10:36.83	Best:22.1
2024-12-27 23:36:47,026: Snapshot:1	Epoch:11	Loss:2.913	translation_Loss:2.104	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:22.29	Hits@10:37.12	Best:22.29
2024-12-27 23:36:54,051: Snapshot:1	Epoch:12	Loss:2.693	translation_Loss:1.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.852                                                   	MRR:22.43	Hits@10:37.41	Best:22.43
2024-12-27 23:37:01,070: Snapshot:1	Epoch:13	Loss:2.497	translation_Loss:1.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.889                                                   	MRR:22.56	Hits@10:37.59	Best:22.56
2024-12-27 23:37:08,538: Snapshot:1	Epoch:14	Loss:2.358	translation_Loss:1.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.92                                                   	MRR:22.69	Hits@10:37.75	Best:22.69
2024-12-27 23:37:15,604: Snapshot:1	Epoch:15	Loss:2.254	translation_Loss:1.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.947                                                   	MRR:22.73	Hits@10:37.88	Best:22.73
2024-12-27 23:37:22,694: Snapshot:1	Epoch:16	Loss:2.156	translation_Loss:1.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.969                                                   	MRR:22.78	Hits@10:37.93	Best:22.78
2024-12-27 23:37:29,839: Snapshot:1	Epoch:17	Loss:2.092	translation_Loss:1.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.988                                                   	MRR:22.87	Hits@10:38.06	Best:22.87
2024-12-27 23:37:36,797: Snapshot:1	Epoch:18	Loss:2.034	translation_Loss:1.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.004                                                   	MRR:22.87	Hits@10:38.21	Best:22.87
2024-12-27 23:37:43,836: Snapshot:1	Epoch:19	Loss:1.989	translation_Loss:0.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.018                                                   	MRR:22.92	Hits@10:38.41	Best:22.92
2024-12-27 23:37:51,355: Snapshot:1	Epoch:20	Loss:1.936	translation_Loss:0.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.03                                                   	MRR:22.93	Hits@10:38.34	Best:22.93
2024-12-27 23:37:58,362: Snapshot:1	Epoch:21	Loss:1.9	translation_Loss:0.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.039                                                   	MRR:23.0	Hits@10:38.39	Best:23.0
2024-12-27 23:38:05,343: Snapshot:1	Epoch:22	Loss:1.875	translation_Loss:0.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.047                                                   	MRR:22.96	Hits@10:38.37	Best:23.0
2024-12-27 23:38:12,380: Snapshot:1	Epoch:23	Loss:1.857	translation_Loss:0.802	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.055                                                   	MRR:23.03	Hits@10:38.52	Best:23.03
2024-12-27 23:38:19,454: Snapshot:1	Epoch:24	Loss:1.832	translation_Loss:0.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.062                                                   	MRR:22.99	Hits@10:38.49	Best:23.03
2024-12-27 23:38:26,845: Snapshot:1	Epoch:25	Loss:1.806	translation_Loss:0.74	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.067                                                   	MRR:23.02	Hits@10:38.56	Best:23.03
2024-12-27 23:38:33,918: Snapshot:1	Epoch:26	Loss:1.793	translation_Loss:0.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.072                                                   	MRR:23.09	Hits@10:38.5	Best:23.09
2024-12-27 23:38:41,023: Snapshot:1	Epoch:27	Loss:1.789	translation_Loss:0.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.076                                                   	MRR:23.11	Hits@10:38.5	Best:23.11
2024-12-27 23:38:47,989: Snapshot:1	Epoch:28	Loss:1.754	translation_Loss:0.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.079                                                   	MRR:23.05	Hits@10:38.57	Best:23.11
2024-12-27 23:38:54,980: Snapshot:1	Epoch:29	Loss:1.747	translation_Loss:0.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.083                                                   	MRR:23.1	Hits@10:38.57	Best:23.11
2024-12-27 23:39:01,936: Snapshot:1	Epoch:30	Loss:1.73	translation_Loss:0.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.085                                                   	MRR:23.09	Hits@10:38.56	Best:23.11
2024-12-27 23:39:09,357: Snapshot:1	Epoch:31	Loss:1.727	translation_Loss:0.64	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.087                                                   	MRR:23.13	Hits@10:38.63	Best:23.13
2024-12-27 23:39:16,485: Snapshot:1	Epoch:32	Loss:1.717	translation_Loss:0.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.09                                                   	MRR:23.09	Hits@10:38.57	Best:23.13
2024-12-27 23:39:23,427: Snapshot:1	Epoch:33	Loss:1.705	translation_Loss:0.612	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.093                                                   	MRR:23.1	Hits@10:38.61	Best:23.13
2024-12-27 23:39:30,487: Snapshot:1	Epoch:34	Loss:1.695	translation_Loss:0.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.094                                                   	MRR:23.17	Hits@10:38.68	Best:23.17
2024-12-27 23:39:37,478: Snapshot:1	Epoch:35	Loss:1.696	translation_Loss:0.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.095                                                   	MRR:23.1	Hits@10:38.71	Best:23.17
2024-12-27 23:39:44,988: Snapshot:1	Epoch:36	Loss:1.689	translation_Loss:0.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:23.15	Hits@10:38.72	Best:23.17
2024-12-27 23:39:51,967: Snapshot:1	Epoch:37	Loss:1.668	translation_Loss:0.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:23.12	Hits@10:38.75	Best:23.17
2024-12-27 23:39:58,954: Snapshot:1	Epoch:38	Loss:1.666	translation_Loss:0.568	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.098                                                   	MRR:23.15	Hits@10:38.72	Best:23.17
2024-12-27 23:40:06,003: Early Stopping! Snapshot: 1 Epoch: 39 Best Results: 23.17
2024-12-27 23:40:06,003: Start to training tokens! Snapshot: 1 Epoch: 39 Loss:1.665 MRR:23.16 Best Results: 23.17
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:40:06,003: Snapshot:1	Epoch:39	Loss:1.665	translation_Loss:0.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.099                                                   	MRR:23.16	Hits@10:38.71	Best:23.17
2024-12-27 23:40:12,920: Snapshot:1	Epoch:40	Loss:20.361	translation_Loss:19.744	multi_layer_Loss:0.617	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.16	Hits@10:38.71	Best:23.17
2024-12-27 23:40:19,805: End of token training: 1 Epoch: 41 Loss:20.081 MRR:23.16 Best Results: 23.17
2024-12-27 23:40:19,806: Snapshot:1	Epoch:41	Loss:20.081	translation_Loss:19.719	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.16	Hits@10:38.71	Best:23.17
2024-12-27 23:40:20,063: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_1000/1model_best.tar'
2024-12-27 23:40:26,207: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1561 | 0.2912 | 0.3483 |  0.4195 |
|     1      | 0.2335 | 0.1489 | 0.273  | 0.323  |  0.387  |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:40:50,717: Snapshot:2	Epoch:0	Loss:6.773	translation_Loss:6.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.025                                                   	MRR:20.71	Hits@10:35.95	Best:20.71
2024-12-27 23:40:57,924: Snapshot:2	Epoch:1	Loss:5.186	translation_Loss:5.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.078                                                   	MRR:21.25	Hits@10:36.89	Best:21.25
2024-12-27 23:41:05,522: Snapshot:2	Epoch:2	Loss:4.172	translation_Loss:4.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.129                                                   	MRR:21.6	Hits@10:37.57	Best:21.6
2024-12-27 23:41:12,710: Snapshot:2	Epoch:3	Loss:3.434	translation_Loss:3.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:21.91	Hits@10:38.01	Best:21.91
2024-12-27 23:41:19,952: Snapshot:2	Epoch:4	Loss:2.876	translation_Loss:2.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.228                                                   	MRR:22.04	Hits@10:38.31	Best:22.04
2024-12-27 23:41:27,218: Snapshot:2	Epoch:5	Loss:2.42	translation_Loss:2.147	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.273                                                   	MRR:22.23	Hits@10:38.71	Best:22.23
2024-12-27 23:41:34,531: Snapshot:2	Epoch:6	Loss:2.048	translation_Loss:1.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.313                                                   	MRR:22.34	Hits@10:38.82	Best:22.34
2024-12-27 23:41:42,156: Snapshot:2	Epoch:7	Loss:1.78	translation_Loss:1.433	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.347                                                   	MRR:22.37	Hits@10:38.87	Best:22.37
2024-12-27 23:41:49,377: Snapshot:2	Epoch:8	Loss:1.564	translation_Loss:1.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.376                                                   	MRR:22.48	Hits@10:39.03	Best:22.48
2024-12-27 23:41:56,649: Snapshot:2	Epoch:9	Loss:1.412	translation_Loss:1.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:22.53	Hits@10:39.15	Best:22.53
2024-12-27 23:42:03,923: Snapshot:2	Epoch:10	Loss:1.293	translation_Loss:0.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.422                                                   	MRR:22.56	Hits@10:39.13	Best:22.56
2024-12-27 23:42:11,118: Snapshot:2	Epoch:11	Loss:1.203	translation_Loss:0.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.439                                                   	MRR:22.58	Hits@10:39.11	Best:22.58
2024-12-27 23:42:18,302: Snapshot:2	Epoch:12	Loss:1.137	translation_Loss:0.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.453                                                   	MRR:22.56	Hits@10:39.15	Best:22.58
2024-12-27 23:42:25,936: Snapshot:2	Epoch:13	Loss:1.075	translation_Loss:0.61	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.465                                                   	MRR:22.59	Hits@10:39.13	Best:22.59
2024-12-27 23:42:33,082: Snapshot:2	Epoch:14	Loss:1.04	translation_Loss:0.566	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.474                                                   	MRR:22.54	Hits@10:39.29	Best:22.59
2024-12-27 23:42:40,221: Snapshot:2	Epoch:15	Loss:1.016	translation_Loss:0.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.483                                                   	MRR:22.51	Hits@10:39.33	Best:22.59
2024-12-27 23:42:47,430: Snapshot:2	Epoch:16	Loss:0.987	translation_Loss:0.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.49                                                   	MRR:22.48	Hits@10:39.28	Best:22.59
2024-12-27 23:42:54,570: Snapshot:2	Epoch:17	Loss:0.968	translation_Loss:0.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.496                                                   	MRR:22.52	Hits@10:39.21	Best:22.59
2024-12-27 23:43:02,148: Early Stopping! Snapshot: 2 Epoch: 18 Best Results: 22.59
2024-12-27 23:43:02,148: Start to training tokens! Snapshot: 2 Epoch: 18 Loss:0.954 MRR:22.5 Best Results: 22.59
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:43:02,149: Snapshot:2	Epoch:18	Loss:0.954	translation_Loss:0.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.502                                                   	MRR:22.5	Hits@10:39.24	Best:22.59
2024-12-27 23:43:09,206: Snapshot:2	Epoch:19	Loss:20.313	translation_Loss:19.679	multi_layer_Loss:0.634	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.5	Hits@10:39.24	Best:22.59
2024-12-27 23:43:16,271: End of token training: 2 Epoch: 20 Loss:20.054 MRR:22.5 Best Results: 22.59
2024-12-27 23:43:16,271: Snapshot:2	Epoch:20	Loss:20.054	translation_Loss:19.675	multi_layer_Loss:0.379	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.5	Hits@10:39.24	Best:22.59
2024-12-27 23:43:16,566: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_1000/2model_best.tar'
2024-12-27 23:43:25,615: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2358 | 0.1474 | 0.2738 | 0.3304 |  0.4033 |
|     1      | 0.2299 | 0.1441 | 0.2659 | 0.3209 |  0.394  |
|     2      | 0.2272 | 0.1403 | 0.2628 | 0.3198 |  0.3925 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:43:50,362: Snapshot:3	Epoch:0	Loss:3.382	translation_Loss:3.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.023                                                   	MRR:19.4	Hits@10:36.9	Best:19.4
2024-12-27 23:43:57,623: Snapshot:3	Epoch:1	Loss:2.27	translation_Loss:2.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.065                                                   	MRR:19.92	Hits@10:38.07	Best:19.92
2024-12-27 23:44:05,253: Snapshot:3	Epoch:2	Loss:1.721	translation_Loss:1.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.097                                                   	MRR:20.19	Hits@10:38.65	Best:20.19
2024-12-27 23:44:12,541: Snapshot:3	Epoch:3	Loss:1.366	translation_Loss:1.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.124                                                   	MRR:20.37	Hits@10:38.92	Best:20.37
2024-12-27 23:44:19,844: Snapshot:3	Epoch:4	Loss:1.113	translation_Loss:0.967	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.146                                                   	MRR:20.56	Hits@10:39.13	Best:20.56
2024-12-27 23:44:27,100: Snapshot:3	Epoch:5	Loss:0.923	translation_Loss:0.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:20.62	Hits@10:39.25	Best:20.62
2024-12-27 23:44:34,361: Snapshot:3	Epoch:6	Loss:0.797	translation_Loss:0.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:20.69	Hits@10:39.37	Best:20.69
2024-12-27 23:44:41,676: Snapshot:3	Epoch:7	Loss:0.692	translation_Loss:0.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.19                                                   	MRR:20.66	Hits@10:39.49	Best:20.69
2024-12-27 23:44:49,411: Snapshot:3	Epoch:8	Loss:0.621	translation_Loss:0.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.199                                                   	MRR:20.71	Hits@10:39.52	Best:20.71
2024-12-27 23:44:56,665: Snapshot:3	Epoch:9	Loss:0.573	translation_Loss:0.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.207                                                   	MRR:20.77	Hits@10:39.67	Best:20.77
2024-12-27 23:45:03,914: Snapshot:3	Epoch:10	Loss:0.541	translation_Loss:0.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:20.76	Hits@10:39.74	Best:20.77
2024-12-27 23:45:11,255: Snapshot:3	Epoch:11	Loss:0.516	translation_Loss:0.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.218                                                   	MRR:20.77	Hits@10:39.65	Best:20.77
2024-12-27 23:45:18,468: Snapshot:3	Epoch:12	Loss:0.5	translation_Loss:0.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.222                                                   	MRR:20.74	Hits@10:39.69	Best:20.77
2024-12-27 23:45:26,108: Snapshot:3	Epoch:13	Loss:0.482	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.226                                                   	MRR:20.78	Hits@10:39.72	Best:20.78
2024-12-27 23:45:33,373: Snapshot:3	Epoch:14	Loss:0.469	translation_Loss:0.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:20.85	Hits@10:39.65	Best:20.85
2024-12-27 23:45:40,654: Snapshot:3	Epoch:15	Loss:0.463	translation_Loss:0.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.232                                                   	MRR:20.88	Hits@10:39.75	Best:20.88
2024-12-27 23:45:47,940: Snapshot:3	Epoch:16	Loss:0.452	translation_Loss:0.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.235                                                   	MRR:20.81	Hits@10:39.62	Best:20.88
2024-12-27 23:45:55,152: Snapshot:3	Epoch:17	Loss:0.452	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:20.84	Hits@10:39.63	Best:20.88
2024-12-27 23:46:02,779: Snapshot:3	Epoch:18	Loss:0.444	translation_Loss:0.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:20.79	Hits@10:39.63	Best:20.88
2024-12-27 23:46:10,002: Snapshot:3	Epoch:19	Loss:0.436	translation_Loss:0.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:20.73	Hits@10:39.6	Best:20.88
2024-12-27 23:46:17,272: Early Stopping! Snapshot: 3 Epoch: 20 Best Results: 20.88
2024-12-27 23:46:17,272: Start to training tokens! Snapshot: 3 Epoch: 20 Loss:0.435 MRR:20.73 Best Results: 20.88
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:46:17,273: Snapshot:3	Epoch:20	Loss:0.435	translation_Loss:0.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:20.73	Hits@10:39.75	Best:20.88
2024-12-27 23:46:24,361: Snapshot:3	Epoch:21	Loss:19.409	translation_Loss:18.787	multi_layer_Loss:0.621	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.73	Hits@10:39.75	Best:20.88
2024-12-27 23:46:31,442: End of token training: 3 Epoch: 22 Loss:19.158 MRR:20.73 Best Results: 20.88
2024-12-27 23:46:31,443: Snapshot:3	Epoch:22	Loss:19.158	translation_Loss:18.798	multi_layer_Loss:0.36	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.73	Hits@10:39.75	Best:20.88
2024-12-27 23:46:31,706: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_1000/3model_best.tar'
2024-12-27 23:46:44,389: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2146 | 0.1312 | 0.2476 | 0.3012 |  0.3728 |
|     1      | 0.2087 | 0.1266 | 0.2383 | 0.2917 |  0.3696 |
|     2      | 0.2099 | 0.1229 | 0.2407 | 0.3026 |  0.3832 |
|     3      | 0.2088 | 0.1138 | 0.2419 | 0.3105 |  0.3973 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 23:47:09,667: Snapshot:4	Epoch:0	Loss:2.052	translation_Loss:2.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.019                                                   	MRR:19.4	Hits@10:42.88	Best:19.4
2024-12-27 23:47:17,029: Snapshot:4	Epoch:1	Loss:1.448	translation_Loss:1.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.049                                                   	MRR:20.52	Hits@10:45.08	Best:20.52
2024-12-27 23:47:24,339: Snapshot:4	Epoch:2	Loss:1.15	translation_Loss:1.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.064                                                   	MRR:21.02	Hits@10:45.81	Best:21.02
2024-12-27 23:47:31,743: Snapshot:4	Epoch:3	Loss:0.901	translation_Loss:0.827	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.075                                                   	MRR:21.36	Hits@10:46.53	Best:21.36
2024-12-27 23:47:39,103: Snapshot:4	Epoch:4	Loss:0.699	translation_Loss:0.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.082                                                   	MRR:21.63	Hits@10:47.07	Best:21.63
2024-12-27 23:47:46,577: Snapshot:4	Epoch:5	Loss:0.546	translation_Loss:0.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.087                                                   	MRR:21.77	Hits@10:47.21	Best:21.77
2024-12-27 23:47:53,960: Snapshot:4	Epoch:6	Loss:0.426	translation_Loss:0.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.09                                                   	MRR:22.13	Hits@10:47.54	Best:22.13
2024-12-27 23:48:01,290: Snapshot:4	Epoch:7	Loss:0.342	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.089                                                   	MRR:22.23	Hits@10:47.67	Best:22.23
2024-12-27 23:48:08,732: Snapshot:4	Epoch:8	Loss:0.279	translation_Loss:0.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.086                                                   	MRR:22.43	Hits@10:47.71	Best:22.43
2024-12-27 23:48:16,112: Snapshot:4	Epoch:9	Loss:0.24	translation_Loss:0.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.083                                                   	MRR:22.61	Hits@10:47.83	Best:22.61
2024-12-27 23:48:23,483: Snapshot:4	Epoch:10	Loss:0.211	translation_Loss:0.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.08                                                   	MRR:22.86	Hits@10:48.09	Best:22.86
2024-12-27 23:48:30,878: Snapshot:4	Epoch:11	Loss:0.186	translation_Loss:0.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.077                                                   	MRR:22.87	Hits@10:48.0	Best:22.87
2024-12-27 23:48:38,271: Snapshot:4	Epoch:12	Loss:0.176	translation_Loss:0.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.075                                                   	MRR:22.89	Hits@10:48.03	Best:22.89
2024-12-27 23:48:45,688: Snapshot:4	Epoch:13	Loss:0.16	translation_Loss:0.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.073                                                   	MRR:22.98	Hits@10:48.28	Best:22.98
2024-12-27 23:48:53,484: Snapshot:4	Epoch:14	Loss:0.156	translation_Loss:0.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.072                                                   	MRR:22.84	Hits@10:48.22	Best:22.98
2024-12-27 23:49:00,844: Snapshot:4	Epoch:15	Loss:0.145	translation_Loss:0.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.071                                                   	MRR:22.88	Hits@10:48.15	Best:22.98
2024-12-27 23:49:08,144: Snapshot:4	Epoch:16	Loss:0.14	translation_Loss:0.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:22.89	Hits@10:48.17	Best:22.98
2024-12-27 23:49:15,465: Snapshot:4	Epoch:17	Loss:0.138	translation_Loss:0.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:22.84	Hits@10:48.11	Best:22.98
2024-12-27 23:49:22,806: Snapshot:4	Epoch:18	Loss:0.137	translation_Loss:0.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.04	Hits@10:48.52	Best:23.04
2024-12-27 23:49:30,524: Snapshot:4	Epoch:19	Loss:0.131	translation_Loss:0.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.29	Hits@10:48.59	Best:23.29
2024-12-27 23:49:37,868: Snapshot:4	Epoch:20	Loss:0.133	translation_Loss:0.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.29	Hits@10:48.74	Best:23.29
2024-12-27 23:49:45,306: Snapshot:4	Epoch:21	Loss:0.129	translation_Loss:0.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.37	Hits@10:48.75	Best:23.37
2024-12-27 23:49:52,596: Snapshot:4	Epoch:22	Loss:0.126	translation_Loss:0.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.36	Hits@10:48.68	Best:23.37
2024-12-27 23:49:59,917: Snapshot:4	Epoch:23	Loss:0.13	translation_Loss:0.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.53	Hits@10:48.84	Best:23.53
2024-12-27 23:50:07,350: Snapshot:4	Epoch:24	Loss:0.125	translation_Loss:0.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.47	Hits@10:48.89	Best:23.53
2024-12-27 23:50:15,150: Snapshot:4	Epoch:25	Loss:0.122	translation_Loss:0.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.48	Hits@10:48.96	Best:23.53
2024-12-27 23:50:22,422: Snapshot:4	Epoch:26	Loss:0.123	translation_Loss:0.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.41	Hits@10:49.02	Best:23.53
2024-12-27 23:50:29,691: Snapshot:4	Epoch:27	Loss:0.123	translation_Loss:0.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.33	Hits@10:48.96	Best:23.53
2024-12-27 23:50:36,973: Early Stopping! Snapshot: 4 Epoch: 28 Best Results: 23.53
2024-12-27 23:50:36,973: Start to training tokens! Snapshot: 4 Epoch: 28 Loss:0.118 MRR:23.37 Best Results: 23.53
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 23:50:36,973: Snapshot:4	Epoch:28	Loss:0.118	translation_Loss:0.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.07                                                   	MRR:23.37	Hits@10:48.73	Best:23.53
2024-12-27 23:50:44,117: Snapshot:4	Epoch:29	Loss:16.384	translation_Loss:15.759	multi_layer_Loss:0.625	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.37	Hits@10:48.73	Best:23.53
2024-12-27 23:50:51,686: End of token training: 4 Epoch: 30 Loss:16.131 MRR:23.37 Best Results: 23.53
2024-12-27 23:50:51,686: Snapshot:4	Epoch:30	Loss:16.131	translation_Loss:15.769	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.37	Hits@10:48.73	Best:23.53
2024-12-27 23:50:51,941: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_1000/4model_best.tar'
2024-12-27 23:51:07,990: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1949 | 0.1148 | 0.2245 | 0.2747 |  0.3487 |
|     1      | 0.1871 | 0.1083 | 0.2146 | 0.2673 |  0.3416 |
|     2      | 0.1854 | 0.1025 | 0.2125 | 0.2699 |  0.3508 |
|     3      | 0.1838 | 0.0933 | 0.2091 | 0.2757 |  0.3726 |
|     4      | 0.2365 | 0.115  | 0.2798 | 0.3668 |  0.4858 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 23:51:07,992: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2344 | 0.147  | 0.2788 | 0.3251 |  0.3887 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1561 | 0.2912 | 0.3483 |  0.4195 |
|     1      | 0.2335 | 0.1489 | 0.273  | 0.323  |  0.387  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2358 | 0.1474 | 0.2738 | 0.3304 |  0.4033 |
|     1      | 0.2299 | 0.1441 | 0.2659 | 0.3209 |  0.394  |
|     2      | 0.2272 | 0.1403 | 0.2628 | 0.3198 |  0.3925 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2146 | 0.1312 | 0.2476 | 0.3012 |  0.3728 |
|     1      | 0.2087 | 0.1266 | 0.2383 | 0.2917 |  0.3696 |
|     2      | 0.2099 | 0.1229 | 0.2407 | 0.3026 |  0.3832 |
|     3      | 0.2088 | 0.1138 | 0.2419 | 0.3105 |  0.3973 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1949 | 0.1148 | 0.2245 | 0.2747 |  0.3487 |
|     1      | 0.1871 | 0.1083 | 0.2146 | 0.2673 |  0.3416 |
|     2      | 0.1854 | 0.1025 | 0.2125 | 0.2699 |  0.3508 |
|     3      | 0.1838 | 0.0933 | 0.2091 | 0.2757 |  0.3726 |
|     4      | 0.2365 | 0.115  | 0.2798 | 0.3668 |  0.4858 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 23:51:07,993: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     |  861.85635638237   |   0.234   |    0.147     |    0.279     |     0.389     |
|    1     | 312.9955897331238  |   0.241   |    0.152     |    0.282     |     0.403     |
|    2     | 166.75832843780518 |   0.231   |    0.144     |    0.268     |     0.397     |
|    3     | 182.5070505142212  |    0.21   |    0.124     |    0.242     |     0.381     |
|    4     | 243.87661838531494 |   0.198   |    0.107     |    0.228     |      0.38     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 23:51:07,993: Sum_Training_Time:1767.993943452835
2024-12-27 23:51:07,993: Every_Training_Time:[861.85635638237, 312.9955897331238, 166.75832843780518, 182.5070505142212, 243.87661838531494]
2024-12-27 23:51:07,993: Forward transfer: 0.17545 Backward transfer: -0.038175000000000014
2024-12-27 23:51:47,045: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227235112/FACTfact_0.0001_2048_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_2048_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_2048_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 23:51:57,041: Snapshot:0	Epoch:0	Loss:26.399	translation_Loss:26.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.43	Best:1.42
2024-12-27 23:52:03,500: Snapshot:0	Epoch:1	Loss:25.069	translation_Loss:25.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.81	Hits@10:2.56	Best:1.81
2024-12-27 23:52:10,337: Snapshot:0	Epoch:2	Loss:23.874	translation_Loss:23.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.3	Best:2.98
2024-12-27 23:52:16,794: Snapshot:0	Epoch:3	Loss:22.731	translation_Loss:22.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.96	Hits@10:7.35	Best:3.96
2024-12-27 23:52:23,193: Snapshot:0	Epoch:4	Loss:21.623	translation_Loss:21.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.72	Hits@10:9.23	Best:4.72
2024-12-27 23:52:29,637: Snapshot:0	Epoch:5	Loss:20.546	translation_Loss:20.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.38	Hits@10:10.87	Best:5.38
2024-12-27 23:52:36,120: Snapshot:0	Epoch:6	Loss:19.518	translation_Loss:19.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.01	Hits@10:12.51	Best:6.01
2024-12-27 23:52:43,103: Snapshot:0	Epoch:7	Loss:18.524	translation_Loss:18.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.64	Hits@10:14.27	Best:6.64
2024-12-27 23:52:49,561: Snapshot:0	Epoch:8	Loss:17.557	translation_Loss:17.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.25	Hits@10:15.93	Best:7.25
2024-12-27 23:52:55,975: Snapshot:0	Epoch:9	Loss:16.633	translation_Loss:16.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.8	Hits@10:17.36	Best:7.8
2024-12-27 23:53:02,390: Snapshot:0	Epoch:10	Loss:15.731	translation_Loss:15.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.35	Hits@10:18.98	Best:8.35
2024-12-27 23:53:08,845: Snapshot:0	Epoch:11	Loss:14.856	translation_Loss:14.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.92	Hits@10:20.48	Best:8.92
2024-12-27 23:53:15,291: Snapshot:0	Epoch:12	Loss:14.025	translation_Loss:14.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.48	Hits@10:21.89	Best:9.48
2024-12-27 23:53:22,161: Snapshot:0	Epoch:13	Loss:13.222	translation_Loss:13.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.05	Hits@10:23.26	Best:10.05
2024-12-27 23:53:28,661: Snapshot:0	Epoch:14	Loss:12.461	translation_Loss:12.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.58	Hits@10:24.57	Best:10.58
2024-12-27 23:53:35,114: Snapshot:0	Epoch:15	Loss:11.742	translation_Loss:11.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.15	Hits@10:25.75	Best:11.15
2024-12-27 23:53:41,574: Snapshot:0	Epoch:16	Loss:11.041	translation_Loss:11.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.72	Hits@10:26.86	Best:11.72
2024-12-27 23:53:48,004: Snapshot:0	Epoch:17	Loss:10.379	translation_Loss:10.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.24	Hits@10:27.79	Best:12.24
2024-12-27 23:53:54,892: Snapshot:0	Epoch:18	Loss:9.757	translation_Loss:9.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.71	Hits@10:28.74	Best:12.71
2024-12-27 23:54:01,346: Snapshot:0	Epoch:19	Loss:9.158	translation_Loss:9.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.2	Hits@10:29.55	Best:13.2
2024-12-27 23:54:07,760: Snapshot:0	Epoch:20	Loss:8.589	translation_Loss:8.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.66	Hits@10:30.21	Best:13.66
2024-12-27 23:54:14,181: Snapshot:0	Epoch:21	Loss:8.072	translation_Loss:8.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.11	Hits@10:31.02	Best:14.11
2024-12-27 23:54:20,660: Snapshot:0	Epoch:22	Loss:7.58	translation_Loss:7.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.57	Hits@10:31.79	Best:14.57
2024-12-27 23:54:27,082: Snapshot:0	Epoch:23	Loss:7.1	translation_Loss:7.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.02	Hits@10:32.43	Best:15.02
2024-12-27 23:54:34,010: Snapshot:0	Epoch:24	Loss:6.658	translation_Loss:6.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.41	Hits@10:33.0	Best:15.41
2024-12-27 23:54:40,437: Snapshot:0	Epoch:25	Loss:6.25	translation_Loss:6.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.83	Hits@10:33.51	Best:15.83
2024-12-27 23:54:46,939: Snapshot:0	Epoch:26	Loss:5.851	translation_Loss:5.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.24	Hits@10:34.03	Best:16.24
2024-12-27 23:54:53,324: Snapshot:0	Epoch:27	Loss:5.48	translation_Loss:5.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:34.45	Best:16.66
2024-12-27 23:54:59,813: Snapshot:0	Epoch:28	Loss:5.134	translation_Loss:5.134	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.05	Hits@10:34.84	Best:17.05
2024-12-27 23:55:06,747: Snapshot:0	Epoch:29	Loss:4.788	translation_Loss:4.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.44	Hits@10:35.28	Best:17.44
2024-12-27 23:55:13,258: Snapshot:0	Epoch:30	Loss:4.495	translation_Loss:4.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.8	Hits@10:35.57	Best:17.8
2024-12-27 23:55:19,693: Snapshot:0	Epoch:31	Loss:4.201	translation_Loss:4.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.14	Hits@10:35.97	Best:18.14
2024-12-27 23:55:26,125: Snapshot:0	Epoch:32	Loss:3.933	translation_Loss:3.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.45	Hits@10:36.25	Best:18.45
2024-12-27 23:55:32,552: Snapshot:0	Epoch:33	Loss:3.658	translation_Loss:3.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.78	Hits@10:36.65	Best:18.78
2024-12-27 23:55:39,000: Snapshot:0	Epoch:34	Loss:3.417	translation_Loss:3.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.12	Hits@10:36.9	Best:19.12
2024-12-27 23:55:45,965: Snapshot:0	Epoch:35	Loss:3.193	translation_Loss:3.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.42	Hits@10:37.17	Best:19.42
2024-12-27 23:55:52,417: Snapshot:0	Epoch:36	Loss:2.978	translation_Loss:2.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.73	Hits@10:37.42	Best:19.73
2024-12-27 23:55:58,867: Snapshot:0	Epoch:37	Loss:2.784	translation_Loss:2.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.95	Hits@10:37.54	Best:19.95
2024-12-27 23:56:05,332: Snapshot:0	Epoch:38	Loss:2.59	translation_Loss:2.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.17	Hits@10:37.72	Best:20.17
2024-12-27 23:56:11,735: Snapshot:0	Epoch:39	Loss:2.419	translation_Loss:2.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.42	Hits@10:37.95	Best:20.42
2024-12-27 23:56:18,559: Snapshot:0	Epoch:40	Loss:2.264	translation_Loss:2.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.65	Hits@10:38.16	Best:20.65
2024-12-27 23:56:24,969: Snapshot:0	Epoch:41	Loss:2.112	translation_Loss:2.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.86	Hits@10:38.21	Best:20.86
2024-12-27 23:56:31,379: Snapshot:0	Epoch:42	Loss:1.97	translation_Loss:1.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.05	Hits@10:38.4	Best:21.05
2024-12-27 23:56:37,819: Snapshot:0	Epoch:43	Loss:1.836	translation_Loss:1.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.26	Hits@10:38.54	Best:21.26
2024-12-27 23:56:44,290: Snapshot:0	Epoch:44	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.42	Hits@10:38.76	Best:21.42
2024-12-27 23:56:50,728: Snapshot:0	Epoch:45	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.58	Hits@10:38.88	Best:21.58
2024-12-27 23:56:57,579: Snapshot:0	Epoch:46	Loss:1.495	translation_Loss:1.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.7	Hits@10:38.94	Best:21.7
2024-12-27 23:57:03,980: Snapshot:0	Epoch:47	Loss:1.406	translation_Loss:1.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.85	Hits@10:39.0	Best:21.85
2024-12-27 23:57:10,404: Snapshot:0	Epoch:48	Loss:1.32	translation_Loss:1.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.99	Hits@10:39.06	Best:21.99
2024-12-27 23:57:16,866: Snapshot:0	Epoch:49	Loss:1.234	translation_Loss:1.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.15	Hits@10:39.21	Best:22.15
2024-12-27 23:57:23,269: Snapshot:0	Epoch:50	Loss:1.157	translation_Loss:1.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.28	Hits@10:39.13	Best:22.28
2024-12-27 23:57:30,098: Snapshot:0	Epoch:51	Loss:1.086	translation_Loss:1.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.41	Hits@10:39.24	Best:22.41
2024-12-27 23:57:36,531: Snapshot:0	Epoch:52	Loss:1.024	translation_Loss:1.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.54	Hits@10:39.23	Best:22.54
2024-12-27 23:57:43,000: Snapshot:0	Epoch:53	Loss:0.965	translation_Loss:0.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.63	Hits@10:39.29	Best:22.63
2024-12-27 23:57:49,418: Snapshot:0	Epoch:54	Loss:0.91	translation_Loss:0.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.77	Hits@10:39.36	Best:22.77
2024-12-27 23:57:55,862: Snapshot:0	Epoch:55	Loss:0.856	translation_Loss:0.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.81	Hits@10:39.36	Best:22.81
2024-12-27 23:58:02,305: Snapshot:0	Epoch:56	Loss:0.816	translation_Loss:0.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.89	Hits@10:39.37	Best:22.89
2024-12-27 23:58:09,233: Snapshot:0	Epoch:57	Loss:0.769	translation_Loss:0.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.95	Hits@10:39.31	Best:22.95
2024-12-27 23:58:15,658: Snapshot:0	Epoch:58	Loss:0.733	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.01	Hits@10:39.43	Best:23.01
2024-12-27 23:58:22,088: Snapshot:0	Epoch:59	Loss:0.688	translation_Loss:0.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.12	Hits@10:39.44	Best:23.12
2024-12-27 23:58:28,563: Snapshot:0	Epoch:60	Loss:0.649	translation_Loss:0.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.13	Hits@10:39.47	Best:23.13
2024-12-27 23:58:34,983: Snapshot:0	Epoch:61	Loss:0.625	translation_Loss:0.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.18	Hits@10:39.56	Best:23.18
2024-12-27 23:58:41,791: Snapshot:0	Epoch:62	Loss:0.592	translation_Loss:0.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.22	Hits@10:39.47	Best:23.22
2024-12-27 23:58:48,265: Snapshot:0	Epoch:63	Loss:0.563	translation_Loss:0.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:39.46	Best:23.24
2024-12-27 23:58:54,694: Snapshot:0	Epoch:64	Loss:0.532	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.26	Hits@10:39.5	Best:23.26
2024-12-27 23:59:01,106: Snapshot:0	Epoch:65	Loss:0.51	translation_Loss:0.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.32	Hits@10:39.55	Best:23.32
2024-12-27 23:59:07,523: Snapshot:0	Epoch:66	Loss:0.494	translation_Loss:0.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.36	Hits@10:39.51	Best:23.36
2024-12-27 23:59:14,034: Snapshot:0	Epoch:67	Loss:0.479	translation_Loss:0.479	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.39	Hits@10:39.64	Best:23.39
2024-12-27 23:59:20,863: Snapshot:0	Epoch:68	Loss:0.446	translation_Loss:0.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.44	Hits@10:39.67	Best:23.44
2024-12-27 23:59:27,281: Snapshot:0	Epoch:69	Loss:0.427	translation_Loss:0.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:39.75	Best:23.45
2024-12-27 23:59:33,718: Snapshot:0	Epoch:70	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.52	Hits@10:39.79	Best:23.52
2024-12-27 23:59:40,170: Snapshot:0	Epoch:71	Loss:0.395	translation_Loss:0.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.55	Hits@10:39.79	Best:23.55
2024-12-27 23:59:46,673: Snapshot:0	Epoch:72	Loss:0.38	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.77	Best:23.58
2024-12-27 23:59:53,476: Snapshot:0	Epoch:73	Loss:0.367	translation_Loss:0.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.85	Best:23.67
2024-12-27 23:59:59,852: Snapshot:0	Epoch:74	Loss:0.353	translation_Loss:0.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.92	Best:23.67
2024-12-28 00:00:06,314: Snapshot:0	Epoch:75	Loss:0.345	translation_Loss:0.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.85	Best:23.67
2024-12-28 00:00:12,800: Snapshot:0	Epoch:76	Loss:0.332	translation_Loss:0.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.71	Hits@10:39.81	Best:23.71
2024-12-28 00:00:19,278: Snapshot:0	Epoch:77	Loss:0.324	translation_Loss:0.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.85	Best:23.71
2024-12-28 00:00:25,702: Snapshot:0	Epoch:78	Loss:0.307	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:39.91	Best:23.74
2024-12-28 00:00:32,468: Snapshot:0	Epoch:79	Loss:0.299	translation_Loss:0.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.73	Hits@10:39.83	Best:23.74
2024-12-28 00:00:38,922: Snapshot:0	Epoch:80	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:39.86	Best:23.75
2024-12-28 00:00:45,401: Snapshot:0	Epoch:81	Loss:0.284	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.79	Best:23.76
2024-12-28 00:00:51,769: Snapshot:0	Epoch:82	Loss:0.279	translation_Loss:0.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.74	Hits@10:39.86	Best:23.76
2024-12-28 00:00:58,214: Snapshot:0	Epoch:83	Loss:0.263	translation_Loss:0.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.78	Hits@10:39.92	Best:23.78
2024-12-28 00:01:05,091: Snapshot:0	Epoch:84	Loss:0.256	translation_Loss:0.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.78	Hits@10:39.93	Best:23.78
2024-12-28 00:01:11,502: Snapshot:0	Epoch:85	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.88	Hits@10:39.88	Best:23.88
2024-12-28 00:01:18,010: Snapshot:0	Epoch:86	Loss:0.244	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:39.9	Best:23.89
2024-12-28 00:01:24,456: Snapshot:0	Epoch:87	Loss:0.242	translation_Loss:0.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:39.96	Best:23.95
2024-12-28 00:01:30,972: Snapshot:0	Epoch:88	Loss:0.232	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.97	Hits@10:40.0	Best:23.97
2024-12-28 00:01:37,346: Snapshot:0	Epoch:89	Loss:0.228	translation_Loss:0.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.93	Hits@10:39.98	Best:23.97
2024-12-28 00:01:44,180: Snapshot:0	Epoch:90	Loss:0.224	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:40.02	Best:23.97
2024-12-28 00:01:50,633: Snapshot:0	Epoch:91	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:40.01	Best:23.98
2024-12-28 00:01:57,012: Snapshot:0	Epoch:92	Loss:0.21	translation_Loss:0.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:39.92	Best:23.98
2024-12-28 00:02:03,361: Snapshot:0	Epoch:93	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:40.0	Best:23.98
2024-12-28 00:02:09,798: Snapshot:0	Epoch:94	Loss:0.203	translation_Loss:0.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:39.99	Best:23.98
2024-12-28 00:02:16,595: Snapshot:0	Epoch:95	Loss:0.196	translation_Loss:0.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:40.08	Best:23.99
2024-12-28 00:02:22,978: Snapshot:0	Epoch:96	Loss:0.194	translation_Loss:0.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.0	Hits@10:40.09	Best:24.0
2024-12-28 00:02:29,322: Snapshot:0	Epoch:97	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.97	Hits@10:40.09	Best:24.0
2024-12-28 00:02:35,702: Snapshot:0	Epoch:98	Loss:0.184	translation_Loss:0.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.94	Hits@10:40.02	Best:24.0
2024-12-28 00:02:42,121: Snapshot:0	Epoch:99	Loss:0.183	translation_Loss:0.183	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.01	Hits@10:40.06	Best:24.01
2024-12-28 00:02:48,535: Snapshot:0	Epoch:100	Loss:0.179	translation_Loss:0.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.02	Hits@10:39.95	Best:24.02
2024-12-28 00:02:55,311: Snapshot:0	Epoch:101	Loss:0.173	translation_Loss:0.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.9	Best:24.08
2024-12-28 00:03:01,725: Snapshot:0	Epoch:102	Loss:0.169	translation_Loss:0.169	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:39.77	Best:24.08
2024-12-28 00:03:08,084: Snapshot:0	Epoch:103	Loss:0.17	translation_Loss:0.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.05	Hits@10:39.9	Best:24.08
2024-12-28 00:03:14,429: Snapshot:0	Epoch:104	Loss:0.162	translation_Loss:0.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.98	Best:24.08
2024-12-28 00:03:20,833: Snapshot:0	Epoch:105	Loss:0.162	translation_Loss:0.162	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.08	Hits@10:39.91	Best:24.08
2024-12-28 00:03:27,615: Snapshot:0	Epoch:106	Loss:0.156	translation_Loss:0.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:39.96	Best:24.11
2024-12-28 00:03:33,977: Snapshot:0	Epoch:107	Loss:0.155	translation_Loss:0.155	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.1	Hits@10:39.98	Best:24.11
2024-12-28 00:03:40,421: Snapshot:0	Epoch:108	Loss:0.153	translation_Loss:0.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:39.91	Best:24.13
2024-12-28 00:03:46,858: Snapshot:0	Epoch:109	Loss:0.146	translation_Loss:0.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.09	Hits@10:39.97	Best:24.13
2024-12-28 00:03:53,211: Snapshot:0	Epoch:110	Loss:0.145	translation_Loss:0.145	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:39.98	Best:24.13
2024-12-28 00:03:59,586: Snapshot:0	Epoch:111	Loss:0.149	translation_Loss:0.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.11	Hits@10:39.95	Best:24.13
2024-12-28 00:04:06,335: Snapshot:0	Epoch:112	Loss:0.144	translation_Loss:0.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.12	Hits@10:39.9	Best:24.13
2024-12-28 00:04:12,738: Early Stopping! Snapshot: 0 Epoch: 113 Best Results: 24.13
2024-12-28 00:04:12,738: Start to training tokens! Snapshot: 0 Epoch: 113 Loss:0.139 MRR:24.13 Best Results: 24.13
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:04:12,739: Snapshot:0	Epoch:113	Loss:0.139	translation_Loss:0.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:39.92	Best:24.13
2024-12-28 00:04:19,684: Snapshot:0	Epoch:114	Loss:18.777	translation_Loss:18.151	multi_layer_Loss:0.626	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.13	Hits@10:39.92	Best:24.13
2024-12-28 00:04:26,072: End of token training: 0 Epoch: 115 Loss:18.517 MRR:24.13 Best Results: 24.13
2024-12-28 00:04:26,072: Snapshot:0	Epoch:115	Loss:18.517	translation_Loss:18.142	multi_layer_Loss:0.375	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.13	Hits@10:39.92	Best:24.13
2024-12-28 00:04:26,275: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_5000/0model_best.tar'
2024-12-28 00:04:29,447: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.234 | 0.1469 | 0.2792 | 0.3266 |  0.389  |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:04:53,948: Snapshot:1	Epoch:0	Loss:12.206	translation_Loss:12.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.075                                                   	MRR:17.72	Hits@10:30.55	Best:17.72
2024-12-28 00:05:01,369: Snapshot:1	Epoch:1	Loss:10.608	translation_Loss:10.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.224                                                   	MRR:18.41	Hits@10:31.62	Best:18.41
2024-12-28 00:05:08,488: Snapshot:1	Epoch:2	Loss:9.578	translation_Loss:9.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.424                                                   	MRR:18.9	Hits@10:32.45	Best:18.9
2024-12-28 00:05:15,676: Snapshot:1	Epoch:3	Loss:8.801	translation_Loss:8.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.642                                                   	MRR:19.3	Hits@10:33.07	Best:19.3
2024-12-28 00:05:22,723: Snapshot:1	Epoch:4	Loss:8.184	translation_Loss:7.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.852                                                   	MRR:19.66	Hits@10:33.59	Best:19.66
2024-12-28 00:05:29,828: Snapshot:1	Epoch:5	Loss:7.652	translation_Loss:6.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.045                                                   	MRR:19.94	Hits@10:34.03	Best:19.94
2024-12-28 00:05:37,256: Snapshot:1	Epoch:6	Loss:7.202	translation_Loss:5.986	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.216                                                   	MRR:20.24	Hits@10:34.49	Best:20.24
2024-12-28 00:05:44,347: Snapshot:1	Epoch:7	Loss:6.803	translation_Loss:5.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.367                                                   	MRR:20.55	Hits@10:34.87	Best:20.55
2024-12-28 00:05:51,392: Snapshot:1	Epoch:8	Loss:6.464	translation_Loss:4.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.496                                                   	MRR:20.73	Hits@10:35.18	Best:20.73
2024-12-28 00:05:58,508: Snapshot:1	Epoch:9	Loss:6.158	translation_Loss:4.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.606                                                   	MRR:20.89	Hits@10:35.42	Best:20.89
2024-12-28 00:06:05,647: Snapshot:1	Epoch:10	Loss:5.898	translation_Loss:4.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.699                                                   	MRR:21.0	Hits@10:35.8	Best:21.0
2024-12-28 00:06:12,754: Snapshot:1	Epoch:11	Loss:5.686	translation_Loss:3.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.777                                                   	MRR:21.2	Hits@10:36.05	Best:21.2
2024-12-28 00:06:20,299: Snapshot:1	Epoch:12	Loss:5.508	translation_Loss:3.666	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.842                                                   	MRR:21.35	Hits@10:36.27	Best:21.35
2024-12-28 00:06:27,320: Snapshot:1	Epoch:13	Loss:5.355	translation_Loss:3.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.897                                                   	MRR:21.49	Hits@10:36.36	Best:21.49
2024-12-28 00:06:34,409: Snapshot:1	Epoch:14	Loss:5.236	translation_Loss:3.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.943                                                   	MRR:21.59	Hits@10:36.53	Best:21.59
2024-12-28 00:06:41,494: Snapshot:1	Epoch:15	Loss:5.14	translation_Loss:3.16	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.98                                                   	MRR:21.67	Hits@10:36.71	Best:21.67
2024-12-28 00:06:48,609: Snapshot:1	Epoch:16	Loss:5.041	translation_Loss:3.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.012                                                   	MRR:21.75	Hits@10:36.81	Best:21.75
2024-12-28 00:06:56,057: Snapshot:1	Epoch:17	Loss:4.977	translation_Loss:2.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.036                                                   	MRR:21.81	Hits@10:36.8	Best:21.81
2024-12-28 00:07:03,097: Snapshot:1	Epoch:18	Loss:4.925	translation_Loss:2.867	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.058                                                   	MRR:21.85	Hits@10:36.82	Best:21.85
2024-12-28 00:07:10,131: Snapshot:1	Epoch:19	Loss:4.894	translation_Loss:2.817	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.077                                                   	MRR:21.83	Hits@10:36.96	Best:21.85
2024-12-28 00:07:17,232: Snapshot:1	Epoch:20	Loss:4.851	translation_Loss:2.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.093                                                   	MRR:21.98	Hits@10:37.06	Best:21.98
2024-12-28 00:07:24,346: Snapshot:1	Epoch:21	Loss:4.806	translation_Loss:2.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.108                                                   	MRR:21.96	Hits@10:37.11	Best:21.98
2024-12-28 00:07:31,843: Snapshot:1	Epoch:22	Loss:4.779	translation_Loss:2.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.118                                                   	MRR:21.99	Hits@10:37.13	Best:21.99
2024-12-28 00:07:38,902: Snapshot:1	Epoch:23	Loss:4.748	translation_Loss:2.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.127                                                   	MRR:22.05	Hits@10:37.13	Best:22.05
2024-12-28 00:07:45,943: Snapshot:1	Epoch:24	Loss:4.732	translation_Loss:2.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.136                                                   	MRR:22.0	Hits@10:37.13	Best:22.05
2024-12-28 00:07:52,984: Snapshot:1	Epoch:25	Loss:4.724	translation_Loss:2.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.144                                                   	MRR:22.05	Hits@10:37.2	Best:22.05
2024-12-28 00:07:59,969: Snapshot:1	Epoch:26	Loss:4.697	translation_Loss:2.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.15                                                   	MRR:22.04	Hits@10:37.3	Best:22.05
2024-12-28 00:08:07,016: Snapshot:1	Epoch:27	Loss:4.672	translation_Loss:2.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.157                                                   	MRR:22.07	Hits@10:37.3	Best:22.07
2024-12-28 00:08:14,439: Snapshot:1	Epoch:28	Loss:4.667	translation_Loss:2.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.161                                                   	MRR:22.09	Hits@10:37.36	Best:22.09
2024-12-28 00:08:21,549: Snapshot:1	Epoch:29	Loss:4.664	translation_Loss:2.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.167                                                   	MRR:22.14	Hits@10:37.34	Best:22.14
2024-12-28 00:08:28,548: Snapshot:1	Epoch:30	Loss:4.644	translation_Loss:2.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.172                                                   	MRR:22.14	Hits@10:37.42	Best:22.14
2024-12-28 00:08:35,584: Snapshot:1	Epoch:31	Loss:4.633	translation_Loss:2.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.175                                                   	MRR:22.17	Hits@10:37.48	Best:22.17
2024-12-28 00:08:42,724: Snapshot:1	Epoch:32	Loss:4.613	translation_Loss:2.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.178                                                   	MRR:22.17	Hits@10:37.34	Best:22.17
2024-12-28 00:08:50,176: Snapshot:1	Epoch:33	Loss:4.613	translation_Loss:2.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.182                                                   	MRR:22.13	Hits@10:37.35	Best:22.17
2024-12-28 00:08:57,216: Snapshot:1	Epoch:34	Loss:4.602	translation_Loss:2.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.185                                                   	MRR:22.13	Hits@10:37.42	Best:22.17
2024-12-28 00:09:04,241: Snapshot:1	Epoch:35	Loss:4.593	translation_Loss:2.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.187                                                   	MRR:22.13	Hits@10:37.44	Best:22.17
2024-12-28 00:09:11,254: Early Stopping! Snapshot: 1 Epoch: 36 Best Results: 22.17
2024-12-28 00:09:11,255: Start to training tokens! Snapshot: 1 Epoch: 36 Loss:4.586 MRR:22.14 Best Results: 22.17
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:09:11,255: Snapshot:1	Epoch:36	Loss:4.586	translation_Loss:2.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.19                                                   	MRR:22.14	Hits@10:37.36	Best:22.17
2024-12-28 00:09:18,118: Snapshot:1	Epoch:37	Loss:21.236	translation_Loss:20.619	multi_layer_Loss:0.617	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.14	Hits@10:37.36	Best:22.17
2024-12-28 00:09:24,961: End of token training: 1 Epoch: 38 Loss:20.972 MRR:22.14 Best Results: 22.17
2024-12-28 00:09:24,961: Snapshot:1	Epoch:38	Loss:20.972	translation_Loss:20.61	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.14	Hits@10:37.36	Best:22.17
2024-12-28 00:09:25,240: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_5000/1model_best.tar'
2024-12-28 00:09:31,695: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2509 | 0.1605 | 0.2941 | 0.3482 |  0.4172 |
|     1      | 0.2215 | 0.1362 | 0.2637 | 0.3137 |  0.3756 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:09:56,759: Snapshot:2	Epoch:0	Loss:8.101	translation_Loss:8.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.075                                                   	MRR:19.74	Hits@10:34.8	Best:19.74
2024-12-28 00:10:04,039: Snapshot:2	Epoch:1	Loss:6.581	translation_Loss:6.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.21                                                   	MRR:20.27	Hits@10:35.68	Best:20.27
2024-12-28 00:10:11,386: Snapshot:2	Epoch:2	Loss:5.687	translation_Loss:5.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:20.62	Hits@10:36.31	Best:20.62
2024-12-28 00:10:18,673: Snapshot:2	Epoch:3	Loss:5.069	translation_Loss:4.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.551                                                   	MRR:20.91	Hits@10:36.75	Best:20.91
2024-12-28 00:10:25,909: Snapshot:2	Epoch:4	Loss:4.612	translation_Loss:3.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.713                                                   	MRR:21.05	Hits@10:37.06	Best:21.05
2024-12-28 00:10:33,150: Snapshot:2	Epoch:5	Loss:4.266	translation_Loss:3.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.856                                                   	MRR:21.29	Hits@10:37.3	Best:21.29
2024-12-28 00:10:40,431: Snapshot:2	Epoch:6	Loss:3.985	translation_Loss:3.005	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.98                                                   	MRR:21.39	Hits@10:37.6	Best:21.39
2024-12-28 00:10:47,775: Snapshot:2	Epoch:7	Loss:3.761	translation_Loss:2.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.084                                                   	MRR:21.52	Hits@10:37.81	Best:21.52
2024-12-28 00:10:55,043: Snapshot:2	Epoch:8	Loss:3.571	translation_Loss:2.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.169                                                   	MRR:21.65	Hits@10:37.86	Best:21.65
2024-12-28 00:11:02,364: Snapshot:2	Epoch:9	Loss:3.455	translation_Loss:2.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.24                                                   	MRR:21.74	Hits@10:38.02	Best:21.74
2024-12-28 00:11:09,600: Snapshot:2	Epoch:10	Loss:3.323	translation_Loss:2.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.3                                                   	MRR:21.79	Hits@10:38.21	Best:21.79
2024-12-28 00:11:16,893: Snapshot:2	Epoch:11	Loss:3.25	translation_Loss:1.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.346                                                   	MRR:21.87	Hits@10:38.25	Best:21.87
2024-12-28 00:11:24,146: Snapshot:2	Epoch:12	Loss:3.18	translation_Loss:1.796	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.385                                                   	MRR:21.96	Hits@10:38.47	Best:21.96
2024-12-28 00:11:31,348: Snapshot:2	Epoch:13	Loss:3.124	translation_Loss:1.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.414                                                   	MRR:21.96	Hits@10:38.5	Best:21.96
2024-12-28 00:11:39,039: Snapshot:2	Epoch:14	Loss:3.081	translation_Loss:1.642	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.439                                                   	MRR:22.03	Hits@10:38.47	Best:22.03
2024-12-28 00:11:46,282: Snapshot:2	Epoch:15	Loss:3.059	translation_Loss:1.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.463                                                   	MRR:21.98	Hits@10:38.58	Best:22.03
2024-12-28 00:11:53,467: Snapshot:2	Epoch:16	Loss:3.04	translation_Loss:1.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.479                                                   	MRR:22.02	Hits@10:38.56	Best:22.03
2024-12-28 00:12:00,683: Snapshot:2	Epoch:17	Loss:3.004	translation_Loss:1.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.493                                                   	MRR:22.06	Hits@10:38.54	Best:22.06
2024-12-28 00:12:07,956: Snapshot:2	Epoch:18	Loss:2.993	translation_Loss:1.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.506                                                   	MRR:22.04	Hits@10:38.55	Best:22.06
2024-12-28 00:12:15,191: Snapshot:2	Epoch:19	Loss:2.979	translation_Loss:1.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.516                                                   	MRR:22.08	Hits@10:38.6	Best:22.08
2024-12-28 00:12:22,823: Snapshot:2	Epoch:20	Loss:2.97	translation_Loss:1.441	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.529                                                   	MRR:22.01	Hits@10:38.63	Best:22.08
2024-12-28 00:12:30,041: Snapshot:2	Epoch:21	Loss:2.958	translation_Loss:1.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.537                                                   	MRR:22.1	Hits@10:38.61	Best:22.1
2024-12-28 00:12:37,222: Snapshot:2	Epoch:22	Loss:2.951	translation_Loss:1.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.545                                                   	MRR:22.1	Hits@10:38.66	Best:22.1
2024-12-28 00:12:44,415: Snapshot:2	Epoch:23	Loss:2.954	translation_Loss:1.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.553                                                   	MRR:22.09	Hits@10:38.6	Best:22.1
2024-12-28 00:12:51,762: Snapshot:2	Epoch:24	Loss:2.945	translation_Loss:1.386	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.558                                                   	MRR:22.11	Hits@10:38.6	Best:22.11
2024-12-28 00:12:59,508: Snapshot:2	Epoch:25	Loss:2.943	translation_Loss:1.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.564                                                   	MRR:22.2	Hits@10:38.61	Best:22.2
2024-12-28 00:13:06,796: Snapshot:2	Epoch:26	Loss:2.937	translation_Loss:1.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.568                                                   	MRR:22.16	Hits@10:38.63	Best:22.2
2024-12-28 00:13:13,955: Snapshot:2	Epoch:27	Loss:2.932	translation_Loss:1.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.572                                                   	MRR:22.16	Hits@10:38.67	Best:22.2
2024-12-28 00:13:21,129: Snapshot:2	Epoch:28	Loss:2.928	translation_Loss:1.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.576                                                   	MRR:22.11	Hits@10:38.67	Best:22.2
2024-12-28 00:13:28,279: Snapshot:2	Epoch:29	Loss:2.923	translation_Loss:1.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.579                                                   	MRR:22.15	Hits@10:38.74	Best:22.2
2024-12-28 00:13:35,465: Early Stopping! Snapshot: 2 Epoch: 30 Best Results: 22.2
2024-12-28 00:13:35,466: Start to training tokens! Snapshot: 2 Epoch: 30 Loss:2.913 MRR:22.14 Best Results: 22.2
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:13:35,466: Snapshot:2	Epoch:30	Loss:2.913	translation_Loss:1.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.582                                                   	MRR:22.14	Hits@10:38.71	Best:22.2
2024-12-28 00:13:42,922: Snapshot:2	Epoch:31	Loss:20.793	translation_Loss:20.159	multi_layer_Loss:0.634	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.14	Hits@10:38.71	Best:22.2
2024-12-28 00:13:50,091: End of token training: 2 Epoch: 32 Loss:20.539 MRR:22.14 Best Results: 22.2
2024-12-28 00:13:50,091: Snapshot:2	Epoch:32	Loss:20.539	translation_Loss:20.16	multi_layer_Loss:0.379	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.14	Hits@10:38.71	Best:22.2
2024-12-28 00:13:50,353: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_5000/2model_best.tar'
2024-12-28 00:13:59,747: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2467 | 0.157  | 0.2859 | 0.3423 |  0.4162 |
|     1      | 0.2286 | 0.1427 | 0.2667 | 0.3216 |  0.3923 |
|     2      | 0.2225 | 0.1355 | 0.2583 | 0.3147 |  0.3875 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:14:24,702: Snapshot:3	Epoch:0	Loss:4.299	translation_Loss:4.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.073                                                   	MRR:19.33	Hits@10:36.73	Best:19.33
2024-12-28 00:14:32,036: Snapshot:3	Epoch:1	Loss:3.137	translation_Loss:2.954	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:19.77	Hits@10:37.9	Best:19.77
2024-12-28 00:14:39,331: Snapshot:3	Epoch:2	Loss:2.595	translation_Loss:2.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.284                                                   	MRR:19.98	Hits@10:38.37	Best:19.98
2024-12-28 00:14:46,611: Snapshot:3	Epoch:3	Loss:2.244	translation_Loss:1.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.374                                                   	MRR:20.17	Hits@10:38.71	Best:20.17
2024-12-28 00:14:53,911: Snapshot:3	Epoch:4	Loss:2.03	translation_Loss:1.582	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.448                                                   	MRR:20.29	Hits@10:38.95	Best:20.29
2024-12-28 00:15:01,202: Snapshot:3	Epoch:5	Loss:1.861	translation_Loss:1.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.51                                                   	MRR:20.39	Hits@10:39.04	Best:20.39
2024-12-28 00:15:08,565: Snapshot:3	Epoch:6	Loss:1.74	translation_Loss:1.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.56                                                   	MRR:20.45	Hits@10:39.1	Best:20.45
2024-12-28 00:15:15,876: Snapshot:3	Epoch:7	Loss:1.644	translation_Loss:1.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.599                                                   	MRR:20.51	Hits@10:39.3	Best:20.51
2024-12-28 00:15:23,169: Snapshot:3	Epoch:8	Loss:1.578	translation_Loss:0.947	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.631                                                   	MRR:20.59	Hits@10:39.41	Best:20.59
2024-12-28 00:15:30,932: Snapshot:3	Epoch:9	Loss:1.534	translation_Loss:0.878	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.656                                                   	MRR:20.68	Hits@10:39.35	Best:20.68
2024-12-28 00:15:38,237: Snapshot:3	Epoch:10	Loss:1.495	translation_Loss:0.821	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.674                                                   	MRR:20.68	Hits@10:39.39	Best:20.68
2024-12-28 00:15:45,611: Snapshot:3	Epoch:11	Loss:1.473	translation_Loss:0.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.689                                                   	MRR:20.7	Hits@10:39.57	Best:20.7
2024-12-28 00:15:52,843: Snapshot:3	Epoch:12	Loss:1.458	translation_Loss:0.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.704                                                   	MRR:20.7	Hits@10:39.56	Best:20.7
2024-12-28 00:16:00,104: Snapshot:3	Epoch:13	Loss:1.437	translation_Loss:0.724	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.713                                                   	MRR:20.71	Hits@10:39.46	Best:20.71
2024-12-28 00:16:07,808: Snapshot:3	Epoch:14	Loss:1.436	translation_Loss:0.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.72                                                   	MRR:20.72	Hits@10:39.52	Best:20.72
2024-12-28 00:16:15,043: Snapshot:3	Epoch:15	Loss:1.419	translation_Loss:0.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.726                                                   	MRR:20.7	Hits@10:39.54	Best:20.72
2024-12-28 00:16:22,294: Snapshot:3	Epoch:16	Loss:1.412	translation_Loss:0.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.732                                                   	MRR:20.69	Hits@10:39.54	Best:20.72
2024-12-28 00:16:29,556: Snapshot:3	Epoch:17	Loss:1.415	translation_Loss:0.678	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.737                                                   	MRR:20.7	Hits@10:39.66	Best:20.72
2024-12-28 00:16:36,826: Snapshot:3	Epoch:18	Loss:1.4	translation_Loss:0.66	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.741                                                   	MRR:20.7	Hits@10:39.51	Best:20.72
2024-12-28 00:16:44,064: Early Stopping! Snapshot: 3 Epoch: 19 Best Results: 20.72
2024-12-28 00:16:44,065: Start to training tokens! Snapshot: 3 Epoch: 19 Loss:1.407 MRR:20.69 Best Results: 20.72
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:16:44,065: Snapshot:3	Epoch:19	Loss:1.407	translation_Loss:0.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.743                                                   	MRR:20.69	Hits@10:39.62	Best:20.72
2024-12-28 00:16:51,656: Snapshot:3	Epoch:20	Loss:19.953	translation_Loss:19.331	multi_layer_Loss:0.621	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.69	Hits@10:39.62	Best:20.72
2024-12-28 00:16:58,771: End of token training: 3 Epoch: 21 Loss:19.685 MRR:20.69 Best Results: 20.72
2024-12-28 00:16:58,772: Snapshot:3	Epoch:21	Loss:19.685	translation_Loss:19.325	multi_layer_Loss:0.36	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.69	Hits@10:39.62	Best:20.72
2024-12-28 00:16:59,026: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_5000/3model_best.tar'
2024-12-28 00:17:11,560: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2333 | 0.1468 | 0.2679 | 0.3238 |  0.3988 |
|     1      | 0.2186 | 0.1345 | 0.2516 | 0.306  |  0.3807 |
|     2      | 0.2153 | 0.1261 | 0.2502 |  0.31  |  0.3912 |
|     3      | 0.2079 | 0.1124 | 0.2439 | 0.3083 |  0.3964 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:17:36,572: Snapshot:4	Epoch:0	Loss:2.569	translation_Loss:2.508	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.061                                                   	MRR:19.68	Hits@10:43.35	Best:19.68
2024-12-28 00:17:43,985: Snapshot:4	Epoch:1	Loss:1.803	translation_Loss:1.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.12                                                   	MRR:21.0	Hits@10:46.03	Best:21.0
2024-12-28 00:17:51,399: Snapshot:4	Epoch:2	Loss:1.47	translation_Loss:1.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.14                                                   	MRR:21.52	Hits@10:46.8	Best:21.52
2024-12-28 00:17:58,795: Snapshot:4	Epoch:3	Loss:1.227	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.16                                                   	MRR:21.61	Hits@10:47.05	Best:21.61
2024-12-28 00:18:06,157: Snapshot:4	Epoch:4	Loss:1.019	translation_Loss:0.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:21.69	Hits@10:47.07	Best:21.69
2024-12-28 00:18:13,543: Snapshot:4	Epoch:5	Loss:0.852	translation_Loss:0.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:22.02	Hits@10:47.46	Best:22.02
2024-12-28 00:18:20,994: Snapshot:4	Epoch:6	Loss:0.724	translation_Loss:0.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.201                                                   	MRR:22.09	Hits@10:47.59	Best:22.09
2024-12-28 00:18:28,790: Snapshot:4	Epoch:7	Loss:0.624	translation_Loss:0.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.206                                                   	MRR:22.28	Hits@10:47.9	Best:22.28
2024-12-28 00:18:36,149: Snapshot:4	Epoch:8	Loss:0.554	translation_Loss:0.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.209                                                   	MRR:22.58	Hits@10:48.13	Best:22.58
2024-12-28 00:18:43,608: Snapshot:4	Epoch:9	Loss:0.506	translation_Loss:0.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.21                                                   	MRR:22.58	Hits@10:48.04	Best:22.58
2024-12-28 00:18:50,981: Snapshot:4	Epoch:10	Loss:0.469	translation_Loss:0.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.21                                                   	MRR:22.72	Hits@10:47.96	Best:22.72
2024-12-28 00:18:58,362: Snapshot:4	Epoch:11	Loss:0.451	translation_Loss:0.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:22.77	Hits@10:47.86	Best:22.77
2024-12-28 00:19:05,749: Snapshot:4	Epoch:12	Loss:0.437	translation_Loss:0.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.212                                                   	MRR:22.94	Hits@10:48.19	Best:22.94
2024-12-28 00:19:13,420: Snapshot:4	Epoch:13	Loss:0.427	translation_Loss:0.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:22.92	Hits@10:48.07	Best:22.94
2024-12-28 00:19:20,760: Snapshot:4	Epoch:14	Loss:0.418	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:22.84	Hits@10:48.44	Best:22.94
2024-12-28 00:19:28,044: Snapshot:4	Epoch:15	Loss:0.401	translation_Loss:0.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:22.79	Hits@10:48.12	Best:22.94
2024-12-28 00:19:35,298: Snapshot:4	Epoch:16	Loss:0.396	translation_Loss:0.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.212                                                   	MRR:22.89	Hits@10:48.4	Best:22.94
2024-12-28 00:19:42,615: Early Stopping! Snapshot: 4 Epoch: 17 Best Results: 22.94
2024-12-28 00:19:42,615: Start to training tokens! Snapshot: 4 Epoch: 17 Loss:0.399 MRR:22.84 Best Results: 22.94
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:19:42,616: Snapshot:4	Epoch:17	Loss:0.399	translation_Loss:0.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:22.84	Hits@10:48.3	Best:22.94
2024-12-28 00:19:50,352: Snapshot:4	Epoch:18	Loss:17.033	translation_Loss:16.408	multi_layer_Loss:0.625	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.84	Hits@10:48.3	Best:22.94
2024-12-28 00:19:57,516: End of token training: 4 Epoch: 19 Loss:16.768 MRR:22.84 Best Results: 22.94
2024-12-28 00:19:57,516: Snapshot:4	Epoch:19	Loss:16.768	translation_Loss:16.406	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.84	Hits@10:48.3	Best:22.94
2024-12-28 00:19:57,772: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_5000/4model_best.tar'
2024-12-28 00:20:14,201: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2213 | 0.138  | 0.2526 | 0.3079 |  0.3802 |
|     1      | 0.2036 | 0.1222 | 0.2336 | 0.2885 |  0.3628 |
|     2      | 0.1988 | 0.1137 | 0.2273 | 0.2875 |  0.3729 |
|     3      | 0.1929 | 0.0986 | 0.2237 | 0.2917 |  0.3844 |
|     4      | 0.2294 | 0.1069 | 0.273  | 0.3616 |  0.4826 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 00:20:14,204: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.234 | 0.1469 | 0.2792 | 0.3266 |  0.389  |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2509 | 0.1605 | 0.2941 | 0.3482 |  0.4172 |
|     1      | 0.2215 | 0.1362 | 0.2637 | 0.3137 |  0.3756 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2467 | 0.157  | 0.2859 | 0.3423 |  0.4162 |
|     1      | 0.2286 | 0.1427 | 0.2667 | 0.3216 |  0.3923 |
|     2      | 0.2225 | 0.1355 | 0.2583 | 0.3147 |  0.3875 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2333 | 0.1468 | 0.2679 | 0.3238 |  0.3988 |
|     1      | 0.2186 | 0.1345 | 0.2516 | 0.306  |  0.3807 |
|     2      | 0.2153 | 0.1261 | 0.2502 |  0.31  |  0.3912 |
|     3      | 0.2079 | 0.1124 | 0.2439 | 0.3083 |  0.3964 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2213 | 0.138  | 0.2526 | 0.3079 |  0.3802 |
|     1      | 0.2036 | 0.1222 | 0.2336 | 0.2885 |  0.3628 |
|     2      | 0.1988 | 0.1137 | 0.2273 | 0.2875 |  0.3729 |
|     3      | 0.1929 | 0.0986 | 0.2237 | 0.2917 |  0.3844 |
|     4      | 0.2294 | 0.1069 | 0.273  | 0.3616 |  0.4826 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 00:20:14,204: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 759.0260574817657  |   0.234   |    0.147     |    0.279     |     0.389     |
|    1     | 292.4348261356354  |   0.236   |    0.148     |    0.279     |     0.396     |
|    2     | 255.0866894721985  |   0.233   |    0.145     |     0.27     |     0.399     |
|    3     | 175.6797924041748  |   0.219   |     0.13     |    0.253     |     0.392     |
|    4     | 162.54367017745972 |   0.209   |    0.116     |    0.242     |     0.397     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 00:20:14,204: Sum_Training_Time:1644.7710356712341
2024-12-28 00:20:14,204: Every_Training_Time:[759.0260574817657, 292.4348261356354, 255.0866894721985, 175.6797924041748, 162.54367017745972]
2024-12-28 00:20:14,204: Forward transfer: 0.1731 Backward transfer: -0.017325000000000007
2024-12-28 00:20:53,704: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241228002018/FACTfact_0.0001_2048_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.0001_2048_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.0001_2048_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 00:21:03,599: Snapshot:0	Epoch:0	Loss:26.399	translation_Loss:26.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.43	Best:1.42
2024-12-28 00:21:09,989: Snapshot:0	Epoch:1	Loss:25.069	translation_Loss:25.069	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.81	Hits@10:2.56	Best:1.81
2024-12-28 00:21:16,800: Snapshot:0	Epoch:2	Loss:23.874	translation_Loss:23.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.3	Best:2.98
2024-12-28 00:21:23,173: Snapshot:0	Epoch:3	Loss:22.731	translation_Loss:22.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.96	Hits@10:7.35	Best:3.96
2024-12-28 00:21:29,543: Snapshot:0	Epoch:4	Loss:21.623	translation_Loss:21.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.72	Hits@10:9.23	Best:4.72
2024-12-28 00:21:35,883: Snapshot:0	Epoch:5	Loss:20.546	translation_Loss:20.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.38	Hits@10:10.87	Best:5.38
2024-12-28 00:21:42,230: Snapshot:0	Epoch:6	Loss:19.518	translation_Loss:19.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.01	Hits@10:12.51	Best:6.01
2024-12-28 00:21:49,125: Snapshot:0	Epoch:7	Loss:18.524	translation_Loss:18.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.64	Hits@10:14.27	Best:6.64
2024-12-28 00:21:55,461: Snapshot:0	Epoch:8	Loss:17.557	translation_Loss:17.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.25	Hits@10:15.93	Best:7.25
2024-12-28 00:22:01,831: Snapshot:0	Epoch:9	Loss:16.633	translation_Loss:16.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.8	Hits@10:17.36	Best:7.8
2024-12-28 00:22:08,182: Snapshot:0	Epoch:10	Loss:15.731	translation_Loss:15.731	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.35	Hits@10:18.99	Best:8.35
2024-12-28 00:22:14,541: Snapshot:0	Epoch:11	Loss:14.856	translation_Loss:14.856	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.92	Hits@10:20.48	Best:8.92
2024-12-28 00:22:20,918: Snapshot:0	Epoch:12	Loss:14.025	translation_Loss:14.025	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.48	Hits@10:21.91	Best:9.48
2024-12-28 00:22:27,745: Snapshot:0	Epoch:13	Loss:13.222	translation_Loss:13.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.04	Hits@10:23.26	Best:10.04
2024-12-28 00:22:34,088: Snapshot:0	Epoch:14	Loss:12.461	translation_Loss:12.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.59	Hits@10:24.57	Best:10.59
2024-12-28 00:22:40,421: Snapshot:0	Epoch:15	Loss:11.742	translation_Loss:11.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.15	Hits@10:25.74	Best:11.15
2024-12-28 00:22:46,828: Snapshot:0	Epoch:16	Loss:11.041	translation_Loss:11.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.71	Hits@10:26.83	Best:11.71
2024-12-28 00:22:53,178: Snapshot:0	Epoch:17	Loss:10.379	translation_Loss:10.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.23	Hits@10:27.77	Best:12.23
2024-12-28 00:22:59,961: Snapshot:0	Epoch:18	Loss:9.757	translation_Loss:9.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.71	Hits@10:28.75	Best:12.71
2024-12-28 00:23:06,310: Snapshot:0	Epoch:19	Loss:9.158	translation_Loss:9.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.2	Hits@10:29.56	Best:13.2
2024-12-28 00:23:12,680: Snapshot:0	Epoch:20	Loss:8.589	translation_Loss:8.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.66	Hits@10:30.25	Best:13.66
2024-12-28 00:23:19,081: Snapshot:0	Epoch:21	Loss:8.072	translation_Loss:8.072	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.11	Hits@10:31.03	Best:14.11
2024-12-28 00:23:25,410: Snapshot:0	Epoch:22	Loss:7.581	translation_Loss:7.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.57	Hits@10:31.79	Best:14.57
2024-12-28 00:23:31,791: Snapshot:0	Epoch:23	Loss:7.1	translation_Loss:7.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.02	Hits@10:32.45	Best:15.02
2024-12-28 00:23:38,675: Snapshot:0	Epoch:24	Loss:6.658	translation_Loss:6.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.38	Hits@10:32.99	Best:15.38
2024-12-28 00:23:45,101: Snapshot:0	Epoch:25	Loss:6.25	translation_Loss:6.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.84	Hits@10:33.53	Best:15.84
2024-12-28 00:23:51,458: Snapshot:0	Epoch:26	Loss:5.851	translation_Loss:5.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.24	Hits@10:34.07	Best:16.24
2024-12-28 00:23:57,824: Snapshot:0	Epoch:27	Loss:5.48	translation_Loss:5.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:34.46	Best:16.66
2024-12-28 00:24:04,273: Snapshot:0	Epoch:28	Loss:5.133	translation_Loss:5.133	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.04	Hits@10:34.87	Best:17.04
2024-12-28 00:24:11,055: Snapshot:0	Epoch:29	Loss:4.788	translation_Loss:4.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.44	Hits@10:35.32	Best:17.44
2024-12-28 00:24:17,415: Snapshot:0	Epoch:30	Loss:4.495	translation_Loss:4.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.82	Hits@10:35.61	Best:17.82
2024-12-28 00:24:23,789: Snapshot:0	Epoch:31	Loss:4.201	translation_Loss:4.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.16	Hits@10:36.0	Best:18.16
2024-12-28 00:24:30,157: Snapshot:0	Epoch:32	Loss:3.932	translation_Loss:3.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.46	Hits@10:36.26	Best:18.46
2024-12-28 00:24:36,555: Snapshot:0	Epoch:33	Loss:3.658	translation_Loss:3.658	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.79	Hits@10:36.63	Best:18.79
2024-12-28 00:24:42,984: Snapshot:0	Epoch:34	Loss:3.417	translation_Loss:3.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.11	Hits@10:36.96	Best:19.11
2024-12-28 00:24:49,909: Snapshot:0	Epoch:35	Loss:3.194	translation_Loss:3.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.42	Hits@10:37.17	Best:19.42
2024-12-28 00:24:56,244: Snapshot:0	Epoch:36	Loss:2.978	translation_Loss:2.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.72	Hits@10:37.42	Best:19.72
2024-12-28 00:25:02,621: Snapshot:0	Epoch:37	Loss:2.785	translation_Loss:2.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.95	Hits@10:37.61	Best:19.95
2024-12-28 00:25:09,086: Snapshot:0	Epoch:38	Loss:2.59	translation_Loss:2.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.18	Hits@10:37.79	Best:20.18
2024-12-28 00:25:15,459: Snapshot:0	Epoch:39	Loss:2.42	translation_Loss:2.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.46	Hits@10:37.95	Best:20.46
2024-12-28 00:25:22,236: Snapshot:0	Epoch:40	Loss:2.264	translation_Loss:2.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.63	Hits@10:38.13	Best:20.63
2024-12-28 00:25:28,689: Snapshot:0	Epoch:41	Loss:2.112	translation_Loss:2.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.82	Hits@10:38.25	Best:20.82
2024-12-28 00:25:35,043: Snapshot:0	Epoch:42	Loss:1.97	translation_Loss:1.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.06	Hits@10:38.38	Best:21.06
2024-12-28 00:25:41,429: Snapshot:0	Epoch:43	Loss:1.836	translation_Loss:1.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.25	Hits@10:38.57	Best:21.25
2024-12-28 00:25:47,865: Snapshot:0	Epoch:44	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.43	Hits@10:38.64	Best:21.43
2024-12-28 00:25:54,243: Snapshot:0	Epoch:45	Loss:1.609	translation_Loss:1.609	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.58	Hits@10:38.86	Best:21.58
2024-12-28 00:26:01,054: Snapshot:0	Epoch:46	Loss:1.496	translation_Loss:1.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.68	Hits@10:38.91	Best:21.68
2024-12-28 00:26:07,429: Snapshot:0	Epoch:47	Loss:1.406	translation_Loss:1.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.85	Hits@10:38.95	Best:21.85
2024-12-28 00:26:13,787: Snapshot:0	Epoch:48	Loss:1.322	translation_Loss:1.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.0	Hits@10:39.01	Best:22.0
2024-12-28 00:26:20,143: Snapshot:0	Epoch:49	Loss:1.235	translation_Loss:1.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.15	Hits@10:39.13	Best:22.15
2024-12-28 00:26:26,515: Snapshot:0	Epoch:50	Loss:1.157	translation_Loss:1.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.31	Hits@10:39.15	Best:22.31
2024-12-28 00:26:33,279: Snapshot:0	Epoch:51	Loss:1.086	translation_Loss:1.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.41	Hits@10:39.24	Best:22.41
2024-12-28 00:26:39,633: Snapshot:0	Epoch:52	Loss:1.024	translation_Loss:1.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.55	Hits@10:39.29	Best:22.55
2024-12-28 00:26:46,010: Snapshot:0	Epoch:53	Loss:0.965	translation_Loss:0.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.62	Hits@10:39.25	Best:22.62
2024-12-28 00:26:52,404: Snapshot:0	Epoch:54	Loss:0.91	translation_Loss:0.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.73	Hits@10:39.34	Best:22.73
2024-12-28 00:26:58,787: Snapshot:0	Epoch:55	Loss:0.857	translation_Loss:0.857	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.79	Hits@10:39.38	Best:22.79
2024-12-28 00:27:05,180: Snapshot:0	Epoch:56	Loss:0.815	translation_Loss:0.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.86	Hits@10:39.35	Best:22.86
2024-12-28 00:27:11,985: Snapshot:0	Epoch:57	Loss:0.77	translation_Loss:0.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.93	Hits@10:39.4	Best:22.93
2024-12-28 00:27:18,421: Snapshot:0	Epoch:58	Loss:0.733	translation_Loss:0.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.99	Hits@10:39.38	Best:22.99
2024-12-28 00:27:24,770: Snapshot:0	Epoch:59	Loss:0.688	translation_Loss:0.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.11	Hits@10:39.52	Best:23.11
2024-12-28 00:27:31,156: Snapshot:0	Epoch:60	Loss:0.649	translation_Loss:0.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.16	Hits@10:39.47	Best:23.16
2024-12-28 00:27:37,524: Snapshot:0	Epoch:61	Loss:0.625	translation_Loss:0.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.14	Hits@10:39.43	Best:23.16
2024-12-28 00:27:44,318: Snapshot:0	Epoch:62	Loss:0.593	translation_Loss:0.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.25	Hits@10:39.46	Best:23.25
2024-12-28 00:27:50,669: Snapshot:0	Epoch:63	Loss:0.562	translation_Loss:0.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.25	Hits@10:39.45	Best:23.25
2024-12-28 00:27:57,037: Snapshot:0	Epoch:64	Loss:0.532	translation_Loss:0.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.29	Hits@10:39.52	Best:23.29
2024-12-28 00:28:03,390: Snapshot:0	Epoch:65	Loss:0.511	translation_Loss:0.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.34	Hits@10:39.6	Best:23.34
2024-12-28 00:28:09,722: Snapshot:0	Epoch:66	Loss:0.493	translation_Loss:0.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.34	Hits@10:39.52	Best:23.34
2024-12-28 00:28:16,065: Snapshot:0	Epoch:67	Loss:0.478	translation_Loss:0.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.38	Hits@10:39.56	Best:23.38
2024-12-28 00:28:22,862: Snapshot:0	Epoch:68	Loss:0.447	translation_Loss:0.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.45	Hits@10:39.65	Best:23.45
2024-12-28 00:28:29,233: Snapshot:0	Epoch:69	Loss:0.427	translation_Loss:0.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.46	Hits@10:39.75	Best:23.46
2024-12-28 00:28:35,669: Snapshot:0	Epoch:70	Loss:0.419	translation_Loss:0.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.51	Hits@10:39.74	Best:23.51
2024-12-28 00:28:42,010: Snapshot:0	Epoch:71	Loss:0.396	translation_Loss:0.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.5	Hits@10:39.82	Best:23.51
2024-12-28 00:28:48,383: Snapshot:0	Epoch:72	Loss:0.38	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.58	Hits@10:39.85	Best:23.58
2024-12-28 00:28:55,133: Snapshot:0	Epoch:73	Loss:0.368	translation_Loss:0.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.61	Hits@10:39.79	Best:23.61
2024-12-28 00:29:01,507: Snapshot:0	Epoch:74	Loss:0.353	translation_Loss:0.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.66	Hits@10:39.85	Best:23.66
2024-12-28 00:29:07,902: Snapshot:0	Epoch:75	Loss:0.345	translation_Loss:0.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.84	Best:23.67
2024-12-28 00:29:14,330: Snapshot:0	Epoch:76	Loss:0.332	translation_Loss:0.332	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.68	Hits@10:39.84	Best:23.68
2024-12-28 00:29:20,713: Snapshot:0	Epoch:77	Loss:0.324	translation_Loss:0.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.67	Hits@10:39.87	Best:23.68
2024-12-28 00:29:27,090: Snapshot:0	Epoch:78	Loss:0.308	translation_Loss:0.308	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.7	Hits@10:39.92	Best:23.7
2024-12-28 00:29:33,834: Snapshot:0	Epoch:79	Loss:0.298	translation_Loss:0.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.7	Hits@10:39.87	Best:23.7
2024-12-28 00:29:40,178: Snapshot:0	Epoch:80	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.76	Hits@10:39.87	Best:23.76
2024-12-28 00:29:46,589: Snapshot:0	Epoch:81	Loss:0.284	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.79	Hits@10:39.86	Best:23.79
2024-12-28 00:29:52,938: Snapshot:0	Epoch:82	Loss:0.279	translation_Loss:0.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.77	Hits@10:39.94	Best:23.79
2024-12-28 00:29:59,308: Snapshot:0	Epoch:83	Loss:0.263	translation_Loss:0.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.78	Hits@10:39.96	Best:23.79
2024-12-28 00:30:06,166: Snapshot:0	Epoch:84	Loss:0.257	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.8	Hits@10:39.94	Best:23.8
2024-12-28 00:30:12,611: Snapshot:0	Epoch:85	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:40.0	Best:23.87
2024-12-28 00:30:18,997: Snapshot:0	Epoch:86	Loss:0.244	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.86	Hits@10:40.0	Best:23.87
2024-12-28 00:30:25,347: Snapshot:0	Epoch:87	Loss:0.243	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.9	Hits@10:40.02	Best:23.9
2024-12-28 00:30:31,725: Snapshot:0	Epoch:88	Loss:0.231	translation_Loss:0.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.91	Hits@10:39.98	Best:23.91
2024-12-28 00:30:38,146: Snapshot:0	Epoch:89	Loss:0.228	translation_Loss:0.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.92	Hits@10:40.08	Best:23.92
2024-12-28 00:30:44,891: Snapshot:0	Epoch:90	Loss:0.223	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.92	Hits@10:40.04	Best:23.92
2024-12-28 00:30:51,329: Snapshot:0	Epoch:91	Loss:0.215	translation_Loss:0.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:40.02	Best:23.98
2024-12-28 00:30:57,663: Snapshot:0	Epoch:92	Loss:0.211	translation_Loss:0.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.99	Hits@10:39.98	Best:23.99
2024-12-28 00:31:03,980: Snapshot:0	Epoch:93	Loss:0.204	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:40.01	Best:23.99
2024-12-28 00:31:10,317: Snapshot:0	Epoch:94	Loss:0.203	translation_Loss:0.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.97	Hits@10:39.93	Best:23.99
2024-12-28 00:31:17,025: Snapshot:0	Epoch:95	Loss:0.195	translation_Loss:0.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.95	Hits@10:40.02	Best:23.99
2024-12-28 00:31:23,351: Snapshot:0	Epoch:96	Loss:0.193	translation_Loss:0.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.96	Hits@10:40.09	Best:23.99
2024-12-28 00:31:29,666: Early Stopping! Snapshot: 0 Epoch: 97 Best Results: 23.99
2024-12-28 00:31:29,666: Start to training tokens! Snapshot: 0 Epoch: 97 Loss:0.184 MRR:23.98 Best Results: 23.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:31:29,667: Snapshot:0	Epoch:97	Loss:0.184	translation_Loss:0.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:40.0	Best:23.99
2024-12-28 00:31:36,576: Snapshot:0	Epoch:98	Loss:18.767	translation_Loss:18.141	multi_layer_Loss:0.626	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.98	Hits@10:40.0	Best:23.99
2024-12-28 00:31:42,950: End of token training: 0 Epoch: 99 Loss:18.529 MRR:23.98 Best Results: 23.99
2024-12-28 00:31:42,952: Snapshot:0	Epoch:99	Loss:18.529	translation_Loss:18.153	multi_layer_Loss:0.375	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.98	Hits@10:40.0	Best:23.99
2024-12-28 00:31:43,156: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_10000/0model_best.tar'
2024-12-28 00:31:45,820: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2325 | 0.1444 | 0.2778 | 0.3275 |   0.39  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:32:10,850: Snapshot:1	Epoch:0	Loss:12.636	translation_Loss:12.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.113                                                   	MRR:17.63	Hits@10:30.57	Best:17.63
2024-12-28 00:32:17,838: Snapshot:1	Epoch:1	Loss:11.213	translation_Loss:10.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.337                                                   	MRR:18.23	Hits@10:31.5	Best:18.23
2024-12-28 00:32:24,865: Snapshot:1	Epoch:2	Loss:10.423	translation_Loss:9.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.613                                                   	MRR:18.66	Hits@10:32.14	Best:18.66
2024-12-28 00:32:31,865: Snapshot:1	Epoch:3	Loss:9.865	translation_Loss:8.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.868                                                   	MRR:18.96	Hits@10:32.58	Best:18.96
2024-12-28 00:32:38,852: Snapshot:1	Epoch:4	Loss:9.402	translation_Loss:8.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.084                                                   	MRR:19.22	Hits@10:33.14	Best:19.22
2024-12-28 00:32:45,871: Snapshot:1	Epoch:5	Loss:9.03	translation_Loss:7.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.258                                                   	MRR:19.45	Hits@10:33.45	Best:19.45
2024-12-28 00:32:52,869: Snapshot:1	Epoch:6	Loss:8.697	translation_Loss:7.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.399                                                   	MRR:19.67	Hits@10:33.78	Best:19.67
2024-12-28 00:32:59,906: Snapshot:1	Epoch:7	Loss:8.386	translation_Loss:6.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.513                                                   	MRR:19.81	Hits@10:34.13	Best:19.81
2024-12-28 00:33:06,916: Snapshot:1	Epoch:8	Loss:8.086	translation_Loss:6.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.602                                                   	MRR:19.98	Hits@10:34.35	Best:19.98
2024-12-28 00:33:13,942: Snapshot:1	Epoch:9	Loss:7.841	translation_Loss:6.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.675                                                   	MRR:20.11	Hits@10:34.58	Best:20.11
2024-12-28 00:33:20,933: Snapshot:1	Epoch:10	Loss:7.616	translation_Loss:5.885	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.731                                                   	MRR:20.24	Hits@10:34.69	Best:20.24
2024-12-28 00:33:27,992: Snapshot:1	Epoch:11	Loss:7.429	translation_Loss:5.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.778                                                   	MRR:20.38	Hits@10:34.95	Best:20.38
2024-12-28 00:33:35,010: Snapshot:1	Epoch:12	Loss:7.265	translation_Loss:5.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.816                                                   	MRR:20.53	Hits@10:35.13	Best:20.53
2024-12-28 00:33:42,459: Snapshot:1	Epoch:13	Loss:7.115	translation_Loss:5.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.843                                                   	MRR:20.58	Hits@10:35.25	Best:20.58
2024-12-28 00:33:49,454: Snapshot:1	Epoch:14	Loss:7.012	translation_Loss:5.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.864                                                   	MRR:20.68	Hits@10:35.45	Best:20.68
2024-12-28 00:33:56,497: Snapshot:1	Epoch:15	Loss:6.889	translation_Loss:5.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.879                                                   	MRR:20.77	Hits@10:35.49	Best:20.77
2024-12-28 00:34:03,522: Snapshot:1	Epoch:16	Loss:6.809	translation_Loss:4.919	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.89                                                   	MRR:20.83	Hits@10:35.6	Best:20.83
2024-12-28 00:34:10,634: Snapshot:1	Epoch:17	Loss:6.738	translation_Loss:4.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.899                                                   	MRR:20.84	Hits@10:35.71	Best:20.84
2024-12-28 00:34:18,132: Snapshot:1	Epoch:18	Loss:6.672	translation_Loss:4.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.906                                                   	MRR:20.94	Hits@10:35.81	Best:20.94
2024-12-28 00:34:25,173: Snapshot:1	Epoch:19	Loss:6.624	translation_Loss:4.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.909                                                   	MRR:20.97	Hits@10:35.79	Best:20.97
2024-12-28 00:34:32,155: Snapshot:1	Epoch:20	Loss:6.591	translation_Loss:4.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.911                                                   	MRR:21.01	Hits@10:35.88	Best:21.01
2024-12-28 00:34:39,240: Snapshot:1	Epoch:21	Loss:6.56	translation_Loss:4.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.913                                                   	MRR:21.07	Hits@10:35.86	Best:21.07
2024-12-28 00:34:46,265: Snapshot:1	Epoch:22	Loss:6.513	translation_Loss:4.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.914                                                   	MRR:21.08	Hits@10:35.8	Best:21.08
2024-12-28 00:34:53,209: Snapshot:1	Epoch:23	Loss:6.49	translation_Loss:4.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.914                                                   	MRR:21.07	Hits@10:35.89	Best:21.08
2024-12-28 00:35:00,564: Snapshot:1	Epoch:24	Loss:6.473	translation_Loss:4.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.915                                                   	MRR:21.1	Hits@10:35.86	Best:21.1
2024-12-28 00:35:07,638: Snapshot:1	Epoch:25	Loss:6.45	translation_Loss:4.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.916                                                   	MRR:21.08	Hits@10:35.9	Best:21.1
2024-12-28 00:35:14,680: Snapshot:1	Epoch:26	Loss:6.433	translation_Loss:4.518	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.915                                                   	MRR:21.11	Hits@10:35.96	Best:21.11
2024-12-28 00:35:21,662: Snapshot:1	Epoch:27	Loss:6.42	translation_Loss:4.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.915                                                   	MRR:21.12	Hits@10:35.93	Best:21.12
2024-12-28 00:35:28,648: Snapshot:1	Epoch:28	Loss:6.399	translation_Loss:4.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.914                                                   	MRR:21.16	Hits@10:35.94	Best:21.16
2024-12-28 00:35:36,089: Snapshot:1	Epoch:29	Loss:6.397	translation_Loss:4.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.914                                                   	MRR:21.18	Hits@10:35.98	Best:21.18
2024-12-28 00:35:43,104: Snapshot:1	Epoch:30	Loss:6.378	translation_Loss:4.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.912                                                   	MRR:21.19	Hits@10:35.98	Best:21.19
2024-12-28 00:35:50,160: Snapshot:1	Epoch:31	Loss:6.37	translation_Loss:4.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.911                                                   	MRR:21.21	Hits@10:36.02	Best:21.21
2024-12-28 00:35:57,102: Snapshot:1	Epoch:32	Loss:6.363	translation_Loss:4.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.91                                                   	MRR:21.21	Hits@10:35.95	Best:21.21
2024-12-28 00:36:04,054: Snapshot:1	Epoch:33	Loss:6.358	translation_Loss:4.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.909                                                   	MRR:21.2	Hits@10:35.95	Best:21.21
2024-12-28 00:36:11,062: Snapshot:1	Epoch:34	Loss:6.351	translation_Loss:4.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.908                                                   	MRR:21.23	Hits@10:36.0	Best:21.23
2024-12-28 00:36:18,419: Snapshot:1	Epoch:35	Loss:6.344	translation_Loss:4.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.907                                                   	MRR:21.23	Hits@10:35.99	Best:21.23
2024-12-28 00:36:25,420: Snapshot:1	Epoch:36	Loss:6.321	translation_Loss:4.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.907                                                   	MRR:21.22	Hits@10:36.01	Best:21.23
2024-12-28 00:36:32,338: Snapshot:1	Epoch:37	Loss:6.309	translation_Loss:4.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.907                                                   	MRR:21.15	Hits@10:36.03	Best:21.23
2024-12-28 00:36:39,332: Snapshot:1	Epoch:38	Loss:6.308	translation_Loss:4.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.905                                                   	MRR:21.18	Hits@10:35.98	Best:21.23
2024-12-28 00:36:46,296: Early Stopping! Snapshot: 1 Epoch: 39 Best Results: 21.23
2024-12-28 00:36:46,296: Start to training tokens! Snapshot: 1 Epoch: 39 Loss:6.31 MRR:21.19 Best Results: 21.23
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:36:46,297: Snapshot:1	Epoch:39	Loss:6.31	translation_Loss:4.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.905                                                   	MRR:21.19	Hits@10:35.95	Best:21.23
2024-12-28 00:36:53,553: Snapshot:1	Epoch:40	Loss:21.888	translation_Loss:21.271	multi_layer_Loss:0.617	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.19	Hits@10:35.95	Best:21.23
2024-12-28 00:37:00,360: End of token training: 1 Epoch: 41 Loss:21.647 MRR:21.19 Best Results: 21.23
2024-12-28 00:37:00,360: Snapshot:1	Epoch:41	Loss:21.647	translation_Loss:21.285	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.19	Hits@10:35.95	Best:21.23
2024-12-28 00:37:00,614: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_10000/1model_best.tar'
2024-12-28 00:37:06,470: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.246  | 0.1562 | 0.2905 | 0.3405 |  0.4086 |
|     1      | 0.2101 | 0.126  | 0.2526 | 0.302  |  0.3632 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:37:31,370: Snapshot:2	Epoch:0	Loss:9.266	translation_Loss:9.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.114                                                   	MRR:18.95	Hits@10:33.78	Best:18.95
2024-12-28 00:37:38,572: Snapshot:2	Epoch:1	Loss:7.857	translation_Loss:7.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.324                                                   	MRR:19.47	Hits@10:34.65	Best:19.47
2024-12-28 00:37:45,839: Snapshot:2	Epoch:2	Loss:7.137	translation_Loss:6.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.585                                                   	MRR:19.84	Hits@10:35.2	Best:19.84
2024-12-28 00:37:53,054: Snapshot:2	Epoch:3	Loss:6.676	translation_Loss:5.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.832                                                   	MRR:20.03	Hits@10:35.52	Best:20.03
2024-12-28 00:38:00,226: Snapshot:2	Epoch:4	Loss:6.332	translation_Loss:5.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.045                                                   	MRR:20.22	Hits@10:35.74	Best:20.22
2024-12-28 00:38:07,401: Snapshot:2	Epoch:5	Loss:6.082	translation_Loss:4.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.22                                                   	MRR:20.4	Hits@10:35.98	Best:20.4
2024-12-28 00:38:14,578: Snapshot:2	Epoch:6	Loss:5.861	translation_Loss:4.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.362                                                   	MRR:20.56	Hits@10:36.28	Best:20.56
2024-12-28 00:38:21,758: Snapshot:2	Epoch:7	Loss:5.711	translation_Loss:4.233	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.478                                                   	MRR:20.61	Hits@10:36.4	Best:20.61
2024-12-28 00:38:28,941: Snapshot:2	Epoch:8	Loss:5.542	translation_Loss:3.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.568                                                   	MRR:20.72	Hits@10:36.53	Best:20.72
2024-12-28 00:38:36,197: Snapshot:2	Epoch:9	Loss:5.416	translation_Loss:3.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.642                                                   	MRR:20.79	Hits@10:36.72	Best:20.79
2024-12-28 00:38:43,374: Snapshot:2	Epoch:10	Loss:5.311	translation_Loss:3.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.701                                                   	MRR:20.93	Hits@10:36.74	Best:20.93
2024-12-28 00:38:50,608: Snapshot:2	Epoch:11	Loss:5.256	translation_Loss:3.508	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.748                                                   	MRR:20.94	Hits@10:36.9	Best:20.94
2024-12-28 00:38:57,754: Snapshot:2	Epoch:12	Loss:5.186	translation_Loss:3.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.786                                                   	MRR:21.0	Hits@10:36.99	Best:21.0
2024-12-28 00:39:05,393: Snapshot:2	Epoch:13	Loss:5.142	translation_Loss:3.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.815                                                   	MRR:21.05	Hits@10:36.97	Best:21.05
2024-12-28 00:39:12,526: Snapshot:2	Epoch:14	Loss:5.101	translation_Loss:3.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.84                                                   	MRR:21.05	Hits@10:37.0	Best:21.05
2024-12-28 00:39:19,737: Snapshot:2	Epoch:15	Loss:5.055	translation_Loss:3.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.862                                                   	MRR:21.06	Hits@10:37.1	Best:21.06
2024-12-28 00:39:26,902: Snapshot:2	Epoch:16	Loss:5.044	translation_Loss:3.166	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.878                                                   	MRR:21.13	Hits@10:37.04	Best:21.13
2024-12-28 00:39:34,070: Snapshot:2	Epoch:17	Loss:5.025	translation_Loss:3.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.889                                                   	MRR:21.14	Hits@10:37.09	Best:21.14
2024-12-28 00:39:41,684: Snapshot:2	Epoch:18	Loss:5.004	translation_Loss:3.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.899                                                   	MRR:21.2	Hits@10:37.13	Best:21.2
2024-12-28 00:39:48,811: Snapshot:2	Epoch:19	Loss:4.99	translation_Loss:3.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.908                                                   	MRR:21.15	Hits@10:37.15	Best:21.2
2024-12-28 00:39:55,943: Snapshot:2	Epoch:20	Loss:4.976	translation_Loss:3.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.916                                                   	MRR:21.16	Hits@10:37.19	Best:21.2
2024-12-28 00:40:03,112: Snapshot:2	Epoch:21	Loss:4.97	translation_Loss:3.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.923                                                   	MRR:21.16	Hits@10:37.16	Best:21.2
2024-12-28 00:40:10,391: Snapshot:2	Epoch:22	Loss:4.966	translation_Loss:3.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.928                                                   	MRR:21.15	Hits@10:37.17	Best:21.2
2024-12-28 00:40:17,527: Early Stopping! Snapshot: 2 Epoch: 23 Best Results: 21.2
2024-12-28 00:40:17,527: Start to training tokens! Snapshot: 2 Epoch: 23 Loss:4.967 MRR:21.14 Best Results: 21.2
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:40:17,528: Snapshot:2	Epoch:23	Loss:4.967	translation_Loss:3.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.937                                                   	MRR:21.14	Hits@10:37.21	Best:21.2
2024-12-28 00:40:24,961: Snapshot:2	Epoch:24	Loss:21.529	translation_Loss:20.895	multi_layer_Loss:0.634	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.14	Hits@10:37.21	Best:21.2
2024-12-28 00:40:31,963: End of token training: 2 Epoch: 25 Loss:21.269 MRR:21.14 Best Results: 21.2
2024-12-28 00:40:31,964: Snapshot:2	Epoch:25	Loss:21.269	translation_Loss:20.89	multi_layer_Loss:0.379	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.14	Hits@10:37.21	Best:21.2
2024-12-28 00:40:32,208: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_10000/2model_best.tar'
2024-12-28 00:40:41,381: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2451 | 0.155  | 0.288  | 0.3417 |  0.4125 |
|     1      | 0.2202 | 0.1345 | 0.2603 | 0.3117 |  0.3803 |
|     2      | 0.2126 | 0.127  | 0.2522 | 0.303  |  0.3723 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:41:06,339: Snapshot:3	Epoch:0	Loss:5.823	translation_Loss:5.709	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.114                                                   	MRR:18.52	Hits@10:35.24	Best:18.52
2024-12-28 00:41:13,561: Snapshot:3	Epoch:1	Loss:4.56	translation_Loss:4.264	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.296                                                   	MRR:19.02	Hits@10:36.4	Best:19.02
2024-12-28 00:41:20,854: Snapshot:3	Epoch:2	Loss:3.979	translation_Loss:3.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.488                                                   	MRR:19.33	Hits@10:36.99	Best:19.33
2024-12-28 00:41:28,157: Snapshot:3	Epoch:3	Loss:3.637	translation_Loss:2.979	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.658                                                   	MRR:19.44	Hits@10:37.44	Best:19.44
2024-12-28 00:41:35,430: Snapshot:3	Epoch:4	Loss:3.419	translation_Loss:2.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.795                                                   	MRR:19.57	Hits@10:37.68	Best:19.57
2024-12-28 00:41:42,767: Snapshot:3	Epoch:5	Loss:3.262	translation_Loss:2.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.904                                                   	MRR:19.72	Hits@10:37.98	Best:19.72
2024-12-28 00:41:50,083: Snapshot:3	Epoch:6	Loss:3.146	translation_Loss:2.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.988                                                   	MRR:19.79	Hits@10:38.09	Best:19.79
2024-12-28 00:41:57,299: Snapshot:3	Epoch:7	Loss:3.057	translation_Loss:2.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.053                                                   	MRR:19.91	Hits@10:38.09	Best:19.91
2024-12-28 00:42:04,626: Snapshot:3	Epoch:8	Loss:2.992	translation_Loss:1.89	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.102                                                   	MRR:19.94	Hits@10:38.17	Best:19.94
2024-12-28 00:42:11,918: Snapshot:3	Epoch:9	Loss:2.936	translation_Loss:1.795	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.141                                                   	MRR:19.95	Hits@10:38.25	Best:19.95
2024-12-28 00:42:19,641: Snapshot:3	Epoch:10	Loss:2.908	translation_Loss:1.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.171                                                   	MRR:20.06	Hits@10:38.21	Best:20.06
2024-12-28 00:42:26,841: Snapshot:3	Epoch:11	Loss:2.875	translation_Loss:1.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.196                                                   	MRR:20.12	Hits@10:38.45	Best:20.12
2024-12-28 00:42:34,088: Snapshot:3	Epoch:12	Loss:2.863	translation_Loss:1.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.215                                                   	MRR:20.04	Hits@10:38.34	Best:20.12
2024-12-28 00:42:41,299: Snapshot:3	Epoch:13	Loss:2.848	translation_Loss:1.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.23                                                   	MRR:20.02	Hits@10:38.27	Best:20.12
2024-12-28 00:42:48,503: Snapshot:3	Epoch:14	Loss:2.84	translation_Loss:1.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.242                                                   	MRR:20.1	Hits@10:38.36	Best:20.12
2024-12-28 00:42:56,163: Snapshot:3	Epoch:15	Loss:2.819	translation_Loss:1.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.252                                                   	MRR:20.17	Hits@10:38.4	Best:20.17
2024-12-28 00:43:03,357: Snapshot:3	Epoch:16	Loss:2.822	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.261                                                   	MRR:20.09	Hits@10:38.43	Best:20.17
2024-12-28 00:43:10,590: Snapshot:3	Epoch:17	Loss:2.818	translation_Loss:1.548	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.27                                                   	MRR:20.18	Hits@10:38.5	Best:20.18
2024-12-28 00:43:17,791: Snapshot:3	Epoch:18	Loss:2.807	translation_Loss:1.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.274                                                   	MRR:20.08	Hits@10:38.52	Best:20.18
2024-12-28 00:43:24,965: Snapshot:3	Epoch:19	Loss:2.812	translation_Loss:1.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.278                                                   	MRR:20.18	Hits@10:38.49	Best:20.18
2024-12-28 00:43:32,154: Snapshot:3	Epoch:20	Loss:2.806	translation_Loss:1.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.283                                                   	MRR:20.13	Hits@10:38.53	Best:20.18
2024-12-28 00:43:39,814: Snapshot:3	Epoch:21	Loss:2.79	translation_Loss:1.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.288                                                   	MRR:20.12	Hits@10:38.49	Best:20.18
2024-12-28 00:43:47,067: Early Stopping! Snapshot: 3 Epoch: 22 Best Results: 20.18
2024-12-28 00:43:47,067: Start to training tokens! Snapshot: 3 Epoch: 22 Loss:2.8 MRR:20.11 Best Results: 20.18
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:43:47,067: Snapshot:3	Epoch:22	Loss:2.8	translation_Loss:1.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.291                                                   	MRR:20.11	Hits@10:38.52	Best:20.18
2024-12-28 00:43:54,167: Snapshot:3	Epoch:23	Loss:20.482	translation_Loss:19.861	multi_layer_Loss:0.621	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.11	Hits@10:38.52	Best:20.18
2024-12-28 00:44:01,199: End of token training: 3 Epoch: 24 Loss:20.224 MRR:20.11 Best Results: 20.18
2024-12-28 00:44:01,199: Snapshot:3	Epoch:24	Loss:20.224	translation_Loss:19.864	multi_layer_Loss:0.36	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.11	Hits@10:38.52	Best:20.18
2024-12-28 00:44:01,464: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_10000/3model_best.tar'
2024-12-28 00:44:14,182: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2357 | 0.149  | 0.2722 | 0.3258 |  0.4001 |
|     1      | 0.2151 | 0.131  | 0.2501 | 0.3034 |  0.3773 |
|     2      | 0.2093 | 0.1205 | 0.2464 | 0.3058 |  0.382  |
|     3      | 0.2011 | 0.1073 | 0.236  | 0.3006 |  0.3865 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-28 00:44:39,386: Snapshot:4	Epoch:0	Loss:3.27	translation_Loss:3.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.097                                                   	MRR:19.23	Hits@10:42.55	Best:19.23
2024-12-28 00:44:46,760: Snapshot:4	Epoch:1	Loss:2.297	translation_Loss:2.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:20.56	Hits@10:45.45	Best:20.56
2024-12-28 00:44:54,114: Snapshot:4	Epoch:2	Loss:1.904	translation_Loss:1.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.251                                                   	MRR:21.0	Hits@10:46.61	Best:21.0
2024-12-28 00:45:01,399: Snapshot:4	Epoch:3	Loss:1.652	translation_Loss:1.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.298                                                   	MRR:21.32	Hits@10:47.04	Best:21.32
2024-12-28 00:45:08,785: Snapshot:4	Epoch:4	Loss:1.448	translation_Loss:1.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:21.55	Hits@10:47.21	Best:21.55
2024-12-28 00:45:16,125: Snapshot:4	Epoch:5	Loss:1.272	translation_Loss:0.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.355                                                   	MRR:21.71	Hits@10:47.22	Best:21.71
2024-12-28 00:45:23,426: Snapshot:4	Epoch:6	Loss:1.146	translation_Loss:0.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.371                                                   	MRR:21.75	Hits@10:47.4	Best:21.75
2024-12-28 00:45:30,774: Snapshot:4	Epoch:7	Loss:1.042	translation_Loss:0.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.381                                                   	MRR:21.89	Hits@10:47.58	Best:21.89
2024-12-28 00:45:38,099: Snapshot:4	Epoch:8	Loss:0.966	translation_Loss:0.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.387                                                   	MRR:22.0	Hits@10:47.31	Best:22.0
2024-12-28 00:45:45,423: Snapshot:4	Epoch:9	Loss:0.907	translation_Loss:0.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.39                                                   	MRR:22.21	Hits@10:47.63	Best:22.21
2024-12-28 00:45:52,811: Snapshot:4	Epoch:10	Loss:0.88	translation_Loss:0.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.392                                                   	MRR:22.25	Hits@10:47.63	Best:22.25
2024-12-28 00:46:00,130: Snapshot:4	Epoch:11	Loss:0.852	translation_Loss:0.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.394                                                   	MRR:22.43	Hits@10:47.83	Best:22.43
2024-12-28 00:46:07,425: Snapshot:4	Epoch:12	Loss:0.827	translation_Loss:0.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.395                                                   	MRR:22.52	Hits@10:47.82	Best:22.52
2024-12-28 00:46:15,141: Snapshot:4	Epoch:13	Loss:0.823	translation_Loss:0.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.395                                                   	MRR:22.51	Hits@10:47.84	Best:22.52
2024-12-28 00:46:22,445: Snapshot:4	Epoch:14	Loss:0.809	translation_Loss:0.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:22.62	Hits@10:48.0	Best:22.62
2024-12-28 00:46:29,836: Snapshot:4	Epoch:15	Loss:0.803	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:22.67	Hits@10:48.09	Best:22.67
2024-12-28 00:46:37,072: Snapshot:4	Epoch:16	Loss:0.794	translation_Loss:0.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:22.64	Hits@10:48.01	Best:22.67
2024-12-28 00:46:44,359: Snapshot:4	Epoch:17	Loss:0.795	translation_Loss:0.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:22.47	Hits@10:47.84	Best:22.67
2024-12-28 00:46:51,675: Snapshot:4	Epoch:18	Loss:0.779	translation_Loss:0.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:22.57	Hits@10:47.86	Best:22.67
2024-12-28 00:46:59,343: Snapshot:4	Epoch:19	Loss:0.778	translation_Loss:0.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:22.69	Hits@10:47.71	Best:22.69
2024-12-28 00:47:06,591: Snapshot:4	Epoch:20	Loss:0.779	translation_Loss:0.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:22.57	Hits@10:47.55	Best:22.69
2024-12-28 00:47:13,919: Snapshot:4	Epoch:21	Loss:0.788	translation_Loss:0.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:22.78	Hits@10:47.81	Best:22.78
2024-12-28 00:47:21,162: Snapshot:4	Epoch:22	Loss:0.782	translation_Loss:0.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.399                                                   	MRR:22.74	Hits@10:47.79	Best:22.78
2024-12-28 00:47:28,400: Snapshot:4	Epoch:23	Loss:0.774	translation_Loss:0.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.4                                                   	MRR:22.7	Hits@10:47.78	Best:22.78
2024-12-28 00:47:36,060: Snapshot:4	Epoch:24	Loss:0.773	translation_Loss:0.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:22.67	Hits@10:47.91	Best:22.78
2024-12-28 00:47:43,373: Snapshot:4	Epoch:25	Loss:0.778	translation_Loss:0.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.398                                                   	MRR:22.65	Hits@10:47.8	Best:22.78
2024-12-28 00:47:50,648: Early Stopping! Snapshot: 4 Epoch: 26 Best Results: 22.78
2024-12-28 00:47:50,649: Start to training tokens! Snapshot: 4 Epoch: 26 Loss:0.781 MRR:22.72 Best Results: 22.78
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-28 00:47:50,649: Snapshot:4	Epoch:26	Loss:0.781	translation_Loss:0.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.4                                                   	MRR:22.72	Hits@10:47.57	Best:22.78
2024-12-28 00:47:57,755: Snapshot:4	Epoch:27	Loss:17.283	translation_Loss:16.658	multi_layer_Loss:0.625	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.72	Hits@10:47.57	Best:22.78
2024-12-28 00:48:04,898: End of token training: 4 Epoch: 28 Loss:17.044 MRR:22.72 Best Results: 22.78
2024-12-28 00:48:04,898: Snapshot:4	Epoch:28	Loss:17.044	translation_Loss:16.682	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.72	Hits@10:47.57	Best:22.78
2024-12-28 00:48:05,152: => loading checkpoint './checkpoint/FACTfact_0.0001_2048_10000/4model_best.tar'
2024-12-28 00:48:21,349: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2226 | 0.1376 | 0.2555 | 0.3096 |  0.385  |
|     1      | 0.2025 | 0.1218 | 0.2333 | 0.2852 |  0.3606 |
|     2      | 0.1954 | 0.1099 | 0.2261 | 0.2836 |  0.3659 |
|     3      | 0.1892 | 0.0957 | 0.2196 | 0.2872 |  0.3825 |
|     4      | 0.2287 | 0.1083 | 0.2693 | 0.3584 |  0.4784 |
+------------+--------+--------+--------+--------+---------+
2024-12-28 00:48:21,358: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2325 | 0.1444 | 0.2778 | 0.3275 |   0.39  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.246  | 0.1562 | 0.2905 | 0.3405 |  0.4086 |
|     1      | 0.2101 | 0.126  | 0.2526 | 0.302  |  0.3632 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2451 | 0.155  | 0.288  | 0.3417 |  0.4125 |
|     1      | 0.2202 | 0.1345 | 0.2603 | 0.3117 |  0.3803 |
|     2      | 0.2126 | 0.127  | 0.2522 | 0.303  |  0.3723 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2357 | 0.149  | 0.2722 | 0.3258 |  0.4001 |
|     1      | 0.2151 | 0.131  | 0.2501 | 0.3034 |  0.3773 |
|     2      | 0.2093 | 0.1205 | 0.2464 | 0.3058 |  0.382  |
|     3      | 0.2011 | 0.1073 | 0.236  | 0.3006 |  0.3865 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2226 | 0.1376 | 0.2555 | 0.3096 |  0.385  |
|     1      | 0.2025 | 0.1218 | 0.2333 | 0.2852 |  0.3606 |
|     2      | 0.1954 | 0.1099 | 0.2261 | 0.2836 |  0.3659 |
|     3      | 0.1892 | 0.0957 | 0.2196 | 0.2872 |  0.3825 |
|     4      | 0.2287 | 0.1083 | 0.2693 | 0.3584 |  0.4784 |
+------------+--------+--------+--------+--------+---------+]
2024-12-28 00:48:21,359: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 649.2469079494476  |   0.233   |    0.144     |    0.278     |      0.39     |
|    1     | 311.4652404785156  |   0.228   |    0.141     |    0.272     |     0.386     |
|    2     | 202.1966323852539  |   0.226   |    0.139     |    0.267     |     0.388     |
|    3     | 196.46752429008484 |   0.215   |    0.127     |    0.251     |     0.386     |
|    4     | 227.60666513442993 |   0.208   |    0.115     |    0.241     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-28 00:48:21,359: Sum_Training_Time:1586.982970237732
2024-12-28 00:48:21,359: Every_Training_Time:[649.2469079494476, 311.4652404785156, 202.1966323852539, 196.46752429008484, 227.60666513442993]
2024-12-28 00:48:21,359: Forward transfer: 0.1684 Backward transfer: -0.011650000000000008
2024-12-28 00:49:00,332: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228004825/FACTfact_0.00001_512_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_512_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_512_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 00:49:10,402: Snapshot:0	Epoch:0	Loss:104.79	translation_Loss:104.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 00:49:16,946: Snapshot:0	Epoch:1	Loss:103.761	translation_Loss:103.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 00:49:23,534: Snapshot:0	Epoch:2	Loss:102.703	translation_Loss:102.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.39	Best:1.39
2024-12-28 00:49:30,108: Snapshot:0	Epoch:3	Loss:101.68	translation_Loss:101.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.47	Best:1.42
2024-12-28 00:49:36,641: Snapshot:0	Epoch:4	Loss:100.655	translation_Loss:100.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.48	Hits@10:1.58	Best:1.48
2024-12-28 00:49:43,853: Snapshot:0	Epoch:5	Loss:99.608	translation_Loss:99.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.6	Hits@10:2.0	Best:1.6
2024-12-28 00:49:51,049: Snapshot:0	Epoch:6	Loss:98.592	translation_Loss:98.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.8	Hits@10:2.56	Best:1.8
2024-12-28 00:49:58,142: Snapshot:0	Epoch:7	Loss:97.569	translation_Loss:97.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.08	Hits@10:3.35	Best:2.08
2024-12-28 00:50:05,798: Snapshot:0	Epoch:8	Loss:96.556	translation_Loss:96.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.42	Hits@10:4.09	Best:2.42
2024-12-28 00:50:12,953: Snapshot:0	Epoch:9	Loss:95.574	translation_Loss:95.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.72	Hits@10:4.59	Best:2.72
2024-12-28 00:50:20,147: Snapshot:0	Epoch:10	Loss:94.553	translation_Loss:94.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.06	Best:2.98
2024-12-28 00:50:27,252: Snapshot:0	Epoch:11	Loss:93.569	translation_Loss:93.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.2	Hits@10:5.48	Best:3.2
2024-12-28 00:50:34,412: Snapshot:0	Epoch:12	Loss:92.595	translation_Loss:92.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.38	Hits@10:5.9	Best:3.38
2024-12-28 00:50:41,538: Snapshot:0	Epoch:13	Loss:91.639	translation_Loss:91.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.55	Hits@10:6.28	Best:3.55
2024-12-28 00:50:48,691: Snapshot:0	Epoch:14	Loss:90.71	translation_Loss:90.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.7	Hits@10:6.67	Best:3.7
2024-12-28 00:50:55,789: Snapshot:0	Epoch:15	Loss:89.806	translation_Loss:89.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.87	Hits@10:7.02	Best:3.87
2024-12-28 00:51:02,977: Snapshot:0	Epoch:16	Loss:88.895	translation_Loss:88.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.02	Hits@10:7.38	Best:4.02
2024-12-28 00:51:10,084: Snapshot:0	Epoch:17	Loss:87.984	translation_Loss:87.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.18	Hits@10:7.69	Best:4.18
2024-12-28 00:51:17,199: Snapshot:0	Epoch:18	Loss:87.067	translation_Loss:87.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.32	Hits@10:8.03	Best:4.32
2024-12-28 00:51:24,387: Snapshot:0	Epoch:19	Loss:86.195	translation_Loss:86.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.46	Hits@10:8.38	Best:4.46
2024-12-28 00:51:30,961: Snapshot:0	Epoch:20	Loss:85.315	translation_Loss:85.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.58	Hits@10:8.68	Best:4.58
2024-12-28 00:51:37,545: Snapshot:0	Epoch:21	Loss:84.452	translation_Loss:84.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.71	Hits@10:8.98	Best:4.71
2024-12-28 00:51:44,771: Snapshot:0	Epoch:22	Loss:83.641	translation_Loss:83.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.26	Best:4.83
2024-12-28 00:51:51,922: Snapshot:0	Epoch:23	Loss:82.76	translation_Loss:82.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.95	Hits@10:9.58	Best:4.95
2024-12-28 00:51:59,013: Snapshot:0	Epoch:24	Loss:81.949	translation_Loss:81.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.06	Hits@10:9.87	Best:5.06
2024-12-28 00:52:06,136: Snapshot:0	Epoch:25	Loss:81.161	translation_Loss:81.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.16	Hits@10:10.09	Best:5.16
2024-12-28 00:52:12,709: Snapshot:0	Epoch:26	Loss:80.331	translation_Loss:80.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.27	Hits@10:10.35	Best:5.27
2024-12-28 00:52:19,856: Snapshot:0	Epoch:27	Loss:79.511	translation_Loss:79.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.38	Hits@10:10.62	Best:5.38
2024-12-28 00:52:26,981: Snapshot:0	Epoch:28	Loss:78.715	translation_Loss:78.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:10.91	Best:5.49
2024-12-28 00:52:34,100: Snapshot:0	Epoch:29	Loss:77.907	translation_Loss:77.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.59	Hits@10:11.18	Best:5.59
2024-12-28 00:52:41,250: Snapshot:0	Epoch:30	Loss:77.157	translation_Loss:77.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.7	Hits@10:11.47	Best:5.7
2024-12-28 00:52:47,865: Snapshot:0	Epoch:31	Loss:76.376	translation_Loss:76.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.81	Hits@10:11.8	Best:5.81
2024-12-28 00:52:55,048: Snapshot:0	Epoch:32	Loss:75.646	translation_Loss:75.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.93	Hits@10:12.17	Best:5.93
2024-12-28 00:53:02,166: Snapshot:0	Epoch:33	Loss:74.81	translation_Loss:74.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.05	Hits@10:12.56	Best:6.05
2024-12-28 00:53:09,297: Snapshot:0	Epoch:34	Loss:74.074	translation_Loss:74.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.16	Hits@10:12.86	Best:6.16
2024-12-28 00:53:16,429: Snapshot:0	Epoch:35	Loss:73.322	translation_Loss:73.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.28	Hits@10:13.26	Best:6.28
2024-12-28 00:53:23,540: Snapshot:0	Epoch:36	Loss:72.556	translation_Loss:72.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.39	Hits@10:13.61	Best:6.39
2024-12-28 00:53:30,712: Snapshot:0	Epoch:37	Loss:71.861	translation_Loss:71.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.51	Hits@10:13.87	Best:6.51
2024-12-28 00:53:37,849: Snapshot:0	Epoch:38	Loss:71.111	translation_Loss:71.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.63	Hits@10:14.23	Best:6.63
2024-12-28 00:53:45,043: Snapshot:0	Epoch:39	Loss:70.375	translation_Loss:70.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.76	Hits@10:14.54	Best:6.76
2024-12-28 00:53:52,202: Snapshot:0	Epoch:40	Loss:69.71	translation_Loss:69.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.87	Hits@10:14.84	Best:6.87
2024-12-28 00:53:58,761: Snapshot:0	Epoch:41	Loss:68.945	translation_Loss:68.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.99	Hits@10:15.15	Best:6.99
2024-12-28 00:54:05,876: Snapshot:0	Epoch:42	Loss:68.253	translation_Loss:68.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.11	Hits@10:15.44	Best:7.11
2024-12-28 00:54:12,994: Snapshot:0	Epoch:43	Loss:67.532	translation_Loss:67.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.23	Hits@10:15.76	Best:7.23
2024-12-28 00:54:20,164: Snapshot:0	Epoch:44	Loss:66.851	translation_Loss:66.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.36	Hits@10:16.04	Best:7.36
2024-12-28 00:54:26,765: Snapshot:0	Epoch:45	Loss:66.17	translation_Loss:66.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.48	Hits@10:16.28	Best:7.48
2024-12-28 00:54:33,958: Snapshot:0	Epoch:46	Loss:65.437	translation_Loss:65.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.58	Hits@10:16.56	Best:7.58
2024-12-28 00:54:41,078: Snapshot:0	Epoch:47	Loss:64.763	translation_Loss:64.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.72	Hits@10:16.87	Best:7.72
2024-12-28 00:54:48,242: Snapshot:0	Epoch:48	Loss:64.121	translation_Loss:64.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.85	Hits@10:17.17	Best:7.85
2024-12-28 00:54:55,506: Snapshot:0	Epoch:49	Loss:63.439	translation_Loss:63.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.99	Hits@10:17.47	Best:7.99
2024-12-28 00:55:02,139: Snapshot:0	Epoch:50	Loss:62.772	translation_Loss:62.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:17.71	Best:8.1
2024-12-28 00:55:08,806: Snapshot:0	Epoch:51	Loss:62.09	translation_Loss:62.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.23	Hits@10:18.0	Best:8.23
2024-12-28 00:55:15,390: Snapshot:0	Epoch:52	Loss:61.502	translation_Loss:61.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.35	Hits@10:18.29	Best:8.35
2024-12-28 00:55:21,997: Snapshot:0	Epoch:53	Loss:60.811	translation_Loss:60.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.49	Hits@10:18.6	Best:8.49
2024-12-28 00:55:28,559: Snapshot:0	Epoch:54	Loss:60.142	translation_Loss:60.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.63	Hits@10:18.96	Best:8.63
2024-12-28 00:55:35,154: Snapshot:0	Epoch:55	Loss:59.514	translation_Loss:59.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.76	Hits@10:19.25	Best:8.76
2024-12-28 00:55:41,772: Snapshot:0	Epoch:56	Loss:58.915	translation_Loss:58.915	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:19.5	Best:8.89
2024-12-28 00:55:48,412: Snapshot:0	Epoch:57	Loss:58.265	translation_Loss:58.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.03	Hits@10:19.82	Best:9.03
2024-12-28 00:55:55,001: Snapshot:0	Epoch:58	Loss:57.652	translation_Loss:57.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.18	Hits@10:20.08	Best:9.18
2024-12-28 00:56:01,598: Snapshot:0	Epoch:59	Loss:57.039	translation_Loss:57.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.3	Hits@10:20.35	Best:9.3
2024-12-28 00:56:08,248: Snapshot:0	Epoch:60	Loss:56.374	translation_Loss:56.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.42	Hits@10:20.6	Best:9.42
2024-12-28 00:56:14,846: Snapshot:0	Epoch:61	Loss:55.824	translation_Loss:55.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.56	Hits@10:20.85	Best:9.56
2024-12-28 00:56:21,451: Snapshot:0	Epoch:62	Loss:55.188	translation_Loss:55.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:21.13	Best:9.68
2024-12-28 00:56:28,055: Snapshot:0	Epoch:63	Loss:54.6	translation_Loss:54.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.79	Hits@10:21.41	Best:9.79
2024-12-28 00:56:34,654: Snapshot:0	Epoch:64	Loss:53.969	translation_Loss:53.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.93	Hits@10:21.75	Best:9.93
2024-12-28 00:56:41,344: Snapshot:0	Epoch:65	Loss:53.411	translation_Loss:53.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.06	Hits@10:22.0	Best:10.06
2024-12-28 00:56:47,945: Snapshot:0	Epoch:66	Loss:52.843	translation_Loss:52.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.17	Hits@10:22.29	Best:10.17
2024-12-28 00:56:54,525: Snapshot:0	Epoch:67	Loss:52.292	translation_Loss:52.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.31	Hits@10:22.61	Best:10.31
2024-12-28 00:57:01,110: Snapshot:0	Epoch:68	Loss:51.697	translation_Loss:51.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.45	Hits@10:22.82	Best:10.45
2024-12-28 00:57:08,188: Snapshot:0	Epoch:69	Loss:51.028	translation_Loss:51.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.58	Hits@10:23.06	Best:10.58
2024-12-28 00:57:15,317: Snapshot:0	Epoch:70	Loss:50.5	translation_Loss:50.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.7	Hits@10:23.25	Best:10.7
2024-12-28 00:57:22,429: Snapshot:0	Epoch:71	Loss:49.969	translation_Loss:49.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:23.5	Best:10.83
2024-12-28 00:57:29,530: Snapshot:0	Epoch:72	Loss:49.379	translation_Loss:49.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:23.76	Best:10.96
2024-12-28 00:57:36,675: Snapshot:0	Epoch:73	Loss:48.845	translation_Loss:48.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.08	Hits@10:23.98	Best:11.08
2024-12-28 00:57:43,888: Snapshot:0	Epoch:74	Loss:48.309	translation_Loss:48.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.2	Hits@10:24.19	Best:11.2
2024-12-28 00:57:51,081: Snapshot:0	Epoch:75	Loss:47.822	translation_Loss:47.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.29	Hits@10:24.48	Best:11.29
2024-12-28 00:57:58,189: Snapshot:0	Epoch:76	Loss:47.241	translation_Loss:47.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.41	Hits@10:24.73	Best:11.41
2024-12-28 00:58:05,323: Snapshot:0	Epoch:77	Loss:46.726	translation_Loss:46.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.52	Hits@10:24.98	Best:11.52
2024-12-28 00:58:12,462: Snapshot:0	Epoch:78	Loss:46.128	translation_Loss:46.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.62	Hits@10:25.22	Best:11.62
2024-12-28 00:58:19,591: Snapshot:0	Epoch:79	Loss:45.681	translation_Loss:45.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.73	Hits@10:25.46	Best:11.73
2024-12-28 00:58:26,713: Snapshot:0	Epoch:80	Loss:45.146	translation_Loss:45.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.82	Hits@10:25.64	Best:11.82
2024-12-28 00:58:33,861: Snapshot:0	Epoch:81	Loss:44.617	translation_Loss:44.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.91	Hits@10:25.81	Best:11.91
2024-12-28 00:58:40,996: Snapshot:0	Epoch:82	Loss:44.111	translation_Loss:44.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.0	Hits@10:26.03	Best:12.0
2024-12-28 00:58:48,158: Snapshot:0	Epoch:83	Loss:43.576	translation_Loss:43.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.11	Hits@10:26.27	Best:12.11
2024-12-28 00:58:54,767: Snapshot:0	Epoch:84	Loss:43.08	translation_Loss:43.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.22	Hits@10:26.48	Best:12.22
2024-12-28 00:59:01,407: Snapshot:0	Epoch:85	Loss:42.655	translation_Loss:42.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:26.7	Best:12.31
2024-12-28 00:59:08,518: Snapshot:0	Epoch:86	Loss:42.117	translation_Loss:42.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:26.86	Best:12.4
2024-12-28 00:59:15,665: Snapshot:0	Epoch:87	Loss:41.663	translation_Loss:41.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.51	Hits@10:27.06	Best:12.51
2024-12-28 00:59:22,799: Snapshot:0	Epoch:88	Loss:41.132	translation_Loss:41.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.6	Hits@10:27.2	Best:12.6
2024-12-28 00:59:30,268: Snapshot:0	Epoch:89	Loss:40.679	translation_Loss:40.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.69	Hits@10:27.4	Best:12.69
2024-12-28 00:59:37,435: Snapshot:0	Epoch:90	Loss:40.213	translation_Loss:40.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.78	Hits@10:27.59	Best:12.78
2024-12-28 00:59:44,642: Snapshot:0	Epoch:91	Loss:39.716	translation_Loss:39.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.87	Hits@10:27.75	Best:12.87
2024-12-28 00:59:51,797: Snapshot:0	Epoch:92	Loss:39.278	translation_Loss:39.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.96	Hits@10:27.94	Best:12.96
2024-12-28 00:59:58,920: Snapshot:0	Epoch:93	Loss:38.763	translation_Loss:38.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.02	Hits@10:28.07	Best:13.02
2024-12-28 01:00:06,145: Snapshot:0	Epoch:94	Loss:38.379	translation_Loss:38.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.12	Hits@10:28.23	Best:13.12
2024-12-28 01:00:13,314: Snapshot:0	Epoch:95	Loss:37.872	translation_Loss:37.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.22	Hits@10:28.4	Best:13.22
2024-12-28 01:00:20,470: Snapshot:0	Epoch:96	Loss:37.486	translation_Loss:37.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.31	Hits@10:28.51	Best:13.31
2024-12-28 01:00:27,673: Snapshot:0	Epoch:97	Loss:36.963	translation_Loss:36.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.39	Hits@10:28.69	Best:13.39
2024-12-28 01:00:34,823: Snapshot:0	Epoch:98	Loss:36.562	translation_Loss:36.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.46	Hits@10:28.86	Best:13.46
2024-12-28 01:00:41,990: Snapshot:0	Epoch:99	Loss:36.161	translation_Loss:36.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.55	Hits@10:29.01	Best:13.55
2024-12-28 01:00:49,144: Snapshot:0	Epoch:100	Loss:35.648	translation_Loss:35.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.63	Hits@10:29.19	Best:13.63
2024-12-28 01:00:56,268: Snapshot:0	Epoch:101	Loss:35.27	translation_Loss:35.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.72	Hits@10:29.32	Best:13.72
2024-12-28 01:01:03,420: Snapshot:0	Epoch:102	Loss:34.842	translation_Loss:34.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.8	Hits@10:29.52	Best:13.8
2024-12-28 01:01:10,596: Snapshot:0	Epoch:103	Loss:34.466	translation_Loss:34.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.88	Hits@10:29.67	Best:13.88
2024-12-28 01:01:17,197: Snapshot:0	Epoch:104	Loss:33.99	translation_Loss:33.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.98	Hits@10:29.86	Best:13.98
2024-12-28 01:01:24,405: Snapshot:0	Epoch:105	Loss:33.586	translation_Loss:33.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.06	Hits@10:30.05	Best:14.06
2024-12-28 01:01:31,639: Snapshot:0	Epoch:106	Loss:33.174	translation_Loss:33.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.15	Hits@10:30.22	Best:14.15
2024-12-28 01:01:38,780: Snapshot:0	Epoch:107	Loss:32.866	translation_Loss:32.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.22	Hits@10:30.38	Best:14.22
2024-12-28 01:01:45,995: Snapshot:0	Epoch:108	Loss:32.414	translation_Loss:32.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.31	Hits@10:30.5	Best:14.31
2024-12-28 01:01:53,639: Snapshot:0	Epoch:109	Loss:32.026	translation_Loss:32.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.39	Hits@10:30.65	Best:14.39
2024-12-28 01:02:00,770: Snapshot:0	Epoch:110	Loss:31.607	translation_Loss:31.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.47	Hits@10:30.76	Best:14.47
2024-12-28 01:02:07,897: Snapshot:0	Epoch:111	Loss:31.296	translation_Loss:31.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.56	Hits@10:30.87	Best:14.56
2024-12-28 01:02:14,498: Snapshot:0	Epoch:112	Loss:30.814	translation_Loss:30.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.64	Hits@10:30.98	Best:14.64
2024-12-28 01:02:21,664: Snapshot:0	Epoch:113	Loss:30.478	translation_Loss:30.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.73	Hits@10:31.1	Best:14.73
2024-12-28 01:02:28,787: Snapshot:0	Epoch:114	Loss:30.117	translation_Loss:30.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.81	Hits@10:31.22	Best:14.81
2024-12-28 01:02:35,918: Snapshot:0	Epoch:115	Loss:29.694	translation_Loss:29.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.88	Hits@10:31.4	Best:14.88
2024-12-28 01:02:43,031: Snapshot:0	Epoch:116	Loss:29.431	translation_Loss:29.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.95	Hits@10:31.5	Best:14.95
2024-12-28 01:02:49,626: Snapshot:0	Epoch:117	Loss:28.995	translation_Loss:28.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.02	Hits@10:31.62	Best:15.02
2024-12-28 01:02:56,774: Snapshot:0	Epoch:118	Loss:28.674	translation_Loss:28.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.09	Hits@10:31.76	Best:15.09
2024-12-28 01:03:03,348: Snapshot:0	Epoch:119	Loss:28.336	translation_Loss:28.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.16	Hits@10:31.88	Best:15.16
2024-12-28 01:03:10,569: Snapshot:0	Epoch:120	Loss:27.946	translation_Loss:27.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.24	Hits@10:32.02	Best:15.24
2024-12-28 01:03:17,181: Snapshot:0	Epoch:121	Loss:27.593	translation_Loss:27.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.32	Hits@10:32.12	Best:15.32
2024-12-28 01:03:24,357: Snapshot:0	Epoch:122	Loss:27.173	translation_Loss:27.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.41	Hits@10:32.25	Best:15.41
2024-12-28 01:03:30,952: Snapshot:0	Epoch:123	Loss:26.942	translation_Loss:26.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.48	Hits@10:32.38	Best:15.48
2024-12-28 01:03:38,095: Snapshot:0	Epoch:124	Loss:26.577	translation_Loss:26.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.56	Hits@10:32.5	Best:15.56
2024-12-28 01:03:45,313: Snapshot:0	Epoch:125	Loss:26.189	translation_Loss:26.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.63	Hits@10:32.66	Best:15.63
2024-12-28 01:03:52,456: Snapshot:0	Epoch:126	Loss:25.85	translation_Loss:25.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.71	Hits@10:32.72	Best:15.71
2024-12-28 01:03:59,599: Snapshot:0	Epoch:127	Loss:25.587	translation_Loss:25.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.78	Hits@10:32.81	Best:15.78
2024-12-28 01:04:06,712: Snapshot:0	Epoch:128	Loss:25.237	translation_Loss:25.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.87	Hits@10:32.95	Best:15.87
2024-12-28 01:04:13,904: Snapshot:0	Epoch:129	Loss:24.904	translation_Loss:24.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.95	Hits@10:33.06	Best:15.95
2024-12-28 01:04:21,042: Snapshot:0	Epoch:130	Loss:24.618	translation_Loss:24.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.0	Hits@10:33.14	Best:16.0
2024-12-28 01:04:27,623: Snapshot:0	Epoch:131	Loss:24.323	translation_Loss:24.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.08	Hits@10:33.24	Best:16.08
2024-12-28 01:04:34,305: Snapshot:0	Epoch:132	Loss:23.926	translation_Loss:23.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.14	Hits@10:33.3	Best:16.14
2024-12-28 01:04:40,925: Snapshot:0	Epoch:133	Loss:23.688	translation_Loss:23.688	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.22	Hits@10:33.42	Best:16.22
2024-12-28 01:04:47,554: Snapshot:0	Epoch:134	Loss:23.327	translation_Loss:23.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.3	Hits@10:33.52	Best:16.3
2024-12-28 01:04:54,176: Snapshot:0	Epoch:135	Loss:23.05	translation_Loss:23.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.38	Hits@10:33.62	Best:16.38
2024-12-28 01:05:00,761: Snapshot:0	Epoch:136	Loss:22.742	translation_Loss:22.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.45	Hits@10:33.72	Best:16.45
2024-12-28 01:05:07,441: Snapshot:0	Epoch:137	Loss:22.463	translation_Loss:22.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.53	Hits@10:33.8	Best:16.53
2024-12-28 01:05:14,024: Snapshot:0	Epoch:138	Loss:22.185	translation_Loss:22.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.6	Hits@10:33.94	Best:16.6
2024-12-28 01:05:20,675: Snapshot:0	Epoch:139	Loss:21.887	translation_Loss:21.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:34.05	Best:16.66
2024-12-28 01:05:27,271: Snapshot:0	Epoch:140	Loss:21.617	translation_Loss:21.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.74	Hits@10:34.12	Best:16.74
2024-12-28 01:05:33,933: Snapshot:0	Epoch:141	Loss:21.389	translation_Loss:21.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.83	Hits@10:34.22	Best:16.83
2024-12-28 01:05:40,555: Snapshot:0	Epoch:142	Loss:21.043	translation_Loss:21.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.9	Hits@10:34.28	Best:16.9
2024-12-28 01:05:47,193: Snapshot:0	Epoch:143	Loss:20.743	translation_Loss:20.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.98	Hits@10:34.4	Best:16.98
2024-12-28 01:05:53,833: Snapshot:0	Epoch:144	Loss:20.497	translation_Loss:20.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.04	Hits@10:34.53	Best:17.04
2024-12-28 01:06:00,460: Snapshot:0	Epoch:145	Loss:20.226	translation_Loss:20.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.13	Hits@10:34.57	Best:17.13
2024-12-28 01:06:07,126: Snapshot:0	Epoch:146	Loss:20.007	translation_Loss:20.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.21	Hits@10:34.64	Best:17.21
2024-12-28 01:06:13,756: Snapshot:0	Epoch:147	Loss:19.699	translation_Loss:19.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.28	Hits@10:34.69	Best:17.28
2024-12-28 01:06:20,355: Snapshot:0	Epoch:148	Loss:19.387	translation_Loss:19.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.35	Hits@10:34.79	Best:17.35
2024-12-28 01:06:26,939: Snapshot:0	Epoch:149	Loss:19.12	translation_Loss:19.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.42	Hits@10:34.89	Best:17.42
2024-12-28 01:06:34,014: Snapshot:0	Epoch:150	Loss:18.845	translation_Loss:18.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.48	Hits@10:34.94	Best:17.48
2024-12-28 01:06:41,251: Snapshot:0	Epoch:151	Loss:18.615	translation_Loss:18.615	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.54	Hits@10:35.05	Best:17.54
2024-12-28 01:06:48,501: Snapshot:0	Epoch:152	Loss:18.38	translation_Loss:18.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.61	Hits@10:35.09	Best:17.61
2024-12-28 01:06:55,644: Snapshot:0	Epoch:153	Loss:18.148	translation_Loss:18.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.69	Hits@10:35.19	Best:17.69
2024-12-28 01:07:02,789: Snapshot:0	Epoch:154	Loss:17.871	translation_Loss:17.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.75	Hits@10:35.24	Best:17.75
2024-12-28 01:07:10,000: Snapshot:0	Epoch:155	Loss:17.664	translation_Loss:17.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.8	Hits@10:35.36	Best:17.8
2024-12-28 01:07:17,155: Snapshot:0	Epoch:156	Loss:17.37	translation_Loss:17.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.88	Hits@10:35.46	Best:17.88
2024-12-28 01:07:24,287: Snapshot:0	Epoch:157	Loss:17.195	translation_Loss:17.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.95	Hits@10:35.54	Best:17.95
2024-12-28 01:07:31,477: Snapshot:0	Epoch:158	Loss:16.887	translation_Loss:16.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.03	Hits@10:35.6	Best:18.03
2024-12-28 01:07:38,708: Snapshot:0	Epoch:159	Loss:16.698	translation_Loss:16.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.1	Hits@10:35.66	Best:18.1
2024-12-28 01:07:45,979: Snapshot:0	Epoch:160	Loss:16.41	translation_Loss:16.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.16	Hits@10:35.73	Best:18.16
2024-12-28 01:07:53,202: Snapshot:0	Epoch:161	Loss:16.204	translation_Loss:16.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.22	Hits@10:35.84	Best:18.22
2024-12-28 01:07:59,881: Snapshot:0	Epoch:162	Loss:15.995	translation_Loss:15.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.3	Hits@10:35.88	Best:18.3
2024-12-28 01:08:07,061: Snapshot:0	Epoch:163	Loss:15.785	translation_Loss:15.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.35	Hits@10:35.95	Best:18.35
2024-12-28 01:08:14,203: Snapshot:0	Epoch:164	Loss:15.511	translation_Loss:15.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.41	Hits@10:36.01	Best:18.41
2024-12-28 01:08:21,373: Snapshot:0	Epoch:165	Loss:15.347	translation_Loss:15.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.47	Hits@10:36.1	Best:18.47
2024-12-28 01:08:27,987: Snapshot:0	Epoch:166	Loss:15.129	translation_Loss:15.129	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.52	Hits@10:36.17	Best:18.52
2024-12-28 01:08:35,141: Snapshot:0	Epoch:167	Loss:14.83	translation_Loss:14.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.61	Hits@10:36.22	Best:18.61
2024-12-28 01:08:42,329: Snapshot:0	Epoch:168	Loss:14.772	translation_Loss:14.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.67	Hits@10:36.25	Best:18.67
2024-12-28 01:08:49,507: Snapshot:0	Epoch:169	Loss:14.423	translation_Loss:14.423	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.74	Hits@10:36.28	Best:18.74
2024-12-28 01:08:56,891: Snapshot:0	Epoch:170	Loss:14.278	translation_Loss:14.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.79	Hits@10:36.36	Best:18.79
2024-12-28 01:09:04,042: Snapshot:0	Epoch:171	Loss:14.041	translation_Loss:14.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.84	Hits@10:36.4	Best:18.84
2024-12-28 01:09:11,232: Snapshot:0	Epoch:172	Loss:13.935	translation_Loss:13.935	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.91	Hits@10:36.47	Best:18.91
2024-12-28 01:09:18,416: Snapshot:0	Epoch:173	Loss:13.643	translation_Loss:13.643	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.95	Hits@10:36.5	Best:18.95
2024-12-28 01:09:25,652: Snapshot:0	Epoch:174	Loss:13.493	translation_Loss:13.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.02	Hits@10:36.61	Best:19.02
2024-12-28 01:09:32,808: Snapshot:0	Epoch:175	Loss:13.255	translation_Loss:13.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.08	Hits@10:36.68	Best:19.08
2024-12-28 01:09:39,450: Snapshot:0	Epoch:176	Loss:13.116	translation_Loss:13.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.14	Hits@10:36.73	Best:19.14
2024-12-28 01:09:46,684: Snapshot:0	Epoch:177	Loss:12.887	translation_Loss:12.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.2	Hits@10:36.76	Best:19.2
2024-12-28 01:09:53,843: Snapshot:0	Epoch:178	Loss:12.733	translation_Loss:12.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.28	Hits@10:36.78	Best:19.28
2024-12-28 01:10:01,033: Snapshot:0	Epoch:179	Loss:12.54	translation_Loss:12.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.34	Hits@10:36.84	Best:19.34
2024-12-28 01:10:08,292: Snapshot:0	Epoch:180	Loss:12.313	translation_Loss:12.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.38	Hits@10:36.87	Best:19.38
2024-12-28 01:10:14,859: Snapshot:0	Epoch:181	Loss:12.126	translation_Loss:12.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.44	Hits@10:36.95	Best:19.44
2024-12-28 01:10:22,038: Snapshot:0	Epoch:182	Loss:11.965	translation_Loss:11.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.52	Hits@10:37.01	Best:19.52
2024-12-28 01:10:29,201: Snapshot:0	Epoch:183	Loss:11.801	translation_Loss:11.801	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.58	Hits@10:37.02	Best:19.58
2024-12-28 01:10:36,414: Snapshot:0	Epoch:184	Loss:11.629	translation_Loss:11.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.64	Hits@10:37.07	Best:19.64
2024-12-28 01:10:43,637: Snapshot:0	Epoch:185	Loss:11.463	translation_Loss:11.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.7	Hits@10:37.11	Best:19.7
2024-12-28 01:10:50,909: Snapshot:0	Epoch:186	Loss:11.252	translation_Loss:11.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:37.17	Best:19.77
2024-12-28 01:10:57,502: Snapshot:0	Epoch:187	Loss:11.085	translation_Loss:11.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.84	Hits@10:37.23	Best:19.84
2024-12-28 01:11:04,079: Snapshot:0	Epoch:188	Loss:10.88	translation_Loss:10.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.89	Hits@10:37.26	Best:19.89
2024-12-28 01:11:11,328: Snapshot:0	Epoch:189	Loss:10.772	translation_Loss:10.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.95	Hits@10:37.3	Best:19.95
2024-12-28 01:11:18,466: Snapshot:0	Epoch:190	Loss:10.596	translation_Loss:10.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.0	Hits@10:37.37	Best:20.0
2024-12-28 01:11:25,075: Snapshot:0	Epoch:191	Loss:10.435	translation_Loss:10.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.08	Hits@10:37.41	Best:20.08
2024-12-28 01:11:31,668: Snapshot:0	Epoch:192	Loss:10.253	translation_Loss:10.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.12	Hits@10:37.44	Best:20.12
2024-12-28 01:11:38,282: Snapshot:0	Epoch:193	Loss:10.135	translation_Loss:10.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.18	Hits@10:37.49	Best:20.18
2024-12-28 01:11:44,911: Snapshot:0	Epoch:194	Loss:9.974	translation_Loss:9.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.23	Hits@10:37.51	Best:20.23
2024-12-28 01:11:51,551: Snapshot:0	Epoch:195	Loss:9.815	translation_Loss:9.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.29	Hits@10:37.55	Best:20.29
2024-12-28 01:11:58,168: Snapshot:0	Epoch:196	Loss:9.685	translation_Loss:9.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.34	Hits@10:37.64	Best:20.34
2024-12-28 01:12:04,853: Snapshot:0	Epoch:197	Loss:9.544	translation_Loss:9.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.38	Hits@10:37.67	Best:20.38
2024-12-28 01:12:11,464: Snapshot:0	Epoch:198	Loss:9.322	translation_Loss:9.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.43	Hits@10:37.73	Best:20.43
2024-12-28 01:12:18,055: Snapshot:0	Epoch:199	Loss:9.195	translation_Loss:9.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.47	Hits@10:37.73	Best:20.47
2024-12-28 01:12:18,398: => loading checkpoint './checkpoint/FACTfact_0.00001_512_1000/0model_best.tar'
2024-12-28 01:12:21,190: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1974 | 0.0993 | 0.2546 | 0.3044 |  0.3675 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 01:13:03,464: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228011228/FACTfact_0.00001_512_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_512_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_512_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 01:13:13,535: Snapshot:0	Epoch:0	Loss:104.79	translation_Loss:104.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 01:13:20,102: Snapshot:0	Epoch:1	Loss:103.761	translation_Loss:103.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 01:13:26,636: Snapshot:0	Epoch:2	Loss:102.703	translation_Loss:102.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.39	Best:1.39
2024-12-28 01:13:33,219: Snapshot:0	Epoch:3	Loss:101.68	translation_Loss:101.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.47	Best:1.42
2024-12-28 01:13:39,762: Snapshot:0	Epoch:4	Loss:100.655	translation_Loss:100.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.48	Hits@10:1.58	Best:1.48
2024-12-28 01:13:46,393: Snapshot:0	Epoch:5	Loss:99.608	translation_Loss:99.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.6	Hits@10:2.0	Best:1.6
2024-12-28 01:13:52,985: Snapshot:0	Epoch:6	Loss:98.592	translation_Loss:98.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.8	Hits@10:2.56	Best:1.8
2024-12-28 01:13:59,555: Snapshot:0	Epoch:7	Loss:97.569	translation_Loss:97.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.08	Hits@10:3.35	Best:2.08
2024-12-28 01:14:06,627: Snapshot:0	Epoch:8	Loss:96.556	translation_Loss:96.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.42	Hits@10:4.09	Best:2.42
2024-12-28 01:14:13,175: Snapshot:0	Epoch:9	Loss:95.574	translation_Loss:95.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.72	Hits@10:4.59	Best:2.72
2024-12-28 01:14:19,802: Snapshot:0	Epoch:10	Loss:94.553	translation_Loss:94.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.06	Best:2.98
2024-12-28 01:14:26,409: Snapshot:0	Epoch:11	Loss:93.569	translation_Loss:93.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.2	Hits@10:5.48	Best:3.2
2024-12-28 01:14:32,967: Snapshot:0	Epoch:12	Loss:92.595	translation_Loss:92.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.38	Hits@10:5.9	Best:3.38
2024-12-28 01:14:39,594: Snapshot:0	Epoch:13	Loss:91.639	translation_Loss:91.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.55	Hits@10:6.28	Best:3.55
2024-12-28 01:14:46,181: Snapshot:0	Epoch:14	Loss:90.71	translation_Loss:90.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.7	Hits@10:6.67	Best:3.7
2024-12-28 01:14:52,737: Snapshot:0	Epoch:15	Loss:89.806	translation_Loss:89.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.87	Hits@10:7.02	Best:3.87
2024-12-28 01:14:59,331: Snapshot:0	Epoch:16	Loss:88.895	translation_Loss:88.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.02	Hits@10:7.38	Best:4.02
2024-12-28 01:15:05,985: Snapshot:0	Epoch:17	Loss:87.984	translation_Loss:87.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.18	Hits@10:7.69	Best:4.18
2024-12-28 01:15:12,550: Snapshot:0	Epoch:18	Loss:87.067	translation_Loss:87.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.32	Hits@10:8.03	Best:4.32
2024-12-28 01:15:19,124: Snapshot:0	Epoch:19	Loss:86.195	translation_Loss:86.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.46	Hits@10:8.38	Best:4.46
2024-12-28 01:15:25,698: Snapshot:0	Epoch:20	Loss:85.315	translation_Loss:85.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.58	Hits@10:8.68	Best:4.58
2024-12-28 01:15:32,283: Snapshot:0	Epoch:21	Loss:84.452	translation_Loss:84.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.71	Hits@10:8.98	Best:4.71
2024-12-28 01:15:38,901: Snapshot:0	Epoch:22	Loss:83.641	translation_Loss:83.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.26	Best:4.83
2024-12-28 01:15:45,549: Snapshot:0	Epoch:23	Loss:82.76	translation_Loss:82.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.95	Hits@10:9.58	Best:4.95
2024-12-28 01:15:52,163: Snapshot:0	Epoch:24	Loss:81.949	translation_Loss:81.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.06	Hits@10:9.86	Best:5.06
2024-12-28 01:15:58,749: Snapshot:0	Epoch:25	Loss:81.161	translation_Loss:81.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.16	Hits@10:10.09	Best:5.16
2024-12-28 01:16:05,302: Snapshot:0	Epoch:26	Loss:80.331	translation_Loss:80.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.27	Hits@10:10.35	Best:5.27
2024-12-28 01:16:11,920: Snapshot:0	Epoch:27	Loss:79.511	translation_Loss:79.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.38	Hits@10:10.62	Best:5.38
2024-12-28 01:16:18,504: Snapshot:0	Epoch:28	Loss:78.715	translation_Loss:78.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:10.91	Best:5.49
2024-12-28 01:16:25,594: Snapshot:0	Epoch:29	Loss:77.907	translation_Loss:77.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.59	Hits@10:11.18	Best:5.59
2024-12-28 01:16:32,181: Snapshot:0	Epoch:30	Loss:77.157	translation_Loss:77.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.7	Hits@10:11.47	Best:5.7
2024-12-28 01:16:38,819: Snapshot:0	Epoch:31	Loss:76.376	translation_Loss:76.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.81	Hits@10:11.8	Best:5.81
2024-12-28 01:16:45,460: Snapshot:0	Epoch:32	Loss:75.646	translation_Loss:75.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.93	Hits@10:12.17	Best:5.93
2024-12-28 01:16:52,114: Snapshot:0	Epoch:33	Loss:74.81	translation_Loss:74.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.05	Hits@10:12.56	Best:6.05
2024-12-28 01:16:58,689: Snapshot:0	Epoch:34	Loss:74.074	translation_Loss:74.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.16	Hits@10:12.86	Best:6.16
2024-12-28 01:17:05,247: Snapshot:0	Epoch:35	Loss:73.322	translation_Loss:73.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.28	Hits@10:13.26	Best:6.28
2024-12-28 01:17:11,837: Snapshot:0	Epoch:36	Loss:72.556	translation_Loss:72.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.39	Hits@10:13.61	Best:6.39
2024-12-28 01:17:18,413: Snapshot:0	Epoch:37	Loss:71.861	translation_Loss:71.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.51	Hits@10:13.87	Best:6.51
2024-12-28 01:17:24,995: Snapshot:0	Epoch:38	Loss:71.111	translation_Loss:71.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.63	Hits@10:14.23	Best:6.63
2024-12-28 01:17:31,582: Snapshot:0	Epoch:39	Loss:70.375	translation_Loss:70.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.76	Hits@10:14.54	Best:6.76
2024-12-28 01:17:38,227: Snapshot:0	Epoch:40	Loss:69.71	translation_Loss:69.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.88	Hits@10:14.84	Best:6.88
2024-12-28 01:17:44,869: Snapshot:0	Epoch:41	Loss:68.945	translation_Loss:68.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.99	Hits@10:15.15	Best:6.99
2024-12-28 01:17:51,497: Snapshot:0	Epoch:42	Loss:68.253	translation_Loss:68.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.11	Hits@10:15.43	Best:7.11
2024-12-28 01:17:58,115: Snapshot:0	Epoch:43	Loss:67.532	translation_Loss:67.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.23	Hits@10:15.75	Best:7.23
2024-12-28 01:18:04,683: Snapshot:0	Epoch:44	Loss:66.851	translation_Loss:66.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.36	Hits@10:16.03	Best:7.36
2024-12-28 01:18:11,273: Snapshot:0	Epoch:45	Loss:66.17	translation_Loss:66.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.48	Hits@10:16.28	Best:7.48
2024-12-28 01:18:17,883: Snapshot:0	Epoch:46	Loss:65.437	translation_Loss:65.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.59	Hits@10:16.57	Best:7.59
2024-12-28 01:18:24,472: Snapshot:0	Epoch:47	Loss:64.763	translation_Loss:64.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.72	Hits@10:16.86	Best:7.72
2024-12-28 01:18:31,093: Snapshot:0	Epoch:48	Loss:64.121	translation_Loss:64.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.84	Hits@10:17.17	Best:7.84
2024-12-28 01:18:38,216: Snapshot:0	Epoch:49	Loss:63.439	translation_Loss:63.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.99	Hits@10:17.48	Best:7.99
2024-12-28 01:18:44,793: Snapshot:0	Epoch:50	Loss:62.772	translation_Loss:62.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:17.7	Best:8.1
2024-12-28 01:18:51,445: Snapshot:0	Epoch:51	Loss:62.09	translation_Loss:62.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.23	Hits@10:18.0	Best:8.23
2024-12-28 01:18:58,038: Snapshot:0	Epoch:52	Loss:61.502	translation_Loss:61.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.35	Hits@10:18.28	Best:8.35
2024-12-28 01:19:04,634: Snapshot:0	Epoch:53	Loss:60.811	translation_Loss:60.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.49	Hits@10:18.6	Best:8.49
2024-12-28 01:19:11,246: Snapshot:0	Epoch:54	Loss:60.142	translation_Loss:60.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.63	Hits@10:18.96	Best:8.63
2024-12-28 01:19:17,841: Snapshot:0	Epoch:55	Loss:59.514	translation_Loss:59.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.76	Hits@10:19.23	Best:8.76
2024-12-28 01:19:24,409: Snapshot:0	Epoch:56	Loss:58.915	translation_Loss:58.915	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:19.5	Best:8.89
2024-12-28 01:19:30,995: Snapshot:0	Epoch:57	Loss:58.265	translation_Loss:58.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.04	Hits@10:19.83	Best:9.04
2024-12-28 01:19:37,640: Snapshot:0	Epoch:58	Loss:57.652	translation_Loss:57.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.18	Hits@10:20.08	Best:9.18
2024-12-28 01:19:44,315: Snapshot:0	Epoch:59	Loss:57.039	translation_Loss:57.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.3	Hits@10:20.36	Best:9.3
2024-12-28 01:19:50,995: Snapshot:0	Epoch:60	Loss:56.374	translation_Loss:56.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.42	Hits@10:20.59	Best:9.42
2024-12-28 01:19:57,593: Snapshot:0	Epoch:61	Loss:55.824	translation_Loss:55.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.56	Hits@10:20.85	Best:9.56
2024-12-28 01:20:04,214: Snapshot:0	Epoch:62	Loss:55.188	translation_Loss:55.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:21.12	Best:9.68
2024-12-28 01:20:10,886: Snapshot:0	Epoch:63	Loss:54.6	translation_Loss:54.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.79	Hits@10:21.43	Best:9.79
2024-12-28 01:20:17,515: Snapshot:0	Epoch:64	Loss:53.968	translation_Loss:53.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.93	Hits@10:21.76	Best:9.93
2024-12-28 01:20:24,103: Snapshot:0	Epoch:65	Loss:53.411	translation_Loss:53.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.06	Hits@10:22.0	Best:10.06
2024-12-28 01:20:30,709: Snapshot:0	Epoch:66	Loss:52.843	translation_Loss:52.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.17	Hits@10:22.3	Best:10.17
2024-12-28 01:20:37,374: Snapshot:0	Epoch:67	Loss:52.292	translation_Loss:52.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.3	Hits@10:22.6	Best:10.3
2024-12-28 01:20:43,996: Snapshot:0	Epoch:68	Loss:51.697	translation_Loss:51.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.45	Hits@10:22.82	Best:10.45
2024-12-28 01:20:51,142: Snapshot:0	Epoch:69	Loss:51.028	translation_Loss:51.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.58	Hits@10:23.05	Best:10.58
2024-12-28 01:20:57,805: Snapshot:0	Epoch:70	Loss:50.5	translation_Loss:50.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.7	Hits@10:23.25	Best:10.7
2024-12-28 01:21:04,414: Snapshot:0	Epoch:71	Loss:49.969	translation_Loss:49.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:23.49	Best:10.83
2024-12-28 01:21:10,989: Snapshot:0	Epoch:72	Loss:49.379	translation_Loss:49.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:23.75	Best:10.96
2024-12-28 01:21:17,590: Snapshot:0	Epoch:73	Loss:48.845	translation_Loss:48.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.08	Hits@10:23.97	Best:11.08
2024-12-28 01:21:24,172: Snapshot:0	Epoch:74	Loss:48.309	translation_Loss:48.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.2	Hits@10:24.2	Best:11.2
2024-12-28 01:21:30,783: Snapshot:0	Epoch:75	Loss:47.822	translation_Loss:47.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.29	Hits@10:24.46	Best:11.29
2024-12-28 01:21:37,410: Snapshot:0	Epoch:76	Loss:47.241	translation_Loss:47.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.41	Hits@10:24.74	Best:11.41
2024-12-28 01:21:44,082: Snapshot:0	Epoch:77	Loss:46.726	translation_Loss:46.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.52	Hits@10:24.99	Best:11.52
2024-12-28 01:21:50,700: Snapshot:0	Epoch:78	Loss:46.128	translation_Loss:46.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.62	Hits@10:25.22	Best:11.62
2024-12-28 01:21:57,350: Snapshot:0	Epoch:79	Loss:45.681	translation_Loss:45.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.73	Hits@10:25.45	Best:11.73
2024-12-28 01:22:03,962: Snapshot:0	Epoch:80	Loss:45.146	translation_Loss:45.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.81	Hits@10:25.62	Best:11.81
2024-12-28 01:22:10,600: Snapshot:0	Epoch:81	Loss:44.617	translation_Loss:44.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.91	Hits@10:25.81	Best:11.91
2024-12-28 01:22:17,171: Snapshot:0	Epoch:82	Loss:44.111	translation_Loss:44.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.0	Hits@10:26.03	Best:12.0
2024-12-28 01:22:23,753: Snapshot:0	Epoch:83	Loss:43.576	translation_Loss:43.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.11	Hits@10:26.27	Best:12.11
2024-12-28 01:22:30,326: Snapshot:0	Epoch:84	Loss:43.08	translation_Loss:43.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.22	Hits@10:26.47	Best:12.22
2024-12-28 01:22:36,929: Snapshot:0	Epoch:85	Loss:42.654	translation_Loss:42.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.32	Hits@10:26.71	Best:12.32
2024-12-28 01:22:43,552: Snapshot:0	Epoch:86	Loss:42.116	translation_Loss:42.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:26.86	Best:12.4
2024-12-28 01:22:50,229: Snapshot:0	Epoch:87	Loss:41.663	translation_Loss:41.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.51	Hits@10:27.05	Best:12.51
2024-12-28 01:22:56,806: Snapshot:0	Epoch:88	Loss:41.132	translation_Loss:41.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.6	Hits@10:27.21	Best:12.6
2024-12-28 01:23:03,901: Snapshot:0	Epoch:89	Loss:40.679	translation_Loss:40.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.69	Hits@10:27.42	Best:12.69
2024-12-28 01:23:10,493: Snapshot:0	Epoch:90	Loss:40.213	translation_Loss:40.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.78	Hits@10:27.57	Best:12.78
2024-12-28 01:23:17,123: Snapshot:0	Epoch:91	Loss:39.715	translation_Loss:39.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.87	Hits@10:27.74	Best:12.87
2024-12-28 01:23:23,702: Snapshot:0	Epoch:92	Loss:39.278	translation_Loss:39.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.95	Hits@10:27.94	Best:12.95
2024-12-28 01:23:30,295: Snapshot:0	Epoch:93	Loss:38.762	translation_Loss:38.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.02	Hits@10:28.06	Best:13.02
2024-12-28 01:23:36,913: Snapshot:0	Epoch:94	Loss:38.379	translation_Loss:38.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.12	Hits@10:28.22	Best:13.12
2024-12-28 01:23:43,582: Snapshot:0	Epoch:95	Loss:37.872	translation_Loss:37.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.21	Hits@10:28.4	Best:13.21
2024-12-28 01:23:50,234: Snapshot:0	Epoch:96	Loss:37.486	translation_Loss:37.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.31	Hits@10:28.54	Best:13.31
2024-12-28 01:23:56,838: Snapshot:0	Epoch:97	Loss:36.963	translation_Loss:36.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.38	Hits@10:28.7	Best:13.38
2024-12-28 01:24:03,494: Snapshot:0	Epoch:98	Loss:36.561	translation_Loss:36.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.46	Hits@10:28.87	Best:13.46
2024-12-28 01:24:10,138: Snapshot:0	Epoch:99	Loss:36.161	translation_Loss:36.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.54	Hits@10:29.04	Best:13.54
2024-12-28 01:24:16,710: Snapshot:0	Epoch:100	Loss:35.648	translation_Loss:35.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.63	Hits@10:29.21	Best:13.63
2024-12-28 01:24:23,289: Snapshot:0	Epoch:101	Loss:35.27	translation_Loss:35.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.71	Hits@10:29.34	Best:13.71
2024-12-28 01:24:29,929: Snapshot:0	Epoch:102	Loss:34.842	translation_Loss:34.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.79	Hits@10:29.53	Best:13.79
2024-12-28 01:24:36,580: Snapshot:0	Epoch:103	Loss:34.466	translation_Loss:34.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.87	Hits@10:29.68	Best:13.87
2024-12-28 01:24:43,161: Snapshot:0	Epoch:104	Loss:33.989	translation_Loss:33.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.98	Hits@10:29.88	Best:13.98
2024-12-28 01:24:49,797: Snapshot:0	Epoch:105	Loss:33.586	translation_Loss:33.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.06	Hits@10:30.05	Best:14.06
2024-12-28 01:24:56,357: Snapshot:0	Epoch:106	Loss:33.174	translation_Loss:33.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.15	Hits@10:30.25	Best:14.15
2024-12-28 01:25:02,991: Snapshot:0	Epoch:107	Loss:32.866	translation_Loss:32.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.22	Hits@10:30.33	Best:14.22
2024-12-28 01:25:09,645: Snapshot:0	Epoch:108	Loss:32.414	translation_Loss:32.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.31	Hits@10:30.5	Best:14.31
2024-12-28 01:25:16,746: Snapshot:0	Epoch:109	Loss:32.026	translation_Loss:32.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.39	Hits@10:30.63	Best:14.39
2024-12-28 01:25:23,353: Snapshot:0	Epoch:110	Loss:31.607	translation_Loss:31.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.48	Hits@10:30.76	Best:14.48
2024-12-28 01:25:30,034: Snapshot:0	Epoch:111	Loss:31.296	translation_Loss:31.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.56	Hits@10:30.85	Best:14.56
2024-12-28 01:25:36,620: Snapshot:0	Epoch:112	Loss:30.814	translation_Loss:30.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.65	Hits@10:30.98	Best:14.65
2024-12-28 01:25:43,274: Snapshot:0	Epoch:113	Loss:30.478	translation_Loss:30.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.72	Hits@10:31.1	Best:14.72
2024-12-28 01:25:49,953: Snapshot:0	Epoch:114	Loss:30.117	translation_Loss:30.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.8	Hits@10:31.25	Best:14.8
2024-12-28 01:25:56,614: Snapshot:0	Epoch:115	Loss:29.694	translation_Loss:29.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.88	Hits@10:31.41	Best:14.88
2024-12-28 01:26:03,209: Snapshot:0	Epoch:116	Loss:29.432	translation_Loss:29.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.95	Hits@10:31.5	Best:14.95
2024-12-28 01:26:09,786: Snapshot:0	Epoch:117	Loss:28.995	translation_Loss:28.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.03	Hits@10:31.63	Best:15.03
2024-12-28 01:26:16,364: Snapshot:0	Epoch:118	Loss:28.674	translation_Loss:28.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.09	Hits@10:31.75	Best:15.09
2024-12-28 01:26:22,958: Snapshot:0	Epoch:119	Loss:28.336	translation_Loss:28.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.16	Hits@10:31.9	Best:15.16
2024-12-28 01:26:29,526: Snapshot:0	Epoch:120	Loss:27.946	translation_Loss:27.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.24	Hits@10:32.02	Best:15.24
2024-12-28 01:26:36,136: Snapshot:0	Epoch:121	Loss:27.593	translation_Loss:27.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.32	Hits@10:32.13	Best:15.32
2024-12-28 01:26:42,810: Snapshot:0	Epoch:122	Loss:27.173	translation_Loss:27.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.41	Hits@10:32.26	Best:15.41
2024-12-28 01:26:49,421: Snapshot:0	Epoch:123	Loss:26.943	translation_Loss:26.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.47	Hits@10:32.37	Best:15.47
2024-12-28 01:26:56,010: Snapshot:0	Epoch:124	Loss:26.577	translation_Loss:26.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.55	Hits@10:32.49	Best:15.55
2024-12-28 01:27:02,592: Snapshot:0	Epoch:125	Loss:26.189	translation_Loss:26.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.64	Hits@10:32.62	Best:15.64
2024-12-28 01:27:09,198: Snapshot:0	Epoch:126	Loss:25.85	translation_Loss:25.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.71	Hits@10:32.71	Best:15.71
2024-12-28 01:27:15,854: Snapshot:0	Epoch:127	Loss:25.588	translation_Loss:25.588	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.79	Hits@10:32.82	Best:15.79
2024-12-28 01:27:22,443: Snapshot:0	Epoch:128	Loss:25.237	translation_Loss:25.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.87	Hits@10:32.97	Best:15.87
2024-12-28 01:27:29,008: Snapshot:0	Epoch:129	Loss:24.904	translation_Loss:24.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.94	Hits@10:33.06	Best:15.94
2024-12-28 01:27:36,111: Snapshot:0	Epoch:130	Loss:24.618	translation_Loss:24.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.01	Hits@10:33.14	Best:16.01
2024-12-28 01:27:42,726: Snapshot:0	Epoch:131	Loss:24.323	translation_Loss:24.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.08	Hits@10:33.22	Best:16.08
2024-12-28 01:27:49,309: Snapshot:0	Epoch:132	Loss:23.926	translation_Loss:23.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.14	Hits@10:33.3	Best:16.14
2024-12-28 01:27:55,986: Snapshot:0	Epoch:133	Loss:23.689	translation_Loss:23.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.22	Hits@10:33.44	Best:16.22
2024-12-28 01:28:02,564: Snapshot:0	Epoch:134	Loss:23.328	translation_Loss:23.328	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.31	Hits@10:33.51	Best:16.31
2024-12-28 01:28:09,137: Snapshot:0	Epoch:135	Loss:23.05	translation_Loss:23.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.38	Hits@10:33.61	Best:16.38
2024-12-28 01:28:15,721: Snapshot:0	Epoch:136	Loss:22.742	translation_Loss:22.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.45	Hits@10:33.72	Best:16.45
2024-12-28 01:28:22,325: Snapshot:0	Epoch:137	Loss:22.464	translation_Loss:22.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.53	Hits@10:33.81	Best:16.53
2024-12-28 01:28:28,921: Snapshot:0	Epoch:138	Loss:22.185	translation_Loss:22.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.6	Hits@10:33.94	Best:16.6
2024-12-28 01:28:35,504: Snapshot:0	Epoch:139	Loss:21.888	translation_Loss:21.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:34.03	Best:16.66
2024-12-28 01:28:42,079: Snapshot:0	Epoch:140	Loss:21.617	translation_Loss:21.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.74	Hits@10:34.12	Best:16.74
2024-12-28 01:28:48,690: Snapshot:0	Epoch:141	Loss:21.389	translation_Loss:21.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.83	Hits@10:34.19	Best:16.83
2024-12-28 01:28:55,315: Snapshot:0	Epoch:142	Loss:21.043	translation_Loss:21.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.9	Hits@10:34.3	Best:16.9
2024-12-28 01:29:01,931: Snapshot:0	Epoch:143	Loss:20.743	translation_Loss:20.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.98	Hits@10:34.42	Best:16.98
2024-12-28 01:29:08,522: Snapshot:0	Epoch:144	Loss:20.497	translation_Loss:20.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.05	Hits@10:34.52	Best:17.05
2024-12-28 01:29:15,155: Snapshot:0	Epoch:145	Loss:20.227	translation_Loss:20.227	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.14	Hits@10:34.57	Best:17.14
2024-12-28 01:29:21,768: Snapshot:0	Epoch:146	Loss:20.007	translation_Loss:20.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.21	Hits@10:34.64	Best:17.21
2024-12-28 01:29:28,346: Snapshot:0	Epoch:147	Loss:19.7	translation_Loss:19.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.29	Hits@10:34.7	Best:17.29
2024-12-28 01:29:34,927: Snapshot:0	Epoch:148	Loss:19.388	translation_Loss:19.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.35	Hits@10:34.78	Best:17.35
2024-12-28 01:29:41,531: Snapshot:0	Epoch:149	Loss:19.121	translation_Loss:19.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.43	Hits@10:34.88	Best:17.43
2024-12-28 01:29:48,644: Snapshot:0	Epoch:150	Loss:18.845	translation_Loss:18.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.49	Hits@10:34.97	Best:17.49
2024-12-28 01:29:55,233: Snapshot:0	Epoch:151	Loss:18.614	translation_Loss:18.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.54	Hits@10:35.02	Best:17.54
2024-12-28 01:30:01,920: Snapshot:0	Epoch:152	Loss:18.38	translation_Loss:18.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.62	Hits@10:35.08	Best:17.62
2024-12-28 01:30:08,616: Snapshot:0	Epoch:153	Loss:18.148	translation_Loss:18.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.69	Hits@10:35.16	Best:17.69
2024-12-28 01:30:15,204: Snapshot:0	Epoch:154	Loss:17.872	translation_Loss:17.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.75	Hits@10:35.24	Best:17.75
2024-12-28 01:30:21,846: Snapshot:0	Epoch:155	Loss:17.665	translation_Loss:17.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.8	Hits@10:35.36	Best:17.8
2024-12-28 01:30:28,423: Snapshot:0	Epoch:156	Loss:17.371	translation_Loss:17.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.89	Hits@10:35.46	Best:17.89
2024-12-28 01:30:35,007: Snapshot:0	Epoch:157	Loss:17.196	translation_Loss:17.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.95	Hits@10:35.52	Best:17.95
2024-12-28 01:30:41,596: Snapshot:0	Epoch:158	Loss:16.887	translation_Loss:16.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.03	Hits@10:35.59	Best:18.03
2024-12-28 01:30:48,240: Snapshot:0	Epoch:159	Loss:16.699	translation_Loss:16.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.1	Hits@10:35.68	Best:18.1
2024-12-28 01:30:54,832: Snapshot:0	Epoch:160	Loss:16.41	translation_Loss:16.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.16	Hits@10:35.75	Best:18.16
2024-12-28 01:31:01,419: Snapshot:0	Epoch:161	Loss:16.203	translation_Loss:16.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.22	Hits@10:35.83	Best:18.22
2024-12-28 01:31:08,023: Snapshot:0	Epoch:162	Loss:15.996	translation_Loss:15.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.3	Hits@10:35.84	Best:18.3
2024-12-28 01:31:14,631: Snapshot:0	Epoch:163	Loss:15.785	translation_Loss:15.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.35	Hits@10:35.96	Best:18.35
2024-12-28 01:31:21,248: Snapshot:0	Epoch:164	Loss:15.511	translation_Loss:15.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.41	Hits@10:36.03	Best:18.41
2024-12-28 01:31:27,922: Snapshot:0	Epoch:165	Loss:15.348	translation_Loss:15.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.48	Hits@10:36.08	Best:18.48
2024-12-28 01:31:34,541: Snapshot:0	Epoch:166	Loss:15.13	translation_Loss:15.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.52	Hits@10:36.13	Best:18.52
2024-12-28 01:31:41,142: Snapshot:0	Epoch:167	Loss:14.831	translation_Loss:14.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.62	Hits@10:36.24	Best:18.62
2024-12-28 01:31:47,741: Snapshot:0	Epoch:168	Loss:14.771	translation_Loss:14.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.69	Hits@10:36.26	Best:18.69
2024-12-28 01:31:54,325: Snapshot:0	Epoch:169	Loss:14.424	translation_Loss:14.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.74	Hits@10:36.3	Best:18.74
2024-12-28 01:32:01,432: Snapshot:0	Epoch:170	Loss:14.278	translation_Loss:14.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.79	Hits@10:36.37	Best:18.79
2024-12-28 01:32:08,038: Snapshot:0	Epoch:171	Loss:14.041	translation_Loss:14.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.83	Hits@10:36.4	Best:18.83
2024-12-28 01:32:14,623: Snapshot:0	Epoch:172	Loss:13.936	translation_Loss:13.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.91	Hits@10:36.44	Best:18.91
2024-12-28 01:32:21,236: Snapshot:0	Epoch:173	Loss:13.644	translation_Loss:13.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.95	Hits@10:36.51	Best:18.95
2024-12-28 01:32:27,859: Snapshot:0	Epoch:174	Loss:13.494	translation_Loss:13.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.01	Hits@10:36.59	Best:19.01
2024-12-28 01:32:34,430: Snapshot:0	Epoch:175	Loss:13.255	translation_Loss:13.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.06	Hits@10:36.63	Best:19.06
2024-12-28 01:32:41,031: Snapshot:0	Epoch:176	Loss:13.117	translation_Loss:13.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.13	Hits@10:36.73	Best:19.13
2024-12-28 01:32:47,650: Snapshot:0	Epoch:177	Loss:12.887	translation_Loss:12.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.19	Hits@10:36.76	Best:19.19
2024-12-28 01:32:54,298: Snapshot:0	Epoch:178	Loss:12.734	translation_Loss:12.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.27	Hits@10:36.79	Best:19.27
2024-12-28 01:33:00,895: Snapshot:0	Epoch:179	Loss:12.54	translation_Loss:12.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.35	Hits@10:36.85	Best:19.35
2024-12-28 01:33:07,526: Snapshot:0	Epoch:180	Loss:12.313	translation_Loss:12.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.38	Hits@10:36.87	Best:19.38
2024-12-28 01:33:14,189: Snapshot:0	Epoch:181	Loss:12.127	translation_Loss:12.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.44	Hits@10:36.92	Best:19.44
2024-12-28 01:33:20,804: Snapshot:0	Epoch:182	Loss:11.966	translation_Loss:11.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.52	Hits@10:37.0	Best:19.52
2024-12-28 01:33:27,394: Snapshot:0	Epoch:183	Loss:11.801	translation_Loss:11.801	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.58	Hits@10:37.03	Best:19.58
2024-12-28 01:33:33,980: Snapshot:0	Epoch:184	Loss:11.629	translation_Loss:11.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.63	Hits@10:37.11	Best:19.63
2024-12-28 01:33:40,640: Snapshot:0	Epoch:185	Loss:11.463	translation_Loss:11.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.69	Hits@10:37.11	Best:19.69
2024-12-28 01:33:47,274: Snapshot:0	Epoch:186	Loss:11.253	translation_Loss:11.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:37.19	Best:19.77
2024-12-28 01:33:53,896: Snapshot:0	Epoch:187	Loss:11.085	translation_Loss:11.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.83	Hits@10:37.23	Best:19.83
2024-12-28 01:34:00,497: Snapshot:0	Epoch:188	Loss:10.881	translation_Loss:10.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.89	Hits@10:37.24	Best:19.89
2024-12-28 01:34:07,107: Snapshot:0	Epoch:189	Loss:10.772	translation_Loss:10.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.96	Hits@10:37.3	Best:19.96
2024-12-28 01:34:14,229: Snapshot:0	Epoch:190	Loss:10.596	translation_Loss:10.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.0	Hits@10:37.38	Best:20.0
2024-12-28 01:34:20,893: Snapshot:0	Epoch:191	Loss:10.435	translation_Loss:10.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.07	Hits@10:37.39	Best:20.07
2024-12-28 01:34:27,490: Snapshot:0	Epoch:192	Loss:10.254	translation_Loss:10.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.12	Hits@10:37.44	Best:20.12
2024-12-28 01:34:34,141: Snapshot:0	Epoch:193	Loss:10.135	translation_Loss:10.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.18	Hits@10:37.47	Best:20.18
2024-12-28 01:34:40,739: Snapshot:0	Epoch:194	Loss:9.974	translation_Loss:9.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.25	Hits@10:37.5	Best:20.25
2024-12-28 01:34:47,382: Snapshot:0	Epoch:195	Loss:9.816	translation_Loss:9.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.29	Hits@10:37.58	Best:20.29
2024-12-28 01:34:53,979: Snapshot:0	Epoch:196	Loss:9.685	translation_Loss:9.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.34	Hits@10:37.65	Best:20.34
2024-12-28 01:35:00,583: Snapshot:0	Epoch:197	Loss:9.544	translation_Loss:9.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.39	Hits@10:37.68	Best:20.39
2024-12-28 01:35:07,269: Snapshot:0	Epoch:198	Loss:9.322	translation_Loss:9.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.43	Hits@10:37.71	Best:20.43
2024-12-28 01:35:13,872: Snapshot:0	Epoch:199	Loss:9.196	translation_Loss:9.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.47	Hits@10:37.72	Best:20.47
2024-12-28 01:35:14,216: => loading checkpoint './checkpoint/FACTfact_0.00001_512_5000/0model_best.tar'
2024-12-28 01:35:17,012: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1974 | 0.0992 | 0.2546 | 0.3043 |  0.3673 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 01:35:59,589: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='512', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228013524/FACTfact_0.00001_512_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_512_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_512_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 01:36:09,791: Snapshot:0	Epoch:0	Loss:104.79	translation_Loss:104.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 01:36:16,499: Snapshot:0	Epoch:1	Loss:103.761	translation_Loss:103.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 01:36:23,126: Snapshot:0	Epoch:2	Loss:102.703	translation_Loss:102.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.39	Best:1.39
2024-12-28 01:36:29,754: Snapshot:0	Epoch:3	Loss:101.68	translation_Loss:101.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.47	Best:1.42
2024-12-28 01:36:36,393: Snapshot:0	Epoch:4	Loss:100.655	translation_Loss:100.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.48	Hits@10:1.58	Best:1.48
2024-12-28 01:36:43,019: Snapshot:0	Epoch:5	Loss:99.608	translation_Loss:99.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.6	Hits@10:2.0	Best:1.6
2024-12-28 01:36:49,665: Snapshot:0	Epoch:6	Loss:98.592	translation_Loss:98.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.8	Hits@10:2.56	Best:1.8
2024-12-28 01:36:56,331: Snapshot:0	Epoch:7	Loss:97.569	translation_Loss:97.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.08	Hits@10:3.35	Best:2.08
2024-12-28 01:37:03,554: Snapshot:0	Epoch:8	Loss:96.556	translation_Loss:96.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.42	Hits@10:4.09	Best:2.42
2024-12-28 01:37:10,202: Snapshot:0	Epoch:9	Loss:95.574	translation_Loss:95.574	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.72	Hits@10:4.59	Best:2.72
2024-12-28 01:37:16,860: Snapshot:0	Epoch:10	Loss:94.553	translation_Loss:94.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.06	Best:2.98
2024-12-28 01:37:23,528: Snapshot:0	Epoch:11	Loss:93.569	translation_Loss:93.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.2	Hits@10:5.48	Best:3.2
2024-12-28 01:37:30,185: Snapshot:0	Epoch:12	Loss:92.595	translation_Loss:92.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.38	Hits@10:5.9	Best:3.38
2024-12-28 01:37:36,876: Snapshot:0	Epoch:13	Loss:91.639	translation_Loss:91.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.55	Hits@10:6.28	Best:3.55
2024-12-28 01:37:43,648: Snapshot:0	Epoch:14	Loss:90.71	translation_Loss:90.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.7	Hits@10:6.67	Best:3.7
2024-12-28 01:37:50,401: Snapshot:0	Epoch:15	Loss:89.806	translation_Loss:89.806	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.87	Hits@10:7.02	Best:3.87
2024-12-28 01:37:57,061: Snapshot:0	Epoch:16	Loss:88.895	translation_Loss:88.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.02	Hits@10:7.38	Best:4.02
2024-12-28 01:38:03,703: Snapshot:0	Epoch:17	Loss:87.984	translation_Loss:87.984	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.18	Hits@10:7.69	Best:4.18
2024-12-28 01:38:10,426: Snapshot:0	Epoch:18	Loss:87.067	translation_Loss:87.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.32	Hits@10:8.03	Best:4.32
2024-12-28 01:38:17,091: Snapshot:0	Epoch:19	Loss:86.195	translation_Loss:86.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.46	Hits@10:8.38	Best:4.46
2024-12-28 01:38:23,807: Snapshot:0	Epoch:20	Loss:85.315	translation_Loss:85.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.58	Hits@10:8.68	Best:4.58
2024-12-28 01:38:30,503: Snapshot:0	Epoch:21	Loss:84.452	translation_Loss:84.452	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.71	Hits@10:8.98	Best:4.71
2024-12-28 01:38:37,215: Snapshot:0	Epoch:22	Loss:83.641	translation_Loss:83.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.26	Best:4.83
2024-12-28 01:38:43,891: Snapshot:0	Epoch:23	Loss:82.76	translation_Loss:82.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.95	Hits@10:9.58	Best:4.95
2024-12-28 01:38:50,570: Snapshot:0	Epoch:24	Loss:81.949	translation_Loss:81.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.06	Hits@10:9.86	Best:5.06
2024-12-28 01:38:57,292: Snapshot:0	Epoch:25	Loss:81.161	translation_Loss:81.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.16	Hits@10:10.09	Best:5.16
2024-12-28 01:39:03,981: Snapshot:0	Epoch:26	Loss:80.331	translation_Loss:80.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.27	Hits@10:10.35	Best:5.27
2024-12-28 01:39:10,669: Snapshot:0	Epoch:27	Loss:79.511	translation_Loss:79.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.38	Hits@10:10.62	Best:5.38
2024-12-28 01:39:17,423: Snapshot:0	Epoch:28	Loss:78.715	translation_Loss:78.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.49	Hits@10:10.91	Best:5.49
2024-12-28 01:39:24,678: Snapshot:0	Epoch:29	Loss:77.907	translation_Loss:77.907	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:11.18	Best:5.6
2024-12-28 01:39:31,393: Snapshot:0	Epoch:30	Loss:77.157	translation_Loss:77.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.7	Hits@10:11.47	Best:5.7
2024-12-28 01:39:38,105: Snapshot:0	Epoch:31	Loss:76.376	translation_Loss:76.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.81	Hits@10:11.8	Best:5.81
2024-12-28 01:39:44,872: Snapshot:0	Epoch:32	Loss:75.646	translation_Loss:75.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.93	Hits@10:12.17	Best:5.93
2024-12-28 01:39:51,549: Snapshot:0	Epoch:33	Loss:74.81	translation_Loss:74.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.05	Hits@10:12.56	Best:6.05
2024-12-28 01:39:58,223: Snapshot:0	Epoch:34	Loss:74.074	translation_Loss:74.074	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.16	Hits@10:12.86	Best:6.16
2024-12-28 01:40:05,002: Snapshot:0	Epoch:35	Loss:73.322	translation_Loss:73.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.28	Hits@10:13.26	Best:6.28
2024-12-28 01:40:11,740: Snapshot:0	Epoch:36	Loss:72.556	translation_Loss:72.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.39	Hits@10:13.61	Best:6.39
2024-12-28 01:40:18,421: Snapshot:0	Epoch:37	Loss:71.861	translation_Loss:71.861	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.51	Hits@10:13.88	Best:6.51
2024-12-28 01:40:25,100: Snapshot:0	Epoch:38	Loss:71.111	translation_Loss:71.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.63	Hits@10:14.23	Best:6.63
2024-12-28 01:40:31,793: Snapshot:0	Epoch:39	Loss:70.375	translation_Loss:70.375	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.76	Hits@10:14.54	Best:6.76
2024-12-28 01:40:38,499: Snapshot:0	Epoch:40	Loss:69.71	translation_Loss:69.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.88	Hits@10:14.85	Best:6.88
2024-12-28 01:40:45,196: Snapshot:0	Epoch:41	Loss:68.945	translation_Loss:68.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.99	Hits@10:15.15	Best:6.99
2024-12-28 01:40:51,949: Snapshot:0	Epoch:42	Loss:68.253	translation_Loss:68.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.11	Hits@10:15.44	Best:7.11
2024-12-28 01:40:58,600: Snapshot:0	Epoch:43	Loss:67.532	translation_Loss:67.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.23	Hits@10:15.76	Best:7.23
2024-12-28 01:41:05,262: Snapshot:0	Epoch:44	Loss:66.851	translation_Loss:66.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.36	Hits@10:16.04	Best:7.36
2024-12-28 01:41:11,933: Snapshot:0	Epoch:45	Loss:66.17	translation_Loss:66.17	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.48	Hits@10:16.29	Best:7.48
2024-12-28 01:41:18,622: Snapshot:0	Epoch:46	Loss:65.437	translation_Loss:65.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.58	Hits@10:16.57	Best:7.58
2024-12-28 01:41:25,297: Snapshot:0	Epoch:47	Loss:64.763	translation_Loss:64.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.72	Hits@10:16.87	Best:7.72
2024-12-28 01:41:32,034: Snapshot:0	Epoch:48	Loss:64.121	translation_Loss:64.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.85	Hits@10:17.17	Best:7.85
2024-12-28 01:41:39,252: Snapshot:0	Epoch:49	Loss:63.439	translation_Loss:63.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.99	Hits@10:17.48	Best:7.99
2024-12-28 01:41:46,045: Snapshot:0	Epoch:50	Loss:62.772	translation_Loss:62.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:17.71	Best:8.1
2024-12-28 01:41:52,753: Snapshot:0	Epoch:51	Loss:62.09	translation_Loss:62.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.23	Hits@10:18.0	Best:8.23
2024-12-28 01:41:59,460: Snapshot:0	Epoch:52	Loss:61.502	translation_Loss:61.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.36	Hits@10:18.29	Best:8.36
2024-12-28 01:42:06,127: Snapshot:0	Epoch:53	Loss:60.811	translation_Loss:60.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.49	Hits@10:18.61	Best:8.49
2024-12-28 01:42:12,836: Snapshot:0	Epoch:54	Loss:60.142	translation_Loss:60.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.63	Hits@10:18.96	Best:8.63
2024-12-28 01:42:19,520: Snapshot:0	Epoch:55	Loss:59.514	translation_Loss:59.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.76	Hits@10:19.24	Best:8.76
2024-12-28 01:42:26,197: Snapshot:0	Epoch:56	Loss:58.915	translation_Loss:58.915	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:19.5	Best:8.89
2024-12-28 01:42:32,921: Snapshot:0	Epoch:57	Loss:58.265	translation_Loss:58.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.03	Hits@10:19.83	Best:9.03
2024-12-28 01:42:39,671: Snapshot:0	Epoch:58	Loss:57.652	translation_Loss:57.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.18	Hits@10:20.08	Best:9.18
2024-12-28 01:42:46,338: Snapshot:0	Epoch:59	Loss:57.039	translation_Loss:57.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.3	Hits@10:20.37	Best:9.3
2024-12-28 01:42:53,062: Snapshot:0	Epoch:60	Loss:56.374	translation_Loss:56.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.42	Hits@10:20.58	Best:9.42
2024-12-28 01:42:59,756: Snapshot:0	Epoch:61	Loss:55.824	translation_Loss:55.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.56	Hits@10:20.85	Best:9.56
2024-12-28 01:43:06,416: Snapshot:0	Epoch:62	Loss:55.188	translation_Loss:55.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:21.12	Best:9.68
2024-12-28 01:43:13,096: Snapshot:0	Epoch:63	Loss:54.6	translation_Loss:54.6	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.79	Hits@10:21.41	Best:9.79
2024-12-28 01:43:19,818: Snapshot:0	Epoch:64	Loss:53.968	translation_Loss:53.968	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.93	Hits@10:21.75	Best:9.93
2024-12-28 01:43:26,474: Snapshot:0	Epoch:65	Loss:53.411	translation_Loss:53.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.06	Hits@10:22.0	Best:10.06
2024-12-28 01:43:33,194: Snapshot:0	Epoch:66	Loss:52.843	translation_Loss:52.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.17	Hits@10:22.3	Best:10.17
2024-12-28 01:43:39,950: Snapshot:0	Epoch:67	Loss:52.292	translation_Loss:52.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.31	Hits@10:22.6	Best:10.31
2024-12-28 01:43:46,694: Snapshot:0	Epoch:68	Loss:51.697	translation_Loss:51.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.45	Hits@10:22.82	Best:10.45
2024-12-28 01:43:53,904: Snapshot:0	Epoch:69	Loss:51.028	translation_Loss:51.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.57	Hits@10:23.05	Best:10.57
2024-12-28 01:44:00,567: Snapshot:0	Epoch:70	Loss:50.5	translation_Loss:50.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.7	Hits@10:23.24	Best:10.7
2024-12-28 01:44:07,257: Snapshot:0	Epoch:71	Loss:49.969	translation_Loss:49.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.83	Hits@10:23.49	Best:10.83
2024-12-28 01:44:13,960: Snapshot:0	Epoch:72	Loss:49.379	translation_Loss:49.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:23.75	Best:10.96
2024-12-28 01:44:20,664: Snapshot:0	Epoch:73	Loss:48.844	translation_Loss:48.844	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.08	Hits@10:23.97	Best:11.08
2024-12-28 01:44:27,340: Snapshot:0	Epoch:74	Loss:48.309	translation_Loss:48.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.2	Hits@10:24.18	Best:11.2
2024-12-28 01:44:34,013: Snapshot:0	Epoch:75	Loss:47.822	translation_Loss:47.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.29	Hits@10:24.46	Best:11.29
2024-12-28 01:44:40,708: Snapshot:0	Epoch:76	Loss:47.241	translation_Loss:47.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.41	Hits@10:24.72	Best:11.41
2024-12-28 01:44:47,438: Snapshot:0	Epoch:77	Loss:46.726	translation_Loss:46.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.52	Hits@10:24.99	Best:11.52
2024-12-28 01:44:54,177: Snapshot:0	Epoch:78	Loss:46.128	translation_Loss:46.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.62	Hits@10:25.23	Best:11.62
2024-12-28 01:45:00,859: Snapshot:0	Epoch:79	Loss:45.681	translation_Loss:45.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.72	Hits@10:25.44	Best:11.72
2024-12-28 01:45:07,628: Snapshot:0	Epoch:80	Loss:45.146	translation_Loss:45.146	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.82	Hits@10:25.62	Best:11.82
2024-12-28 01:45:14,333: Snapshot:0	Epoch:81	Loss:44.616	translation_Loss:44.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.91	Hits@10:25.81	Best:11.91
2024-12-28 01:45:21,021: Snapshot:0	Epoch:82	Loss:44.111	translation_Loss:44.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.0	Hits@10:26.02	Best:12.0
2024-12-28 01:45:27,686: Snapshot:0	Epoch:83	Loss:43.576	translation_Loss:43.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.11	Hits@10:26.25	Best:12.11
2024-12-28 01:45:34,410: Snapshot:0	Epoch:84	Loss:43.08	translation_Loss:43.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.22	Hits@10:26.48	Best:12.22
2024-12-28 01:45:41,129: Snapshot:0	Epoch:85	Loss:42.654	translation_Loss:42.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:26.7	Best:12.31
2024-12-28 01:45:47,832: Snapshot:0	Epoch:86	Loss:42.116	translation_Loss:42.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:26.86	Best:12.4
2024-12-28 01:45:54,545: Snapshot:0	Epoch:87	Loss:41.663	translation_Loss:41.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.5	Hits@10:27.04	Best:12.5
2024-12-28 01:46:01,226: Snapshot:0	Epoch:88	Loss:41.131	translation_Loss:41.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.6	Hits@10:27.19	Best:12.6
2024-12-28 01:46:08,431: Snapshot:0	Epoch:89	Loss:40.679	translation_Loss:40.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.69	Hits@10:27.4	Best:12.69
2024-12-28 01:46:15,100: Snapshot:0	Epoch:90	Loss:40.213	translation_Loss:40.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.78	Hits@10:27.59	Best:12.78
2024-12-28 01:46:21,759: Snapshot:0	Epoch:91	Loss:39.715	translation_Loss:39.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.87	Hits@10:27.75	Best:12.87
2024-12-28 01:46:28,472: Snapshot:0	Epoch:92	Loss:39.278	translation_Loss:39.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.96	Hits@10:27.93	Best:12.96
2024-12-28 01:46:35,182: Snapshot:0	Epoch:93	Loss:38.763	translation_Loss:38.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.02	Hits@10:28.07	Best:13.02
2024-12-28 01:46:41,870: Snapshot:0	Epoch:94	Loss:38.379	translation_Loss:38.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.12	Hits@10:28.23	Best:13.12
2024-12-28 01:46:48,570: Snapshot:0	Epoch:95	Loss:37.872	translation_Loss:37.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.22	Hits@10:28.39	Best:13.22
2024-12-28 01:46:55,241: Snapshot:0	Epoch:96	Loss:37.486	translation_Loss:37.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.32	Hits@10:28.52	Best:13.32
2024-12-28 01:47:01,911: Snapshot:0	Epoch:97	Loss:36.962	translation_Loss:36.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.38	Hits@10:28.67	Best:13.38
2024-12-28 01:47:08,611: Snapshot:0	Epoch:98	Loss:36.562	translation_Loss:36.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.46	Hits@10:28.86	Best:13.46
2024-12-28 01:47:15,329: Snapshot:0	Epoch:99	Loss:36.161	translation_Loss:36.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.55	Hits@10:29.05	Best:13.55
2024-12-28 01:47:22,017: Snapshot:0	Epoch:100	Loss:35.649	translation_Loss:35.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.63	Hits@10:29.19	Best:13.63
2024-12-28 01:47:28,660: Snapshot:0	Epoch:101	Loss:35.27	translation_Loss:35.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.72	Hits@10:29.35	Best:13.72
2024-12-28 01:47:35,383: Snapshot:0	Epoch:102	Loss:34.842	translation_Loss:34.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.79	Hits@10:29.5	Best:13.79
2024-12-28 01:47:42,151: Snapshot:0	Epoch:103	Loss:34.466	translation_Loss:34.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.87	Hits@10:29.69	Best:13.87
2024-12-28 01:47:48,874: Snapshot:0	Epoch:104	Loss:33.99	translation_Loss:33.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.98	Hits@10:29.87	Best:13.98
2024-12-28 01:47:55,631: Snapshot:0	Epoch:105	Loss:33.587	translation_Loss:33.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.06	Hits@10:30.05	Best:14.06
2024-12-28 01:48:02,359: Snapshot:0	Epoch:106	Loss:33.174	translation_Loss:33.174	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.15	Hits@10:30.24	Best:14.15
2024-12-28 01:48:09,131: Snapshot:0	Epoch:107	Loss:32.866	translation_Loss:32.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.23	Hits@10:30.36	Best:14.23
2024-12-28 01:48:15,819: Snapshot:0	Epoch:108	Loss:32.414	translation_Loss:32.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.31	Hits@10:30.51	Best:14.31
2024-12-28 01:48:23,050: Snapshot:0	Epoch:109	Loss:32.026	translation_Loss:32.026	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.39	Hits@10:30.65	Best:14.39
2024-12-28 01:48:29,733: Snapshot:0	Epoch:110	Loss:31.608	translation_Loss:31.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.47	Hits@10:30.76	Best:14.47
2024-12-28 01:48:36,412: Snapshot:0	Epoch:111	Loss:31.296	translation_Loss:31.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.56	Hits@10:30.86	Best:14.56
2024-12-28 01:48:43,081: Snapshot:0	Epoch:112	Loss:30.815	translation_Loss:30.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.64	Hits@10:30.98	Best:14.64
2024-12-28 01:48:49,765: Snapshot:0	Epoch:113	Loss:30.478	translation_Loss:30.478	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.72	Hits@10:31.11	Best:14.72
2024-12-28 01:48:56,484: Snapshot:0	Epoch:114	Loss:30.117	translation_Loss:30.117	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.8	Hits@10:31.25	Best:14.8
2024-12-28 01:49:03,189: Snapshot:0	Epoch:115	Loss:29.694	translation_Loss:29.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.88	Hits@10:31.41	Best:14.88
2024-12-28 01:49:09,882: Snapshot:0	Epoch:116	Loss:29.432	translation_Loss:29.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.95	Hits@10:31.51	Best:14.95
2024-12-28 01:49:16,573: Snapshot:0	Epoch:117	Loss:28.995	translation_Loss:28.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.03	Hits@10:31.64	Best:15.03
2024-12-28 01:49:23,248: Snapshot:0	Epoch:118	Loss:28.674	translation_Loss:28.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.08	Hits@10:31.75	Best:15.08
2024-12-28 01:49:29,986: Snapshot:0	Epoch:119	Loss:28.336	translation_Loss:28.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.16	Hits@10:31.89	Best:15.16
2024-12-28 01:49:36,705: Snapshot:0	Epoch:120	Loss:27.946	translation_Loss:27.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.24	Hits@10:32.04	Best:15.24
2024-12-28 01:49:43,405: Snapshot:0	Epoch:121	Loss:27.593	translation_Loss:27.593	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.32	Hits@10:32.14	Best:15.32
2024-12-28 01:49:50,143: Snapshot:0	Epoch:122	Loss:27.173	translation_Loss:27.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.4	Hits@10:32.23	Best:15.4
2024-12-28 01:49:56,931: Snapshot:0	Epoch:123	Loss:26.943	translation_Loss:26.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.48	Hits@10:32.36	Best:15.48
2024-12-28 01:50:03,699: Snapshot:0	Epoch:124	Loss:26.577	translation_Loss:26.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.56	Hits@10:32.51	Best:15.56
2024-12-28 01:50:10,478: Snapshot:0	Epoch:125	Loss:26.189	translation_Loss:26.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.64	Hits@10:32.63	Best:15.64
2024-12-28 01:50:17,156: Snapshot:0	Epoch:126	Loss:25.85	translation_Loss:25.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.71	Hits@10:32.71	Best:15.71
2024-12-28 01:50:23,903: Snapshot:0	Epoch:127	Loss:25.587	translation_Loss:25.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.79	Hits@10:32.81	Best:15.79
2024-12-28 01:50:30,629: Snapshot:0	Epoch:128	Loss:25.237	translation_Loss:25.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.87	Hits@10:32.95	Best:15.87
2024-12-28 01:50:37,317: Snapshot:0	Epoch:129	Loss:24.904	translation_Loss:24.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.94	Hits@10:33.04	Best:15.94
2024-12-28 01:50:44,542: Snapshot:0	Epoch:130	Loss:24.618	translation_Loss:24.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.01	Hits@10:33.15	Best:16.01
2024-12-28 01:50:51,222: Snapshot:0	Epoch:131	Loss:24.323	translation_Loss:24.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.08	Hits@10:33.22	Best:16.08
2024-12-28 01:50:57,912: Snapshot:0	Epoch:132	Loss:23.927	translation_Loss:23.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.14	Hits@10:33.29	Best:16.14
2024-12-28 01:51:04,623: Snapshot:0	Epoch:133	Loss:23.689	translation_Loss:23.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.22	Hits@10:33.41	Best:16.22
2024-12-28 01:51:11,345: Snapshot:0	Epoch:134	Loss:23.327	translation_Loss:23.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.3	Hits@10:33.52	Best:16.3
2024-12-28 01:51:18,090: Snapshot:0	Epoch:135	Loss:23.05	translation_Loss:23.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.38	Hits@10:33.61	Best:16.38
2024-12-28 01:51:24,846: Snapshot:0	Epoch:136	Loss:22.742	translation_Loss:22.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.44	Hits@10:33.71	Best:16.44
2024-12-28 01:51:31,542: Snapshot:0	Epoch:137	Loss:22.464	translation_Loss:22.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.52	Hits@10:33.82	Best:16.52
2024-12-28 01:51:38,288: Snapshot:0	Epoch:138	Loss:22.185	translation_Loss:22.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.59	Hits@10:33.95	Best:16.59
2024-12-28 01:51:45,004: Snapshot:0	Epoch:139	Loss:21.888	translation_Loss:21.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:34.03	Best:16.66
2024-12-28 01:51:51,733: Snapshot:0	Epoch:140	Loss:21.617	translation_Loss:21.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.75	Hits@10:34.13	Best:16.75
2024-12-28 01:51:58,437: Snapshot:0	Epoch:141	Loss:21.389	translation_Loss:21.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.83	Hits@10:34.22	Best:16.83
2024-12-28 01:52:05,211: Snapshot:0	Epoch:142	Loss:21.043	translation_Loss:21.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.9	Hits@10:34.3	Best:16.9
2024-12-28 01:52:11,878: Snapshot:0	Epoch:143	Loss:20.743	translation_Loss:20.743	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.98	Hits@10:34.4	Best:16.98
2024-12-28 01:52:18,607: Snapshot:0	Epoch:144	Loss:20.497	translation_Loss:20.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.05	Hits@10:34.52	Best:17.05
2024-12-28 01:52:25,273: Snapshot:0	Epoch:145	Loss:20.226	translation_Loss:20.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.14	Hits@10:34.59	Best:17.14
2024-12-28 01:52:31,990: Snapshot:0	Epoch:146	Loss:20.007	translation_Loss:20.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.22	Hits@10:34.63	Best:17.22
2024-12-28 01:52:38,660: Snapshot:0	Epoch:147	Loss:19.7	translation_Loss:19.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.29	Hits@10:34.72	Best:17.29
2024-12-28 01:52:45,328: Snapshot:0	Epoch:148	Loss:19.388	translation_Loss:19.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.36	Hits@10:34.79	Best:17.36
2024-12-28 01:52:52,061: Snapshot:0	Epoch:149	Loss:19.121	translation_Loss:19.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.42	Hits@10:34.87	Best:17.42
2024-12-28 01:52:59,225: Snapshot:0	Epoch:150	Loss:18.845	translation_Loss:18.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.48	Hits@10:34.94	Best:17.48
2024-12-28 01:53:05,985: Snapshot:0	Epoch:151	Loss:18.614	translation_Loss:18.614	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.54	Hits@10:35.02	Best:17.54
2024-12-28 01:53:12,669: Snapshot:0	Epoch:152	Loss:18.38	translation_Loss:18.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.61	Hits@10:35.09	Best:17.61
2024-12-28 01:53:19,372: Snapshot:0	Epoch:153	Loss:18.148	translation_Loss:18.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.69	Hits@10:35.16	Best:17.69
2024-12-28 01:53:26,040: Snapshot:0	Epoch:154	Loss:17.872	translation_Loss:17.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.75	Hits@10:35.26	Best:17.75
2024-12-28 01:53:32,766: Snapshot:0	Epoch:155	Loss:17.665	translation_Loss:17.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.81	Hits@10:35.37	Best:17.81
2024-12-28 01:53:39,488: Snapshot:0	Epoch:156	Loss:17.371	translation_Loss:17.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.89	Hits@10:35.45	Best:17.89
2024-12-28 01:53:46,206: Snapshot:0	Epoch:157	Loss:17.196	translation_Loss:17.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:17.96	Hits@10:35.55	Best:17.96
2024-12-28 01:53:52,901: Snapshot:0	Epoch:158	Loss:16.887	translation_Loss:16.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.03	Hits@10:35.6	Best:18.03
2024-12-28 01:53:59,587: Snapshot:0	Epoch:159	Loss:16.699	translation_Loss:16.699	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.1	Hits@10:35.66	Best:18.1
2024-12-28 01:54:06,311: Snapshot:0	Epoch:160	Loss:16.41	translation_Loss:16.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.16	Hits@10:35.74	Best:18.16
2024-12-28 01:54:13,031: Snapshot:0	Epoch:161	Loss:16.204	translation_Loss:16.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.23	Hits@10:35.82	Best:18.23
2024-12-28 01:54:19,775: Snapshot:0	Epoch:162	Loss:15.996	translation_Loss:15.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.3	Hits@10:35.88	Best:18.3
2024-12-28 01:54:26,473: Snapshot:0	Epoch:163	Loss:15.785	translation_Loss:15.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.36	Hits@10:35.97	Best:18.36
2024-12-28 01:54:33,209: Snapshot:0	Epoch:164	Loss:15.511	translation_Loss:15.511	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.42	Hits@10:36.01	Best:18.42
2024-12-28 01:54:39,922: Snapshot:0	Epoch:165	Loss:15.348	translation_Loss:15.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.48	Hits@10:36.11	Best:18.48
2024-12-28 01:54:46,616: Snapshot:0	Epoch:166	Loss:15.13	translation_Loss:15.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.53	Hits@10:36.13	Best:18.53
2024-12-28 01:54:53,331: Snapshot:0	Epoch:167	Loss:14.831	translation_Loss:14.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.61	Hits@10:36.23	Best:18.61
2024-12-28 01:55:00,009: Snapshot:0	Epoch:168	Loss:14.771	translation_Loss:14.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.67	Hits@10:36.26	Best:18.67
2024-12-28 01:55:06,812: Snapshot:0	Epoch:169	Loss:14.424	translation_Loss:14.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.73	Hits@10:36.29	Best:18.73
2024-12-28 01:55:14,001: Snapshot:0	Epoch:170	Loss:14.279	translation_Loss:14.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.79	Hits@10:36.36	Best:18.79
2024-12-28 01:55:20,711: Snapshot:0	Epoch:171	Loss:14.041	translation_Loss:14.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.84	Hits@10:36.39	Best:18.84
2024-12-28 01:55:27,402: Snapshot:0	Epoch:172	Loss:13.936	translation_Loss:13.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.9	Hits@10:36.46	Best:18.9
2024-12-28 01:55:34,099: Snapshot:0	Epoch:173	Loss:13.644	translation_Loss:13.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.95	Hits@10:36.5	Best:18.95
2024-12-28 01:55:40,839: Snapshot:0	Epoch:174	Loss:13.493	translation_Loss:13.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.03	Hits@10:36.57	Best:19.03
2024-12-28 01:55:47,545: Snapshot:0	Epoch:175	Loss:13.256	translation_Loss:13.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.07	Hits@10:36.63	Best:19.07
2024-12-28 01:55:54,295: Snapshot:0	Epoch:176	Loss:13.116	translation_Loss:13.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.12	Hits@10:36.71	Best:19.12
2024-12-28 01:56:00,969: Snapshot:0	Epoch:177	Loss:12.887	translation_Loss:12.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.2	Hits@10:36.76	Best:19.2
2024-12-28 01:56:07,648: Snapshot:0	Epoch:178	Loss:12.734	translation_Loss:12.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.27	Hits@10:36.76	Best:19.27
2024-12-28 01:56:14,363: Snapshot:0	Epoch:179	Loss:12.54	translation_Loss:12.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.35	Hits@10:36.84	Best:19.35
2024-12-28 01:56:21,095: Snapshot:0	Epoch:180	Loss:12.313	translation_Loss:12.313	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.38	Hits@10:36.89	Best:19.38
2024-12-28 01:56:27,778: Snapshot:0	Epoch:181	Loss:12.127	translation_Loss:12.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.45	Hits@10:36.94	Best:19.45
2024-12-28 01:56:34,533: Snapshot:0	Epoch:182	Loss:11.966	translation_Loss:11.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.52	Hits@10:37.01	Best:19.52
2024-12-28 01:56:41,230: Snapshot:0	Epoch:183	Loss:11.801	translation_Loss:11.801	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.58	Hits@10:37.03	Best:19.58
2024-12-28 01:56:47,906: Snapshot:0	Epoch:184	Loss:11.63	translation_Loss:11.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.64	Hits@10:37.09	Best:19.64
2024-12-28 01:56:54,598: Snapshot:0	Epoch:185	Loss:11.463	translation_Loss:11.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.7	Hits@10:37.11	Best:19.7
2024-12-28 01:57:01,288: Snapshot:0	Epoch:186	Loss:11.252	translation_Loss:11.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.76	Hits@10:37.18	Best:19.76
2024-12-28 01:57:08,058: Snapshot:0	Epoch:187	Loss:11.085	translation_Loss:11.085	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.82	Hits@10:37.21	Best:19.82
2024-12-28 01:57:14,767: Snapshot:0	Epoch:188	Loss:10.88	translation_Loss:10.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.88	Hits@10:37.25	Best:19.88
2024-12-28 01:57:21,479: Snapshot:0	Epoch:189	Loss:10.772	translation_Loss:10.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.95	Hits@10:37.31	Best:19.95
2024-12-28 01:57:28,704: Snapshot:0	Epoch:190	Loss:10.596	translation_Loss:10.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.01	Hits@10:37.36	Best:20.01
2024-12-28 01:57:35,416: Snapshot:0	Epoch:191	Loss:10.435	translation_Loss:10.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.07	Hits@10:37.42	Best:20.07
2024-12-28 01:57:42,125: Snapshot:0	Epoch:192	Loss:10.253	translation_Loss:10.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.13	Hits@10:37.45	Best:20.13
2024-12-28 01:57:48,845: Snapshot:0	Epoch:193	Loss:10.135	translation_Loss:10.135	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.19	Hits@10:37.48	Best:20.19
2024-12-28 01:57:55,557: Snapshot:0	Epoch:194	Loss:9.974	translation_Loss:9.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.24	Hits@10:37.51	Best:20.24
2024-12-28 01:58:02,272: Snapshot:0	Epoch:195	Loss:9.815	translation_Loss:9.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.29	Hits@10:37.58	Best:20.29
2024-12-28 01:58:08,970: Snapshot:0	Epoch:196	Loss:9.684	translation_Loss:9.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.34	Hits@10:37.64	Best:20.34
2024-12-28 01:58:15,645: Snapshot:0	Epoch:197	Loss:9.544	translation_Loss:9.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.39	Hits@10:37.65	Best:20.39
2024-12-28 01:58:22,337: Snapshot:0	Epoch:198	Loss:9.322	translation_Loss:9.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.44	Hits@10:37.69	Best:20.44
2024-12-28 01:58:29,083: Snapshot:0	Epoch:199	Loss:9.196	translation_Loss:9.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.47	Hits@10:37.73	Best:20.47
2024-12-28 01:58:29,436: => loading checkpoint './checkpoint/FACTfact_0.00001_512_10000/0model_best.tar'
2024-12-28 01:58:32,235: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1974 | 0.0993 | 0.2545 | 0.3042 |  0.3672 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 01:59:14,581: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228015839/FACTfact_0.00001_1024_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_1024_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_1024_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 01:59:24,637: Snapshot:0	Epoch:0	Loss:52.432	translation_Loss:52.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 01:59:31,103: Snapshot:0	Epoch:1	Loss:52.062	translation_Loss:52.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 01:59:37,583: Snapshot:0	Epoch:2	Loss:51.695	translation_Loss:51.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 01:59:44,124: Snapshot:0	Epoch:3	Loss:51.346	translation_Loss:51.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 01:59:51,101: Snapshot:0	Epoch:4	Loss:50.994	translation_Loss:50.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-28 01:59:57,580: Snapshot:0	Epoch:5	Loss:50.629	translation_Loss:50.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.41	Hits@10:1.43	Best:1.41
2024-12-28 02:00:04,114: Snapshot:0	Epoch:6	Loss:50.279	translation_Loss:50.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.44	Hits@10:1.5	Best:1.44
2024-12-28 02:00:10,717: Snapshot:0	Epoch:7	Loss:49.921	translation_Loss:49.921	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.47	Hits@10:1.57	Best:1.47
2024-12-28 02:00:17,190: Snapshot:0	Epoch:8	Loss:49.567	translation_Loss:49.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.53	Hits@10:1.72	Best:1.53
2024-12-28 02:00:23,733: Snapshot:0	Epoch:9	Loss:49.224	translation_Loss:49.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.99	Best:1.61
2024-12-28 02:00:30,200: Snapshot:0	Epoch:10	Loss:48.855	translation_Loss:48.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.71	Hits@10:2.32	Best:1.71
2024-12-28 02:00:36,690: Snapshot:0	Epoch:11	Loss:48.502	translation_Loss:48.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.86	Hits@10:2.75	Best:1.86
2024-12-28 02:00:43,197: Snapshot:0	Epoch:12	Loss:48.148	translation_Loss:48.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.04	Hits@10:3.23	Best:2.04
2024-12-28 02:00:49,717: Snapshot:0	Epoch:13	Loss:47.8	translation_Loss:47.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.23	Hits@10:3.74	Best:2.23
2024-12-28 02:00:56,240: Snapshot:0	Epoch:14	Loss:47.46	translation_Loss:47.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.44	Hits@10:4.15	Best:2.44
2024-12-28 02:01:02,759: Snapshot:0	Epoch:15	Loss:47.128	translation_Loss:47.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.65	Hits@10:4.53	Best:2.65
2024-12-28 02:01:09,306: Snapshot:0	Epoch:16	Loss:46.788	translation_Loss:46.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.84	Hits@10:4.86	Best:2.84
2024-12-28 02:01:15,830: Snapshot:0	Epoch:17	Loss:46.447	translation_Loss:46.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.02	Hits@10:5.24	Best:3.02
2024-12-28 02:01:22,311: Snapshot:0	Epoch:18	Loss:46.099	translation_Loss:46.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.17	Hits@10:5.55	Best:3.17
2024-12-28 02:01:28,858: Snapshot:0	Epoch:19	Loss:45.768	translation_Loss:45.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.32	Hits@10:5.9	Best:3.32
2024-12-28 02:01:35,352: Snapshot:0	Epoch:20	Loss:45.435	translation_Loss:45.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.45	Hits@10:6.16	Best:3.45
2024-12-28 02:01:41,876: Snapshot:0	Epoch:21	Loss:45.1	translation_Loss:45.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.57	Hits@10:6.41	Best:3.57
2024-12-28 02:01:48,396: Snapshot:0	Epoch:22	Loss:44.79	translation_Loss:44.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.7	Hits@10:6.66	Best:3.7
2024-12-28 02:01:54,883: Snapshot:0	Epoch:23	Loss:44.447	translation_Loss:44.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.81	Hits@10:6.97	Best:3.81
2024-12-28 02:02:01,870: Snapshot:0	Epoch:24	Loss:44.138	translation_Loss:44.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.93	Hits@10:7.21	Best:3.93
2024-12-28 02:02:08,343: Snapshot:0	Epoch:25	Loss:43.829	translation_Loss:43.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.04	Hits@10:7.42	Best:4.04
2024-12-28 02:02:14,848: Snapshot:0	Epoch:26	Loss:43.507	translation_Loss:43.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.15	Hits@10:7.68	Best:4.15
2024-12-28 02:02:21,333: Snapshot:0	Epoch:27	Loss:43.186	translation_Loss:43.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.26	Hits@10:7.96	Best:4.26
2024-12-28 02:02:27,918: Snapshot:0	Epoch:28	Loss:42.876	translation_Loss:42.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.36	Hits@10:8.18	Best:4.36
2024-12-28 02:02:34,453: Snapshot:0	Epoch:29	Loss:42.562	translation_Loss:42.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.46	Hits@10:8.41	Best:4.46
2024-12-28 02:02:40,980: Snapshot:0	Epoch:30	Loss:42.269	translation_Loss:42.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.55	Hits@10:8.68	Best:4.55
2024-12-28 02:02:47,575: Snapshot:0	Epoch:31	Loss:41.959	translation_Loss:41.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.65	Hits@10:8.91	Best:4.65
2024-12-28 02:02:54,090: Snapshot:0	Epoch:32	Loss:41.679	translation_Loss:41.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.74	Hits@10:9.13	Best:4.74
2024-12-28 02:03:00,579: Snapshot:0	Epoch:33	Loss:41.35	translation_Loss:41.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.33	Best:4.83
2024-12-28 02:03:07,082: Snapshot:0	Epoch:34	Loss:41.061	translation_Loss:41.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.93	Hits@10:9.57	Best:4.93
2024-12-28 02:03:14,097: Snapshot:0	Epoch:35	Loss:40.768	translation_Loss:40.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.01	Hits@10:9.77	Best:5.01
2024-12-28 02:03:20,593: Snapshot:0	Epoch:36	Loss:40.464	translation_Loss:40.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.09	Hits@10:10.02	Best:5.09
2024-12-28 02:03:27,091: Snapshot:0	Epoch:37	Loss:40.192	translation_Loss:40.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.17	Hits@10:10.22	Best:5.17
2024-12-28 02:03:33,583: Snapshot:0	Epoch:38	Loss:39.894	translation_Loss:39.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.25	Hits@10:10.44	Best:5.25
2024-12-28 02:03:40,154: Snapshot:0	Epoch:39	Loss:39.604	translation_Loss:39.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.34	Hits@10:10.61	Best:5.34
2024-12-28 02:03:46,700: Snapshot:0	Epoch:40	Loss:39.342	translation_Loss:39.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.42	Hits@10:10.77	Best:5.42
2024-12-28 02:03:53,233: Snapshot:0	Epoch:41	Loss:39.037	translation_Loss:39.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.5	Hits@10:10.95	Best:5.5
2024-12-28 02:03:59,745: Snapshot:0	Epoch:42	Loss:38.764	translation_Loss:38.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.57	Hits@10:11.16	Best:5.57
2024-12-28 02:04:06,221: Snapshot:0	Epoch:43	Loss:38.476	translation_Loss:38.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:11.38	Best:5.66
2024-12-28 02:04:12,732: Snapshot:0	Epoch:44	Loss:38.208	translation_Loss:38.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.74	Hits@10:11.6	Best:5.74
2024-12-28 02:04:19,287: Snapshot:0	Epoch:45	Loss:37.931	translation_Loss:37.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.82	Hits@10:11.8	Best:5.82
2024-12-28 02:04:25,882: Snapshot:0	Epoch:46	Loss:37.638	translation_Loss:37.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.9	Hits@10:12.01	Best:5.9
2024-12-28 02:04:32,384: Snapshot:0	Epoch:47	Loss:37.369	translation_Loss:37.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.98	Hits@10:12.31	Best:5.98
2024-12-28 02:04:38,917: Snapshot:0	Epoch:48	Loss:37.113	translation_Loss:37.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.06	Hits@10:12.62	Best:6.06
2024-12-28 02:04:45,507: Snapshot:0	Epoch:49	Loss:36.841	translation_Loss:36.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.15	Hits@10:12.9	Best:6.15
2024-12-28 02:04:52,036: Snapshot:0	Epoch:50	Loss:36.571	translation_Loss:36.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.19	Best:6.23
2024-12-28 02:04:58,508: Snapshot:0	Epoch:51	Loss:36.292	translation_Loss:36.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.31	Hits@10:13.37	Best:6.31
2024-12-28 02:05:05,045: Snapshot:0	Epoch:52	Loss:36.059	translation_Loss:36.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.39	Hits@10:13.6	Best:6.39
2024-12-28 02:05:11,581: Snapshot:0	Epoch:53	Loss:35.778	translation_Loss:35.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.48	Hits@10:13.83	Best:6.48
2024-12-28 02:05:18,679: Snapshot:0	Epoch:54	Loss:35.507	translation_Loss:35.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.57	Hits@10:14.05	Best:6.57
2024-12-28 02:05:25,240: Snapshot:0	Epoch:55	Loss:35.249	translation_Loss:35.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.66	Hits@10:14.28	Best:6.66
2024-12-28 02:05:31,778: Snapshot:0	Epoch:56	Loss:35.006	translation_Loss:35.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.74	Hits@10:14.49	Best:6.74
2024-12-28 02:05:38,287: Snapshot:0	Epoch:57	Loss:34.741	translation_Loss:34.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.82	Hits@10:14.73	Best:6.82
2024-12-28 02:05:44,887: Snapshot:0	Epoch:58	Loss:34.484	translation_Loss:34.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.91	Hits@10:14.95	Best:6.91
2024-12-28 02:05:51,441: Snapshot:0	Epoch:59	Loss:34.237	translation_Loss:34.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.99	Hits@10:15.16	Best:6.99
2024-12-28 02:05:57,939: Snapshot:0	Epoch:60	Loss:33.958	translation_Loss:33.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.08	Hits@10:15.38	Best:7.08
2024-12-28 02:06:04,470: Snapshot:0	Epoch:61	Loss:33.733	translation_Loss:33.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.16	Hits@10:15.62	Best:7.16
2024-12-28 02:06:10,998: Snapshot:0	Epoch:62	Loss:33.473	translation_Loss:33.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.23	Hits@10:15.8	Best:7.23
2024-12-28 02:06:17,536: Snapshot:0	Epoch:63	Loss:33.231	translation_Loss:33.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.31	Hits@10:16.0	Best:7.31
2024-12-28 02:06:24,566: Snapshot:0	Epoch:64	Loss:32.965	translation_Loss:32.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.39	Hits@10:16.15	Best:7.39
2024-12-28 02:06:31,153: Snapshot:0	Epoch:65	Loss:32.738	translation_Loss:32.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.47	Hits@10:16.34	Best:7.47
2024-12-28 02:06:37,747: Snapshot:0	Epoch:66	Loss:32.499	translation_Loss:32.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.54	Hits@10:16.57	Best:7.54
2024-12-28 02:06:44,314: Snapshot:0	Epoch:67	Loss:32.266	translation_Loss:32.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.62	Hits@10:16.77	Best:7.62
2024-12-28 02:06:50,888: Snapshot:0	Epoch:68	Loss:32.02	translation_Loss:32.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.69	Hits@10:17.01	Best:7.69
2024-12-28 02:06:57,398: Snapshot:0	Epoch:69	Loss:31.733	translation_Loss:31.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.77	Hits@10:17.23	Best:7.77
2024-12-28 02:07:03,938: Snapshot:0	Epoch:70	Loss:31.512	translation_Loss:31.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.85	Hits@10:17.48	Best:7.85
2024-12-28 02:07:10,449: Snapshot:0	Epoch:71	Loss:31.288	translation_Loss:31.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.69	Best:7.94
2024-12-28 02:07:16,966: Snapshot:0	Epoch:72	Loss:31.039	translation_Loss:31.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.01	Hits@10:17.89	Best:8.01
2024-12-28 02:07:23,468: Snapshot:0	Epoch:73	Loss:30.809	translation_Loss:30.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:18.1	Best:8.1
2024-12-28 02:07:30,066: Snapshot:0	Epoch:74	Loss:30.581	translation_Loss:30.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.18	Hits@10:18.29	Best:8.18
2024-12-28 02:07:36,615: Snapshot:0	Epoch:75	Loss:30.376	translation_Loss:30.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.28	Hits@10:18.5	Best:8.28
2024-12-28 02:07:43,155: Snapshot:0	Epoch:76	Loss:30.127	translation_Loss:30.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.37	Hits@10:18.72	Best:8.37
2024-12-28 02:07:49,726: Snapshot:0	Epoch:77	Loss:29.899	translation_Loss:29.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.47	Hits@10:18.93	Best:8.47
2024-12-28 02:07:56,244: Snapshot:0	Epoch:78	Loss:29.646	translation_Loss:29.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.55	Hits@10:19.16	Best:8.55
2024-12-28 02:08:02,749: Snapshot:0	Epoch:79	Loss:29.455	translation_Loss:29.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.64	Hits@10:19.37	Best:8.64
2024-12-28 02:08:09,252: Snapshot:0	Epoch:80	Loss:29.214	translation_Loss:29.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.73	Hits@10:19.59	Best:8.73
2024-12-28 02:08:15,777: Snapshot:0	Epoch:81	Loss:28.989	translation_Loss:28.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.82	Hits@10:19.8	Best:8.82
2024-12-28 02:08:22,302: Snapshot:0	Epoch:82	Loss:28.772	translation_Loss:28.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.9	Hits@10:19.98	Best:8.9
2024-12-28 02:08:28,901: Snapshot:0	Epoch:83	Loss:28.528	translation_Loss:28.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.0	Hits@10:20.17	Best:9.0
2024-12-28 02:08:35,960: Snapshot:0	Epoch:84	Loss:28.319	translation_Loss:28.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.07	Hits@10:20.37	Best:9.07
2024-12-28 02:08:42,477: Snapshot:0	Epoch:85	Loss:28.127	translation_Loss:28.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.17	Hits@10:20.6	Best:9.17
2024-12-28 02:08:49,094: Snapshot:0	Epoch:86	Loss:27.889	translation_Loss:27.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.26	Hits@10:20.74	Best:9.26
2024-12-28 02:08:55,659: Snapshot:0	Epoch:87	Loss:27.68	translation_Loss:27.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.34	Hits@10:20.96	Best:9.34
2024-12-28 02:09:02,251: Snapshot:0	Epoch:88	Loss:27.445	translation_Loss:27.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.43	Hits@10:21.23	Best:9.43
2024-12-28 02:09:08,769: Snapshot:0	Epoch:89	Loss:27.241	translation_Loss:27.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.52	Hits@10:21.41	Best:9.52
2024-12-28 02:09:15,283: Snapshot:0	Epoch:90	Loss:27.038	translation_Loss:27.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.6	Hits@10:21.65	Best:9.6
2024-12-28 02:09:21,822: Snapshot:0	Epoch:91	Loss:26.807	translation_Loss:26.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.69	Hits@10:21.85	Best:9.69
2024-12-28 02:09:28,344: Snapshot:0	Epoch:92	Loss:26.611	translation_Loss:26.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.78	Hits@10:22.02	Best:9.78
2024-12-28 02:09:34,887: Snapshot:0	Epoch:93	Loss:26.378	translation_Loss:26.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.89	Hits@10:22.25	Best:9.89
2024-12-28 02:09:41,947: Snapshot:0	Epoch:94	Loss:26.202	translation_Loss:26.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.97	Hits@10:22.43	Best:9.97
2024-12-28 02:09:48,528: Snapshot:0	Epoch:95	Loss:25.978	translation_Loss:25.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.05	Hits@10:22.58	Best:10.05
2024-12-28 02:09:55,041: Snapshot:0	Epoch:96	Loss:25.788	translation_Loss:25.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.13	Hits@10:22.77	Best:10.13
2024-12-28 02:10:01,550: Snapshot:0	Epoch:97	Loss:25.553	translation_Loss:25.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.21	Hits@10:22.93	Best:10.21
2024-12-28 02:10:08,172: Snapshot:0	Epoch:98	Loss:25.371	translation_Loss:25.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:23.11	Best:10.29
2024-12-28 02:10:14,665: Snapshot:0	Epoch:99	Loss:25.186	translation_Loss:25.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.37	Hits@10:23.28	Best:10.37
2024-12-28 02:10:21,181: Snapshot:0	Epoch:100	Loss:24.941	translation_Loss:24.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.46	Hits@10:23.41	Best:10.46
2024-12-28 02:10:27,691: Snapshot:0	Epoch:101	Loss:24.764	translation_Loss:24.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.55	Hits@10:23.57	Best:10.55
2024-12-28 02:10:34,189: Snapshot:0	Epoch:102	Loss:24.564	translation_Loss:24.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.63	Hits@10:23.72	Best:10.63
2024-12-28 02:10:40,724: Snapshot:0	Epoch:103	Loss:24.381	translation_Loss:24.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.72	Hits@10:23.9	Best:10.72
2024-12-28 02:10:47,254: Snapshot:0	Epoch:104	Loss:24.165	translation_Loss:24.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.81	Hits@10:24.06	Best:10.81
2024-12-28 02:10:53,856: Snapshot:0	Epoch:105	Loss:23.974	translation_Loss:23.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.9	Hits@10:24.23	Best:10.9
2024-12-28 02:11:00,376: Snapshot:0	Epoch:106	Loss:23.776	translation_Loss:23.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.98	Hits@10:24.37	Best:10.98
2024-12-28 02:11:06,878: Snapshot:0	Epoch:107	Loss:23.617	translation_Loss:23.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.07	Hits@10:24.54	Best:11.07
2024-12-28 02:11:13,381: Snapshot:0	Epoch:108	Loss:23.407	translation_Loss:23.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.15	Hits@10:24.73	Best:11.15
2024-12-28 02:11:19,901: Snapshot:0	Epoch:109	Loss:23.225	translation_Loss:23.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.22	Hits@10:24.91	Best:11.22
2024-12-28 02:11:26,453: Snapshot:0	Epoch:110	Loss:23.015	translation_Loss:23.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.31	Hits@10:25.06	Best:11.31
2024-12-28 02:11:32,962: Snapshot:0	Epoch:111	Loss:22.868	translation_Loss:22.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.38	Hits@10:25.19	Best:11.38
2024-12-28 02:11:39,539: Snapshot:0	Epoch:112	Loss:22.634	translation_Loss:22.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.45	Hits@10:25.33	Best:11.45
2024-12-28 02:11:46,118: Snapshot:0	Epoch:113	Loss:22.471	translation_Loss:22.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.53	Hits@10:25.53	Best:11.53
2024-12-28 02:11:53,196: Snapshot:0	Epoch:114	Loss:22.292	translation_Loss:22.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.59	Hits@10:25.72	Best:11.59
2024-12-28 02:11:59,783: Snapshot:0	Epoch:115	Loss:22.086	translation_Loss:22.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.66	Hits@10:25.86	Best:11.66
2024-12-28 02:12:06,319: Snapshot:0	Epoch:116	Loss:21.953	translation_Loss:21.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.74	Hits@10:25.97	Best:11.74
2024-12-28 02:12:12,898: Snapshot:0	Epoch:117	Loss:21.739	translation_Loss:21.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.81	Hits@10:26.13	Best:11.81
2024-12-28 02:12:19,493: Snapshot:0	Epoch:118	Loss:21.573	translation_Loss:21.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.87	Hits@10:26.27	Best:11.87
2024-12-28 02:12:26,082: Snapshot:0	Epoch:119	Loss:21.408	translation_Loss:21.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.94	Hits@10:26.42	Best:11.94
2024-12-28 02:12:32,588: Snapshot:0	Epoch:120	Loss:21.219	translation_Loss:21.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.02	Hits@10:26.55	Best:12.02
2024-12-28 02:12:39,168: Snapshot:0	Epoch:121	Loss:21.038	translation_Loss:21.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.1	Hits@10:26.72	Best:12.1
2024-12-28 02:12:45,698: Snapshot:0	Epoch:122	Loss:20.826	translation_Loss:20.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.17	Hits@10:26.88	Best:12.17
2024-12-28 02:12:52,231: Snapshot:0	Epoch:123	Loss:20.711	translation_Loss:20.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.24	Hits@10:27.0	Best:12.24
2024-12-28 02:12:59,279: Snapshot:0	Epoch:124	Loss:20.524	translation_Loss:20.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.14	Best:12.31
2024-12-28 02:13:05,788: Snapshot:0	Epoch:125	Loss:20.325	translation_Loss:20.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.38	Hits@10:27.23	Best:12.38
2024-12-28 02:13:12,314: Snapshot:0	Epoch:126	Loss:20.157	translation_Loss:20.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.45	Hits@10:27.39	Best:12.45
2024-12-28 02:13:18,903: Snapshot:0	Epoch:127	Loss:20.01	translation_Loss:20.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.52	Hits@10:27.53	Best:12.52
2024-12-28 02:13:25,466: Snapshot:0	Epoch:128	Loss:19.836	translation_Loss:19.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.58	Hits@10:27.65	Best:12.58
2024-12-28 02:13:32,075: Snapshot:0	Epoch:129	Loss:19.665	translation_Loss:19.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.66	Hits@10:27.74	Best:12.66
2024-12-28 02:13:38,613: Snapshot:0	Epoch:130	Loss:19.516	translation_Loss:19.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.72	Hits@10:27.9	Best:12.72
2024-12-28 02:13:45,204: Snapshot:0	Epoch:131	Loss:19.366	translation_Loss:19.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.79	Hits@10:28.05	Best:12.79
2024-12-28 02:13:51,783: Snapshot:0	Epoch:132	Loss:19.151	translation_Loss:19.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.86	Hits@10:28.15	Best:12.86
2024-12-28 02:13:58,304: Snapshot:0	Epoch:133	Loss:19.032	translation_Loss:19.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.92	Hits@10:28.24	Best:12.92
2024-12-28 02:14:04,820: Snapshot:0	Epoch:134	Loss:18.843	translation_Loss:18.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.98	Hits@10:28.34	Best:12.98
2024-12-28 02:14:11,858: Snapshot:0	Epoch:135	Loss:18.701	translation_Loss:18.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.05	Hits@10:28.43	Best:13.05
2024-12-28 02:14:18,377: Snapshot:0	Epoch:136	Loss:18.533	translation_Loss:18.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.12	Hits@10:28.57	Best:13.12
2024-12-28 02:14:24,983: Snapshot:0	Epoch:137	Loss:18.391	translation_Loss:18.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.18	Hits@10:28.68	Best:13.18
2024-12-28 02:14:31,527: Snapshot:0	Epoch:138	Loss:18.234	translation_Loss:18.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.24	Hits@10:28.77	Best:13.24
2024-12-28 02:14:38,063: Snapshot:0	Epoch:139	Loss:18.086	translation_Loss:18.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.3	Hits@10:28.86	Best:13.3
2024-12-28 02:14:44,602: Snapshot:0	Epoch:140	Loss:17.934	translation_Loss:17.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.36	Hits@10:29.0	Best:13.36
2024-12-28 02:14:51,150: Snapshot:0	Epoch:141	Loss:17.811	translation_Loss:17.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.42	Hits@10:29.16	Best:13.42
2024-12-28 02:14:57,661: Snapshot:0	Epoch:142	Loss:17.639	translation_Loss:17.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.48	Hits@10:29.27	Best:13.48
2024-12-28 02:15:04,251: Snapshot:0	Epoch:143	Loss:17.472	translation_Loss:17.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.54	Hits@10:29.43	Best:13.54
2024-12-28 02:15:10,802: Snapshot:0	Epoch:144	Loss:17.333	translation_Loss:17.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.6	Hits@10:29.55	Best:13.6
2024-12-28 02:15:17,335: Snapshot:0	Epoch:145	Loss:17.19	translation_Loss:17.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.66	Hits@10:29.69	Best:13.66
2024-12-28 02:15:23,894: Snapshot:0	Epoch:146	Loss:17.068	translation_Loss:17.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.72	Hits@10:29.82	Best:13.72
2024-12-28 02:15:30,511: Snapshot:0	Epoch:147	Loss:16.9	translation_Loss:16.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.79	Hits@10:29.93	Best:13.79
2024-12-28 02:15:37,071: Snapshot:0	Epoch:148	Loss:16.737	translation_Loss:16.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.85	Hits@10:30.07	Best:13.85
2024-12-28 02:15:43,702: Snapshot:0	Epoch:149	Loss:16.595	translation_Loss:16.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.91	Hits@10:30.12	Best:13.91
2024-12-28 02:15:50,271: Snapshot:0	Epoch:150	Loss:16.431	translation_Loss:16.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.98	Hits@10:30.2	Best:13.98
2024-12-28 02:15:56,825: Snapshot:0	Epoch:151	Loss:16.304	translation_Loss:16.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.03	Hits@10:30.28	Best:14.03
2024-12-28 02:16:03,357: Snapshot:0	Epoch:152	Loss:16.173	translation_Loss:16.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.09	Hits@10:30.38	Best:14.09
2024-12-28 02:16:09,877: Snapshot:0	Epoch:153	Loss:16.047	translation_Loss:16.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.16	Hits@10:30.49	Best:14.16
2024-12-28 02:16:16,927: Snapshot:0	Epoch:154	Loss:15.897	translation_Loss:15.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.22	Hits@10:30.6	Best:14.22
2024-12-28 02:16:23,483: Snapshot:0	Epoch:155	Loss:15.777	translation_Loss:15.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.27	Hits@10:30.67	Best:14.27
2024-12-28 02:16:30,038: Snapshot:0	Epoch:156	Loss:15.611	translation_Loss:15.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.33	Hits@10:30.75	Best:14.33
2024-12-28 02:16:36,567: Snapshot:0	Epoch:157	Loss:15.505	translation_Loss:15.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.38	Hits@10:30.84	Best:14.38
2024-12-28 02:16:43,137: Snapshot:0	Epoch:158	Loss:15.339	translation_Loss:15.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.44	Hits@10:30.92	Best:14.44
2024-12-28 02:16:49,729: Snapshot:0	Epoch:159	Loss:15.228	translation_Loss:15.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.49	Hits@10:31.0	Best:14.49
2024-12-28 02:16:56,258: Snapshot:0	Epoch:160	Loss:15.057	translation_Loss:15.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.55	Hits@10:31.11	Best:14.55
2024-12-28 02:17:02,801: Snapshot:0	Epoch:161	Loss:14.953	translation_Loss:14.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.63	Hits@10:31.21	Best:14.63
2024-12-28 02:17:09,328: Snapshot:0	Epoch:162	Loss:14.824	translation_Loss:14.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.68	Hits@10:31.34	Best:14.68
2024-12-28 02:17:15,866: Snapshot:0	Epoch:163	Loss:14.703	translation_Loss:14.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.72	Hits@10:31.43	Best:14.72
2024-12-28 02:17:22,897: Snapshot:0	Epoch:164	Loss:14.544	translation_Loss:14.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.79	Hits@10:31.54	Best:14.79
2024-12-28 02:17:29,407: Snapshot:0	Epoch:165	Loss:14.445	translation_Loss:14.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.84	Hits@10:31.62	Best:14.84
2024-12-28 02:17:35,988: Snapshot:0	Epoch:166	Loss:14.321	translation_Loss:14.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.9	Hits@10:31.67	Best:14.9
2024-12-28 02:17:42,557: Snapshot:0	Epoch:167	Loss:14.137	translation_Loss:14.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.96	Hits@10:31.78	Best:14.96
2024-12-28 02:17:49,112: Snapshot:0	Epoch:168	Loss:14.1	translation_Loss:14.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.01	Hits@10:31.87	Best:15.01
2024-12-28 02:17:55,663: Snapshot:0	Epoch:169	Loss:13.91	translation_Loss:13.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.08	Hits@10:31.95	Best:15.08
2024-12-28 02:18:02,227: Snapshot:0	Epoch:170	Loss:13.809	translation_Loss:13.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.13	Hits@10:32.02	Best:15.13
2024-12-28 02:18:08,849: Snapshot:0	Epoch:171	Loss:13.669	translation_Loss:13.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.19	Hits@10:32.12	Best:15.19
2024-12-28 02:18:15,368: Snapshot:0	Epoch:172	Loss:13.603	translation_Loss:13.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.25	Hits@10:32.2	Best:15.25
2024-12-28 02:18:21,908: Snapshot:0	Epoch:173	Loss:13.435	translation_Loss:13.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.3	Hits@10:32.31	Best:15.3
2024-12-28 02:18:28,441: Snapshot:0	Epoch:174	Loss:13.345	translation_Loss:13.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.36	Hits@10:32.41	Best:15.36
2024-12-28 02:18:35,016: Snapshot:0	Epoch:175	Loss:13.199	translation_Loss:13.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.41	Hits@10:32.52	Best:15.41
2024-12-28 02:18:41,547: Snapshot:0	Epoch:176	Loss:13.116	translation_Loss:13.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.46	Hits@10:32.56	Best:15.46
2024-12-28 02:18:48,108: Snapshot:0	Epoch:177	Loss:12.97	translation_Loss:12.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.51	Hits@10:32.67	Best:15.51
2024-12-28 02:18:54,648: Snapshot:0	Epoch:178	Loss:12.887	translation_Loss:12.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.57	Hits@10:32.71	Best:15.57
2024-12-28 02:19:01,173: Snapshot:0	Epoch:179	Loss:12.754	translation_Loss:12.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.62	Hits@10:32.8	Best:15.62
2024-12-28 02:19:07,695: Snapshot:0	Epoch:180	Loss:12.623	translation_Loss:12.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.69	Hits@10:32.86	Best:15.69
2024-12-28 02:19:14,214: Snapshot:0	Epoch:181	Loss:12.5	translation_Loss:12.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.75	Hits@10:32.91	Best:15.75
2024-12-28 02:19:20,754: Snapshot:0	Epoch:182	Loss:12.402	translation_Loss:12.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.8	Hits@10:32.99	Best:15.8
2024-12-28 02:19:27,262: Snapshot:0	Epoch:183	Loss:12.3	translation_Loss:12.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.85	Hits@10:33.05	Best:15.85
2024-12-28 02:19:34,317: Snapshot:0	Epoch:184	Loss:12.197	translation_Loss:12.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.9	Hits@10:33.13	Best:15.9
2024-12-28 02:19:40,909: Snapshot:0	Epoch:185	Loss:12.075	translation_Loss:12.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.95	Hits@10:33.18	Best:15.95
2024-12-28 02:19:47,470: Snapshot:0	Epoch:186	Loss:11.948	translation_Loss:11.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.0	Hits@10:33.29	Best:16.0
2024-12-28 02:19:54,053: Snapshot:0	Epoch:187	Loss:11.847	translation_Loss:11.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.06	Hits@10:33.37	Best:16.06
2024-12-28 02:20:00,688: Snapshot:0	Epoch:188	Loss:11.713	translation_Loss:11.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.11	Hits@10:33.44	Best:16.11
2024-12-28 02:20:07,324: Snapshot:0	Epoch:189	Loss:11.649	translation_Loss:11.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.15	Hits@10:33.51	Best:16.15
2024-12-28 02:20:13,904: Snapshot:0	Epoch:190	Loss:11.532	translation_Loss:11.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.18	Hits@10:33.6	Best:16.18
2024-12-28 02:20:20,481: Snapshot:0	Epoch:191	Loss:11.418	translation_Loss:11.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.25	Hits@10:33.68	Best:16.25
2024-12-28 02:20:27,038: Snapshot:0	Epoch:192	Loss:11.302	translation_Loss:11.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.29	Hits@10:33.74	Best:16.29
2024-12-28 02:20:33,565: Snapshot:0	Epoch:193	Loss:11.219	translation_Loss:11.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.35	Hits@10:33.82	Best:16.35
2024-12-28 02:20:40,648: Snapshot:0	Epoch:194	Loss:11.109	translation_Loss:11.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.4	Hits@10:33.89	Best:16.4
2024-12-28 02:20:47,229: Snapshot:0	Epoch:195	Loss:11.002	translation_Loss:11.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.46	Hits@10:33.97	Best:16.46
2024-12-28 02:20:53,846: Snapshot:0	Epoch:196	Loss:10.911	translation_Loss:10.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.52	Hits@10:34.05	Best:16.52
2024-12-28 02:21:00,435: Snapshot:0	Epoch:197	Loss:10.814	translation_Loss:10.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.57	Hits@10:34.1	Best:16.57
2024-12-28 02:21:06,962: Snapshot:0	Epoch:198	Loss:10.673	translation_Loss:10.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.62	Hits@10:34.16	Best:16.62
2024-12-28 02:21:13,477: Snapshot:0	Epoch:199	Loss:10.576	translation_Loss:10.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.66	Hits@10:34.24	Best:16.66
2024-12-28 02:21:13,824: => loading checkpoint './checkpoint/FACTfact_0.00001_1024_1000/0model_best.tar'
2024-12-28 02:21:16,630: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1602 | 0.061  | 0.2208 | 0.2717 |  0.3316 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 02:21:59,502: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228022123/FACTfact_0.00001_1024_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_1024_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_1024_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 02:22:09,514: Snapshot:0	Epoch:0	Loss:52.432	translation_Loss:52.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 02:22:16,009: Snapshot:0	Epoch:1	Loss:52.062	translation_Loss:52.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 02:22:22,459: Snapshot:0	Epoch:2	Loss:51.695	translation_Loss:51.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 02:22:28,941: Snapshot:0	Epoch:3	Loss:51.346	translation_Loss:51.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 02:22:35,889: Snapshot:0	Epoch:4	Loss:50.994	translation_Loss:50.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-28 02:22:42,350: Snapshot:0	Epoch:5	Loss:50.629	translation_Loss:50.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.41	Hits@10:1.43	Best:1.41
2024-12-28 02:22:48,852: Snapshot:0	Epoch:6	Loss:50.279	translation_Loss:50.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.44	Hits@10:1.5	Best:1.44
2024-12-28 02:22:55,367: Snapshot:0	Epoch:7	Loss:49.921	translation_Loss:49.921	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.47	Hits@10:1.57	Best:1.47
2024-12-28 02:23:01,860: Snapshot:0	Epoch:8	Loss:49.567	translation_Loss:49.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.53	Hits@10:1.72	Best:1.53
2024-12-28 02:23:08,317: Snapshot:0	Epoch:9	Loss:49.224	translation_Loss:49.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.99	Best:1.61
2024-12-28 02:23:14,890: Snapshot:0	Epoch:10	Loss:48.855	translation_Loss:48.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.71	Hits@10:2.32	Best:1.71
2024-12-28 02:23:21,385: Snapshot:0	Epoch:11	Loss:48.502	translation_Loss:48.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.86	Hits@10:2.75	Best:1.86
2024-12-28 02:23:27,854: Snapshot:0	Epoch:12	Loss:48.148	translation_Loss:48.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.04	Hits@10:3.23	Best:2.04
2024-12-28 02:23:34,303: Snapshot:0	Epoch:13	Loss:47.8	translation_Loss:47.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.23	Hits@10:3.74	Best:2.23
2024-12-28 02:23:40,799: Snapshot:0	Epoch:14	Loss:47.46	translation_Loss:47.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.44	Hits@10:4.15	Best:2.44
2024-12-28 02:23:47,314: Snapshot:0	Epoch:15	Loss:47.128	translation_Loss:47.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.65	Hits@10:4.53	Best:2.65
2024-12-28 02:23:53,805: Snapshot:0	Epoch:16	Loss:46.788	translation_Loss:46.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.84	Hits@10:4.86	Best:2.84
2024-12-28 02:24:00,290: Snapshot:0	Epoch:17	Loss:46.447	translation_Loss:46.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.02	Hits@10:5.24	Best:3.02
2024-12-28 02:24:06,791: Snapshot:0	Epoch:18	Loss:46.099	translation_Loss:46.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.17	Hits@10:5.55	Best:3.17
2024-12-28 02:24:13,256: Snapshot:0	Epoch:19	Loss:45.768	translation_Loss:45.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.32	Hits@10:5.9	Best:3.32
2024-12-28 02:24:19,747: Snapshot:0	Epoch:20	Loss:45.435	translation_Loss:45.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.45	Hits@10:6.16	Best:3.45
2024-12-28 02:24:26,286: Snapshot:0	Epoch:21	Loss:45.1	translation_Loss:45.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.57	Hits@10:6.41	Best:3.57
2024-12-28 02:24:32,789: Snapshot:0	Epoch:22	Loss:44.79	translation_Loss:44.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.7	Hits@10:6.66	Best:3.7
2024-12-28 02:24:39,262: Snapshot:0	Epoch:23	Loss:44.447	translation_Loss:44.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.81	Hits@10:6.97	Best:3.81
2024-12-28 02:24:46,261: Snapshot:0	Epoch:24	Loss:44.138	translation_Loss:44.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.93	Hits@10:7.21	Best:3.93
2024-12-28 02:24:52,777: Snapshot:0	Epoch:25	Loss:43.829	translation_Loss:43.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.04	Hits@10:7.42	Best:4.04
2024-12-28 02:24:59,274: Snapshot:0	Epoch:26	Loss:43.507	translation_Loss:43.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.15	Hits@10:7.68	Best:4.15
2024-12-28 02:25:05,809: Snapshot:0	Epoch:27	Loss:43.186	translation_Loss:43.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.26	Hits@10:7.96	Best:4.26
2024-12-28 02:25:12,313: Snapshot:0	Epoch:28	Loss:42.876	translation_Loss:42.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.36	Hits@10:8.18	Best:4.36
2024-12-28 02:25:18,811: Snapshot:0	Epoch:29	Loss:42.562	translation_Loss:42.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.46	Hits@10:8.41	Best:4.46
2024-12-28 02:25:25,331: Snapshot:0	Epoch:30	Loss:42.269	translation_Loss:42.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.55	Hits@10:8.68	Best:4.55
2024-12-28 02:25:31,852: Snapshot:0	Epoch:31	Loss:41.959	translation_Loss:41.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.65	Hits@10:8.91	Best:4.65
2024-12-28 02:25:38,412: Snapshot:0	Epoch:32	Loss:41.679	translation_Loss:41.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.74	Hits@10:9.13	Best:4.74
2024-12-28 02:25:44,988: Snapshot:0	Epoch:33	Loss:41.35	translation_Loss:41.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.33	Best:4.83
2024-12-28 02:25:51,553: Snapshot:0	Epoch:34	Loss:41.061	translation_Loss:41.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.93	Hits@10:9.57	Best:4.93
2024-12-28 02:25:58,541: Snapshot:0	Epoch:35	Loss:40.768	translation_Loss:40.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.01	Hits@10:9.77	Best:5.01
2024-12-28 02:26:05,093: Snapshot:0	Epoch:36	Loss:40.464	translation_Loss:40.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.09	Hits@10:10.02	Best:5.09
2024-12-28 02:26:11,577: Snapshot:0	Epoch:37	Loss:40.192	translation_Loss:40.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.17	Hits@10:10.22	Best:5.17
2024-12-28 02:26:18,063: Snapshot:0	Epoch:38	Loss:39.894	translation_Loss:39.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.25	Hits@10:10.44	Best:5.25
2024-12-28 02:26:24,608: Snapshot:0	Epoch:39	Loss:39.604	translation_Loss:39.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.34	Hits@10:10.61	Best:5.34
2024-12-28 02:26:31,122: Snapshot:0	Epoch:40	Loss:39.342	translation_Loss:39.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.42	Hits@10:10.77	Best:5.42
2024-12-28 02:26:37,614: Snapshot:0	Epoch:41	Loss:39.037	translation_Loss:39.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.5	Hits@10:10.95	Best:5.5
2024-12-28 02:26:44,148: Snapshot:0	Epoch:42	Loss:38.764	translation_Loss:38.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.57	Hits@10:11.16	Best:5.57
2024-12-28 02:26:50,666: Snapshot:0	Epoch:43	Loss:38.476	translation_Loss:38.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:11.38	Best:5.66
2024-12-28 02:26:57,165: Snapshot:0	Epoch:44	Loss:38.208	translation_Loss:38.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.74	Hits@10:11.6	Best:5.74
2024-12-28 02:27:03,678: Snapshot:0	Epoch:45	Loss:37.931	translation_Loss:37.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.82	Hits@10:11.81	Best:5.82
2024-12-28 02:27:10,183: Snapshot:0	Epoch:46	Loss:37.638	translation_Loss:37.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.9	Hits@10:12.01	Best:5.9
2024-12-28 02:27:16,680: Snapshot:0	Epoch:47	Loss:37.369	translation_Loss:37.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.98	Hits@10:12.31	Best:5.98
2024-12-28 02:27:23,164: Snapshot:0	Epoch:48	Loss:37.113	translation_Loss:37.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.06	Hits@10:12.62	Best:6.06
2024-12-28 02:27:29,634: Snapshot:0	Epoch:49	Loss:36.841	translation_Loss:36.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.15	Hits@10:12.89	Best:6.15
2024-12-28 02:27:36,113: Snapshot:0	Epoch:50	Loss:36.571	translation_Loss:36.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.18	Best:6.23
2024-12-28 02:27:42,626: Snapshot:0	Epoch:51	Loss:36.292	translation_Loss:36.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.31	Hits@10:13.37	Best:6.31
2024-12-28 02:27:49,191: Snapshot:0	Epoch:52	Loss:36.059	translation_Loss:36.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.39	Hits@10:13.6	Best:6.39
2024-12-28 02:27:55,694: Snapshot:0	Epoch:53	Loss:35.778	translation_Loss:35.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.48	Hits@10:13.83	Best:6.48
2024-12-28 02:28:02,706: Snapshot:0	Epoch:54	Loss:35.507	translation_Loss:35.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.57	Hits@10:14.06	Best:6.57
2024-12-28 02:28:09,188: Snapshot:0	Epoch:55	Loss:35.249	translation_Loss:35.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.66	Hits@10:14.29	Best:6.66
2024-12-28 02:28:15,688: Snapshot:0	Epoch:56	Loss:35.006	translation_Loss:35.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.74	Hits@10:14.49	Best:6.74
2024-12-28 02:28:22,190: Snapshot:0	Epoch:57	Loss:34.741	translation_Loss:34.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.82	Hits@10:14.73	Best:6.82
2024-12-28 02:28:28,717: Snapshot:0	Epoch:58	Loss:34.484	translation_Loss:34.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.91	Hits@10:14.95	Best:6.91
2024-12-28 02:28:35,211: Snapshot:0	Epoch:59	Loss:34.237	translation_Loss:34.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.0	Hits@10:15.16	Best:7.0
2024-12-28 02:28:41,699: Snapshot:0	Epoch:60	Loss:33.958	translation_Loss:33.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.08	Hits@10:15.38	Best:7.08
2024-12-28 02:28:48,216: Snapshot:0	Epoch:61	Loss:33.733	translation_Loss:33.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.16	Hits@10:15.63	Best:7.16
2024-12-28 02:28:54,762: Snapshot:0	Epoch:62	Loss:33.473	translation_Loss:33.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.23	Hits@10:15.8	Best:7.23
2024-12-28 02:29:01,339: Snapshot:0	Epoch:63	Loss:33.231	translation_Loss:33.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.31	Hits@10:16.0	Best:7.31
2024-12-28 02:29:08,358: Snapshot:0	Epoch:64	Loss:32.965	translation_Loss:32.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.39	Hits@10:16.15	Best:7.39
2024-12-28 02:29:14,836: Snapshot:0	Epoch:65	Loss:32.738	translation_Loss:32.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.47	Hits@10:16.34	Best:7.47
2024-12-28 02:29:21,320: Snapshot:0	Epoch:66	Loss:32.499	translation_Loss:32.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.54	Hits@10:16.57	Best:7.54
2024-12-28 02:29:27,816: Snapshot:0	Epoch:67	Loss:32.266	translation_Loss:32.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.62	Hits@10:16.77	Best:7.62
2024-12-28 02:29:34,328: Snapshot:0	Epoch:68	Loss:32.02	translation_Loss:32.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.69	Hits@10:17.01	Best:7.69
2024-12-28 02:29:40,908: Snapshot:0	Epoch:69	Loss:31.733	translation_Loss:31.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.77	Hits@10:17.24	Best:7.77
2024-12-28 02:29:47,473: Snapshot:0	Epoch:70	Loss:31.512	translation_Loss:31.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.85	Hits@10:17.48	Best:7.85
2024-12-28 02:29:53,999: Snapshot:0	Epoch:71	Loss:31.288	translation_Loss:31.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.7	Best:7.94
2024-12-28 02:30:00,515: Snapshot:0	Epoch:72	Loss:31.039	translation_Loss:31.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.01	Hits@10:17.91	Best:8.01
2024-12-28 02:30:07,110: Snapshot:0	Epoch:73	Loss:30.809	translation_Loss:30.809	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:18.09	Best:8.1
2024-12-28 02:30:13,617: Snapshot:0	Epoch:74	Loss:30.581	translation_Loss:30.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.19	Hits@10:18.29	Best:8.19
2024-12-28 02:30:20,166: Snapshot:0	Epoch:75	Loss:30.376	translation_Loss:30.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.28	Hits@10:18.49	Best:8.28
2024-12-28 02:30:26,707: Snapshot:0	Epoch:76	Loss:30.127	translation_Loss:30.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.37	Hits@10:18.72	Best:8.37
2024-12-28 02:30:33,199: Snapshot:0	Epoch:77	Loss:29.899	translation_Loss:29.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.47	Hits@10:18.92	Best:8.47
2024-12-28 02:30:39,685: Snapshot:0	Epoch:78	Loss:29.646	translation_Loss:29.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.55	Hits@10:19.17	Best:8.55
2024-12-28 02:30:46,257: Snapshot:0	Epoch:79	Loss:29.455	translation_Loss:29.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.64	Hits@10:19.36	Best:8.64
2024-12-28 02:30:52,778: Snapshot:0	Epoch:80	Loss:29.214	translation_Loss:29.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.73	Hits@10:19.58	Best:8.73
2024-12-28 02:30:59,371: Snapshot:0	Epoch:81	Loss:28.989	translation_Loss:28.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.82	Hits@10:19.81	Best:8.82
2024-12-28 02:31:05,866: Snapshot:0	Epoch:82	Loss:28.772	translation_Loss:28.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.9	Hits@10:19.98	Best:8.9
2024-12-28 02:31:12,349: Snapshot:0	Epoch:83	Loss:28.528	translation_Loss:28.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.0	Hits@10:20.17	Best:9.0
2024-12-28 02:31:19,370: Snapshot:0	Epoch:84	Loss:28.319	translation_Loss:28.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.08	Hits@10:20.36	Best:9.08
2024-12-28 02:31:25,941: Snapshot:0	Epoch:85	Loss:28.127	translation_Loss:28.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.17	Hits@10:20.59	Best:9.17
2024-12-28 02:31:32,420: Snapshot:0	Epoch:86	Loss:27.889	translation_Loss:27.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.26	Hits@10:20.76	Best:9.26
2024-12-28 02:31:38,984: Snapshot:0	Epoch:87	Loss:27.68	translation_Loss:27.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.34	Hits@10:20.97	Best:9.34
2024-12-28 02:31:45,584: Snapshot:0	Epoch:88	Loss:27.445	translation_Loss:27.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.43	Hits@10:21.21	Best:9.43
2024-12-28 02:31:52,136: Snapshot:0	Epoch:89	Loss:27.241	translation_Loss:27.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.52	Hits@10:21.41	Best:9.52
2024-12-28 02:31:58,640: Snapshot:0	Epoch:90	Loss:27.038	translation_Loss:27.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.6	Hits@10:21.64	Best:9.6
2024-12-28 02:32:05,155: Snapshot:0	Epoch:91	Loss:26.807	translation_Loss:26.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:21.85	Best:9.68
2024-12-28 02:32:11,654: Snapshot:0	Epoch:92	Loss:26.611	translation_Loss:26.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.78	Hits@10:22.03	Best:9.78
2024-12-28 02:32:18,148: Snapshot:0	Epoch:93	Loss:26.378	translation_Loss:26.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.89	Hits@10:22.25	Best:9.89
2024-12-28 02:32:25,187: Snapshot:0	Epoch:94	Loss:26.202	translation_Loss:26.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.97	Hits@10:22.43	Best:9.97
2024-12-28 02:32:31,688: Snapshot:0	Epoch:95	Loss:25.978	translation_Loss:25.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.05	Hits@10:22.58	Best:10.05
2024-12-28 02:32:38,196: Snapshot:0	Epoch:96	Loss:25.788	translation_Loss:25.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.13	Hits@10:22.76	Best:10.13
2024-12-28 02:32:44,714: Snapshot:0	Epoch:97	Loss:25.553	translation_Loss:25.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.22	Hits@10:22.93	Best:10.22
2024-12-28 02:32:51,231: Snapshot:0	Epoch:98	Loss:25.371	translation_Loss:25.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:23.11	Best:10.29
2024-12-28 02:32:57,748: Snapshot:0	Epoch:99	Loss:25.186	translation_Loss:25.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.38	Hits@10:23.3	Best:10.38
2024-12-28 02:33:04,250: Snapshot:0	Epoch:100	Loss:24.941	translation_Loss:24.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.45	Hits@10:23.42	Best:10.45
2024-12-28 02:33:10,791: Snapshot:0	Epoch:101	Loss:24.764	translation_Loss:24.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.55	Hits@10:23.56	Best:10.55
2024-12-28 02:33:17,311: Snapshot:0	Epoch:102	Loss:24.564	translation_Loss:24.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.63	Hits@10:23.73	Best:10.63
2024-12-28 02:33:23,879: Snapshot:0	Epoch:103	Loss:24.381	translation_Loss:24.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.72	Hits@10:23.89	Best:10.72
2024-12-28 02:33:30,415: Snapshot:0	Epoch:104	Loss:24.165	translation_Loss:24.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.81	Hits@10:24.06	Best:10.81
2024-12-28 02:33:36,926: Snapshot:0	Epoch:105	Loss:23.974	translation_Loss:23.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.9	Hits@10:24.22	Best:10.9
2024-12-28 02:33:43,494: Snapshot:0	Epoch:106	Loss:23.776	translation_Loss:23.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.99	Hits@10:24.37	Best:10.99
2024-12-28 02:33:50,004: Snapshot:0	Epoch:107	Loss:23.617	translation_Loss:23.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.07	Hits@10:24.55	Best:11.07
2024-12-28 02:33:56,535: Snapshot:0	Epoch:108	Loss:23.407	translation_Loss:23.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.15	Hits@10:24.73	Best:11.15
2024-12-28 02:34:03,069: Snapshot:0	Epoch:109	Loss:23.226	translation_Loss:23.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.23	Hits@10:24.91	Best:11.23
2024-12-28 02:34:09,590: Snapshot:0	Epoch:110	Loss:23.015	translation_Loss:23.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.31	Hits@10:25.06	Best:11.31
2024-12-28 02:34:16,135: Snapshot:0	Epoch:111	Loss:22.868	translation_Loss:22.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.38	Hits@10:25.2	Best:11.38
2024-12-28 02:34:22,665: Snapshot:0	Epoch:112	Loss:22.634	translation_Loss:22.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.45	Hits@10:25.32	Best:11.45
2024-12-28 02:34:29,185: Snapshot:0	Epoch:113	Loss:22.471	translation_Loss:22.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.53	Hits@10:25.53	Best:11.53
2024-12-28 02:34:36,226: Snapshot:0	Epoch:114	Loss:22.292	translation_Loss:22.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.59	Hits@10:25.7	Best:11.59
2024-12-28 02:34:42,730: Snapshot:0	Epoch:115	Loss:22.086	translation_Loss:22.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.66	Hits@10:25.86	Best:11.66
2024-12-28 02:34:49,279: Snapshot:0	Epoch:116	Loss:21.953	translation_Loss:21.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.74	Hits@10:25.97	Best:11.74
2024-12-28 02:34:55,830: Snapshot:0	Epoch:117	Loss:21.739	translation_Loss:21.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.81	Hits@10:26.14	Best:11.81
2024-12-28 02:35:02,334: Snapshot:0	Epoch:118	Loss:21.573	translation_Loss:21.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.87	Hits@10:26.28	Best:11.87
2024-12-28 02:35:08,943: Snapshot:0	Epoch:119	Loss:21.408	translation_Loss:21.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.94	Hits@10:26.42	Best:11.94
2024-12-28 02:35:15,499: Snapshot:0	Epoch:120	Loss:21.219	translation_Loss:21.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.02	Hits@10:26.57	Best:12.02
2024-12-28 02:35:22,004: Snapshot:0	Epoch:121	Loss:21.038	translation_Loss:21.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.1	Hits@10:26.72	Best:12.1
2024-12-28 02:35:28,534: Snapshot:0	Epoch:122	Loss:20.826	translation_Loss:20.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.17	Hits@10:26.87	Best:12.17
2024-12-28 02:35:35,044: Snapshot:0	Epoch:123	Loss:20.711	translation_Loss:20.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.24	Hits@10:27.0	Best:12.24
2024-12-28 02:35:42,081: Snapshot:0	Epoch:124	Loss:20.524	translation_Loss:20.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.16	Best:12.31
2024-12-28 02:35:48,600: Snapshot:0	Epoch:125	Loss:20.325	translation_Loss:20.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.37	Hits@10:27.23	Best:12.37
2024-12-28 02:35:55,120: Snapshot:0	Epoch:126	Loss:20.157	translation_Loss:20.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.45	Hits@10:27.36	Best:12.45
2024-12-28 02:36:01,630: Snapshot:0	Epoch:127	Loss:20.01	translation_Loss:20.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.52	Hits@10:27.52	Best:12.52
2024-12-28 02:36:08,134: Snapshot:0	Epoch:128	Loss:19.836	translation_Loss:19.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.58	Hits@10:27.66	Best:12.58
2024-12-28 02:36:14,673: Snapshot:0	Epoch:129	Loss:19.665	translation_Loss:19.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.66	Hits@10:27.76	Best:12.66
2024-12-28 02:36:21,238: Snapshot:0	Epoch:130	Loss:19.516	translation_Loss:19.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.72	Hits@10:27.9	Best:12.72
2024-12-28 02:36:27,770: Snapshot:0	Epoch:131	Loss:19.366	translation_Loss:19.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.79	Hits@10:28.05	Best:12.79
2024-12-28 02:36:34,297: Snapshot:0	Epoch:132	Loss:19.151	translation_Loss:19.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.85	Hits@10:28.13	Best:12.85
2024-12-28 02:36:40,815: Snapshot:0	Epoch:133	Loss:19.032	translation_Loss:19.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.92	Hits@10:28.24	Best:12.92
2024-12-28 02:36:47,327: Snapshot:0	Epoch:134	Loss:18.843	translation_Loss:18.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.98	Hits@10:28.34	Best:12.98
2024-12-28 02:36:54,364: Snapshot:0	Epoch:135	Loss:18.701	translation_Loss:18.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.05	Hits@10:28.43	Best:13.05
2024-12-28 02:37:00,921: Snapshot:0	Epoch:136	Loss:18.533	translation_Loss:18.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.12	Hits@10:28.57	Best:13.12
2024-12-28 02:37:07,441: Snapshot:0	Epoch:137	Loss:18.391	translation_Loss:18.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.18	Hits@10:28.68	Best:13.18
2024-12-28 02:37:13,961: Snapshot:0	Epoch:138	Loss:18.234	translation_Loss:18.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.24	Hits@10:28.79	Best:13.24
2024-12-28 02:37:20,492: Snapshot:0	Epoch:139	Loss:18.086	translation_Loss:18.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.29	Hits@10:28.86	Best:13.29
2024-12-28 02:37:27,029: Snapshot:0	Epoch:140	Loss:17.934	translation_Loss:17.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.36	Hits@10:29.0	Best:13.36
2024-12-28 02:37:33,551: Snapshot:0	Epoch:141	Loss:17.811	translation_Loss:17.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.42	Hits@10:29.15	Best:13.42
2024-12-28 02:37:40,081: Snapshot:0	Epoch:142	Loss:17.639	translation_Loss:17.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.48	Hits@10:29.28	Best:13.48
2024-12-28 02:37:46,639: Snapshot:0	Epoch:143	Loss:17.472	translation_Loss:17.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.54	Hits@10:29.44	Best:13.54
2024-12-28 02:37:53,166: Snapshot:0	Epoch:144	Loss:17.333	translation_Loss:17.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.6	Hits@10:29.56	Best:13.6
2024-12-28 02:37:59,667: Snapshot:0	Epoch:145	Loss:17.19	translation_Loss:17.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.67	Hits@10:29.68	Best:13.67
2024-12-28 02:38:06,184: Snapshot:0	Epoch:146	Loss:17.068	translation_Loss:17.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.72	Hits@10:29.82	Best:13.72
2024-12-28 02:38:12,705: Snapshot:0	Epoch:147	Loss:16.9	translation_Loss:16.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.78	Hits@10:29.92	Best:13.78
2024-12-28 02:38:19,270: Snapshot:0	Epoch:148	Loss:16.737	translation_Loss:16.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.85	Hits@10:30.06	Best:13.85
2024-12-28 02:38:25,813: Snapshot:0	Epoch:149	Loss:16.595	translation_Loss:16.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.91	Hits@10:30.12	Best:13.91
2024-12-28 02:38:32,330: Snapshot:0	Epoch:150	Loss:16.431	translation_Loss:16.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.98	Hits@10:30.21	Best:13.98
2024-12-28 02:38:38,842: Snapshot:0	Epoch:151	Loss:16.304	translation_Loss:16.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.03	Hits@10:30.29	Best:14.03
2024-12-28 02:38:45,396: Snapshot:0	Epoch:152	Loss:16.173	translation_Loss:16.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.09	Hits@10:30.38	Best:14.09
2024-12-28 02:38:51,927: Snapshot:0	Epoch:153	Loss:16.047	translation_Loss:16.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.16	Hits@10:30.48	Best:14.16
2024-12-28 02:38:58,953: Snapshot:0	Epoch:154	Loss:15.897	translation_Loss:15.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.22	Hits@10:30.57	Best:14.22
2024-12-28 02:39:05,467: Snapshot:0	Epoch:155	Loss:15.777	translation_Loss:15.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.27	Hits@10:30.66	Best:14.27
2024-12-28 02:39:12,000: Snapshot:0	Epoch:156	Loss:15.611	translation_Loss:15.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.33	Hits@10:30.74	Best:14.33
2024-12-28 02:39:18,547: Snapshot:0	Epoch:157	Loss:15.505	translation_Loss:15.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.38	Hits@10:30.85	Best:14.38
2024-12-28 02:39:25,130: Snapshot:0	Epoch:158	Loss:15.339	translation_Loss:15.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.44	Hits@10:30.93	Best:14.44
2024-12-28 02:39:31,683: Snapshot:0	Epoch:159	Loss:15.228	translation_Loss:15.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.51	Hits@10:31.0	Best:14.51
2024-12-28 02:39:38,279: Snapshot:0	Epoch:160	Loss:15.057	translation_Loss:15.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.56	Hits@10:31.12	Best:14.56
2024-12-28 02:39:44,850: Snapshot:0	Epoch:161	Loss:14.953	translation_Loss:14.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.63	Hits@10:31.2	Best:14.63
2024-12-28 02:39:51,421: Snapshot:0	Epoch:162	Loss:14.824	translation_Loss:14.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.68	Hits@10:31.34	Best:14.68
2024-12-28 02:39:57,969: Snapshot:0	Epoch:163	Loss:14.703	translation_Loss:14.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.72	Hits@10:31.44	Best:14.72
2024-12-28 02:40:05,111: Snapshot:0	Epoch:164	Loss:14.545	translation_Loss:14.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.79	Hits@10:31.54	Best:14.79
2024-12-28 02:40:11,701: Snapshot:0	Epoch:165	Loss:14.445	translation_Loss:14.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.83	Hits@10:31.62	Best:14.83
2024-12-28 02:40:18,309: Snapshot:0	Epoch:166	Loss:14.321	translation_Loss:14.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.9	Hits@10:31.67	Best:14.9
2024-12-28 02:40:24,859: Snapshot:0	Epoch:167	Loss:14.137	translation_Loss:14.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.96	Hits@10:31.78	Best:14.96
2024-12-28 02:40:31,383: Snapshot:0	Epoch:168	Loss:14.1	translation_Loss:14.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.02	Hits@10:31.87	Best:15.02
2024-12-28 02:40:37,900: Snapshot:0	Epoch:169	Loss:13.91	translation_Loss:13.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.08	Hits@10:31.98	Best:15.08
2024-12-28 02:40:44,400: Snapshot:0	Epoch:170	Loss:13.81	translation_Loss:13.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.13	Hits@10:32.02	Best:15.13
2024-12-28 02:40:50,966: Snapshot:0	Epoch:171	Loss:13.669	translation_Loss:13.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.18	Hits@10:32.1	Best:15.18
2024-12-28 02:40:57,563: Snapshot:0	Epoch:172	Loss:13.603	translation_Loss:13.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.25	Hits@10:32.22	Best:15.25
2024-12-28 02:41:04,064: Snapshot:0	Epoch:173	Loss:13.435	translation_Loss:13.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.3	Hits@10:32.31	Best:15.3
2024-12-28 02:41:10,545: Snapshot:0	Epoch:174	Loss:13.345	translation_Loss:13.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.35	Hits@10:32.39	Best:15.35
2024-12-28 02:41:17,115: Snapshot:0	Epoch:175	Loss:13.199	translation_Loss:13.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.41	Hits@10:32.5	Best:15.41
2024-12-28 02:41:23,640: Snapshot:0	Epoch:176	Loss:13.116	translation_Loss:13.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.46	Hits@10:32.58	Best:15.46
2024-12-28 02:41:30,165: Snapshot:0	Epoch:177	Loss:12.97	translation_Loss:12.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.51	Hits@10:32.66	Best:15.51
2024-12-28 02:41:36,691: Snapshot:0	Epoch:178	Loss:12.887	translation_Loss:12.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.57	Hits@10:32.71	Best:15.57
2024-12-28 02:41:43,235: Snapshot:0	Epoch:179	Loss:12.754	translation_Loss:12.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.62	Hits@10:32.8	Best:15.62
2024-12-28 02:41:49,770: Snapshot:0	Epoch:180	Loss:12.623	translation_Loss:12.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.69	Hits@10:32.86	Best:15.69
2024-12-28 02:41:56,287: Snapshot:0	Epoch:181	Loss:12.5	translation_Loss:12.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.75	Hits@10:32.92	Best:15.75
2024-12-28 02:42:02,841: Snapshot:0	Epoch:182	Loss:12.402	translation_Loss:12.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.8	Hits@10:33.01	Best:15.8
2024-12-28 02:42:09,336: Snapshot:0	Epoch:183	Loss:12.3	translation_Loss:12.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.85	Hits@10:33.07	Best:15.85
2024-12-28 02:42:16,396: Snapshot:0	Epoch:184	Loss:12.197	translation_Loss:12.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.9	Hits@10:33.15	Best:15.9
2024-12-28 02:42:22,986: Snapshot:0	Epoch:185	Loss:12.075	translation_Loss:12.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.95	Hits@10:33.19	Best:15.95
2024-12-28 02:42:29,510: Snapshot:0	Epoch:186	Loss:11.948	translation_Loss:11.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.0	Hits@10:33.27	Best:16.0
2024-12-28 02:42:36,037: Snapshot:0	Epoch:187	Loss:11.847	translation_Loss:11.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.06	Hits@10:33.38	Best:16.06
2024-12-28 02:42:42,600: Snapshot:0	Epoch:188	Loss:11.713	translation_Loss:11.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.12	Hits@10:33.45	Best:16.12
2024-12-28 02:42:49,124: Snapshot:0	Epoch:189	Loss:11.649	translation_Loss:11.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.15	Hits@10:33.51	Best:16.15
2024-12-28 02:42:55,684: Snapshot:0	Epoch:190	Loss:11.532	translation_Loss:11.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.18	Hits@10:33.6	Best:16.18
2024-12-28 02:43:02,182: Snapshot:0	Epoch:191	Loss:11.418	translation_Loss:11.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.25	Hits@10:33.69	Best:16.25
2024-12-28 02:43:08,675: Snapshot:0	Epoch:192	Loss:11.302	translation_Loss:11.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.3	Hits@10:33.76	Best:16.3
2024-12-28 02:43:15,212: Snapshot:0	Epoch:193	Loss:11.219	translation_Loss:11.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.35	Hits@10:33.83	Best:16.35
2024-12-28 02:43:22,248: Snapshot:0	Epoch:194	Loss:11.109	translation_Loss:11.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.4	Hits@10:33.89	Best:16.4
2024-12-28 02:43:28,751: Snapshot:0	Epoch:195	Loss:11.002	translation_Loss:11.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.46	Hits@10:33.97	Best:16.46
2024-12-28 02:43:35,293: Snapshot:0	Epoch:196	Loss:10.911	translation_Loss:10.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.52	Hits@10:34.05	Best:16.52
2024-12-28 02:43:41,822: Snapshot:0	Epoch:197	Loss:10.814	translation_Loss:10.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.57	Hits@10:34.13	Best:16.57
2024-12-28 02:43:48,349: Snapshot:0	Epoch:198	Loss:10.673	translation_Loss:10.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.62	Hits@10:34.19	Best:16.62
2024-12-28 02:43:54,899: Snapshot:0	Epoch:199	Loss:10.576	translation_Loss:10.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.67	Hits@10:34.25	Best:16.67
2024-12-28 02:43:55,194: => loading checkpoint './checkpoint/FACTfact_0.00001_1024_5000/0model_best.tar'
2024-12-28 02:43:57,996: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1601 | 0.0609 | 0.2209 | 0.2717 |  0.3316 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 02:44:40,504: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228024405/FACTfact_0.00001_1024_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_1024_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_1024_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 02:44:50,476: Snapshot:0	Epoch:0	Loss:52.432	translation_Loss:52.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 02:44:56,961: Snapshot:0	Epoch:1	Loss:52.062	translation_Loss:52.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 02:45:03,422: Snapshot:0	Epoch:2	Loss:51.695	translation_Loss:51.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 02:45:09,915: Snapshot:0	Epoch:3	Loss:51.346	translation_Loss:51.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 02:45:16,892: Snapshot:0	Epoch:4	Loss:50.994	translation_Loss:50.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-28 02:45:23,344: Snapshot:0	Epoch:5	Loss:50.629	translation_Loss:50.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.41	Hits@10:1.43	Best:1.41
2024-12-28 02:45:29,807: Snapshot:0	Epoch:6	Loss:50.279	translation_Loss:50.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.44	Hits@10:1.5	Best:1.44
2024-12-28 02:45:36,269: Snapshot:0	Epoch:7	Loss:49.921	translation_Loss:49.921	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.47	Hits@10:1.57	Best:1.47
2024-12-28 02:45:42,820: Snapshot:0	Epoch:8	Loss:49.567	translation_Loss:49.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.53	Hits@10:1.72	Best:1.53
2024-12-28 02:45:49,261: Snapshot:0	Epoch:9	Loss:49.224	translation_Loss:49.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.99	Best:1.61
2024-12-28 02:45:55,765: Snapshot:0	Epoch:10	Loss:48.855	translation_Loss:48.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.71	Hits@10:2.32	Best:1.71
2024-12-28 02:46:02,199: Snapshot:0	Epoch:11	Loss:48.502	translation_Loss:48.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.86	Hits@10:2.75	Best:1.86
2024-12-28 02:46:08,689: Snapshot:0	Epoch:12	Loss:48.148	translation_Loss:48.148	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.04	Hits@10:3.23	Best:2.04
2024-12-28 02:46:15,154: Snapshot:0	Epoch:13	Loss:47.8	translation_Loss:47.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.23	Hits@10:3.74	Best:2.23
2024-12-28 02:46:21,699: Snapshot:0	Epoch:14	Loss:47.46	translation_Loss:47.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.44	Hits@10:4.15	Best:2.44
2024-12-28 02:46:28,144: Snapshot:0	Epoch:15	Loss:47.128	translation_Loss:47.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.65	Hits@10:4.53	Best:2.65
2024-12-28 02:46:34,624: Snapshot:0	Epoch:16	Loss:46.788	translation_Loss:46.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.84	Hits@10:4.86	Best:2.84
2024-12-28 02:46:41,083: Snapshot:0	Epoch:17	Loss:46.447	translation_Loss:46.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.02	Hits@10:5.24	Best:3.02
2024-12-28 02:46:47,548: Snapshot:0	Epoch:18	Loss:46.099	translation_Loss:46.099	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.17	Hits@10:5.55	Best:3.17
2024-12-28 02:46:54,001: Snapshot:0	Epoch:19	Loss:45.768	translation_Loss:45.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.32	Hits@10:5.9	Best:3.32
2024-12-28 02:47:00,454: Snapshot:0	Epoch:20	Loss:45.435	translation_Loss:45.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.45	Hits@10:6.16	Best:3.45
2024-12-28 02:47:06,920: Snapshot:0	Epoch:21	Loss:45.1	translation_Loss:45.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.57	Hits@10:6.41	Best:3.57
2024-12-28 02:47:13,392: Snapshot:0	Epoch:22	Loss:44.79	translation_Loss:44.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.7	Hits@10:6.66	Best:3.7
2024-12-28 02:47:19,915: Snapshot:0	Epoch:23	Loss:44.447	translation_Loss:44.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.81	Hits@10:6.97	Best:3.81
2024-12-28 02:47:26,938: Snapshot:0	Epoch:24	Loss:44.138	translation_Loss:44.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.93	Hits@10:7.21	Best:3.93
2024-12-28 02:47:33,388: Snapshot:0	Epoch:25	Loss:43.829	translation_Loss:43.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.04	Hits@10:7.42	Best:4.04
2024-12-28 02:47:39,888: Snapshot:0	Epoch:26	Loss:43.507	translation_Loss:43.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.15	Hits@10:7.68	Best:4.15
2024-12-28 02:47:46,402: Snapshot:0	Epoch:27	Loss:43.186	translation_Loss:43.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.26	Hits@10:7.96	Best:4.26
2024-12-28 02:47:52,871: Snapshot:0	Epoch:28	Loss:42.876	translation_Loss:42.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.36	Hits@10:8.18	Best:4.36
2024-12-28 02:47:59,335: Snapshot:0	Epoch:29	Loss:42.562	translation_Loss:42.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.46	Hits@10:8.41	Best:4.46
2024-12-28 02:48:05,804: Snapshot:0	Epoch:30	Loss:42.269	translation_Loss:42.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.55	Hits@10:8.68	Best:4.55
2024-12-28 02:48:12,278: Snapshot:0	Epoch:31	Loss:41.959	translation_Loss:41.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.65	Hits@10:8.91	Best:4.65
2024-12-28 02:48:18,782: Snapshot:0	Epoch:32	Loss:41.679	translation_Loss:41.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.74	Hits@10:9.13	Best:4.74
2024-12-28 02:48:25,274: Snapshot:0	Epoch:33	Loss:41.35	translation_Loss:41.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.33	Best:4.83
2024-12-28 02:48:31,750: Snapshot:0	Epoch:34	Loss:41.061	translation_Loss:41.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.93	Hits@10:9.57	Best:4.93
2024-12-28 02:48:38,739: Snapshot:0	Epoch:35	Loss:40.768	translation_Loss:40.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.01	Hits@10:9.77	Best:5.01
2024-12-28 02:48:45,215: Snapshot:0	Epoch:36	Loss:40.464	translation_Loss:40.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.09	Hits@10:10.02	Best:5.09
2024-12-28 02:48:51,678: Snapshot:0	Epoch:37	Loss:40.192	translation_Loss:40.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.17	Hits@10:10.22	Best:5.17
2024-12-28 02:48:58,193: Snapshot:0	Epoch:38	Loss:39.894	translation_Loss:39.894	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.25	Hits@10:10.44	Best:5.25
2024-12-28 02:49:04,666: Snapshot:0	Epoch:39	Loss:39.604	translation_Loss:39.604	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.34	Hits@10:10.61	Best:5.34
2024-12-28 02:49:11,168: Snapshot:0	Epoch:40	Loss:39.342	translation_Loss:39.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.42	Hits@10:10.77	Best:5.42
2024-12-28 02:49:17,700: Snapshot:0	Epoch:41	Loss:39.037	translation_Loss:39.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.5	Hits@10:10.95	Best:5.5
2024-12-28 02:49:24,214: Snapshot:0	Epoch:42	Loss:38.764	translation_Loss:38.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.57	Hits@10:11.16	Best:5.57
2024-12-28 02:49:30,678: Snapshot:0	Epoch:43	Loss:38.476	translation_Loss:38.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:11.38	Best:5.66
2024-12-28 02:49:37,159: Snapshot:0	Epoch:44	Loss:38.208	translation_Loss:38.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.74	Hits@10:11.6	Best:5.74
2024-12-28 02:49:43,706: Snapshot:0	Epoch:45	Loss:37.931	translation_Loss:37.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.82	Hits@10:11.81	Best:5.82
2024-12-28 02:49:50,241: Snapshot:0	Epoch:46	Loss:37.638	translation_Loss:37.638	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.9	Hits@10:12.01	Best:5.9
2024-12-28 02:49:56,731: Snapshot:0	Epoch:47	Loss:37.369	translation_Loss:37.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.98	Hits@10:12.31	Best:5.98
2024-12-28 02:50:03,238: Snapshot:0	Epoch:48	Loss:37.113	translation_Loss:37.113	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.06	Hits@10:12.61	Best:6.06
2024-12-28 02:50:09,839: Snapshot:0	Epoch:49	Loss:36.841	translation_Loss:36.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.15	Hits@10:12.9	Best:6.15
2024-12-28 02:50:16,336: Snapshot:0	Epoch:50	Loss:36.571	translation_Loss:36.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.19	Best:6.23
2024-12-28 02:50:22,823: Snapshot:0	Epoch:51	Loss:36.292	translation_Loss:36.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.31	Hits@10:13.37	Best:6.31
2024-12-28 02:50:29,372: Snapshot:0	Epoch:52	Loss:36.059	translation_Loss:36.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.39	Hits@10:13.61	Best:6.39
2024-12-28 02:50:35,862: Snapshot:0	Epoch:53	Loss:35.778	translation_Loss:35.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.48	Hits@10:13.84	Best:6.48
2024-12-28 02:50:42,827: Snapshot:0	Epoch:54	Loss:35.507	translation_Loss:35.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.57	Hits@10:14.06	Best:6.57
2024-12-28 02:50:49,309: Snapshot:0	Epoch:55	Loss:35.249	translation_Loss:35.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.66	Hits@10:14.28	Best:6.66
2024-12-28 02:50:55,837: Snapshot:0	Epoch:56	Loss:35.006	translation_Loss:35.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.74	Hits@10:14.5	Best:6.74
2024-12-28 02:51:02,341: Snapshot:0	Epoch:57	Loss:34.741	translation_Loss:34.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.82	Hits@10:14.73	Best:6.82
2024-12-28 02:51:08,822: Snapshot:0	Epoch:58	Loss:34.484	translation_Loss:34.484	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.91	Hits@10:14.95	Best:6.91
2024-12-28 02:51:15,375: Snapshot:0	Epoch:59	Loss:34.237	translation_Loss:34.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.0	Hits@10:15.16	Best:7.0
2024-12-28 02:51:21,880: Snapshot:0	Epoch:60	Loss:33.958	translation_Loss:33.958	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.08	Hits@10:15.38	Best:7.08
2024-12-28 02:51:28,353: Snapshot:0	Epoch:61	Loss:33.733	translation_Loss:33.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.16	Hits@10:15.63	Best:7.16
2024-12-28 02:51:34,860: Snapshot:0	Epoch:62	Loss:33.473	translation_Loss:33.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.23	Hits@10:15.8	Best:7.23
2024-12-28 02:51:41,376: Snapshot:0	Epoch:63	Loss:33.231	translation_Loss:33.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.31	Hits@10:16.01	Best:7.31
2024-12-28 02:51:48,403: Snapshot:0	Epoch:64	Loss:32.965	translation_Loss:32.965	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.39	Hits@10:16.15	Best:7.39
2024-12-28 02:51:54,879: Snapshot:0	Epoch:65	Loss:32.738	translation_Loss:32.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.47	Hits@10:16.34	Best:7.47
2024-12-28 02:52:01,361: Snapshot:0	Epoch:66	Loss:32.499	translation_Loss:32.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.54	Hits@10:16.57	Best:7.54
2024-12-28 02:52:07,850: Snapshot:0	Epoch:67	Loss:32.266	translation_Loss:32.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.62	Hits@10:16.77	Best:7.62
2024-12-28 02:52:14,419: Snapshot:0	Epoch:68	Loss:32.02	translation_Loss:32.02	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.69	Hits@10:17.01	Best:7.69
2024-12-28 02:52:20,926: Snapshot:0	Epoch:69	Loss:31.733	translation_Loss:31.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.77	Hits@10:17.23	Best:7.77
2024-12-28 02:52:27,472: Snapshot:0	Epoch:70	Loss:31.512	translation_Loss:31.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.85	Hits@10:17.48	Best:7.85
2024-12-28 02:52:33,930: Snapshot:0	Epoch:71	Loss:31.288	translation_Loss:31.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.94	Hits@10:17.7	Best:7.94
2024-12-28 02:52:40,439: Snapshot:0	Epoch:72	Loss:31.039	translation_Loss:31.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.01	Hits@10:17.9	Best:8.01
2024-12-28 02:52:46,990: Snapshot:0	Epoch:73	Loss:30.808	translation_Loss:30.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.1	Hits@10:18.1	Best:8.1
2024-12-28 02:52:53,459: Snapshot:0	Epoch:74	Loss:30.581	translation_Loss:30.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.18	Hits@10:18.28	Best:8.18
2024-12-28 02:52:59,912: Snapshot:0	Epoch:75	Loss:30.376	translation_Loss:30.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.28	Hits@10:18.5	Best:8.28
2024-12-28 02:53:06,376: Snapshot:0	Epoch:76	Loss:30.127	translation_Loss:30.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.37	Hits@10:18.72	Best:8.37
2024-12-28 02:53:12,843: Snapshot:0	Epoch:77	Loss:29.899	translation_Loss:29.899	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.47	Hits@10:18.93	Best:8.47
2024-12-28 02:53:19,317: Snapshot:0	Epoch:78	Loss:29.646	translation_Loss:29.646	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.55	Hits@10:19.16	Best:8.55
2024-12-28 02:53:25,848: Snapshot:0	Epoch:79	Loss:29.455	translation_Loss:29.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.64	Hits@10:19.36	Best:8.64
2024-12-28 02:53:32,384: Snapshot:0	Epoch:80	Loss:29.214	translation_Loss:29.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.74	Hits@10:19.58	Best:8.74
2024-12-28 02:53:38,888: Snapshot:0	Epoch:81	Loss:28.989	translation_Loss:28.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.82	Hits@10:19.81	Best:8.82
2024-12-28 02:53:45,423: Snapshot:0	Epoch:82	Loss:28.772	translation_Loss:28.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.9	Hits@10:19.97	Best:8.9
2024-12-28 02:53:51,891: Snapshot:0	Epoch:83	Loss:28.528	translation_Loss:28.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.0	Hits@10:20.17	Best:9.0
2024-12-28 02:53:58,940: Snapshot:0	Epoch:84	Loss:28.319	translation_Loss:28.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.08	Hits@10:20.37	Best:9.08
2024-12-28 02:54:05,517: Snapshot:0	Epoch:85	Loss:28.127	translation_Loss:28.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.17	Hits@10:20.59	Best:9.17
2024-12-28 02:54:12,009: Snapshot:0	Epoch:86	Loss:27.889	translation_Loss:27.889	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.27	Hits@10:20.75	Best:9.27
2024-12-28 02:54:18,486: Snapshot:0	Epoch:87	Loss:27.68	translation_Loss:27.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.34	Hits@10:20.95	Best:9.34
2024-12-28 02:54:25,019: Snapshot:0	Epoch:88	Loss:27.445	translation_Loss:27.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.43	Hits@10:21.22	Best:9.43
2024-12-28 02:54:31,504: Snapshot:0	Epoch:89	Loss:27.241	translation_Loss:27.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.52	Hits@10:21.41	Best:9.52
2024-12-28 02:54:38,081: Snapshot:0	Epoch:90	Loss:27.038	translation_Loss:27.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.6	Hits@10:21.65	Best:9.6
2024-12-28 02:54:44,613: Snapshot:0	Epoch:91	Loss:26.807	translation_Loss:26.807	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:21.84	Best:9.68
2024-12-28 02:54:51,120: Snapshot:0	Epoch:92	Loss:26.611	translation_Loss:26.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.78	Hits@10:22.02	Best:9.78
2024-12-28 02:54:57,627: Snapshot:0	Epoch:93	Loss:26.378	translation_Loss:26.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.89	Hits@10:22.26	Best:9.89
2024-12-28 02:55:04,688: Snapshot:0	Epoch:94	Loss:26.202	translation_Loss:26.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.97	Hits@10:22.41	Best:9.97
2024-12-28 02:55:11,186: Snapshot:0	Epoch:95	Loss:25.978	translation_Loss:25.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.05	Hits@10:22.58	Best:10.05
2024-12-28 02:55:17,688: Snapshot:0	Epoch:96	Loss:25.788	translation_Loss:25.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.14	Hits@10:22.74	Best:10.14
2024-12-28 02:55:24,175: Snapshot:0	Epoch:97	Loss:25.553	translation_Loss:25.553	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.21	Hits@10:22.94	Best:10.21
2024-12-28 02:55:30,689: Snapshot:0	Epoch:98	Loss:25.371	translation_Loss:25.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:23.1	Best:10.29
2024-12-28 02:55:37,174: Snapshot:0	Epoch:99	Loss:25.186	translation_Loss:25.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.37	Hits@10:23.29	Best:10.37
2024-12-28 02:55:43,774: Snapshot:0	Epoch:100	Loss:24.941	translation_Loss:24.941	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.45	Hits@10:23.41	Best:10.45
2024-12-28 02:55:50,295: Snapshot:0	Epoch:101	Loss:24.764	translation_Loss:24.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.55	Hits@10:23.56	Best:10.55
2024-12-28 02:55:56,773: Snapshot:0	Epoch:102	Loss:24.564	translation_Loss:24.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.64	Hits@10:23.72	Best:10.64
2024-12-28 02:56:03,241: Snapshot:0	Epoch:103	Loss:24.381	translation_Loss:24.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.72	Hits@10:23.9	Best:10.72
2024-12-28 02:56:09,760: Snapshot:0	Epoch:104	Loss:24.165	translation_Loss:24.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.81	Hits@10:24.07	Best:10.81
2024-12-28 02:56:16,274: Snapshot:0	Epoch:105	Loss:23.974	translation_Loss:23.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.89	Hits@10:24.21	Best:10.89
2024-12-28 02:56:22,774: Snapshot:0	Epoch:106	Loss:23.775	translation_Loss:23.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.98	Hits@10:24.36	Best:10.98
2024-12-28 02:56:29,257: Snapshot:0	Epoch:107	Loss:23.617	translation_Loss:23.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.07	Hits@10:24.55	Best:11.07
2024-12-28 02:56:35,772: Snapshot:0	Epoch:108	Loss:23.407	translation_Loss:23.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.15	Hits@10:24.73	Best:11.15
2024-12-28 02:56:42,227: Snapshot:0	Epoch:109	Loss:23.225	translation_Loss:23.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.22	Hits@10:24.9	Best:11.22
2024-12-28 02:56:48,742: Snapshot:0	Epoch:110	Loss:23.015	translation_Loss:23.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.31	Hits@10:25.06	Best:11.31
2024-12-28 02:56:55,220: Snapshot:0	Epoch:111	Loss:22.868	translation_Loss:22.868	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.38	Hits@10:25.2	Best:11.38
2024-12-28 02:57:01,763: Snapshot:0	Epoch:112	Loss:22.634	translation_Loss:22.634	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.45	Hits@10:25.33	Best:11.45
2024-12-28 02:57:08,241: Snapshot:0	Epoch:113	Loss:22.471	translation_Loss:22.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.53	Hits@10:25.53	Best:11.53
2024-12-28 02:57:15,259: Snapshot:0	Epoch:114	Loss:22.292	translation_Loss:22.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.58	Hits@10:25.71	Best:11.58
2024-12-28 02:57:21,754: Snapshot:0	Epoch:115	Loss:22.086	translation_Loss:22.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.67	Hits@10:25.85	Best:11.67
2024-12-28 02:57:28,273: Snapshot:0	Epoch:116	Loss:21.953	translation_Loss:21.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.74	Hits@10:25.99	Best:11.74
2024-12-28 02:57:34,832: Snapshot:0	Epoch:117	Loss:21.739	translation_Loss:21.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.81	Hits@10:26.13	Best:11.81
2024-12-28 02:57:41,348: Snapshot:0	Epoch:118	Loss:21.573	translation_Loss:21.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.87	Hits@10:26.28	Best:11.87
2024-12-28 02:57:47,859: Snapshot:0	Epoch:119	Loss:21.408	translation_Loss:21.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.94	Hits@10:26.42	Best:11.94
2024-12-28 02:57:54,350: Snapshot:0	Epoch:120	Loss:21.219	translation_Loss:21.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.02	Hits@10:26.58	Best:12.02
2024-12-28 02:58:00,853: Snapshot:0	Epoch:121	Loss:21.038	translation_Loss:21.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.1	Hits@10:26.72	Best:12.1
2024-12-28 02:58:07,358: Snapshot:0	Epoch:122	Loss:20.826	translation_Loss:20.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.17	Hits@10:26.88	Best:12.17
2024-12-28 02:58:13,892: Snapshot:0	Epoch:123	Loss:20.711	translation_Loss:20.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.24	Hits@10:26.99	Best:12.24
2024-12-28 02:58:20,910: Snapshot:0	Epoch:124	Loss:20.524	translation_Loss:20.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.14	Best:12.31
2024-12-28 02:58:27,431: Snapshot:0	Epoch:125	Loss:20.325	translation_Loss:20.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.38	Hits@10:27.23	Best:12.38
2024-12-28 02:58:33,930: Snapshot:0	Epoch:126	Loss:20.157	translation_Loss:20.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.45	Hits@10:27.38	Best:12.45
2024-12-28 02:58:40,408: Snapshot:0	Epoch:127	Loss:20.01	translation_Loss:20.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.52	Hits@10:27.52	Best:12.52
2024-12-28 02:58:46,946: Snapshot:0	Epoch:128	Loss:19.836	translation_Loss:19.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.59	Hits@10:27.66	Best:12.59
2024-12-28 02:58:53,413: Snapshot:0	Epoch:129	Loss:19.665	translation_Loss:19.665	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.66	Hits@10:27.75	Best:12.66
2024-12-28 02:58:59,905: Snapshot:0	Epoch:130	Loss:19.516	translation_Loss:19.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.72	Hits@10:27.9	Best:12.72
2024-12-28 02:59:06,365: Snapshot:0	Epoch:131	Loss:19.366	translation_Loss:19.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.79	Hits@10:28.05	Best:12.79
2024-12-28 02:59:12,881: Snapshot:0	Epoch:132	Loss:19.151	translation_Loss:19.151	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.85	Hits@10:28.15	Best:12.85
2024-12-28 02:59:19,375: Snapshot:0	Epoch:133	Loss:19.032	translation_Loss:19.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.92	Hits@10:28.23	Best:12.92
2024-12-28 02:59:25,864: Snapshot:0	Epoch:134	Loss:18.843	translation_Loss:18.843	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.98	Hits@10:28.34	Best:12.98
2024-12-28 02:59:32,886: Snapshot:0	Epoch:135	Loss:18.701	translation_Loss:18.701	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.05	Hits@10:28.41	Best:13.05
2024-12-28 02:59:39,397: Snapshot:0	Epoch:136	Loss:18.533	translation_Loss:18.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.12	Hits@10:28.56	Best:13.12
2024-12-28 02:59:45,921: Snapshot:0	Epoch:137	Loss:18.391	translation_Loss:18.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.18	Hits@10:28.68	Best:13.18
2024-12-28 02:59:52,415: Snapshot:0	Epoch:138	Loss:18.234	translation_Loss:18.234	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.24	Hits@10:28.77	Best:13.24
2024-12-28 02:59:58,945: Snapshot:0	Epoch:139	Loss:18.086	translation_Loss:18.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.3	Hits@10:28.87	Best:13.3
2024-12-28 03:00:05,473: Snapshot:0	Epoch:140	Loss:17.934	translation_Loss:17.934	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.37	Hits@10:29.03	Best:13.37
2024-12-28 03:00:12,020: Snapshot:0	Epoch:141	Loss:17.811	translation_Loss:17.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.42	Hits@10:29.16	Best:13.42
2024-12-28 03:00:18,498: Snapshot:0	Epoch:142	Loss:17.639	translation_Loss:17.639	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.48	Hits@10:29.28	Best:13.48
2024-12-28 03:00:25,006: Snapshot:0	Epoch:143	Loss:17.472	translation_Loss:17.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.54	Hits@10:29.44	Best:13.54
2024-12-28 03:00:31,509: Snapshot:0	Epoch:144	Loss:17.333	translation_Loss:17.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.61	Hits@10:29.56	Best:13.61
2024-12-28 03:00:38,013: Snapshot:0	Epoch:145	Loss:17.19	translation_Loss:17.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.66	Hits@10:29.68	Best:13.66
2024-12-28 03:00:44,512: Snapshot:0	Epoch:146	Loss:17.068	translation_Loss:17.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.72	Hits@10:29.81	Best:13.72
2024-12-28 03:00:51,018: Snapshot:0	Epoch:147	Loss:16.901	translation_Loss:16.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.78	Hits@10:29.91	Best:13.78
2024-12-28 03:00:57,563: Snapshot:0	Epoch:148	Loss:16.737	translation_Loss:16.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.85	Hits@10:30.06	Best:13.85
2024-12-28 03:01:04,076: Snapshot:0	Epoch:149	Loss:16.595	translation_Loss:16.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.91	Hits@10:30.13	Best:13.91
2024-12-28 03:01:10,593: Snapshot:0	Epoch:150	Loss:16.431	translation_Loss:16.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.98	Hits@10:30.2	Best:13.98
2024-12-28 03:01:17,121: Snapshot:0	Epoch:151	Loss:16.304	translation_Loss:16.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.03	Hits@10:30.29	Best:14.03
2024-12-28 03:01:23,636: Snapshot:0	Epoch:152	Loss:16.173	translation_Loss:16.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.09	Hits@10:30.38	Best:14.09
2024-12-28 03:01:30,103: Snapshot:0	Epoch:153	Loss:16.047	translation_Loss:16.047	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.16	Hits@10:30.48	Best:14.16
2024-12-28 03:01:37,147: Snapshot:0	Epoch:154	Loss:15.897	translation_Loss:15.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.23	Hits@10:30.58	Best:14.23
2024-12-28 03:01:43,678: Snapshot:0	Epoch:155	Loss:15.777	translation_Loss:15.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.27	Hits@10:30.68	Best:14.27
2024-12-28 03:01:50,197: Snapshot:0	Epoch:156	Loss:15.611	translation_Loss:15.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.33	Hits@10:30.75	Best:14.33
2024-12-28 03:01:56,705: Snapshot:0	Epoch:157	Loss:15.505	translation_Loss:15.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.38	Hits@10:30.86	Best:14.38
2024-12-28 03:02:03,202: Snapshot:0	Epoch:158	Loss:15.339	translation_Loss:15.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.44	Hits@10:30.91	Best:14.44
2024-12-28 03:02:09,677: Snapshot:0	Epoch:159	Loss:15.228	translation_Loss:15.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.5	Hits@10:31.01	Best:14.5
2024-12-28 03:02:16,189: Snapshot:0	Epoch:160	Loss:15.057	translation_Loss:15.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.55	Hits@10:31.12	Best:14.55
2024-12-28 03:02:22,695: Snapshot:0	Epoch:161	Loss:14.953	translation_Loss:14.953	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.62	Hits@10:31.19	Best:14.62
2024-12-28 03:02:29,208: Snapshot:0	Epoch:162	Loss:14.824	translation_Loss:14.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.68	Hits@10:31.34	Best:14.68
2024-12-28 03:02:35,709: Snapshot:0	Epoch:163	Loss:14.703	translation_Loss:14.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.72	Hits@10:31.43	Best:14.72
2024-12-28 03:02:42,706: Snapshot:0	Epoch:164	Loss:14.544	translation_Loss:14.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.78	Hits@10:31.54	Best:14.78
2024-12-28 03:02:49,250: Snapshot:0	Epoch:165	Loss:14.445	translation_Loss:14.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.84	Hits@10:31.62	Best:14.84
2024-12-28 03:02:55,787: Snapshot:0	Epoch:166	Loss:14.321	translation_Loss:14.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.9	Hits@10:31.69	Best:14.9
2024-12-28 03:03:02,292: Snapshot:0	Epoch:167	Loss:14.137	translation_Loss:14.137	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:14.96	Hits@10:31.79	Best:14.96
2024-12-28 03:03:08,787: Snapshot:0	Epoch:168	Loss:14.1	translation_Loss:14.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.01	Hits@10:31.88	Best:15.01
2024-12-28 03:03:15,277: Snapshot:0	Epoch:169	Loss:13.91	translation_Loss:13.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.08	Hits@10:31.97	Best:15.08
2024-12-28 03:03:21,783: Snapshot:0	Epoch:170	Loss:13.81	translation_Loss:13.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.13	Hits@10:32.02	Best:15.13
2024-12-28 03:03:28,310: Snapshot:0	Epoch:171	Loss:13.669	translation_Loss:13.669	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.19	Hits@10:32.08	Best:15.19
2024-12-28 03:03:34,833: Snapshot:0	Epoch:172	Loss:13.603	translation_Loss:13.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.24	Hits@10:32.2	Best:15.24
2024-12-28 03:03:41,427: Snapshot:0	Epoch:173	Loss:13.435	translation_Loss:13.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.3	Hits@10:32.31	Best:15.3
2024-12-28 03:03:47,946: Snapshot:0	Epoch:174	Loss:13.345	translation_Loss:13.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.36	Hits@10:32.39	Best:15.36
2024-12-28 03:03:54,418: Snapshot:0	Epoch:175	Loss:13.199	translation_Loss:13.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.4	Hits@10:32.51	Best:15.4
2024-12-28 03:04:00,928: Snapshot:0	Epoch:176	Loss:13.116	translation_Loss:13.116	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.46	Hits@10:32.58	Best:15.46
2024-12-28 03:04:07,419: Snapshot:0	Epoch:177	Loss:12.97	translation_Loss:12.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.51	Hits@10:32.66	Best:15.51
2024-12-28 03:04:13,914: Snapshot:0	Epoch:178	Loss:12.887	translation_Loss:12.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.56	Hits@10:32.72	Best:15.56
2024-12-28 03:04:20,389: Snapshot:0	Epoch:179	Loss:12.754	translation_Loss:12.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.62	Hits@10:32.8	Best:15.62
2024-12-28 03:04:26,937: Snapshot:0	Epoch:180	Loss:12.623	translation_Loss:12.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.69	Hits@10:32.86	Best:15.69
2024-12-28 03:04:33,444: Snapshot:0	Epoch:181	Loss:12.5	translation_Loss:12.5	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.75	Hits@10:32.93	Best:15.75
2024-12-28 03:04:39,979: Snapshot:0	Epoch:182	Loss:12.402	translation_Loss:12.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.81	Hits@10:33.0	Best:15.81
2024-12-28 03:04:46,500: Snapshot:0	Epoch:183	Loss:12.3	translation_Loss:12.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.85	Hits@10:33.06	Best:15.85
2024-12-28 03:04:53,512: Snapshot:0	Epoch:184	Loss:12.197	translation_Loss:12.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.91	Hits@10:33.15	Best:15.91
2024-12-28 03:05:00,013: Snapshot:0	Epoch:185	Loss:12.075	translation_Loss:12.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.96	Hits@10:33.18	Best:15.96
2024-12-28 03:05:06,598: Snapshot:0	Epoch:186	Loss:11.948	translation_Loss:11.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.0	Hits@10:33.28	Best:16.0
2024-12-28 03:05:13,233: Snapshot:0	Epoch:187	Loss:11.847	translation_Loss:11.847	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.06	Hits@10:33.39	Best:16.06
2024-12-28 03:05:19,795: Snapshot:0	Epoch:188	Loss:11.713	translation_Loss:11.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.11	Hits@10:33.45	Best:16.11
2024-12-28 03:05:26,306: Snapshot:0	Epoch:189	Loss:11.649	translation_Loss:11.649	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.15	Hits@10:33.5	Best:16.15
2024-12-28 03:05:32,833: Snapshot:0	Epoch:190	Loss:11.532	translation_Loss:11.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.18	Hits@10:33.59	Best:16.18
2024-12-28 03:05:39,412: Snapshot:0	Epoch:191	Loss:11.418	translation_Loss:11.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.25	Hits@10:33.68	Best:16.25
2024-12-28 03:05:46,024: Snapshot:0	Epoch:192	Loss:11.302	translation_Loss:11.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.3	Hits@10:33.76	Best:16.3
2024-12-28 03:05:52,505: Snapshot:0	Epoch:193	Loss:11.218	translation_Loss:11.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.35	Hits@10:33.83	Best:16.35
2024-12-28 03:05:59,578: Snapshot:0	Epoch:194	Loss:11.109	translation_Loss:11.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.4	Hits@10:33.89	Best:16.4
2024-12-28 03:06:06,099: Snapshot:0	Epoch:195	Loss:11.002	translation_Loss:11.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.45	Hits@10:33.97	Best:16.45
2024-12-28 03:06:12,595: Snapshot:0	Epoch:196	Loss:10.911	translation_Loss:10.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.52	Hits@10:34.06	Best:16.52
2024-12-28 03:06:19,093: Snapshot:0	Epoch:197	Loss:10.814	translation_Loss:10.814	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.56	Hits@10:34.12	Best:16.56
2024-12-28 03:06:25,580: Snapshot:0	Epoch:198	Loss:10.672	translation_Loss:10.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.61	Hits@10:34.17	Best:16.61
2024-12-28 03:06:32,066: Snapshot:0	Epoch:199	Loss:10.575	translation_Loss:10.575	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.67	Hits@10:34.25	Best:16.67
2024-12-28 03:06:32,386: => loading checkpoint './checkpoint/FACTfact_0.00001_1024_10000/0model_best.tar'
2024-12-28 03:06:35,191: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1601 | 0.0609 | 0.2208 | 0.2715 |  0.3316 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 03:07:17,480: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228030642/FACTfact_0.00001_2048_1000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_2048_1000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_2048_1000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=1000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 03:07:27,365: Snapshot:0	Epoch:0	Loss:26.587	translation_Loss:26.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 03:07:33,684: Snapshot:0	Epoch:1	Loss:26.448	translation_Loss:26.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 03:07:40,439: Snapshot:0	Epoch:2	Loss:26.318	translation_Loss:26.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 03:07:46,774: Snapshot:0	Epoch:3	Loss:26.197	translation_Loss:26.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 03:07:53,087: Snapshot:0	Epoch:4	Loss:26.076	translation_Loss:26.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.37	Best:1.38
2024-12-28 03:07:59,401: Snapshot:0	Epoch:5	Loss:25.946	translation_Loss:25.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 03:08:05,765: Snapshot:0	Epoch:6	Loss:25.826	translation_Loss:25.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 03:08:12,586: Snapshot:0	Epoch:7	Loss:25.7	translation_Loss:25.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-28 03:08:18,980: Snapshot:0	Epoch:8	Loss:25.577	translation_Loss:25.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.41	Hits@10:1.41	Best:1.41
2024-12-28 03:08:25,316: Snapshot:0	Epoch:9	Loss:25.458	translation_Loss:25.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.44	Best:1.42
2024-12-28 03:08:31,668: Snapshot:0	Epoch:10	Loss:25.326	translation_Loss:25.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.43	Hits@10:1.47	Best:1.43
2024-12-28 03:08:38,028: Snapshot:0	Epoch:11	Loss:25.201	translation_Loss:25.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.44	Hits@10:1.51	Best:1.44
2024-12-28 03:08:44,425: Snapshot:0	Epoch:12	Loss:25.075	translation_Loss:25.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.46	Hits@10:1.57	Best:1.46
2024-12-28 03:08:51,277: Snapshot:0	Epoch:13	Loss:24.951	translation_Loss:24.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.49	Hits@10:1.6	Best:1.49
2024-12-28 03:08:57,613: Snapshot:0	Epoch:14	Loss:24.829	translation_Loss:24.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.52	Hits@10:1.7	Best:1.52
2024-12-28 03:09:03,963: Snapshot:0	Epoch:15	Loss:24.712	translation_Loss:24.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.56	Hits@10:1.83	Best:1.56
2024-12-28 03:09:10,315: Snapshot:0	Epoch:16	Loss:24.591	translation_Loss:24.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-28 03:09:16,671: Snapshot:0	Epoch:17	Loss:24.466	translation_Loss:24.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.66	Hits@10:2.15	Best:1.66
2024-12-28 03:09:23,423: Snapshot:0	Epoch:18	Loss:24.34	translation_Loss:24.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.73	Hits@10:2.36	Best:1.73
2024-12-28 03:09:29,821: Snapshot:0	Epoch:19	Loss:24.219	translation_Loss:24.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.81	Hits@10:2.6	Best:1.81
2024-12-28 03:09:36,149: Snapshot:0	Epoch:20	Loss:24.098	translation_Loss:24.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.9	Hits@10:2.86	Best:1.9
2024-12-28 03:09:42,538: Snapshot:0	Epoch:21	Loss:23.974	translation_Loss:23.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.01	Hits@10:3.15	Best:2.01
2024-12-28 03:09:48,941: Snapshot:0	Epoch:22	Loss:23.86	translation_Loss:23.86	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.12	Hits@10:3.47	Best:2.12
2024-12-28 03:09:55,288: Snapshot:0	Epoch:23	Loss:23.732	translation_Loss:23.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.23	Hits@10:3.76	Best:2.23
2024-12-28 03:10:02,160: Snapshot:0	Epoch:24	Loss:23.618	translation_Loss:23.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.36	Hits@10:4.03	Best:2.36
2024-12-28 03:10:08,601: Snapshot:0	Epoch:25	Loss:23.502	translation_Loss:23.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.49	Hits@10:4.31	Best:2.49
2024-12-28 03:10:14,951: Snapshot:0	Epoch:26	Loss:23.383	translation_Loss:23.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.62	Hits@10:4.53	Best:2.62
2024-12-28 03:10:21,300: Snapshot:0	Epoch:27	Loss:23.259	translation_Loss:23.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.75	Hits@10:4.78	Best:2.75
2024-12-28 03:10:27,691: Snapshot:0	Epoch:28	Loss:23.141	translation_Loss:23.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.86	Hits@10:5.02	Best:2.86
2024-12-28 03:10:34,460: Snapshot:0	Epoch:29	Loss:23.023	translation_Loss:23.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.3	Best:2.98
2024-12-28 03:10:40,896: Snapshot:0	Epoch:30	Loss:22.912	translation_Loss:22.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.09	Hits@10:5.54	Best:3.09
2024-12-28 03:10:47,311: Snapshot:0	Epoch:31	Loss:22.79	translation_Loss:22.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.21	Hits@10:5.82	Best:3.21
2024-12-28 03:10:53,654: Snapshot:0	Epoch:32	Loss:22.686	translation_Loss:22.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.31	Hits@10:6.01	Best:3.31
2024-12-28 03:11:00,021: Snapshot:0	Epoch:33	Loss:22.556	translation_Loss:22.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.41	Hits@10:6.22	Best:3.41
2024-12-28 03:11:06,347: Snapshot:0	Epoch:34	Loss:22.444	translation_Loss:22.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.52	Hits@10:6.45	Best:3.52
2024-12-28 03:11:13,200: Snapshot:0	Epoch:35	Loss:22.331	translation_Loss:22.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.61	Hits@10:6.6	Best:3.61
2024-12-28 03:11:19,605: Snapshot:0	Epoch:36	Loss:22.211	translation_Loss:22.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.69	Hits@10:6.76	Best:3.69
2024-12-28 03:11:25,967: Snapshot:0	Epoch:37	Loss:22.105	translation_Loss:22.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.78	Hits@10:6.95	Best:3.78
2024-12-28 03:11:32,389: Snapshot:0	Epoch:38	Loss:21.989	translation_Loss:21.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.86	Hits@10:7.14	Best:3.86
2024-12-28 03:11:38,776: Snapshot:0	Epoch:39	Loss:21.873	translation_Loss:21.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.94	Hits@10:7.32	Best:3.94
2024-12-28 03:11:45,688: Snapshot:0	Epoch:40	Loss:21.773	translation_Loss:21.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.02	Hits@10:7.49	Best:4.02
2024-12-28 03:11:52,028: Snapshot:0	Epoch:41	Loss:21.65	translation_Loss:21.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.1	Hits@10:7.67	Best:4.1
2024-12-28 03:11:58,398: Snapshot:0	Epoch:42	Loss:21.544	translation_Loss:21.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.17	Hits@10:7.84	Best:4.17
2024-12-28 03:12:04,783: Snapshot:0	Epoch:43	Loss:21.43	translation_Loss:21.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.25	Hits@10:8.05	Best:4.25
2024-12-28 03:12:11,129: Snapshot:0	Epoch:44	Loss:21.324	translation_Loss:21.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.33	Hits@10:8.23	Best:4.33
2024-12-28 03:12:17,494: Snapshot:0	Epoch:45	Loss:21.213	translation_Loss:21.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.41	Hits@10:8.42	Best:4.41
2024-12-28 03:12:24,294: Snapshot:0	Epoch:46	Loss:21.096	translation_Loss:21.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.48	Hits@10:8.61	Best:4.48
2024-12-28 03:12:30,639: Snapshot:0	Epoch:47	Loss:20.99	translation_Loss:20.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.55	Hits@10:8.81	Best:4.55
2024-12-28 03:12:37,004: Snapshot:0	Epoch:48	Loss:20.888	translation_Loss:20.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.62	Hits@10:8.99	Best:4.62
2024-12-28 03:12:43,355: Snapshot:0	Epoch:49	Loss:20.781	translation_Loss:20.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.69	Hits@10:9.16	Best:4.69
2024-12-28 03:12:49,772: Snapshot:0	Epoch:50	Loss:20.671	translation_Loss:20.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.76	Hits@10:9.31	Best:4.76
2024-12-28 03:12:56,634: Snapshot:0	Epoch:51	Loss:20.559	translation_Loss:20.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.46	Best:4.83
2024-12-28 03:13:02,991: Snapshot:0	Epoch:52	Loss:20.468	translation_Loss:20.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.89	Hits@10:9.6	Best:4.89
2024-12-28 03:13:09,335: Snapshot:0	Epoch:53	Loss:20.356	translation_Loss:20.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.96	Hits@10:9.8	Best:4.96
2024-12-28 03:13:15,756: Snapshot:0	Epoch:54	Loss:20.247	translation_Loss:20.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.03	Hits@10:9.97	Best:5.03
2024-12-28 03:13:22,087: Snapshot:0	Epoch:55	Loss:20.142	translation_Loss:20.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.09	Hits@10:10.15	Best:5.09
2024-12-28 03:13:28,422: Snapshot:0	Epoch:56	Loss:20.045	translation_Loss:20.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.16	Hits@10:10.33	Best:5.16
2024-12-28 03:13:35,227: Snapshot:0	Epoch:57	Loss:19.939	translation_Loss:19.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.22	Hits@10:10.49	Best:5.22
2024-12-28 03:13:41,622: Snapshot:0	Epoch:58	Loss:19.835	translation_Loss:19.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.29	Hits@10:10.64	Best:5.29
2024-12-28 03:13:48,006: Snapshot:0	Epoch:59	Loss:19.734	translation_Loss:19.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.35	Hits@10:10.77	Best:5.35
2024-12-28 03:13:54,369: Snapshot:0	Epoch:60	Loss:19.619	translation_Loss:19.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.42	Hits@10:10.88	Best:5.42
2024-12-28 03:14:00,759: Snapshot:0	Epoch:61	Loss:19.529	translation_Loss:19.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.47	Hits@10:11.05	Best:5.47
2024-12-28 03:14:07,617: Snapshot:0	Epoch:62	Loss:19.425	translation_Loss:19.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.53	Hits@10:11.2	Best:5.53
2024-12-28 03:14:13,982: Snapshot:0	Epoch:63	Loss:19.326	translation_Loss:19.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.59	Hits@10:11.36	Best:5.59
2024-12-28 03:14:20,360: Snapshot:0	Epoch:64	Loss:19.217	translation_Loss:19.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:11.5	Best:5.66
2024-12-28 03:14:26,722: Snapshot:0	Epoch:65	Loss:19.128	translation_Loss:19.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.72	Hits@10:11.68	Best:5.72
2024-12-28 03:14:33,087: Snapshot:0	Epoch:66	Loss:19.028	translation_Loss:19.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.78	Hits@10:11.86	Best:5.78
2024-12-28 03:14:39,460: Snapshot:0	Epoch:67	Loss:18.933	translation_Loss:18.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.85	Hits@10:12.07	Best:5.85
2024-12-28 03:14:46,230: Snapshot:0	Epoch:68	Loss:18.834	translation_Loss:18.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.91	Hits@10:12.24	Best:5.91
2024-12-28 03:14:52,586: Snapshot:0	Epoch:69	Loss:18.714	translation_Loss:18.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.98	Hits@10:12.47	Best:5.98
2024-12-28 03:14:58,951: Snapshot:0	Epoch:70	Loss:18.626	translation_Loss:18.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.62	Best:6.03
2024-12-28 03:15:05,375: Snapshot:0	Epoch:71	Loss:18.53	translation_Loss:18.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.09	Hits@10:12.79	Best:6.09
2024-12-28 03:15:11,720: Snapshot:0	Epoch:72	Loss:18.432	translation_Loss:18.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.16	Hits@10:12.98	Best:6.16
2024-12-28 03:15:18,566: Snapshot:0	Epoch:73	Loss:18.336	translation_Loss:18.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.22	Hits@10:13.13	Best:6.22
2024-12-28 03:15:24,932: Snapshot:0	Epoch:74	Loss:18.241	translation_Loss:18.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.28	Hits@10:13.32	Best:6.28
2024-12-28 03:15:31,289: Snapshot:0	Epoch:75	Loss:18.157	translation_Loss:18.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.34	Hits@10:13.49	Best:6.34
2024-12-28 03:15:37,654: Snapshot:0	Epoch:76	Loss:18.054	translation_Loss:18.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.4	Hits@10:13.63	Best:6.4
2024-12-28 03:15:44,091: Snapshot:0	Epoch:77	Loss:17.959	translation_Loss:17.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.45	Hits@10:13.78	Best:6.45
2024-12-28 03:15:50,538: Snapshot:0	Epoch:78	Loss:17.853	translation_Loss:17.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.51	Hits@10:13.94	Best:6.51
2024-12-28 03:15:57,372: Snapshot:0	Epoch:79	Loss:17.777	translation_Loss:17.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.57	Hits@10:14.14	Best:6.57
2024-12-28 03:16:03,794: Snapshot:0	Epoch:80	Loss:17.673	translation_Loss:17.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.63	Hits@10:14.28	Best:6.63
2024-12-28 03:16:10,134: Snapshot:0	Epoch:81	Loss:17.581	translation_Loss:17.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.69	Hits@10:14.45	Best:6.69
2024-12-28 03:16:16,503: Snapshot:0	Epoch:82	Loss:17.493	translation_Loss:17.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.76	Hits@10:14.6	Best:6.76
2024-12-28 03:16:22,866: Snapshot:0	Epoch:83	Loss:17.388	translation_Loss:17.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.82	Hits@10:14.72	Best:6.82
2024-12-28 03:16:29,675: Snapshot:0	Epoch:84	Loss:17.301	translation_Loss:17.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.87	Hits@10:14.89	Best:6.87
2024-12-28 03:16:36,075: Snapshot:0	Epoch:85	Loss:17.22	translation_Loss:17.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.93	Hits@10:15.03	Best:6.93
2024-12-28 03:16:42,432: Snapshot:0	Epoch:86	Loss:17.121	translation_Loss:17.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.0	Hits@10:15.18	Best:7.0
2024-12-28 03:16:48,841: Snapshot:0	Epoch:87	Loss:17.031	translation_Loss:17.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.31	Best:7.06
2024-12-28 03:16:55,207: Snapshot:0	Epoch:88	Loss:16.933	translation_Loss:16.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.11	Hits@10:15.45	Best:7.11
2024-12-28 03:17:01,589: Snapshot:0	Epoch:89	Loss:16.846	translation_Loss:16.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.16	Hits@10:15.59	Best:7.16
2024-12-28 03:17:08,503: Snapshot:0	Epoch:90	Loss:16.759	translation_Loss:16.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.22	Hits@10:15.76	Best:7.22
2024-12-28 03:17:14,853: Snapshot:0	Epoch:91	Loss:16.662	translation_Loss:16.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.27	Hits@10:15.9	Best:7.27
2024-12-28 03:17:21,201: Snapshot:0	Epoch:92	Loss:16.579	translation_Loss:16.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.33	Hits@10:16.05	Best:7.33
2024-12-28 03:17:27,591: Snapshot:0	Epoch:93	Loss:16.481	translation_Loss:16.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.38	Hits@10:16.21	Best:7.38
2024-12-28 03:17:33,941: Snapshot:0	Epoch:94	Loss:16.404	translation_Loss:16.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.43	Hits@10:16.37	Best:7.43
2024-12-28 03:17:40,697: Snapshot:0	Epoch:95	Loss:16.311	translation_Loss:16.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.48	Hits@10:16.48	Best:7.48
2024-12-28 03:17:47,107: Snapshot:0	Epoch:96	Loss:16.228	translation_Loss:16.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.53	Hits@10:16.65	Best:7.53
2024-12-28 03:17:53,473: Snapshot:0	Epoch:97	Loss:16.126	translation_Loss:16.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.59	Hits@10:16.79	Best:7.59
2024-12-28 03:17:59,862: Snapshot:0	Epoch:98	Loss:16.05	translation_Loss:16.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.64	Hits@10:16.92	Best:7.64
2024-12-28 03:18:06,213: Snapshot:0	Epoch:99	Loss:15.97	translation_Loss:15.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.69	Hits@10:17.07	Best:7.69
2024-12-28 03:18:12,593: Snapshot:0	Epoch:100	Loss:15.865	translation_Loss:15.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.74	Hits@10:17.2	Best:7.74
2024-12-28 03:18:19,507: Snapshot:0	Epoch:101	Loss:15.785	translation_Loss:15.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.8	Hits@10:17.36	Best:7.8
2024-12-28 03:18:25,901: Snapshot:0	Epoch:102	Loss:15.698	translation_Loss:15.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.85	Hits@10:17.53	Best:7.85
2024-12-28 03:18:32,236: Snapshot:0	Epoch:103	Loss:15.617	translation_Loss:15.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.91	Hits@10:17.67	Best:7.91
2024-12-28 03:18:38,606: Snapshot:0	Epoch:104	Loss:15.526	translation_Loss:15.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.97	Hits@10:17.84	Best:7.97
2024-12-28 03:18:44,964: Snapshot:0	Epoch:105	Loss:15.445	translation_Loss:15.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.02	Hits@10:18.0	Best:8.02
2024-12-28 03:18:51,841: Snapshot:0	Epoch:106	Loss:15.358	translation_Loss:15.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.07	Hits@10:18.14	Best:8.07
2024-12-28 03:18:58,224: Snapshot:0	Epoch:107	Loss:15.284	translation_Loss:15.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.13	Hits@10:18.27	Best:8.13
2024-12-28 03:19:04,601: Snapshot:0	Epoch:108	Loss:15.192	translation_Loss:15.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.18	Hits@10:18.44	Best:8.18
2024-12-28 03:19:10,955: Snapshot:0	Epoch:109	Loss:15.115	translation_Loss:15.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.24	Hits@10:18.59	Best:8.24
2024-12-28 03:19:17,350: Snapshot:0	Epoch:110	Loss:15.022	translation_Loss:15.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.29	Hits@10:18.77	Best:8.29
2024-12-28 03:19:23,717: Snapshot:0	Epoch:111	Loss:14.955	translation_Loss:14.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.35	Hits@10:18.89	Best:8.35
2024-12-28 03:19:30,541: Snapshot:0	Epoch:112	Loss:14.854	translation_Loss:14.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.4	Hits@10:19.07	Best:8.4
2024-12-28 03:19:36,892: Snapshot:0	Epoch:113	Loss:14.78	translation_Loss:14.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.45	Hits@10:19.22	Best:8.45
2024-12-28 03:19:43,357: Snapshot:0	Epoch:114	Loss:14.697	translation_Loss:14.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.51	Hits@10:19.38	Best:8.51
2024-12-28 03:19:49,764: Snapshot:0	Epoch:115	Loss:14.607	translation_Loss:14.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.55	Hits@10:19.52	Best:8.55
2024-12-28 03:19:56,174: Snapshot:0	Epoch:116	Loss:14.55	translation_Loss:14.55	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.61	Hits@10:19.65	Best:8.61
2024-12-28 03:20:03,042: Snapshot:0	Epoch:117	Loss:14.453	translation_Loss:14.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.66	Hits@10:19.77	Best:8.66
2024-12-28 03:20:09,509: Snapshot:0	Epoch:118	Loss:14.376	translation_Loss:14.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.72	Hits@10:19.91	Best:8.72
2024-12-28 03:20:15,875: Snapshot:0	Epoch:119	Loss:14.306	translation_Loss:14.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.78	Hits@10:20.03	Best:8.78
2024-12-28 03:20:22,269: Snapshot:0	Epoch:120	Loss:14.218	translation_Loss:14.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.83	Hits@10:20.15	Best:8.83
2024-12-28 03:20:28,657: Snapshot:0	Epoch:121	Loss:14.138	translation_Loss:14.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.88	Hits@10:20.29	Best:8.88
2024-12-28 03:20:35,086: Snapshot:0	Epoch:122	Loss:14.039	translation_Loss:14.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.93	Hits@10:20.4	Best:8.93
2024-12-28 03:20:41,866: Snapshot:0	Epoch:123	Loss:13.988	translation_Loss:13.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.98	Hits@10:20.58	Best:8.98
2024-12-28 03:20:48,279: Snapshot:0	Epoch:124	Loss:13.902	translation_Loss:13.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.03	Hits@10:20.73	Best:9.03
2024-12-28 03:20:54,702: Snapshot:0	Epoch:125	Loss:13.81	translation_Loss:13.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.08	Hits@10:20.87	Best:9.08
2024-12-28 03:21:01,050: Snapshot:0	Epoch:126	Loss:13.737	translation_Loss:13.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.13	Hits@10:20.98	Best:9.13
2024-12-28 03:21:07,413: Snapshot:0	Epoch:127	Loss:13.663	translation_Loss:13.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.18	Hits@10:21.12	Best:9.18
2024-12-28 03:21:14,293: Snapshot:0	Epoch:128	Loss:13.586	translation_Loss:13.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.23	Hits@10:21.29	Best:9.23
2024-12-28 03:21:20,674: Snapshot:0	Epoch:129	Loss:13.507	translation_Loss:13.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.28	Hits@10:21.47	Best:9.28
2024-12-28 03:21:27,107: Snapshot:0	Epoch:130	Loss:13.44	translation_Loss:13.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.34	Hits@10:21.55	Best:9.34
2024-12-28 03:21:33,472: Snapshot:0	Epoch:131	Loss:13.368	translation_Loss:13.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.42	Hits@10:21.67	Best:9.42
2024-12-28 03:21:39,877: Snapshot:0	Epoch:132	Loss:13.266	translation_Loss:13.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.47	Hits@10:21.83	Best:9.47
2024-12-28 03:21:46,285: Snapshot:0	Epoch:133	Loss:13.211	translation_Loss:13.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.52	Hits@10:22.01	Best:9.52
2024-12-28 03:21:53,112: Snapshot:0	Epoch:134	Loss:13.124	translation_Loss:13.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.57	Hits@10:22.12	Best:9.57
2024-12-28 03:21:59,503: Snapshot:0	Epoch:135	Loss:13.058	translation_Loss:13.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.63	Hits@10:22.25	Best:9.63
2024-12-28 03:22:05,893: Snapshot:0	Epoch:136	Loss:12.976	translation_Loss:12.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.69	Hits@10:22.39	Best:9.69
2024-12-28 03:22:12,280: Snapshot:0	Epoch:137	Loss:12.91	translation_Loss:12.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.74	Hits@10:22.49	Best:9.74
2024-12-28 03:22:18,656: Snapshot:0	Epoch:138	Loss:12.834	translation_Loss:12.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.79	Hits@10:22.61	Best:9.79
2024-12-28 03:22:25,517: Snapshot:0	Epoch:139	Loss:12.766	translation_Loss:12.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.85	Hits@10:22.74	Best:9.85
2024-12-28 03:22:31,881: Snapshot:0	Epoch:140	Loss:12.696	translation_Loss:12.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.9	Hits@10:22.91	Best:9.9
2024-12-28 03:22:38,281: Snapshot:0	Epoch:141	Loss:12.633	translation_Loss:12.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.95	Hits@10:23.06	Best:9.95
2024-12-28 03:22:44,724: Snapshot:0	Epoch:142	Loss:12.552	translation_Loss:12.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.01	Hits@10:23.13	Best:10.01
2024-12-28 03:22:51,163: Snapshot:0	Epoch:143	Loss:12.472	translation_Loss:12.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.06	Hits@10:23.27	Best:10.06
2024-12-28 03:22:57,553: Snapshot:0	Epoch:144	Loss:12.406	translation_Loss:12.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.13	Hits@10:23.36	Best:10.13
2024-12-28 03:23:04,410: Snapshot:0	Epoch:145	Loss:12.339	translation_Loss:12.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.18	Hits@10:23.46	Best:10.18
2024-12-28 03:23:10,784: Snapshot:0	Epoch:146	Loss:12.278	translation_Loss:12.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.22	Hits@10:23.6	Best:10.22
2024-12-28 03:23:17,161: Snapshot:0	Epoch:147	Loss:12.196	translation_Loss:12.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.28	Hits@10:23.74	Best:10.28
2024-12-28 03:23:23,543: Snapshot:0	Epoch:148	Loss:12.119	translation_Loss:12.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.32	Hits@10:23.83	Best:10.32
2024-12-28 03:23:29,916: Snapshot:0	Epoch:149	Loss:12.052	translation_Loss:12.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.37	Hits@10:23.95	Best:10.37
2024-12-28 03:23:36,766: Snapshot:0	Epoch:150	Loss:11.975	translation_Loss:11.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.43	Hits@10:24.1	Best:10.43
2024-12-28 03:23:43,180: Snapshot:0	Epoch:151	Loss:11.909	translation_Loss:11.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.48	Hits@10:24.2	Best:10.48
2024-12-28 03:23:49,564: Snapshot:0	Epoch:152	Loss:11.845	translation_Loss:11.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.53	Hits@10:24.32	Best:10.53
2024-12-28 03:23:55,941: Snapshot:0	Epoch:153	Loss:11.781	translation_Loss:11.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.58	Hits@10:24.43	Best:10.58
2024-12-28 03:24:02,302: Snapshot:0	Epoch:154	Loss:11.711	translation_Loss:11.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.64	Hits@10:24.54	Best:10.64
2024-12-28 03:24:08,658: Snapshot:0	Epoch:155	Loss:11.653	translation_Loss:11.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.7	Hits@10:24.63	Best:10.7
2024-12-28 03:24:15,518: Snapshot:0	Epoch:156	Loss:11.57	translation_Loss:11.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.74	Hits@10:24.72	Best:10.74
2024-12-28 03:24:21,876: Snapshot:0	Epoch:157	Loss:11.515	translation_Loss:11.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.8	Hits@10:24.87	Best:10.8
2024-12-28 03:24:28,268: Snapshot:0	Epoch:158	Loss:11.437	translation_Loss:11.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.84	Hits@10:24.97	Best:10.84
2024-12-28 03:24:34,615: Snapshot:0	Epoch:159	Loss:11.377	translation_Loss:11.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.9	Hits@10:25.08	Best:10.9
2024-12-28 03:24:41,076: Snapshot:0	Epoch:160	Loss:11.296	translation_Loss:11.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:25.2	Best:10.96
2024-12-28 03:24:47,841: Snapshot:0	Epoch:161	Loss:11.243	translation_Loss:11.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.02	Hits@10:25.32	Best:11.02
2024-12-28 03:24:54,232: Snapshot:0	Epoch:162	Loss:11.178	translation_Loss:11.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.08	Hits@10:25.41	Best:11.08
2024-12-28 03:25:00,623: Snapshot:0	Epoch:163	Loss:11.118	translation_Loss:11.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.12	Hits@10:25.52	Best:11.12
2024-12-28 03:25:07,074: Snapshot:0	Epoch:164	Loss:11.043	translation_Loss:11.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.16	Hits@10:25.61	Best:11.16
2024-12-28 03:25:13,419: Snapshot:0	Epoch:165	Loss:10.989	translation_Loss:10.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.22	Hits@10:25.76	Best:11.22
2024-12-28 03:25:19,804: Snapshot:0	Epoch:166	Loss:10.924	translation_Loss:10.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.27	Hits@10:25.89	Best:11.27
2024-12-28 03:25:26,688: Snapshot:0	Epoch:167	Loss:10.835	translation_Loss:10.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.34	Hits@10:25.97	Best:11.34
2024-12-28 03:25:33,069: Snapshot:0	Epoch:168	Loss:10.811	translation_Loss:10.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.4	Hits@10:26.09	Best:11.4
2024-12-28 03:25:39,433: Snapshot:0	Epoch:169	Loss:10.719	translation_Loss:10.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.46	Hits@10:26.18	Best:11.46
2024-12-28 03:25:45,900: Snapshot:0	Epoch:170	Loss:10.668	translation_Loss:10.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.51	Hits@10:26.32	Best:11.51
2024-12-28 03:25:52,314: Snapshot:0	Epoch:171	Loss:10.596	translation_Loss:10.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.56	Hits@10:26.42	Best:11.56
2024-12-28 03:25:59,208: Snapshot:0	Epoch:172	Loss:10.557	translation_Loss:10.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.61	Hits@10:26.48	Best:11.61
2024-12-28 03:26:05,575: Snapshot:0	Epoch:173	Loss:10.475	translation_Loss:10.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.66	Hits@10:26.58	Best:11.66
2024-12-28 03:26:11,966: Snapshot:0	Epoch:174	Loss:10.432	translation_Loss:10.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.7	Hits@10:26.67	Best:11.7
2024-12-28 03:26:18,326: Snapshot:0	Epoch:175	Loss:10.355	translation_Loss:10.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.75	Hits@10:26.78	Best:11.75
2024-12-28 03:26:24,690: Snapshot:0	Epoch:176	Loss:10.311	translation_Loss:10.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.82	Hits@10:26.9	Best:11.82
2024-12-28 03:26:31,085: Snapshot:0	Epoch:177	Loss:10.235	translation_Loss:10.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.87	Hits@10:26.98	Best:11.87
2024-12-28 03:26:37,992: Snapshot:0	Epoch:178	Loss:10.19	translation_Loss:10.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.92	Hits@10:27.09	Best:11.92
2024-12-28 03:26:44,362: Snapshot:0	Epoch:179	Loss:10.123	translation_Loss:10.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.96	Hits@10:27.2	Best:11.96
2024-12-28 03:26:50,726: Snapshot:0	Epoch:180	Loss:10.056	translation_Loss:10.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.02	Hits@10:27.3	Best:12.02
2024-12-28 03:26:57,195: Snapshot:0	Epoch:181	Loss:9.99	translation_Loss:9.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.06	Hits@10:27.33	Best:12.06
2024-12-28 03:27:03,603: Snapshot:0	Epoch:182	Loss:9.946	translation_Loss:9.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.12	Hits@10:27.43	Best:12.12
2024-12-28 03:27:10,487: Snapshot:0	Epoch:183	Loss:9.886	translation_Loss:9.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.15	Hits@10:27.49	Best:12.15
2024-12-28 03:27:16,884: Snapshot:0	Epoch:184	Loss:9.834	translation_Loss:9.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.21	Hits@10:27.59	Best:12.21
2024-12-28 03:27:23,245: Snapshot:0	Epoch:185	Loss:9.77	translation_Loss:9.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.25	Hits@10:27.68	Best:12.25
2024-12-28 03:27:29,617: Snapshot:0	Epoch:186	Loss:9.704	translation_Loss:9.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.3	Hits@10:27.78	Best:12.3
2024-12-28 03:27:36,014: Snapshot:0	Epoch:187	Loss:9.652	translation_Loss:9.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.35	Hits@10:27.86	Best:12.35
2024-12-28 03:27:42,412: Snapshot:0	Epoch:188	Loss:9.58	translation_Loss:9.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.39	Hits@10:27.91	Best:12.39
2024-12-28 03:27:49,274: Snapshot:0	Epoch:189	Loss:9.543	translation_Loss:9.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.44	Hits@10:27.99	Best:12.44
2024-12-28 03:27:55,673: Snapshot:0	Epoch:190	Loss:9.488	translation_Loss:9.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.48	Hits@10:28.09	Best:12.48
2024-12-28 03:28:02,057: Snapshot:0	Epoch:191	Loss:9.424	translation_Loss:9.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.53	Hits@10:28.18	Best:12.53
2024-12-28 03:28:08,414: Snapshot:0	Epoch:192	Loss:9.361	translation_Loss:9.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.57	Hits@10:28.31	Best:12.57
2024-12-28 03:28:14,792: Snapshot:0	Epoch:193	Loss:9.319	translation_Loss:9.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.61	Hits@10:28.38	Best:12.61
2024-12-28 03:28:21,683: Snapshot:0	Epoch:194	Loss:9.259	translation_Loss:9.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.66	Hits@10:28.43	Best:12.66
2024-12-28 03:28:28,064: Snapshot:0	Epoch:195	Loss:9.198	translation_Loss:9.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.69	Hits@10:28.53	Best:12.69
2024-12-28 03:28:34,410: Snapshot:0	Epoch:196	Loss:9.152	translation_Loss:9.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.74	Hits@10:28.61	Best:12.74
2024-12-28 03:28:40,793: Snapshot:0	Epoch:197	Loss:9.097	translation_Loss:9.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.78	Hits@10:28.69	Best:12.78
2024-12-28 03:28:47,178: Snapshot:0	Epoch:198	Loss:9.023	translation_Loss:9.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.83	Hits@10:28.78	Best:12.83
2024-12-28 03:28:53,579: Snapshot:0	Epoch:199	Loss:8.969	translation_Loss:8.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.88	Hits@10:28.91	Best:12.88
2024-12-28 03:28:53,852: => loading checkpoint './checkpoint/FACTfact_0.00001_2048_1000/0model_best.tar'
2024-12-28 03:28:56,960: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1231 | 0.0299 | 0.1845 | 0.2296 |  0.2788 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 03:29:39,413: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228032904/FACTfact_0.00001_2048_5000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_2048_5000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_2048_5000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=5000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 03:29:49,376: Snapshot:0	Epoch:0	Loss:26.587	translation_Loss:26.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 03:29:55,756: Snapshot:0	Epoch:1	Loss:26.448	translation_Loss:26.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 03:30:02,525: Snapshot:0	Epoch:2	Loss:26.318	translation_Loss:26.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 03:30:08,980: Snapshot:0	Epoch:3	Loss:26.197	translation_Loss:26.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 03:30:15,320: Snapshot:0	Epoch:4	Loss:26.076	translation_Loss:26.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.37	Best:1.38
2024-12-28 03:30:21,678: Snapshot:0	Epoch:5	Loss:25.946	translation_Loss:25.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 03:30:28,016: Snapshot:0	Epoch:6	Loss:25.826	translation_Loss:25.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 03:30:34,849: Snapshot:0	Epoch:7	Loss:25.7	translation_Loss:25.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-28 03:30:41,208: Snapshot:0	Epoch:8	Loss:25.577	translation_Loss:25.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.41	Hits@10:1.41	Best:1.41
2024-12-28 03:30:47,586: Snapshot:0	Epoch:9	Loss:25.458	translation_Loss:25.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.44	Best:1.42
2024-12-28 03:30:53,937: Snapshot:0	Epoch:10	Loss:25.326	translation_Loss:25.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.43	Hits@10:1.47	Best:1.43
2024-12-28 03:31:00,311: Snapshot:0	Epoch:11	Loss:25.201	translation_Loss:25.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.44	Hits@10:1.51	Best:1.44
2024-12-28 03:31:06,697: Snapshot:0	Epoch:12	Loss:25.075	translation_Loss:25.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.46	Hits@10:1.57	Best:1.46
2024-12-28 03:31:13,555: Snapshot:0	Epoch:13	Loss:24.951	translation_Loss:24.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.49	Hits@10:1.6	Best:1.49
2024-12-28 03:31:19,947: Snapshot:0	Epoch:14	Loss:24.829	translation_Loss:24.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.52	Hits@10:1.7	Best:1.52
2024-12-28 03:31:26,312: Snapshot:0	Epoch:15	Loss:24.712	translation_Loss:24.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.56	Hits@10:1.83	Best:1.56
2024-12-28 03:31:32,675: Snapshot:0	Epoch:16	Loss:24.591	translation_Loss:24.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-28 03:31:39,098: Snapshot:0	Epoch:17	Loss:24.466	translation_Loss:24.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.66	Hits@10:2.15	Best:1.66
2024-12-28 03:31:45,973: Snapshot:0	Epoch:18	Loss:24.34	translation_Loss:24.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.73	Hits@10:2.36	Best:1.73
2024-12-28 03:31:52,371: Snapshot:0	Epoch:19	Loss:24.219	translation_Loss:24.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.81	Hits@10:2.6	Best:1.81
2024-12-28 03:31:58,783: Snapshot:0	Epoch:20	Loss:24.098	translation_Loss:24.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.9	Hits@10:2.86	Best:1.9
2024-12-28 03:32:05,156: Snapshot:0	Epoch:21	Loss:23.974	translation_Loss:23.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.01	Hits@10:3.15	Best:2.01
2024-12-28 03:32:11,530: Snapshot:0	Epoch:22	Loss:23.86	translation_Loss:23.86	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.12	Hits@10:3.47	Best:2.12
2024-12-28 03:32:17,909: Snapshot:0	Epoch:23	Loss:23.732	translation_Loss:23.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.23	Hits@10:3.76	Best:2.23
2024-12-28 03:32:24,816: Snapshot:0	Epoch:24	Loss:23.618	translation_Loss:23.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.36	Hits@10:4.03	Best:2.36
2024-12-28 03:32:31,202: Snapshot:0	Epoch:25	Loss:23.502	translation_Loss:23.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.49	Hits@10:4.31	Best:2.49
2024-12-28 03:32:37,599: Snapshot:0	Epoch:26	Loss:23.383	translation_Loss:23.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.62	Hits@10:4.53	Best:2.62
2024-12-28 03:32:44,014: Snapshot:0	Epoch:27	Loss:23.259	translation_Loss:23.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.75	Hits@10:4.78	Best:2.75
2024-12-28 03:32:50,393: Snapshot:0	Epoch:28	Loss:23.141	translation_Loss:23.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.86	Hits@10:5.02	Best:2.86
2024-12-28 03:32:57,218: Snapshot:0	Epoch:29	Loss:23.023	translation_Loss:23.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.3	Best:2.98
2024-12-28 03:33:03,634: Snapshot:0	Epoch:30	Loss:22.912	translation_Loss:22.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.09	Hits@10:5.54	Best:3.09
2024-12-28 03:33:10,036: Snapshot:0	Epoch:31	Loss:22.79	translation_Loss:22.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.21	Hits@10:5.82	Best:3.21
2024-12-28 03:33:16,424: Snapshot:0	Epoch:32	Loss:22.686	translation_Loss:22.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.31	Hits@10:6.01	Best:3.31
2024-12-28 03:33:22,784: Snapshot:0	Epoch:33	Loss:22.556	translation_Loss:22.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.41	Hits@10:6.22	Best:3.41
2024-12-28 03:33:29,186: Snapshot:0	Epoch:34	Loss:22.444	translation_Loss:22.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.52	Hits@10:6.45	Best:3.52
2024-12-28 03:33:36,119: Snapshot:0	Epoch:35	Loss:22.331	translation_Loss:22.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.61	Hits@10:6.6	Best:3.61
2024-12-28 03:33:42,501: Snapshot:0	Epoch:36	Loss:22.211	translation_Loss:22.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.69	Hits@10:6.76	Best:3.69
2024-12-28 03:33:48,894: Snapshot:0	Epoch:37	Loss:22.105	translation_Loss:22.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.78	Hits@10:6.95	Best:3.78
2024-12-28 03:33:55,294: Snapshot:0	Epoch:38	Loss:21.989	translation_Loss:21.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.86	Hits@10:7.14	Best:3.86
2024-12-28 03:34:01,695: Snapshot:0	Epoch:39	Loss:21.873	translation_Loss:21.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.94	Hits@10:7.32	Best:3.94
2024-12-28 03:34:08,546: Snapshot:0	Epoch:40	Loss:21.773	translation_Loss:21.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.02	Hits@10:7.49	Best:4.02
2024-12-28 03:34:14,916: Snapshot:0	Epoch:41	Loss:21.65	translation_Loss:21.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.1	Hits@10:7.67	Best:4.1
2024-12-28 03:34:21,318: Snapshot:0	Epoch:42	Loss:21.544	translation_Loss:21.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.17	Hits@10:7.84	Best:4.17
2024-12-28 03:34:27,783: Snapshot:0	Epoch:43	Loss:21.43	translation_Loss:21.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.25	Hits@10:8.05	Best:4.25
2024-12-28 03:34:34,155: Snapshot:0	Epoch:44	Loss:21.324	translation_Loss:21.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.33	Hits@10:8.23	Best:4.33
2024-12-28 03:34:40,552: Snapshot:0	Epoch:45	Loss:21.213	translation_Loss:21.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.41	Hits@10:8.42	Best:4.41
2024-12-28 03:34:47,434: Snapshot:0	Epoch:46	Loss:21.096	translation_Loss:21.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.48	Hits@10:8.61	Best:4.48
2024-12-28 03:34:53,838: Snapshot:0	Epoch:47	Loss:20.99	translation_Loss:20.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.55	Hits@10:8.81	Best:4.55
2024-12-28 03:35:00,262: Snapshot:0	Epoch:48	Loss:20.888	translation_Loss:20.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.62	Hits@10:8.99	Best:4.62
2024-12-28 03:35:06,721: Snapshot:0	Epoch:49	Loss:20.781	translation_Loss:20.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.69	Hits@10:9.16	Best:4.69
2024-12-28 03:35:13,204: Snapshot:0	Epoch:50	Loss:20.671	translation_Loss:20.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.76	Hits@10:9.3	Best:4.76
2024-12-28 03:35:20,157: Snapshot:0	Epoch:51	Loss:20.559	translation_Loss:20.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.46	Best:4.83
2024-12-28 03:35:26,621: Snapshot:0	Epoch:52	Loss:20.468	translation_Loss:20.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.89	Hits@10:9.6	Best:4.89
2024-12-28 03:35:33,008: Snapshot:0	Epoch:53	Loss:20.356	translation_Loss:20.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.96	Hits@10:9.8	Best:4.96
2024-12-28 03:35:39,425: Snapshot:0	Epoch:54	Loss:20.247	translation_Loss:20.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.03	Hits@10:9.97	Best:5.03
2024-12-28 03:35:45,899: Snapshot:0	Epoch:55	Loss:20.142	translation_Loss:20.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.09	Hits@10:10.15	Best:5.09
2024-12-28 03:35:52,284: Snapshot:0	Epoch:56	Loss:20.045	translation_Loss:20.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.16	Hits@10:10.33	Best:5.16
2024-12-28 03:35:59,120: Snapshot:0	Epoch:57	Loss:19.939	translation_Loss:19.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.22	Hits@10:10.49	Best:5.22
2024-12-28 03:36:05,503: Snapshot:0	Epoch:58	Loss:19.835	translation_Loss:19.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.29	Hits@10:10.64	Best:5.29
2024-12-28 03:36:11,877: Snapshot:0	Epoch:59	Loss:19.734	translation_Loss:19.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.35	Hits@10:10.77	Best:5.35
2024-12-28 03:36:18,263: Snapshot:0	Epoch:60	Loss:19.619	translation_Loss:19.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.42	Hits@10:10.88	Best:5.42
2024-12-28 03:36:24,638: Snapshot:0	Epoch:61	Loss:19.529	translation_Loss:19.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.47	Hits@10:11.05	Best:5.47
2024-12-28 03:36:31,571: Snapshot:0	Epoch:62	Loss:19.425	translation_Loss:19.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.53	Hits@10:11.2	Best:5.53
2024-12-28 03:36:37,998: Snapshot:0	Epoch:63	Loss:19.326	translation_Loss:19.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.59	Hits@10:11.36	Best:5.59
2024-12-28 03:36:44,439: Snapshot:0	Epoch:64	Loss:19.217	translation_Loss:19.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:11.5	Best:5.66
2024-12-28 03:36:50,843: Snapshot:0	Epoch:65	Loss:19.128	translation_Loss:19.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.72	Hits@10:11.68	Best:5.72
2024-12-28 03:36:57,259: Snapshot:0	Epoch:66	Loss:19.028	translation_Loss:19.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.77	Hits@10:11.85	Best:5.77
2024-12-28 03:37:03,713: Snapshot:0	Epoch:67	Loss:18.933	translation_Loss:18.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.85	Hits@10:12.07	Best:5.85
2024-12-28 03:37:10,543: Snapshot:0	Epoch:68	Loss:18.834	translation_Loss:18.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.91	Hits@10:12.24	Best:5.91
2024-12-28 03:37:16,983: Snapshot:0	Epoch:69	Loss:18.714	translation_Loss:18.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.98	Hits@10:12.46	Best:5.98
2024-12-28 03:37:23,371: Snapshot:0	Epoch:70	Loss:18.626	translation_Loss:18.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.62	Best:6.03
2024-12-28 03:37:29,784: Snapshot:0	Epoch:71	Loss:18.53	translation_Loss:18.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.09	Hits@10:12.79	Best:6.09
2024-12-28 03:37:36,203: Snapshot:0	Epoch:72	Loss:18.432	translation_Loss:18.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.16	Hits@10:12.98	Best:6.16
2024-12-28 03:37:43,163: Snapshot:0	Epoch:73	Loss:18.336	translation_Loss:18.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.22	Hits@10:13.12	Best:6.22
2024-12-28 03:37:49,565: Snapshot:0	Epoch:74	Loss:18.241	translation_Loss:18.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.28	Hits@10:13.32	Best:6.28
2024-12-28 03:37:55,950: Snapshot:0	Epoch:75	Loss:18.157	translation_Loss:18.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.34	Hits@10:13.5	Best:6.34
2024-12-28 03:38:02,330: Snapshot:0	Epoch:76	Loss:18.054	translation_Loss:18.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.4	Hits@10:13.63	Best:6.4
2024-12-28 03:38:08,746: Snapshot:0	Epoch:77	Loss:17.959	translation_Loss:17.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.45	Hits@10:13.79	Best:6.45
2024-12-28 03:38:15,142: Snapshot:0	Epoch:78	Loss:17.853	translation_Loss:17.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.51	Hits@10:13.94	Best:6.51
2024-12-28 03:38:22,032: Snapshot:0	Epoch:79	Loss:17.777	translation_Loss:17.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.57	Hits@10:14.14	Best:6.57
2024-12-28 03:38:28,440: Snapshot:0	Epoch:80	Loss:17.673	translation_Loss:17.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.63	Hits@10:14.28	Best:6.63
2024-12-28 03:38:34,811: Snapshot:0	Epoch:81	Loss:17.581	translation_Loss:17.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.69	Hits@10:14.45	Best:6.69
2024-12-28 03:38:41,186: Snapshot:0	Epoch:82	Loss:17.493	translation_Loss:17.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.76	Hits@10:14.6	Best:6.76
2024-12-28 03:38:47,596: Snapshot:0	Epoch:83	Loss:17.388	translation_Loss:17.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.82	Hits@10:14.73	Best:6.82
2024-12-28 03:38:54,413: Snapshot:0	Epoch:84	Loss:17.301	translation_Loss:17.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.87	Hits@10:14.89	Best:6.87
2024-12-28 03:39:00,848: Snapshot:0	Epoch:85	Loss:17.22	translation_Loss:17.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.93	Hits@10:15.02	Best:6.93
2024-12-28 03:39:07,227: Snapshot:0	Epoch:86	Loss:17.121	translation_Loss:17.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.0	Hits@10:15.17	Best:7.0
2024-12-28 03:39:13,614: Snapshot:0	Epoch:87	Loss:17.031	translation_Loss:17.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.31	Best:7.06
2024-12-28 03:39:20,028: Snapshot:0	Epoch:88	Loss:16.933	translation_Loss:16.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.11	Hits@10:15.45	Best:7.11
2024-12-28 03:39:26,431: Snapshot:0	Epoch:89	Loss:16.846	translation_Loss:16.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.16	Hits@10:15.59	Best:7.16
2024-12-28 03:39:33,363: Snapshot:0	Epoch:90	Loss:16.759	translation_Loss:16.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.22	Hits@10:15.76	Best:7.22
2024-12-28 03:39:39,799: Snapshot:0	Epoch:91	Loss:16.662	translation_Loss:16.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.27	Hits@10:15.89	Best:7.27
2024-12-28 03:39:46,234: Snapshot:0	Epoch:92	Loss:16.579	translation_Loss:16.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.33	Hits@10:16.05	Best:7.33
2024-12-28 03:39:52,623: Snapshot:0	Epoch:93	Loss:16.481	translation_Loss:16.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.38	Hits@10:16.21	Best:7.38
2024-12-28 03:39:59,076: Snapshot:0	Epoch:94	Loss:16.404	translation_Loss:16.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.43	Hits@10:16.38	Best:7.43
2024-12-28 03:40:05,994: Snapshot:0	Epoch:95	Loss:16.311	translation_Loss:16.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.48	Hits@10:16.47	Best:7.48
2024-12-28 03:40:12,417: Snapshot:0	Epoch:96	Loss:16.228	translation_Loss:16.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.53	Hits@10:16.66	Best:7.53
2024-12-28 03:40:18,804: Snapshot:0	Epoch:97	Loss:16.126	translation_Loss:16.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.59	Hits@10:16.79	Best:7.59
2024-12-28 03:40:25,217: Snapshot:0	Epoch:98	Loss:16.05	translation_Loss:16.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.64	Hits@10:16.92	Best:7.64
2024-12-28 03:40:31,626: Snapshot:0	Epoch:99	Loss:15.97	translation_Loss:15.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.69	Hits@10:17.07	Best:7.69
2024-12-28 03:40:38,091: Snapshot:0	Epoch:100	Loss:15.865	translation_Loss:15.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.74	Hits@10:17.21	Best:7.74
2024-12-28 03:40:44,966: Snapshot:0	Epoch:101	Loss:15.785	translation_Loss:15.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.8	Hits@10:17.36	Best:7.8
2024-12-28 03:40:51,423: Snapshot:0	Epoch:102	Loss:15.698	translation_Loss:15.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.86	Hits@10:17.53	Best:7.86
2024-12-28 03:40:57,874: Snapshot:0	Epoch:103	Loss:15.617	translation_Loss:15.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.91	Hits@10:17.68	Best:7.91
2024-12-28 03:41:04,265: Snapshot:0	Epoch:104	Loss:15.526	translation_Loss:15.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.97	Hits@10:17.84	Best:7.97
2024-12-28 03:41:10,697: Snapshot:0	Epoch:105	Loss:15.445	translation_Loss:15.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.02	Hits@10:17.99	Best:8.02
2024-12-28 03:41:17,596: Snapshot:0	Epoch:106	Loss:15.358	translation_Loss:15.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.07	Hits@10:18.14	Best:8.07
2024-12-28 03:41:23,980: Snapshot:0	Epoch:107	Loss:15.284	translation_Loss:15.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.13	Hits@10:18.27	Best:8.13
2024-12-28 03:41:30,435: Snapshot:0	Epoch:108	Loss:15.192	translation_Loss:15.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.18	Hits@10:18.44	Best:8.18
2024-12-28 03:41:36,815: Snapshot:0	Epoch:109	Loss:15.115	translation_Loss:15.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.24	Hits@10:18.59	Best:8.24
2024-12-28 03:41:43,220: Snapshot:0	Epoch:110	Loss:15.022	translation_Loss:15.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.29	Hits@10:18.77	Best:8.29
2024-12-28 03:41:49,602: Snapshot:0	Epoch:111	Loss:14.955	translation_Loss:14.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.34	Hits@10:18.89	Best:8.34
2024-12-28 03:41:56,478: Snapshot:0	Epoch:112	Loss:14.854	translation_Loss:14.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.4	Hits@10:19.06	Best:8.4
2024-12-28 03:42:02,862: Snapshot:0	Epoch:113	Loss:14.78	translation_Loss:14.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.45	Hits@10:19.23	Best:8.45
2024-12-28 03:42:09,250: Snapshot:0	Epoch:114	Loss:14.697	translation_Loss:14.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.5	Hits@10:19.38	Best:8.5
2024-12-28 03:42:15,638: Snapshot:0	Epoch:115	Loss:14.607	translation_Loss:14.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.55	Hits@10:19.53	Best:8.55
2024-12-28 03:42:22,046: Snapshot:0	Epoch:116	Loss:14.55	translation_Loss:14.55	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.6	Hits@10:19.63	Best:8.6
2024-12-28 03:42:28,954: Snapshot:0	Epoch:117	Loss:14.453	translation_Loss:14.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.66	Hits@10:19.77	Best:8.66
2024-12-28 03:42:35,334: Snapshot:0	Epoch:118	Loss:14.376	translation_Loss:14.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.72	Hits@10:19.9	Best:8.72
2024-12-28 03:42:41,728: Snapshot:0	Epoch:119	Loss:14.306	translation_Loss:14.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.78	Hits@10:20.04	Best:8.78
2024-12-28 03:42:48,132: Snapshot:0	Epoch:120	Loss:14.218	translation_Loss:14.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.83	Hits@10:20.16	Best:8.83
2024-12-28 03:42:54,518: Snapshot:0	Epoch:121	Loss:14.138	translation_Loss:14.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.88	Hits@10:20.29	Best:8.88
2024-12-28 03:43:00,965: Snapshot:0	Epoch:122	Loss:14.039	translation_Loss:14.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.93	Hits@10:20.41	Best:8.93
2024-12-28 03:43:07,782: Snapshot:0	Epoch:123	Loss:13.988	translation_Loss:13.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.98	Hits@10:20.6	Best:8.98
2024-12-28 03:43:14,187: Snapshot:0	Epoch:124	Loss:13.902	translation_Loss:13.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.03	Hits@10:20.73	Best:9.03
2024-12-28 03:43:20,623: Snapshot:0	Epoch:125	Loss:13.81	translation_Loss:13.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.08	Hits@10:20.85	Best:9.08
2024-12-28 03:43:27,020: Snapshot:0	Epoch:126	Loss:13.737	translation_Loss:13.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.13	Hits@10:20.97	Best:9.13
2024-12-28 03:43:33,402: Snapshot:0	Epoch:127	Loss:13.663	translation_Loss:13.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.18	Hits@10:21.12	Best:9.18
2024-12-28 03:43:40,340: Snapshot:0	Epoch:128	Loss:13.586	translation_Loss:13.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.23	Hits@10:21.28	Best:9.23
2024-12-28 03:43:46,810: Snapshot:0	Epoch:129	Loss:13.507	translation_Loss:13.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.29	Hits@10:21.47	Best:9.29
2024-12-28 03:43:53,184: Snapshot:0	Epoch:130	Loss:13.44	translation_Loss:13.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.34	Hits@10:21.54	Best:9.34
2024-12-28 03:43:59,583: Snapshot:0	Epoch:131	Loss:13.368	translation_Loss:13.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.41	Hits@10:21.66	Best:9.41
2024-12-28 03:44:05,967: Snapshot:0	Epoch:132	Loss:13.266	translation_Loss:13.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.46	Hits@10:21.84	Best:9.46
2024-12-28 03:44:12,432: Snapshot:0	Epoch:133	Loss:13.211	translation_Loss:13.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.52	Hits@10:22.0	Best:9.52
2024-12-28 03:44:19,320: Snapshot:0	Epoch:134	Loss:13.124	translation_Loss:13.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.57	Hits@10:22.13	Best:9.57
2024-12-28 03:44:25,728: Snapshot:0	Epoch:135	Loss:13.058	translation_Loss:13.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.63	Hits@10:22.25	Best:9.63
2024-12-28 03:44:32,102: Snapshot:0	Epoch:136	Loss:12.976	translation_Loss:12.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.69	Hits@10:22.38	Best:9.69
2024-12-28 03:44:38,479: Snapshot:0	Epoch:137	Loss:12.91	translation_Loss:12.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.74	Hits@10:22.48	Best:9.74
2024-12-28 03:44:44,936: Snapshot:0	Epoch:138	Loss:12.834	translation_Loss:12.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.8	Hits@10:22.62	Best:9.8
2024-12-28 03:44:51,764: Snapshot:0	Epoch:139	Loss:12.766	translation_Loss:12.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.84	Hits@10:22.74	Best:9.84
2024-12-28 03:44:58,162: Snapshot:0	Epoch:140	Loss:12.696	translation_Loss:12.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.9	Hits@10:22.9	Best:9.9
2024-12-28 03:45:04,603: Snapshot:0	Epoch:141	Loss:12.633	translation_Loss:12.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.95	Hits@10:23.04	Best:9.95
2024-12-28 03:45:10,998: Snapshot:0	Epoch:142	Loss:12.552	translation_Loss:12.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.01	Hits@10:23.13	Best:10.01
2024-12-28 03:45:17,400: Snapshot:0	Epoch:143	Loss:12.472	translation_Loss:12.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.06	Hits@10:23.26	Best:10.06
2024-12-28 03:45:23,819: Snapshot:0	Epoch:144	Loss:12.407	translation_Loss:12.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.13	Hits@10:23.35	Best:10.13
2024-12-28 03:45:30,768: Snapshot:0	Epoch:145	Loss:12.339	translation_Loss:12.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.18	Hits@10:23.46	Best:10.18
2024-12-28 03:45:37,239: Snapshot:0	Epoch:146	Loss:12.278	translation_Loss:12.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.22	Hits@10:23.6	Best:10.22
2024-12-28 03:45:43,708: Snapshot:0	Epoch:147	Loss:12.196	translation_Loss:12.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.28	Hits@10:23.73	Best:10.28
2024-12-28 03:45:50,100: Snapshot:0	Epoch:148	Loss:12.119	translation_Loss:12.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.32	Hits@10:23.84	Best:10.32
2024-12-28 03:45:56,481: Snapshot:0	Epoch:149	Loss:12.052	translation_Loss:12.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.37	Hits@10:23.97	Best:10.37
2024-12-28 03:46:03,270: Snapshot:0	Epoch:150	Loss:11.975	translation_Loss:11.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.43	Hits@10:24.12	Best:10.43
2024-12-28 03:46:09,672: Snapshot:0	Epoch:151	Loss:11.909	translation_Loss:11.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.48	Hits@10:24.21	Best:10.48
2024-12-28 03:46:16,087: Snapshot:0	Epoch:152	Loss:11.845	translation_Loss:11.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.53	Hits@10:24.31	Best:10.53
2024-12-28 03:46:22,481: Snapshot:0	Epoch:153	Loss:11.781	translation_Loss:11.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.58	Hits@10:24.43	Best:10.58
2024-12-28 03:46:28,943: Snapshot:0	Epoch:154	Loss:11.711	translation_Loss:11.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.64	Hits@10:24.53	Best:10.64
2024-12-28 03:46:35,363: Snapshot:0	Epoch:155	Loss:11.653	translation_Loss:11.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.7	Hits@10:24.63	Best:10.7
2024-12-28 03:46:42,296: Snapshot:0	Epoch:156	Loss:11.57	translation_Loss:11.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.74	Hits@10:24.74	Best:10.74
2024-12-28 03:46:48,759: Snapshot:0	Epoch:157	Loss:11.515	translation_Loss:11.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.8	Hits@10:24.89	Best:10.8
2024-12-28 03:46:55,186: Snapshot:0	Epoch:158	Loss:11.437	translation_Loss:11.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.85	Hits@10:24.98	Best:10.85
2024-12-28 03:47:01,587: Snapshot:0	Epoch:159	Loss:11.377	translation_Loss:11.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.9	Hits@10:25.1	Best:10.9
2024-12-28 03:47:07,996: Snapshot:0	Epoch:160	Loss:11.296	translation_Loss:11.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:25.19	Best:10.96
2024-12-28 03:47:14,813: Snapshot:0	Epoch:161	Loss:11.243	translation_Loss:11.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.02	Hits@10:25.32	Best:11.02
2024-12-28 03:47:21,247: Snapshot:0	Epoch:162	Loss:11.178	translation_Loss:11.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.08	Hits@10:25.42	Best:11.08
2024-12-28 03:47:27,641: Snapshot:0	Epoch:163	Loss:11.118	translation_Loss:11.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.12	Hits@10:25.53	Best:11.12
2024-12-28 03:47:34,101: Snapshot:0	Epoch:164	Loss:11.043	translation_Loss:11.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.16	Hits@10:25.61	Best:11.16
2024-12-28 03:47:40,499: Snapshot:0	Epoch:165	Loss:10.989	translation_Loss:10.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.22	Hits@10:25.76	Best:11.22
2024-12-28 03:47:46,951: Snapshot:0	Epoch:166	Loss:10.924	translation_Loss:10.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.28	Hits@10:25.89	Best:11.28
2024-12-28 03:47:53,834: Snapshot:0	Epoch:167	Loss:10.835	translation_Loss:10.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.34	Hits@10:25.97	Best:11.34
2024-12-28 03:48:00,232: Snapshot:0	Epoch:168	Loss:10.811	translation_Loss:10.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.41	Hits@10:26.11	Best:11.41
2024-12-28 03:48:06,638: Snapshot:0	Epoch:169	Loss:10.719	translation_Loss:10.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.46	Hits@10:26.18	Best:11.46
2024-12-28 03:48:13,033: Snapshot:0	Epoch:170	Loss:10.668	translation_Loss:10.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.51	Hits@10:26.34	Best:11.51
2024-12-28 03:48:19,441: Snapshot:0	Epoch:171	Loss:10.596	translation_Loss:10.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.56	Hits@10:26.42	Best:11.56
2024-12-28 03:48:26,348: Snapshot:0	Epoch:172	Loss:10.557	translation_Loss:10.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.61	Hits@10:26.48	Best:11.61
2024-12-28 03:48:32,751: Snapshot:0	Epoch:173	Loss:10.475	translation_Loss:10.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.65	Hits@10:26.59	Best:11.65
2024-12-28 03:48:39,133: Snapshot:0	Epoch:174	Loss:10.432	translation_Loss:10.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.7	Hits@10:26.66	Best:11.7
2024-12-28 03:48:45,526: Snapshot:0	Epoch:175	Loss:10.355	translation_Loss:10.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.76	Hits@10:26.79	Best:11.76
2024-12-28 03:48:51,936: Snapshot:0	Epoch:176	Loss:10.311	translation_Loss:10.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.82	Hits@10:26.88	Best:11.82
2024-12-28 03:48:58,374: Snapshot:0	Epoch:177	Loss:10.235	translation_Loss:10.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.87	Hits@10:26.97	Best:11.87
2024-12-28 03:49:05,197: Snapshot:0	Epoch:178	Loss:10.19	translation_Loss:10.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.92	Hits@10:27.08	Best:11.92
2024-12-28 03:49:11,615: Snapshot:0	Epoch:179	Loss:10.123	translation_Loss:10.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.96	Hits@10:27.2	Best:11.96
2024-12-28 03:49:18,053: Snapshot:0	Epoch:180	Loss:10.056	translation_Loss:10.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.02	Hits@10:27.29	Best:12.02
2024-12-28 03:49:24,508: Snapshot:0	Epoch:181	Loss:9.99	translation_Loss:9.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.06	Hits@10:27.33	Best:12.06
2024-12-28 03:49:30,899: Snapshot:0	Epoch:182	Loss:9.946	translation_Loss:9.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.11	Hits@10:27.43	Best:12.11
2024-12-28 03:49:37,857: Snapshot:0	Epoch:183	Loss:9.886	translation_Loss:9.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.15	Hits@10:27.5	Best:12.15
2024-12-28 03:49:44,318: Snapshot:0	Epoch:184	Loss:9.834	translation_Loss:9.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.2	Hits@10:27.6	Best:12.2
2024-12-28 03:49:50,727: Snapshot:0	Epoch:185	Loss:9.77	translation_Loss:9.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.25	Hits@10:27.69	Best:12.25
2024-12-28 03:49:57,109: Snapshot:0	Epoch:186	Loss:9.704	translation_Loss:9.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.76	Best:12.31
2024-12-28 03:50:03,527: Snapshot:0	Epoch:187	Loss:9.652	translation_Loss:9.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.35	Hits@10:27.85	Best:12.35
2024-12-28 03:50:10,050: Snapshot:0	Epoch:188	Loss:9.58	translation_Loss:9.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.39	Hits@10:27.91	Best:12.39
2024-12-28 03:50:16,826: Snapshot:0	Epoch:189	Loss:9.543	translation_Loss:9.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.44	Hits@10:28.0	Best:12.44
2024-12-28 03:50:23,213: Snapshot:0	Epoch:190	Loss:9.488	translation_Loss:9.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.48	Hits@10:28.09	Best:12.48
2024-12-28 03:50:29,693: Snapshot:0	Epoch:191	Loss:9.424	translation_Loss:9.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.53	Hits@10:28.19	Best:12.53
2024-12-28 03:50:36,088: Snapshot:0	Epoch:192	Loss:9.361	translation_Loss:9.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.57	Hits@10:28.31	Best:12.57
2024-12-28 03:50:42,458: Snapshot:0	Epoch:193	Loss:9.319	translation_Loss:9.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.61	Hits@10:28.39	Best:12.61
2024-12-28 03:50:49,386: Snapshot:0	Epoch:194	Loss:9.259	translation_Loss:9.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.66	Hits@10:28.41	Best:12.66
2024-12-28 03:50:55,806: Snapshot:0	Epoch:195	Loss:9.198	translation_Loss:9.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.69	Hits@10:28.54	Best:12.69
2024-12-28 03:51:02,279: Snapshot:0	Epoch:196	Loss:9.152	translation_Loss:9.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.74	Hits@10:28.6	Best:12.74
2024-12-28 03:51:08,678: Snapshot:0	Epoch:197	Loss:9.097	translation_Loss:9.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.79	Hits@10:28.69	Best:12.79
2024-12-28 03:51:15,171: Snapshot:0	Epoch:198	Loss:9.023	translation_Loss:9.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.83	Hits@10:28.78	Best:12.83
2024-12-28 03:51:21,623: Snapshot:0	Epoch:199	Loss:8.969	translation_Loss:8.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.88	Hits@10:28.89	Best:12.88
2024-12-28 03:51:21,929: => loading checkpoint './checkpoint/FACTfact_0.00001_2048_5000/0model_best.tar'
2024-12-28 03:51:25,019: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.1231 | 0.0299 | 0.1846 | 0.2297 |  0.2789 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
2024-12-28 03:52:07,136: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='2048', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=1e-05, lifelong_name='double_tokened', log_path='./logs/20241228035131/FACTfact_0.00001_2048_10000', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=0.01, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='fact_0.00001_2048_10000', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACTfact_0.00001_2048_10000', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=10000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-28 03:52:17,239: Snapshot:0	Epoch:0	Loss:26.587	translation_Loss:26.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 03:52:23,735: Snapshot:0	Epoch:1	Loss:26.448	translation_Loss:26.448	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.37	Hits@10:1.36	Best:1.37
2024-12-28 03:52:30,666: Snapshot:0	Epoch:2	Loss:26.318	translation_Loss:26.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 03:52:37,163: Snapshot:0	Epoch:3	Loss:26.197	translation_Loss:26.197	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.36	Best:1.38
2024-12-28 03:52:43,652: Snapshot:0	Epoch:4	Loss:26.076	translation_Loss:26.076	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.38	Hits@10:1.37	Best:1.38
2024-12-28 03:52:50,194: Snapshot:0	Epoch:5	Loss:25.946	translation_Loss:25.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 03:52:56,676: Snapshot:0	Epoch:6	Loss:25.826	translation_Loss:25.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.39	Hits@10:1.38	Best:1.39
2024-12-28 03:53:03,674: Snapshot:0	Epoch:7	Loss:25.7	translation_Loss:25.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.4	Hits@10:1.39	Best:1.4
2024-12-28 03:53:10,186: Snapshot:0	Epoch:8	Loss:25.577	translation_Loss:25.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.41	Hits@10:1.41	Best:1.41
2024-12-28 03:53:16,691: Snapshot:0	Epoch:9	Loss:25.458	translation_Loss:25.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.42	Hits@10:1.44	Best:1.42
2024-12-28 03:53:23,184: Snapshot:0	Epoch:10	Loss:25.326	translation_Loss:25.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.43	Hits@10:1.47	Best:1.43
2024-12-28 03:53:29,793: Snapshot:0	Epoch:11	Loss:25.201	translation_Loss:25.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.44	Hits@10:1.51	Best:1.44
2024-12-28 03:53:36,305: Snapshot:0	Epoch:12	Loss:25.075	translation_Loss:25.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.46	Hits@10:1.57	Best:1.46
2024-12-28 03:53:43,342: Snapshot:0	Epoch:13	Loss:24.951	translation_Loss:24.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.49	Hits@10:1.6	Best:1.49
2024-12-28 03:53:49,857: Snapshot:0	Epoch:14	Loss:24.829	translation_Loss:24.829	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.52	Hits@10:1.7	Best:1.52
2024-12-28 03:53:56,400: Snapshot:0	Epoch:15	Loss:24.712	translation_Loss:24.712	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.56	Hits@10:1.83	Best:1.56
2024-12-28 03:54:02,905: Snapshot:0	Epoch:16	Loss:24.591	translation_Loss:24.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.61	Hits@10:1.98	Best:1.61
2024-12-28 03:54:09,402: Snapshot:0	Epoch:17	Loss:24.466	translation_Loss:24.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.66	Hits@10:2.15	Best:1.66
2024-12-28 03:54:16,409: Snapshot:0	Epoch:18	Loss:24.34	translation_Loss:24.34	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.73	Hits@10:2.36	Best:1.73
2024-12-28 03:54:22,913: Snapshot:0	Epoch:19	Loss:24.219	translation_Loss:24.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.81	Hits@10:2.6	Best:1.81
2024-12-28 03:54:29,515: Snapshot:0	Epoch:20	Loss:24.098	translation_Loss:24.098	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.9	Hits@10:2.86	Best:1.9
2024-12-28 03:54:36,127: Snapshot:0	Epoch:21	Loss:23.974	translation_Loss:23.974	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.01	Hits@10:3.15	Best:2.01
2024-12-28 03:54:42,646: Snapshot:0	Epoch:22	Loss:23.86	translation_Loss:23.86	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.12	Hits@10:3.47	Best:2.12
2024-12-28 03:54:49,239: Snapshot:0	Epoch:23	Loss:23.732	translation_Loss:23.732	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.23	Hits@10:3.76	Best:2.23
2024-12-28 03:54:56,264: Snapshot:0	Epoch:24	Loss:23.618	translation_Loss:23.618	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.36	Hits@10:4.03	Best:2.36
2024-12-28 03:55:02,817: Snapshot:0	Epoch:25	Loss:23.502	translation_Loss:23.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.49	Hits@10:4.31	Best:2.49
2024-12-28 03:55:09,375: Snapshot:0	Epoch:26	Loss:23.383	translation_Loss:23.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.62	Hits@10:4.53	Best:2.62
2024-12-28 03:55:15,917: Snapshot:0	Epoch:27	Loss:23.259	translation_Loss:23.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.75	Hits@10:4.78	Best:2.75
2024-12-28 03:55:22,439: Snapshot:0	Epoch:28	Loss:23.141	translation_Loss:23.141	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.86	Hits@10:5.02	Best:2.86
2024-12-28 03:55:29,430: Snapshot:0	Epoch:29	Loss:23.023	translation_Loss:23.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:2.98	Hits@10:5.3	Best:2.98
2024-12-28 03:55:35,950: Snapshot:0	Epoch:30	Loss:22.912	translation_Loss:22.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.09	Hits@10:5.54	Best:3.09
2024-12-28 03:55:42,504: Snapshot:0	Epoch:31	Loss:22.79	translation_Loss:22.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.21	Hits@10:5.82	Best:3.21
2024-12-28 03:55:49,042: Snapshot:0	Epoch:32	Loss:22.686	translation_Loss:22.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.31	Hits@10:6.01	Best:3.31
2024-12-28 03:55:55,542: Snapshot:0	Epoch:33	Loss:22.556	translation_Loss:22.556	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.41	Hits@10:6.22	Best:3.41
2024-12-28 03:56:02,104: Snapshot:0	Epoch:34	Loss:22.444	translation_Loss:22.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.52	Hits@10:6.45	Best:3.52
2024-12-28 03:56:09,128: Snapshot:0	Epoch:35	Loss:22.331	translation_Loss:22.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.61	Hits@10:6.6	Best:3.61
2024-12-28 03:56:15,715: Snapshot:0	Epoch:36	Loss:22.211	translation_Loss:22.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.69	Hits@10:6.76	Best:3.69
2024-12-28 03:56:22,225: Snapshot:0	Epoch:37	Loss:22.105	translation_Loss:22.105	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.78	Hits@10:6.95	Best:3.78
2024-12-28 03:56:28,742: Snapshot:0	Epoch:38	Loss:21.989	translation_Loss:21.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.86	Hits@10:7.14	Best:3.86
2024-12-28 03:56:35,245: Snapshot:0	Epoch:39	Loss:21.873	translation_Loss:21.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.94	Hits@10:7.32	Best:3.94
2024-12-28 03:56:42,176: Snapshot:0	Epoch:40	Loss:21.773	translation_Loss:21.773	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.02	Hits@10:7.49	Best:4.02
2024-12-28 03:56:48,773: Snapshot:0	Epoch:41	Loss:21.65	translation_Loss:21.65	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.1	Hits@10:7.67	Best:4.1
2024-12-28 03:56:55,305: Snapshot:0	Epoch:42	Loss:21.544	translation_Loss:21.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.17	Hits@10:7.84	Best:4.17
2024-12-28 03:57:01,829: Snapshot:0	Epoch:43	Loss:21.43	translation_Loss:21.43	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.25	Hits@10:8.05	Best:4.25
2024-12-28 03:57:08,339: Snapshot:0	Epoch:44	Loss:21.324	translation_Loss:21.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.33	Hits@10:8.23	Best:4.33
2024-12-28 03:57:14,864: Snapshot:0	Epoch:45	Loss:21.213	translation_Loss:21.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.41	Hits@10:8.42	Best:4.41
2024-12-28 03:57:21,893: Snapshot:0	Epoch:46	Loss:21.096	translation_Loss:21.096	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.48	Hits@10:8.61	Best:4.48
2024-12-28 03:57:28,485: Snapshot:0	Epoch:47	Loss:20.99	translation_Loss:20.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.55	Hits@10:8.81	Best:4.55
2024-12-28 03:57:35,051: Snapshot:0	Epoch:48	Loss:20.888	translation_Loss:20.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.62	Hits@10:8.99	Best:4.62
2024-12-28 03:57:41,604: Snapshot:0	Epoch:49	Loss:20.781	translation_Loss:20.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.69	Hits@10:9.16	Best:4.69
2024-12-28 03:57:48,212: Snapshot:0	Epoch:50	Loss:20.671	translation_Loss:20.671	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.76	Hits@10:9.3	Best:4.76
2024-12-28 03:57:55,174: Snapshot:0	Epoch:51	Loss:20.559	translation_Loss:20.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.83	Hits@10:9.46	Best:4.83
2024-12-28 03:58:01,755: Snapshot:0	Epoch:52	Loss:20.468	translation_Loss:20.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.89	Hits@10:9.59	Best:4.89
2024-12-28 03:58:08,286: Snapshot:0	Epoch:53	Loss:20.356	translation_Loss:20.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:4.96	Hits@10:9.79	Best:4.96
2024-12-28 03:58:14,901: Snapshot:0	Epoch:54	Loss:20.247	translation_Loss:20.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.03	Hits@10:9.97	Best:5.03
2024-12-28 03:58:21,476: Snapshot:0	Epoch:55	Loss:20.142	translation_Loss:20.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.09	Hits@10:10.15	Best:5.09
2024-12-28 03:58:28,120: Snapshot:0	Epoch:56	Loss:20.045	translation_Loss:20.045	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.16	Hits@10:10.34	Best:5.16
2024-12-28 03:58:35,160: Snapshot:0	Epoch:57	Loss:19.939	translation_Loss:19.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.22	Hits@10:10.49	Best:5.22
2024-12-28 03:58:41,681: Snapshot:0	Epoch:58	Loss:19.835	translation_Loss:19.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.28	Hits@10:10.64	Best:5.28
2024-12-28 03:58:48,263: Snapshot:0	Epoch:59	Loss:19.734	translation_Loss:19.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.35	Hits@10:10.77	Best:5.35
2024-12-28 03:58:54,793: Snapshot:0	Epoch:60	Loss:19.619	translation_Loss:19.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.42	Hits@10:10.88	Best:5.42
2024-12-28 03:59:01,360: Snapshot:0	Epoch:61	Loss:19.529	translation_Loss:19.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.47	Hits@10:11.05	Best:5.47
2024-12-28 03:59:08,342: Snapshot:0	Epoch:62	Loss:19.425	translation_Loss:19.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.53	Hits@10:11.2	Best:5.53
2024-12-28 03:59:14,894: Snapshot:0	Epoch:63	Loss:19.326	translation_Loss:19.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.59	Hits@10:11.36	Best:5.59
2024-12-28 03:59:21,459: Snapshot:0	Epoch:64	Loss:19.217	translation_Loss:19.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.66	Hits@10:11.5	Best:5.66
2024-12-28 03:59:28,015: Snapshot:0	Epoch:65	Loss:19.128	translation_Loss:19.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.72	Hits@10:11.68	Best:5.72
2024-12-28 03:59:34,571: Snapshot:0	Epoch:66	Loss:19.028	translation_Loss:19.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.77	Hits@10:11.86	Best:5.77
2024-12-28 03:59:41,105: Snapshot:0	Epoch:67	Loss:18.933	translation_Loss:18.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.84	Hits@10:12.07	Best:5.84
2024-12-28 03:59:48,131: Snapshot:0	Epoch:68	Loss:18.834	translation_Loss:18.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.91	Hits@10:12.24	Best:5.91
2024-12-28 03:59:54,655: Snapshot:0	Epoch:69	Loss:18.714	translation_Loss:18.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.98	Hits@10:12.46	Best:5.98
2024-12-28 04:00:01,260: Snapshot:0	Epoch:70	Loss:18.626	translation_Loss:18.626	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.03	Hits@10:12.62	Best:6.03
2024-12-28 04:00:07,921: Snapshot:0	Epoch:71	Loss:18.53	translation_Loss:18.53	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.09	Hits@10:12.79	Best:6.09
2024-12-28 04:00:14,463: Snapshot:0	Epoch:72	Loss:18.432	translation_Loss:18.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.16	Hits@10:12.98	Best:6.16
2024-12-28 04:00:21,539: Snapshot:0	Epoch:73	Loss:18.336	translation_Loss:18.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.22	Hits@10:13.13	Best:6.22
2024-12-28 04:00:28,120: Snapshot:0	Epoch:74	Loss:18.241	translation_Loss:18.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.28	Hits@10:13.32	Best:6.28
2024-12-28 04:00:34,673: Snapshot:0	Epoch:75	Loss:18.157	translation_Loss:18.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.34	Hits@10:13.49	Best:6.34
2024-12-28 04:00:41,239: Snapshot:0	Epoch:76	Loss:18.054	translation_Loss:18.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.4	Hits@10:13.63	Best:6.4
2024-12-28 04:00:47,798: Snapshot:0	Epoch:77	Loss:17.959	translation_Loss:17.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.45	Hits@10:13.79	Best:6.45
2024-12-28 04:00:54,351: Snapshot:0	Epoch:78	Loss:17.853	translation_Loss:17.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.51	Hits@10:13.94	Best:6.51
2024-12-28 04:01:01,344: Snapshot:0	Epoch:79	Loss:17.777	translation_Loss:17.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.57	Hits@10:14.14	Best:6.57
2024-12-28 04:01:07,976: Snapshot:0	Epoch:80	Loss:17.673	translation_Loss:17.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.63	Hits@10:14.28	Best:6.63
2024-12-28 04:01:14,515: Snapshot:0	Epoch:81	Loss:17.581	translation_Loss:17.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.69	Hits@10:14.45	Best:6.69
2024-12-28 04:01:21,084: Snapshot:0	Epoch:82	Loss:17.493	translation_Loss:17.493	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.76	Hits@10:14.6	Best:6.76
2024-12-28 04:01:27,670: Snapshot:0	Epoch:83	Loss:17.388	translation_Loss:17.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.82	Hits@10:14.72	Best:6.82
2024-12-28 04:01:34,738: Snapshot:0	Epoch:84	Loss:17.301	translation_Loss:17.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.87	Hits@10:14.89	Best:6.87
2024-12-28 04:01:41,302: Snapshot:0	Epoch:85	Loss:17.22	translation_Loss:17.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.93	Hits@10:15.02	Best:6.93
2024-12-28 04:01:47,886: Snapshot:0	Epoch:86	Loss:17.121	translation_Loss:17.121	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.0	Hits@10:15.17	Best:7.0
2024-12-28 04:01:54,423: Snapshot:0	Epoch:87	Loss:17.031	translation_Loss:17.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.06	Hits@10:15.32	Best:7.06
2024-12-28 04:02:00,998: Snapshot:0	Epoch:88	Loss:16.933	translation_Loss:16.933	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.11	Hits@10:15.45	Best:7.11
2024-12-28 04:02:07,528: Snapshot:0	Epoch:89	Loss:16.846	translation_Loss:16.846	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.16	Hits@10:15.59	Best:7.16
2024-12-28 04:02:14,460: Snapshot:0	Epoch:90	Loss:16.759	translation_Loss:16.759	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.22	Hits@10:15.76	Best:7.22
2024-12-28 04:02:21,011: Snapshot:0	Epoch:91	Loss:16.662	translation_Loss:16.662	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.27	Hits@10:15.89	Best:7.27
2024-12-28 04:02:27,560: Snapshot:0	Epoch:92	Loss:16.579	translation_Loss:16.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.33	Hits@10:16.05	Best:7.33
2024-12-28 04:02:34,102: Snapshot:0	Epoch:93	Loss:16.481	translation_Loss:16.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.38	Hits@10:16.21	Best:7.38
2024-12-28 04:02:40,652: Snapshot:0	Epoch:94	Loss:16.404	translation_Loss:16.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.43	Hits@10:16.37	Best:7.43
2024-12-28 04:02:47,703: Snapshot:0	Epoch:95	Loss:16.311	translation_Loss:16.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.48	Hits@10:16.47	Best:7.48
2024-12-28 04:02:54,222: Snapshot:0	Epoch:96	Loss:16.228	translation_Loss:16.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.53	Hits@10:16.66	Best:7.53
2024-12-28 04:03:00,802: Snapshot:0	Epoch:97	Loss:16.126	translation_Loss:16.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.59	Hits@10:16.79	Best:7.59
2024-12-28 04:03:07,434: Snapshot:0	Epoch:98	Loss:16.05	translation_Loss:16.05	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.64	Hits@10:16.92	Best:7.64
2024-12-28 04:03:13,976: Snapshot:0	Epoch:99	Loss:15.97	translation_Loss:15.97	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.69	Hits@10:17.07	Best:7.69
2024-12-28 04:03:20,533: Snapshot:0	Epoch:100	Loss:15.865	translation_Loss:15.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.74	Hits@10:17.2	Best:7.74
2024-12-28 04:03:27,485: Snapshot:0	Epoch:101	Loss:15.785	translation_Loss:15.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.8	Hits@10:17.36	Best:7.8
2024-12-28 04:03:34,059: Snapshot:0	Epoch:102	Loss:15.698	translation_Loss:15.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.86	Hits@10:17.54	Best:7.86
2024-12-28 04:03:40,615: Snapshot:0	Epoch:103	Loss:15.617	translation_Loss:15.617	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.91	Hits@10:17.67	Best:7.91
2024-12-28 04:03:47,258: Snapshot:0	Epoch:104	Loss:15.526	translation_Loss:15.526	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.97	Hits@10:17.84	Best:7.97
2024-12-28 04:03:53,799: Snapshot:0	Epoch:105	Loss:15.445	translation_Loss:15.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.02	Hits@10:17.99	Best:8.02
2024-12-28 04:04:00,874: Snapshot:0	Epoch:106	Loss:15.358	translation_Loss:15.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.07	Hits@10:18.14	Best:8.07
2024-12-28 04:04:07,417: Snapshot:0	Epoch:107	Loss:15.284	translation_Loss:15.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.13	Hits@10:18.27	Best:8.13
2024-12-28 04:04:13,971: Snapshot:0	Epoch:108	Loss:15.192	translation_Loss:15.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.18	Hits@10:18.44	Best:8.18
2024-12-28 04:04:20,579: Snapshot:0	Epoch:109	Loss:15.115	translation_Loss:15.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.24	Hits@10:18.58	Best:8.24
2024-12-28 04:04:27,116: Snapshot:0	Epoch:110	Loss:15.022	translation_Loss:15.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.29	Hits@10:18.76	Best:8.29
2024-12-28 04:04:33,662: Snapshot:0	Epoch:111	Loss:14.955	translation_Loss:14.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.35	Hits@10:18.9	Best:8.35
2024-12-28 04:04:40,642: Snapshot:0	Epoch:112	Loss:14.854	translation_Loss:14.854	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.4	Hits@10:19.06	Best:8.4
2024-12-28 04:04:47,183: Snapshot:0	Epoch:113	Loss:14.78	translation_Loss:14.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.45	Hits@10:19.22	Best:8.45
2024-12-28 04:04:53,734: Snapshot:0	Epoch:114	Loss:14.697	translation_Loss:14.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.5	Hits@10:19.38	Best:8.5
2024-12-28 04:05:00,375: Snapshot:0	Epoch:115	Loss:14.608	translation_Loss:14.608	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.55	Hits@10:19.53	Best:8.55
2024-12-28 04:05:07,009: Snapshot:0	Epoch:116	Loss:14.55	translation_Loss:14.55	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.6	Hits@10:19.64	Best:8.6
2024-12-28 04:05:14,044: Snapshot:0	Epoch:117	Loss:14.453	translation_Loss:14.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.66	Hits@10:19.77	Best:8.66
2024-12-28 04:05:20,632: Snapshot:0	Epoch:118	Loss:14.376	translation_Loss:14.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.72	Hits@10:19.91	Best:8.72
2024-12-28 04:05:27,163: Snapshot:0	Epoch:119	Loss:14.306	translation_Loss:14.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.78	Hits@10:20.04	Best:8.78
2024-12-28 04:05:33,724: Snapshot:0	Epoch:120	Loss:14.218	translation_Loss:14.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.83	Hits@10:20.16	Best:8.83
2024-12-28 04:05:40,288: Snapshot:0	Epoch:121	Loss:14.138	translation_Loss:14.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.88	Hits@10:20.3	Best:8.88
2024-12-28 04:05:46,857: Snapshot:0	Epoch:122	Loss:14.039	translation_Loss:14.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.93	Hits@10:20.4	Best:8.93
2024-12-28 04:05:53,907: Snapshot:0	Epoch:123	Loss:13.988	translation_Loss:13.988	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.98	Hits@10:20.59	Best:8.98
2024-12-28 04:06:00,484: Snapshot:0	Epoch:124	Loss:13.902	translation_Loss:13.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.03	Hits@10:20.72	Best:9.03
2024-12-28 04:06:07,045: Snapshot:0	Epoch:125	Loss:13.81	translation_Loss:13.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.08	Hits@10:20.86	Best:9.08
2024-12-28 04:06:13,624: Snapshot:0	Epoch:126	Loss:13.737	translation_Loss:13.737	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.13	Hits@10:20.98	Best:9.13
2024-12-28 04:06:20,177: Snapshot:0	Epoch:127	Loss:13.663	translation_Loss:13.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.18	Hits@10:21.12	Best:9.18
2024-12-28 04:06:27,159: Snapshot:0	Epoch:128	Loss:13.586	translation_Loss:13.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.23	Hits@10:21.28	Best:9.23
2024-12-28 04:06:33,748: Snapshot:0	Epoch:129	Loss:13.507	translation_Loss:13.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.29	Hits@10:21.47	Best:9.29
2024-12-28 04:06:40,315: Snapshot:0	Epoch:130	Loss:13.44	translation_Loss:13.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.34	Hits@10:21.54	Best:9.34
2024-12-28 04:06:46,956: Snapshot:0	Epoch:131	Loss:13.368	translation_Loss:13.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.42	Hits@10:21.66	Best:9.42
2024-12-28 04:06:53,550: Snapshot:0	Epoch:132	Loss:13.266	translation_Loss:13.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.46	Hits@10:21.84	Best:9.46
2024-12-28 04:07:00,151: Snapshot:0	Epoch:133	Loss:13.211	translation_Loss:13.211	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.52	Hits@10:22.01	Best:9.52
2024-12-28 04:07:07,217: Snapshot:0	Epoch:134	Loss:13.124	translation_Loss:13.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.57	Hits@10:22.13	Best:9.57
2024-12-28 04:07:13,773: Snapshot:0	Epoch:135	Loss:13.058	translation_Loss:13.058	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.63	Hits@10:22.26	Best:9.63
2024-12-28 04:07:20,340: Snapshot:0	Epoch:136	Loss:12.976	translation_Loss:12.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.69	Hits@10:22.38	Best:9.69
2024-12-28 04:07:26,898: Snapshot:0	Epoch:137	Loss:12.91	translation_Loss:12.91	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.74	Hits@10:22.47	Best:9.74
2024-12-28 04:07:33,473: Snapshot:0	Epoch:138	Loss:12.834	translation_Loss:12.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.79	Hits@10:22.61	Best:9.79
2024-12-28 04:07:40,438: Snapshot:0	Epoch:139	Loss:12.766	translation_Loss:12.766	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.85	Hits@10:22.74	Best:9.85
2024-12-28 04:07:47,027: Snapshot:0	Epoch:140	Loss:12.696	translation_Loss:12.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.89	Hits@10:22.91	Best:9.89
2024-12-28 04:07:53,568: Snapshot:0	Epoch:141	Loss:12.633	translation_Loss:12.633	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.95	Hits@10:23.04	Best:9.95
2024-12-28 04:08:00,116: Snapshot:0	Epoch:142	Loss:12.552	translation_Loss:12.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.01	Hits@10:23.13	Best:10.01
2024-12-28 04:08:06,640: Snapshot:0	Epoch:143	Loss:12.472	translation_Loss:12.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.07	Hits@10:23.27	Best:10.07
2024-12-28 04:08:13,282: Snapshot:0	Epoch:144	Loss:12.407	translation_Loss:12.407	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.13	Hits@10:23.36	Best:10.13
2024-12-28 04:08:20,361: Snapshot:0	Epoch:145	Loss:12.339	translation_Loss:12.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.18	Hits@10:23.47	Best:10.18
2024-12-28 04:08:26,899: Snapshot:0	Epoch:146	Loss:12.278	translation_Loss:12.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.22	Hits@10:23.6	Best:10.22
2024-12-28 04:08:33,435: Snapshot:0	Epoch:147	Loss:12.196	translation_Loss:12.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.28	Hits@10:23.73	Best:10.28
2024-12-28 04:08:40,025: Snapshot:0	Epoch:148	Loss:12.119	translation_Loss:12.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.32	Hits@10:23.83	Best:10.32
2024-12-28 04:08:46,558: Snapshot:0	Epoch:149	Loss:12.052	translation_Loss:12.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.37	Hits@10:23.96	Best:10.37
2024-12-28 04:08:53,519: Snapshot:0	Epoch:150	Loss:11.975	translation_Loss:11.975	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.43	Hits@10:24.11	Best:10.43
2024-12-28 04:09:00,110: Snapshot:0	Epoch:151	Loss:11.909	translation_Loss:11.909	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.48	Hits@10:24.21	Best:10.48
2024-12-28 04:09:06,655: Snapshot:0	Epoch:152	Loss:11.845	translation_Loss:11.845	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.53	Hits@10:24.31	Best:10.53
2024-12-28 04:09:13,256: Snapshot:0	Epoch:153	Loss:11.781	translation_Loss:11.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.58	Hits@10:24.43	Best:10.58
2024-12-28 04:09:19,855: Snapshot:0	Epoch:154	Loss:11.711	translation_Loss:11.711	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.64	Hits@10:24.53	Best:10.64
2024-12-28 04:09:26,390: Snapshot:0	Epoch:155	Loss:11.653	translation_Loss:11.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.7	Hits@10:24.62	Best:10.7
2024-12-28 04:09:33,445: Snapshot:0	Epoch:156	Loss:11.57	translation_Loss:11.57	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.74	Hits@10:24.74	Best:10.74
2024-12-28 04:09:40,077: Snapshot:0	Epoch:157	Loss:11.515	translation_Loss:11.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.8	Hits@10:24.89	Best:10.8
2024-12-28 04:09:46,675: Snapshot:0	Epoch:158	Loss:11.437	translation_Loss:11.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.85	Hits@10:24.97	Best:10.85
2024-12-28 04:09:53,220: Snapshot:0	Epoch:159	Loss:11.377	translation_Loss:11.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.9	Hits@10:25.09	Best:10.9
2024-12-28 04:09:59,778: Snapshot:0	Epoch:160	Loss:11.296	translation_Loss:11.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.96	Hits@10:25.18	Best:10.96
2024-12-28 04:10:06,835: Snapshot:0	Epoch:161	Loss:11.243	translation_Loss:11.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.02	Hits@10:25.33	Best:11.02
2024-12-28 04:10:13,407: Snapshot:0	Epoch:162	Loss:11.178	translation_Loss:11.178	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.08	Hits@10:25.41	Best:11.08
2024-12-28 04:10:19,965: Snapshot:0	Epoch:163	Loss:11.118	translation_Loss:11.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.12	Hits@10:25.52	Best:11.12
2024-12-28 04:10:26,498: Snapshot:0	Epoch:164	Loss:11.043	translation_Loss:11.043	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.16	Hits@10:25.62	Best:11.16
2024-12-28 04:10:33,053: Snapshot:0	Epoch:165	Loss:10.989	translation_Loss:10.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.22	Hits@10:25.76	Best:11.22
2024-12-28 04:10:39,687: Snapshot:0	Epoch:166	Loss:10.924	translation_Loss:10.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.27	Hits@10:25.89	Best:11.27
2024-12-28 04:10:46,736: Snapshot:0	Epoch:167	Loss:10.835	translation_Loss:10.835	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.34	Hits@10:25.97	Best:11.34
2024-12-28 04:10:53,285: Snapshot:0	Epoch:168	Loss:10.811	translation_Loss:10.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.4	Hits@10:26.08	Best:11.4
2024-12-28 04:10:59,932: Snapshot:0	Epoch:169	Loss:10.719	translation_Loss:10.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.46	Hits@10:26.17	Best:11.46
2024-12-28 04:11:06,477: Snapshot:0	Epoch:170	Loss:10.668	translation_Loss:10.668	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.51	Hits@10:26.34	Best:11.51
2024-12-28 04:11:13,037: Snapshot:0	Epoch:171	Loss:10.596	translation_Loss:10.596	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.56	Hits@10:26.42	Best:11.56
2024-12-28 04:11:20,107: Snapshot:0	Epoch:172	Loss:10.557	translation_Loss:10.557	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.61	Hits@10:26.47	Best:11.61
2024-12-28 04:11:26,645: Snapshot:0	Epoch:173	Loss:10.475	translation_Loss:10.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.66	Hits@10:26.58	Best:11.66
2024-12-28 04:11:33,250: Snapshot:0	Epoch:174	Loss:10.432	translation_Loss:10.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.7	Hits@10:26.67	Best:11.7
2024-12-28 04:11:39,875: Snapshot:0	Epoch:175	Loss:10.355	translation_Loss:10.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.76	Hits@10:26.78	Best:11.76
2024-12-28 04:11:46,469: Snapshot:0	Epoch:176	Loss:10.311	translation_Loss:10.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.82	Hits@10:26.88	Best:11.82
2024-12-28 04:11:53,035: Snapshot:0	Epoch:177	Loss:10.235	translation_Loss:10.235	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.87	Hits@10:26.98	Best:11.87
2024-12-28 04:12:00,070: Snapshot:0	Epoch:178	Loss:10.19	translation_Loss:10.19	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.92	Hits@10:27.09	Best:11.92
2024-12-28 04:12:06,613: Snapshot:0	Epoch:179	Loss:10.123	translation_Loss:10.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:11.97	Hits@10:27.21	Best:11.97
2024-12-28 04:12:13,194: Snapshot:0	Epoch:180	Loss:10.056	translation_Loss:10.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.02	Hits@10:27.28	Best:12.02
2024-12-28 04:12:19,893: Snapshot:0	Epoch:181	Loss:9.99	translation_Loss:9.99	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.06	Hits@10:27.34	Best:12.06
2024-12-28 04:12:26,449: Snapshot:0	Epoch:182	Loss:9.946	translation_Loss:9.946	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.11	Hits@10:27.44	Best:12.11
2024-12-28 04:12:33,516: Snapshot:0	Epoch:183	Loss:9.886	translation_Loss:9.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.16	Hits@10:27.5	Best:12.16
2024-12-28 04:12:40,098: Snapshot:0	Epoch:184	Loss:9.834	translation_Loss:9.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.21	Hits@10:27.59	Best:12.21
2024-12-28 04:12:46,642: Snapshot:0	Epoch:185	Loss:9.77	translation_Loss:9.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.25	Hits@10:27.69	Best:12.25
2024-12-28 04:12:53,206: Snapshot:0	Epoch:186	Loss:9.704	translation_Loss:9.704	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.31	Hits@10:27.77	Best:12.31
2024-12-28 04:12:59,782: Snapshot:0	Epoch:187	Loss:9.652	translation_Loss:9.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.35	Hits@10:27.86	Best:12.35
2024-12-28 04:13:06,337: Snapshot:0	Epoch:188	Loss:9.58	translation_Loss:9.58	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:27.93	Best:12.4
2024-12-28 04:13:13,355: Snapshot:0	Epoch:189	Loss:9.543	translation_Loss:9.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.44	Hits@10:28.0	Best:12.44
2024-12-28 04:13:19,968: Snapshot:0	Epoch:190	Loss:9.488	translation_Loss:9.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.48	Hits@10:28.1	Best:12.48
2024-12-28 04:13:26,500: Snapshot:0	Epoch:191	Loss:9.424	translation_Loss:9.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.53	Hits@10:28.18	Best:12.53
2024-12-28 04:13:33,149: Snapshot:0	Epoch:192	Loss:9.361	translation_Loss:9.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.57	Hits@10:28.29	Best:12.57
2024-12-28 04:13:39,795: Snapshot:0	Epoch:193	Loss:9.319	translation_Loss:9.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.61	Hits@10:28.36	Best:12.61
2024-12-28 04:13:46,924: Snapshot:0	Epoch:194	Loss:9.259	translation_Loss:9.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.66	Hits@10:28.44	Best:12.66
2024-12-28 04:13:53,473: Snapshot:0	Epoch:195	Loss:9.198	translation_Loss:9.198	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.69	Hits@10:28.54	Best:12.69
2024-12-28 04:14:00,078: Snapshot:0	Epoch:196	Loss:9.152	translation_Loss:9.152	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.74	Hits@10:28.61	Best:12.74
2024-12-28 04:14:06,609: Snapshot:0	Epoch:197	Loss:9.097	translation_Loss:9.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.79	Hits@10:28.69	Best:12.79
2024-12-28 04:14:13,145: Snapshot:0	Epoch:198	Loss:9.023	translation_Loss:9.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.83	Hits@10:28.78	Best:12.83
2024-12-28 04:14:19,701: Snapshot:0	Epoch:199	Loss:8.969	translation_Loss:8.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.88	Hits@10:28.9	Best:12.88
2024-12-28 04:14:19,983: => loading checkpoint './checkpoint/FACTfact_0.00001_2048_10000/0model_best.tar'
2024-12-28 04:14:23,038: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.123 | 0.0299 | 0.1846 | 0.2295 |  0.279  |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
