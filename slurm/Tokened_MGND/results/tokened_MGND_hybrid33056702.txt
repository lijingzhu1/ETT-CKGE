2025-01-03 21:28:27,790: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103212754/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=1, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 21:28:37,062: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 21:28:43,421: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 21:28:49,194: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.68	Best:18.93
2025-01-03 21:28:55,969: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.56	Best:22.69
2025-01-03 21:29:01,634: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:45.23	Best:24.2
2025-01-03 21:29:08,484: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:45.93	Best:25.0
2025-01-03 21:29:14,157: Snapshot:0	Epoch:6	Loss:1.068	translation_Loss:1.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.25	Hits@10:46.26	Best:25.25
2025-01-03 21:29:20,834: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.57	Hits@10:46.41	Best:25.57
2025-01-03 21:29:27,056: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.47	Best:25.7
2025-01-03 21:29:33,757: Snapshot:0	Epoch:9	Loss:0.536	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.53	Best:25.7
2025-01-03 21:29:39,354: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.41	Best:25.72
2025-01-03 21:29:46,007: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.4	Best:25.78
2025-01-03 21:29:51,746: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.35	Best:25.78
2025-01-03 21:29:58,133: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.15	Best:25.78
2025-01-03 21:30:04,293: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.08	Best:25.8
2025-01-03 21:30:10,776: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:45.82	Best:25.83
2025-01-03 21:30:16,984: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.01	Best:25.83
2025-01-03 21:30:23,420: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:45.96	Best:25.83
2025-01-03 21:30:30,170: Snapshot:0	Epoch:18	Loss:0.221	translation_Loss:0.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:45.8	Best:25.83
2025-01-03 21:30:36,011: Snapshot:0	Epoch:19	Loss:0.22	translation_Loss:0.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.46	Hits@10:45.7	Best:25.83
2025-01-03 21:30:42,748: Early Stopping! Snapshot: 0 Epoch: 20 Best Results: 25.83
2025-01-03 21:30:42,748: Start to training tokens! Snapshot: 0 Epoch: 20 Loss:0.196 MRR:25.33 Best Results: 25.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2025-01-03 21:30:42,749: Snapshot:0	Epoch:20	Loss:0.196	translation_Loss:0.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.57	Best:25.83
2025-01-03 21:30:48,892: Snapshot:0	Epoch:21	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.57	Best:25.83
2025-01-03 21:30:55,314: End of token training: 0 Epoch: 22 Loss:nan MRR:25.33 Best Results: 25.83
2025-01-03 21:30:55,314: Snapshot:0	Epoch:22	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.33	Hits@10:45.57	Best:25.83
2025-01-03 21:30:55,553: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 21:30:58,339: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.151  | 0.3126 | 0.3752 |  0.4502 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:31:09,105: Snapshot:1	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:11,599: Snapshot:1	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:13,826: Snapshot:1	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:16,275: Snapshot:1	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:18,476: Snapshot:1	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:20,988: Early Stopping! Snapshot: 1 Epoch: 5 Best Results: 0.3
2025-01-03 21:31:20,988: Start to training tokens! Snapshot: 1 Epoch: 5 Loss:nan MRR:0.3 Best Results: 0.3
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2025-01-03 21:31:20,988: Snapshot:1	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:23,267: Snapshot:1	Epoch:6	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:25,765: End of token training: 1 Epoch: 7 Loss:nan MRR:0.3 Best Results: 0.3
2025-01-03 21:31:25,765: Snapshot:1	Epoch:7	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 21:31:26,004: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 21:31:29,444: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:32:01,221: Snapshot:2	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:32:11,473: Snapshot:2	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:32:20,805: Snapshot:2	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:32:29,656: Snapshot:2	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:32:40,579: Snapshot:2	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:32:49,914: Early Stopping! Snapshot: 2 Epoch: 5 Best Results: 0.27
2025-01-03 21:32:49,915: Start to training tokens! Snapshot: 2 Epoch: 5 Loss:nan MRR:0.27 Best Results: 0.27
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2025-01-03 21:32:49,915: Snapshot:2	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:32:58,874: Snapshot:2	Epoch:6	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:33:08,252: End of token training: 2 Epoch: 7 Loss:nan MRR:0.27 Best Results: 0.27
2025-01-03 21:33:08,252: Snapshot:2	Epoch:7	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.27	Hits@10:1.13	Best:0.27
2025-01-03 21:33:08,490: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 21:33:15,913: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
|     2      | 0.0026 | 0.0001 | 0.0002 | 0.0014 |  0.0107 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:33:53,705: Snapshot:3	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:34:04,978: Snapshot:3	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:34:17,453: Snapshot:3	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:34:28,747: Snapshot:3	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:34:41,175: Snapshot:3	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:34:52,521: Early Stopping! Snapshot: 3 Epoch: 5 Best Results: 0.24
2025-01-03 21:34:52,522: Start to training tokens! Snapshot: 3 Epoch: 5 Loss:nan MRR:0.24 Best Results: 0.24
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2025-01-03 21:34:52,522: Snapshot:3	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:35:05,324: Snapshot:3	Epoch:6	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:35:16,617: End of token training: 3 Epoch: 7 Loss:nan MRR:0.24 Best Results: 0.24
2025-01-03 21:35:16,618: Snapshot:3	Epoch:7	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.24	Hits@10:0.94	Best:0.24
2025-01-03 21:35:16,857: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 21:35:29,036: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
|     2      | 0.0026 | 0.0001 | 0.0002 | 0.0014 |  0.0107 |
|     3      | 0.0024 | 0.0002 | 0.0003 | 0.0015 |  0.0089 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:35:46,559: Snapshot:4	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:35:51,171: Snapshot:4	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:35:56,626: Snapshot:4	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:36:01,492: Snapshot:4	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:36:06,552: Snapshot:4	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:36:11,203: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 0.36
2025-01-03 21:36:11,204: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:nan MRR:0.36 Best Results: 0.36
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2025-01-03 21:36:11,204: Snapshot:4	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:36:16,677: Snapshot:4	Epoch:6	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:36:21,207: End of token training: 4 Epoch: 7 Loss:nan MRR:0.36 Best Results: 0.36
2025-01-03 21:36:21,208: Snapshot:4	Epoch:7	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:0.36	Hits@10:2.19	Best:0.36
2025-01-03 21:36:21,496: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 21:36:36,055: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
|     2      | 0.0026 | 0.0001 | 0.0002 | 0.0014 |  0.0107 |
|     3      | 0.0024 | 0.0002 | 0.0003 | 0.0015 |  0.0089 |
|     4      | 0.0039 |  0.0   |  0.0   |  0.0   |  0.0245 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 21:36:36,058: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.151  | 0.3126 | 0.3752 |  0.4502 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
|     2      | 0.0026 | 0.0001 | 0.0002 | 0.0014 |  0.0107 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
|     2      | 0.0026 | 0.0001 | 0.0002 | 0.0014 |  0.0107 |
|     3      | 0.0024 | 0.0002 | 0.0003 | 0.0015 |  0.0089 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.0081 | 0.0003 | 0.0003 | 0.0046 |  0.0339 |
|     1      | 0.003  |  0.0   |  0.0   | 0.0012 |  0.0104 |
|     2      | 0.0026 | 0.0001 | 0.0002 | 0.0014 |  0.0107 |
|     3      | 0.0024 | 0.0002 | 0.0003 | 0.0015 |  0.0089 |
|     4      | 0.0039 |  0.0   |  0.0   |  0.0   |  0.0245 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 21:36:36,058: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 147.52333807945251 |   0.257   |    0.151     |    0.313     |      0.45     |
|    1     | 26.206024885177612 |   0.007   |     0.0      |     0.0      |     0.028     |
|    2     |  95.0860960483551  |   0.005   |     0.0      |     0.0      |     0.019     |
|    3     | 116.02156019210815 |   0.004   |     0.0      |     0.0      |     0.015     |
|    4     | 49.69709348678589  |   0.004   |     0.0      |     0.0      |     0.016     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 21:36:36,058: Sum_Training_Time:434.5341126918793
2025-01-03 21:36:36,058: Every_Training_Time:[147.52333807945251, 26.206024885177612, 95.0860960483551, 116.02156019210815, 49.69709348678589]
2025-01-03 21:36:36,059: Forward transfer: 0.0146 Backward transfer: -0.06225
2025-01-03 21:37:08,224: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103213641/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 21:37:18,398: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 21:37:23,848: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 21:37:30,030: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.68	Best:18.93
2025-01-03 21:37:36,191: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:43.54	Best:22.67
2025-01-03 21:37:42,538: Snapshot:0	Epoch:4	Loss:2.463	translation_Loss:2.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:45.26	Best:24.2
2025-01-03 21:37:49,362: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:46.0	Best:24.98
2025-01-03 21:37:55,031: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.27	Hits@10:46.31	Best:25.27
2025-01-03 21:38:01,861: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.5	Best:25.58
2025-01-03 21:38:08,305: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.46	Best:25.71
2025-01-03 21:38:14,252: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.5	Best:25.81
2025-01-03 21:38:20,509: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.47	Best:25.81
2025-01-03 21:38:26,393: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.32	Best:25.81
2025-01-03 21:38:32,636: Snapshot:0	Epoch:12	Loss:0.357	translation_Loss:0.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.17	Best:25.81
2025-01-03 21:38:38,249: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.04	Best:25.81
2025-01-03 21:38:44,852: Early Stopping! Snapshot: 0 Epoch: 14 Best Results: 25.81
2025-01-03 21:38:44,852: Start to training tokens! Snapshot: 0 Epoch: 14 Loss:0.293 MRR:25.79 Best Results: 25.81
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2025-01-03 21:38:44,853: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.13	Best:25.81
2025-01-03 21:38:50,902: Snapshot:0	Epoch:15	Loss:21.13	translation_Loss:11.505	multi_layer_Loss:9.624	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.13	Best:25.81
2025-01-03 21:38:57,588: End of token training: 0 Epoch: 16 Loss:11.843 MRR:25.79 Best Results: 25.81
2025-01-03 21:38:57,588: Snapshot:0	Epoch:16	Loss:11.843	translation_Loss:11.485	multi_layer_Loss:0.358	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.79	Hits@10:46.13	Best:25.81
2025-01-03 21:38:57,822: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 21:39:00,524: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2568 | 0.1474 | 0.3154 | 0.3803 |  0.4528 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:39:11,288: Snapshot:1	Epoch:0	Loss:4.93	translation_Loss:4.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.148                                                   	MRR:9.95	Hits@10:18.19	Best:9.95
2025-01-03 21:39:13,781: Snapshot:1	Epoch:1	Loss:2.963	translation_Loss:2.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.323                                                   	MRR:17.01	Hits@10:31.78	Best:17.01
2025-01-03 21:39:16,197: Snapshot:1	Epoch:2	Loss:1.885	translation_Loss:1.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.451                                                   	MRR:21.4	Hits@10:37.74	Best:21.4
2025-01-03 21:39:18,699: Snapshot:1	Epoch:3	Loss:1.343	translation_Loss:0.869	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:24.12	Hits@10:41.25	Best:24.12
2025-01-03 21:39:21,271: Snapshot:1	Epoch:4	Loss:1.032	translation_Loss:0.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.433                                                   	MRR:25.44	Hits@10:43.38	Best:25.44
2025-01-03 21:39:23,737: Snapshot:1	Epoch:5	Loss:0.849	translation_Loss:0.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.374                                                   	MRR:26.06	Hits@10:45.35	Best:26.06
2025-01-03 21:39:25,958: Snapshot:1	Epoch:6	Loss:0.731	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.33                                                   	MRR:26.82	Hits@10:47.01	Best:26.82
2025-01-03 21:39:28,377: Snapshot:1	Epoch:7	Loss:0.66	translation_Loss:0.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.302                                                   	MRR:27.59	Hits@10:48.22	Best:27.59
2025-01-03 21:39:30,765: Snapshot:1	Epoch:8	Loss:0.609	translation_Loss:0.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.284                                                   	MRR:28.16	Hits@10:49.14	Best:28.16
2025-01-03 21:39:33,314: Snapshot:1	Epoch:9	Loss:0.574	translation_Loss:0.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.271                                                   	MRR:28.65	Hits@10:49.92	Best:28.65
2025-01-03 21:39:36,126: Snapshot:1	Epoch:10	Loss:0.543	translation_Loss:0.283	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.26                                                   	MRR:28.98	Hits@10:50.23	Best:28.98
2025-01-03 21:39:38,233: Snapshot:1	Epoch:11	Loss:0.521	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.252                                                   	MRR:29.15	Hits@10:50.31	Best:29.15
2025-01-03 21:39:40,372: Snapshot:1	Epoch:12	Loss:0.503	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.245                                                   	MRR:29.48	Hits@10:50.44	Best:29.48
2025-01-03 21:39:42,470: Snapshot:1	Epoch:13	Loss:0.487	translation_Loss:0.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.238                                                   	MRR:29.7	Hits@10:50.98	Best:29.7
2025-01-03 21:39:44,902: Snapshot:1	Epoch:14	Loss:0.47	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.234                                                   	MRR:30.05	Hits@10:51.38	Best:30.05
2025-01-03 21:39:47,297: Snapshot:1	Epoch:15	Loss:0.461	translation_Loss:0.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:30.24	Hits@10:51.41	Best:30.24
2025-01-03 21:39:49,712: Snapshot:1	Epoch:16	Loss:0.45	translation_Loss:0.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:30.1	Hits@10:51.21	Best:30.24
2025-01-03 21:39:52,100: Snapshot:1	Epoch:17	Loss:0.438	translation_Loss:0.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.222                                                   	MRR:30.17	Hits@10:51.33	Best:30.24
2025-01-03 21:39:54,475: Snapshot:1	Epoch:18	Loss:0.433	translation_Loss:0.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.219                                                   	MRR:30.1	Hits@10:51.17	Best:30.24
2025-01-03 21:39:56,689: Snapshot:1	Epoch:19	Loss:0.419	translation_Loss:0.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.217                                                   	MRR:30.22	Hits@10:51.25	Best:30.24
2025-01-03 21:39:59,202: Early Stopping! Snapshot: 1 Epoch: 20 Best Results: 30.24
2025-01-03 21:39:59,202: Start to training tokens! Snapshot: 1 Epoch: 20 Loss:0.418 MRR:30.06 Best Results: 30.24
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2025-01-03 21:39:59,202: Snapshot:1	Epoch:20	Loss:0.418	translation_Loss:0.205	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:30.06	Hits@10:51.59	Best:30.24
2025-01-03 21:40:01,445: Snapshot:1	Epoch:21	Loss:13.446	translation_Loss:4.306	multi_layer_Loss:9.14	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.06	Hits@10:51.59	Best:30.24
2025-01-03 21:40:03,848: End of token training: 1 Epoch: 22 Loss:5.164 MRR:30.06 Best Results: 30.24
2025-01-03 21:40:03,848: Snapshot:1	Epoch:22	Loss:5.164	translation_Loss:4.304	multi_layer_Loss:0.86	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.06	Hits@10:51.59	Best:30.24
2025-01-03 21:40:04,086: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 21:40:08,246: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2557 | 0.147  | 0.3151 | 0.3804 |  0.4526 |
|     1      | 0.2982 | 0.1902 | 0.3513 | 0.422  |  0.5109 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:40:40,243: Snapshot:2	Epoch:0	Loss:15.898	translation_Loss:14.997	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.902                                                   	MRR:14.6	Hits@10:29.77	Best:14.6
2025-01-03 21:40:49,731: Snapshot:2	Epoch:1	Loss:6.981	translation_Loss:5.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.447                                                   	MRR:19.33	Hits@10:36.44	Best:19.33
2025-01-03 21:40:58,654: Snapshot:2	Epoch:2	Loss:4.547	translation_Loss:3.176	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.371                                                   	MRR:20.79	Hits@10:38.02	Best:20.79
2025-01-03 21:41:07,899: Snapshot:2	Epoch:3	Loss:3.704	translation_Loss:2.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.254                                                   	MRR:21.28	Hits@10:38.53	Best:21.28
2025-01-03 21:41:18,529: Snapshot:2	Epoch:4	Loss:3.364	translation_Loss:2.171	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.193                                                   	MRR:21.57	Hits@10:38.68	Best:21.57
2025-01-03 21:41:27,580: Snapshot:2	Epoch:5	Loss:3.193	translation_Loss:2.034	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.159                                                   	MRR:21.68	Hits@10:38.73	Best:21.68
2025-01-03 21:41:38,346: Snapshot:2	Epoch:6	Loss:3.096	translation_Loss:1.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.136                                                   	MRR:21.67	Hits@10:38.81	Best:21.68
2025-01-03 21:41:48,034: Snapshot:2	Epoch:7	Loss:3.043	translation_Loss:1.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.127                                                   	MRR:21.82	Hits@10:38.68	Best:21.82
2025-01-03 21:41:59,004: Snapshot:2	Epoch:8	Loss:2.993	translation_Loss:1.872	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.121                                                   	MRR:21.83	Hits@10:39.16	Best:21.83
2025-01-03 21:42:08,135: Snapshot:2	Epoch:9	Loss:2.964	translation_Loss:1.852	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.112                                                   	MRR:21.78	Hits@10:38.7	Best:21.83
2025-01-03 21:42:18,822: Snapshot:2	Epoch:10	Loss:2.933	translation_Loss:1.821	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.113                                                   	MRR:21.72	Hits@10:38.65	Best:21.83
2025-01-03 21:42:28,242: Snapshot:2	Epoch:11	Loss:2.908	translation_Loss:1.801	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.107                                                   	MRR:21.81	Hits@10:38.86	Best:21.83
2025-01-03 21:42:38,569: Snapshot:2	Epoch:12	Loss:2.892	translation_Loss:1.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.107                                                   	MRR:21.62	Hits@10:38.85	Best:21.83
2025-01-03 21:42:48,294: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.83
2025-01-03 21:42:48,294: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:2.88 MRR:21.7 Best Results: 21.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2025-01-03 21:42:48,294: Snapshot:2	Epoch:13	Loss:2.88	translation_Loss:1.779	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.102                                                   	MRR:21.7	Hits@10:38.93	Best:21.83
2025-01-03 21:42:57,526: Snapshot:2	Epoch:14	Loss:27.501	translation_Loss:17.546	multi_layer_Loss:9.955	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.7	Hits@10:38.93	Best:21.83
2025-01-03 21:43:08,177: End of token training: 2 Epoch: 15 Loss:17.617 MRR:21.7 Best Results: 21.83
2025-01-03 21:43:08,178: Snapshot:2	Epoch:15	Loss:17.617	translation_Loss:17.518	multi_layer_Loss:0.1	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.7	Hits@10:38.93	Best:21.83
2025-01-03 21:43:08,433: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 21:43:16,283: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1381 | 0.307  | 0.3744 |  0.4575 |
|     1      | 0.2723 | 0.1646 | 0.3165 | 0.3879 |  0.4789 |
|     2      | 0.2193 | 0.1307 | 0.2516 | 0.3105 |  0.3937 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:43:54,034: Snapshot:3	Epoch:0	Loss:14.05	translation_Loss:13.542	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.507                                                   	MRR:16.15	Hits@10:32.79	Best:16.15
2025-01-03 21:44:06,825: Snapshot:3	Epoch:1	Loss:4.558	translation_Loss:3.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.963                                                   	MRR:20.4	Hits@10:38.42	Best:20.4
2025-01-03 21:44:17,910: Snapshot:3	Epoch:2	Loss:2.755	translation_Loss:1.702	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.053                                                   	MRR:21.11	Hits@10:39.28	Best:21.11
2025-01-03 21:44:30,860: Snapshot:3	Epoch:3	Loss:2.218	translation_Loss:1.185	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.033                                                   	MRR:21.58	Hits@10:39.6	Best:21.58
2025-01-03 21:44:42,328: Snapshot:3	Epoch:4	Loss:2.01	translation_Loss:0.995	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.015                                                   	MRR:21.69	Hits@10:40.25	Best:21.69
2025-01-03 21:44:53,434: Snapshot:3	Epoch:5	Loss:1.893	translation_Loss:0.892	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.002                                                   	MRR:21.82	Hits@10:40.34	Best:21.82
2025-01-03 21:45:04,828: Snapshot:3	Epoch:6	Loss:1.837	translation_Loss:0.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.998                                                   	MRR:21.78	Hits@10:40.1	Best:21.82
2025-01-03 21:45:15,742: Snapshot:3	Epoch:7	Loss:1.796	translation_Loss:0.803	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.993                                                   	MRR:22.04	Hits@10:40.48	Best:22.04
2025-01-03 21:45:28,481: Snapshot:3	Epoch:8	Loss:1.77	translation_Loss:0.775	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.996                                                   	MRR:21.95	Hits@10:40.37	Best:22.04
2025-01-03 21:45:39,781: Snapshot:3	Epoch:9	Loss:1.758	translation_Loss:0.763	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.995                                                   	MRR:21.78	Hits@10:40.29	Best:22.04
2025-01-03 21:45:51,082: Snapshot:3	Epoch:10	Loss:1.735	translation_Loss:0.745	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.99                                                   	MRR:22.09	Hits@10:40.37	Best:22.09
2025-01-03 21:46:02,274: Snapshot:3	Epoch:11	Loss:1.735	translation_Loss:0.742	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.993                                                   	MRR:22.16	Hits@10:40.63	Best:22.16
2025-01-03 21:46:15,024: Snapshot:3	Epoch:12	Loss:1.731	translation_Loss:0.736	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.996                                                   	MRR:22.12	Hits@10:40.57	Best:22.16
2025-01-03 21:46:26,192: Snapshot:3	Epoch:13	Loss:1.727	translation_Loss:0.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.994                                                   	MRR:22.07	Hits@10:40.51	Best:22.16
2025-01-03 21:46:36,994: Snapshot:3	Epoch:14	Loss:1.718	translation_Loss:0.724	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.994                                                   	MRR:22.06	Hits@10:40.33	Best:22.16
2025-01-03 21:46:49,886: Snapshot:3	Epoch:15	Loss:1.713	translation_Loss:0.716	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.997                                                   	MRR:21.95	Hits@10:40.35	Best:22.16
2025-01-03 21:47:01,161: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 22.16
2025-01-03 21:47:01,161: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:1.702 MRR:21.99 Best Results: 22.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2025-01-03 21:47:01,161: Snapshot:3	Epoch:16	Loss:1.702	translation_Loss:0.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.997                                                   	MRR:21.99	Hits@10:40.49	Best:22.16
2025-01-03 21:47:12,226: Snapshot:3	Epoch:17	Loss:26.806	translation_Loss:17.481	multi_layer_Loss:9.325	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.99	Hits@10:40.49	Best:22.16
2025-01-03 21:47:25,024: End of token training: 3 Epoch: 18 Loss:17.53 MRR:21.99 Best Results: 22.16
2025-01-03 21:47:25,024: Snapshot:3	Epoch:18	Loss:17.53	translation_Loss:17.478	multi_layer_Loss:0.052	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.99	Hits@10:40.49	Best:22.16
2025-01-03 21:47:25,256: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 21:47:38,845: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.144  | 0.2872 | 0.3503 |  0.4318 |
|     1      | 0.2525 | 0.1554 | 0.2878 | 0.3471 |  0.4504 |
|     2      | 0.2054 | 0.1214 | 0.2333 | 0.2889 |  0.369  |
|     3      | 0.2207 | 0.1227 | 0.2601 | 0.3241 |  0.4056 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:47:56,243: Snapshot:4	Epoch:0	Loss:6.356	translation_Loss:5.92	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.436                                                   	MRR:9.83	Hits@10:21.25	Best:9.83
2025-01-03 21:48:00,721: Snapshot:4	Epoch:1	Loss:3.87	translation_Loss:3.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.832                                                   	MRR:16.17	Hits@10:34.2	Best:16.17
2025-01-03 21:48:05,520: Snapshot:4	Epoch:2	Loss:2.817	translation_Loss:1.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.934                                                   	MRR:18.54	Hits@10:35.92	Best:18.54
2025-01-03 21:48:10,013: Snapshot:4	Epoch:3	Loss:2.303	translation_Loss:1.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.903                                                   	MRR:21.2	Hits@10:36.73	Best:21.2
2025-01-03 21:48:15,276: Snapshot:4	Epoch:4	Loss:1.981	translation_Loss:1.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.85                                                   	MRR:21.98	Hits@10:37.59	Best:21.98
2025-01-03 21:48:20,613: Snapshot:4	Epoch:5	Loss:1.775	translation_Loss:0.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.799                                                   	MRR:22.85	Hits@10:38.49	Best:22.85
2025-01-03 21:48:25,760: Snapshot:4	Epoch:6	Loss:1.639	translation_Loss:0.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.757                                                   	MRR:22.96	Hits@10:38.32	Best:22.96
2025-01-03 21:48:30,342: Snapshot:4	Epoch:7	Loss:1.547	translation_Loss:0.822	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.725                                                   	MRR:22.93	Hits@10:38.54	Best:22.96
2025-01-03 21:48:35,028: Snapshot:4	Epoch:8	Loss:1.493	translation_Loss:0.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.7                                                   	MRR:22.89	Hits@10:38.37	Best:22.96
2025-01-03 21:48:40,131: Snapshot:4	Epoch:9	Loss:1.448	translation_Loss:0.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.679                                                   	MRR:22.97	Hits@10:38.28	Best:22.97
2025-01-03 21:48:44,898: Snapshot:4	Epoch:10	Loss:1.422	translation_Loss:0.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.666                                                   	MRR:22.77	Hits@10:38.51	Best:22.97
2025-01-03 21:48:49,627: Snapshot:4	Epoch:11	Loss:1.41	translation_Loss:0.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.658                                                   	MRR:22.89	Hits@10:38.43	Best:22.97
2025-01-03 21:48:54,591: Snapshot:4	Epoch:12	Loss:1.39	translation_Loss:0.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.652                                                   	MRR:22.94	Hits@10:37.93	Best:22.97
2025-01-03 21:48:59,292: Snapshot:4	Epoch:13	Loss:1.384	translation_Loss:0.738	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.646                                                   	MRR:22.93	Hits@10:38.66	Best:22.97
2025-01-03 21:49:03,967: Early Stopping! Snapshot: 4 Epoch: 14 Best Results: 22.97
2025-01-03 21:49:03,967: Start to training tokens! Snapshot: 4 Epoch: 14 Loss:1.377 MRR:22.86 Best Results: 22.97
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2025-01-03 21:49:03,967: Snapshot:4	Epoch:14	Loss:1.377	translation_Loss:0.735	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.643                                                   	MRR:22.86	Hits@10:37.88	Best:22.97
2025-01-03 21:49:08,326: Snapshot:4	Epoch:15	Loss:18.22	translation_Loss:8.785	multi_layer_Loss:9.435	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.86	Hits@10:37.88	Best:22.97
2025-01-03 21:49:12,728: End of token training: 4 Epoch: 16 Loss:9.327 MRR:22.86 Best Results: 22.97
2025-01-03 21:49:12,729: Snapshot:4	Epoch:16	Loss:9.327	translation_Loss:8.78	multi_layer_Loss:0.547	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.86	Hits@10:37.88	Best:22.97
2025-01-03 21:49:12,956: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 21:49:27,496: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2102 | 0.1149 | 0.2522 | 0.3104 |  0.389  |
|     1      | 0.2473 | 0.1507 | 0.284  | 0.343  |  0.4366 |
|     2      | 0.195  | 0.1125 | 0.2209 | 0.2762 |  0.3546 |
|     3      | 0.2015 | 0.1078 | 0.236  | 0.2992 |  0.3806 |
|     4      | 0.2304 | 0.1479 | 0.2642 | 0.3155 |  0.3848 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 21:49:27,497: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2568 | 0.1474 | 0.3154 | 0.3803 |  0.4528 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2557 | 0.147  | 0.3151 | 0.3804 |  0.4526 |
|     1      | 0.2982 | 0.1902 | 0.3513 | 0.422  |  0.5109 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2498 | 0.1381 | 0.307  | 0.3744 |  0.4575 |
|     1      | 0.2723 | 0.1646 | 0.3165 | 0.3879 |  0.4789 |
|     2      | 0.2193 | 0.1307 | 0.2516 | 0.3105 |  0.3937 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2437 | 0.144  | 0.2872 | 0.3503 |  0.4318 |
|     1      | 0.2525 | 0.1554 | 0.2878 | 0.3471 |  0.4504 |
|     2      | 0.2054 | 0.1214 | 0.2333 | 0.2889 |  0.369  |
|     3      | 0.2207 | 0.1227 | 0.2601 | 0.3241 |  0.4056 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2102 | 0.1149 | 0.2522 | 0.3104 |  0.389  |
|     1      | 0.2473 | 0.1507 | 0.284  | 0.343  |  0.4366 |
|     2      | 0.195  | 0.1125 | 0.2209 | 0.2762 |  0.3546 |
|     3      | 0.2015 | 0.1078 | 0.236  | 0.2992 |  0.3806 |
|     4      | 0.2304 | 0.1479 | 0.2642 | 0.3155 |  0.3848 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 21:49:27,498: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 109.36312198638916 |   0.257   |    0.147     |    0.315     |     0.453     |
|    1     | 62.11675190925598  |   0.267   |    0.158     |    0.325     |     0.468     |
|    2     | 175.55925512313843 |   0.236   |    0.138     |    0.279     |     0.426     |
|    3     | 243.48825430870056 |   0.223   |    0.129     |    0.259     |     0.403     |
|    4     |  91.4896731376648  |   0.208   |    0.119     |    0.242     |     0.379     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 21:49:27,498: Sum_Training_Time:682.0170564651489
2025-01-03 21:49:27,498: Every_Training_Time:[109.36312198638916, 62.11675190925598, 175.55925512313843, 243.48825430870056, 91.4896731376648]
2025-01-03 21:49:27,498: Forward transfer: 0.045325000000000004 Backward transfer: -0.03525
2025-01-03 21:49:59,440: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103214932/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 21:50:08,720: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 21:50:14,135: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 21:50:20,585: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.67	Best:18.93
2025-01-03 21:50:26,791: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.54	Best:22.69
2025-01-03 21:50:33,313: Snapshot:0	Epoch:4	Loss:2.463	translation_Loss:2.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:45.24	Best:24.22
2025-01-03 21:50:39,378: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:46.11	Best:25.0
2025-01-03 21:50:45,770: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.28	Hits@10:46.21	Best:25.28
2025-01-03 21:50:51,654: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:46.45	Best:25.53
2025-01-03 21:50:58,023: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.57	Best:25.73
2025-01-03 21:51:04,448: Snapshot:0	Epoch:9	Loss:0.534	translation_Loss:0.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.6	Best:25.81
2025-01-03 21:51:10,826: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.35	Best:25.81
2025-01-03 21:51:16,904: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.91	Hits@10:46.55	Best:25.91
2025-01-03 21:51:23,367: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.92	Hits@10:46.47	Best:25.92
2025-01-03 21:51:29,129: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:46.26	Best:25.92
2025-01-03 21:51:35,666: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.04	Best:25.92
2025-01-03 21:51:41,219: Snapshot:0	Epoch:15	Loss:0.269	translation_Loss:0.269	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:45.9	Best:25.92
2025-01-03 21:51:47,871: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.15	Best:25.92
2025-01-03 21:51:53,523: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 25.92
2025-01-03 21:51:53,523: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.236 MRR:25.69 Best Results: 25.92
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([3, 200]), requires_grad: True
 - torch.Size([3, 200]), requires_grad: True
2025-01-03 21:51:53,524: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.12	Best:25.92
2025-01-03 21:51:59,717: Snapshot:0	Epoch:18	Loss:23.369	translation_Loss:11.551	multi_layer_Loss:11.818	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.12	Best:25.92
2025-01-03 21:52:06,096: End of token training: 0 Epoch: 19 Loss:11.924 MRR:25.69 Best Results: 25.92
2025-01-03 21:52:06,097: Snapshot:0	Epoch:19	Loss:11.924	translation_Loss:11.558	multi_layer_Loss:0.366	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.69	Hits@10:46.12	Best:25.92
2025-01-03 21:52:06,328: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 21:52:08,840: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1519 | 0.3157 | 0.3798 |  0.4541 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:52:19,955: Snapshot:1	Epoch:0	Loss:4.932	translation_Loss:4.757	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:9.82	Hits@10:17.56	Best:9.82
2025-01-03 21:52:22,053: Snapshot:1	Epoch:1	Loss:2.98	translation_Loss:2.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.363                                                   	MRR:16.72	Hits@10:30.68	Best:16.72
2025-01-03 21:52:24,515: Snapshot:1	Epoch:2	Loss:1.895	translation_Loss:1.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.467                                                   	MRR:20.98	Hits@10:36.8	Best:20.98
2025-01-03 21:52:26,967: Snapshot:1	Epoch:3	Loss:1.351	translation_Loss:0.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.453                                                   	MRR:23.73	Hits@10:40.5	Best:23.73
2025-01-03 21:52:29,172: Snapshot:1	Epoch:4	Loss:1.045	translation_Loss:0.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.391                                                   	MRR:24.84	Hits@10:43.03	Best:24.84
2025-01-03 21:52:31,614: Snapshot:1	Epoch:5	Loss:0.856	translation_Loss:0.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.331                                                   	MRR:25.66	Hits@10:45.33	Best:25.66
2025-01-03 21:52:34,056: Snapshot:1	Epoch:6	Loss:0.741	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.291                                                   	MRR:26.61	Hits@10:47.22	Best:26.61
2025-01-03 21:52:36,554: Snapshot:1	Epoch:7	Loss:0.673	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.267                                                   	MRR:27.5	Hits@10:48.66	Best:27.5
2025-01-03 21:52:38,997: Snapshot:1	Epoch:8	Loss:0.629	translation_Loss:0.379	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.25                                                   	MRR:28.07	Hits@10:49.56	Best:28.07
2025-01-03 21:52:41,433: Snapshot:1	Epoch:9	Loss:0.588	translation_Loss:0.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:28.53	Hits@10:50.0	Best:28.53
2025-01-03 21:52:43,890: Snapshot:1	Epoch:10	Loss:0.557	translation_Loss:0.326	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:28.94	Hits@10:50.01	Best:28.94
2025-01-03 21:52:46,219: Snapshot:1	Epoch:11	Loss:0.533	translation_Loss:0.31	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.223                                                   	MRR:29.29	Hits@10:50.11	Best:29.29
2025-01-03 21:52:48,527: Snapshot:1	Epoch:12	Loss:0.518	translation_Loss:0.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.217                                                   	MRR:29.39	Hits@10:50.41	Best:29.39
2025-01-03 21:52:51,098: Snapshot:1	Epoch:13	Loss:0.498	translation_Loss:0.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:29.68	Hits@10:50.46	Best:29.68
2025-01-03 21:52:53,268: Snapshot:1	Epoch:14	Loss:0.486	translation_Loss:0.279	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.207                                                   	MRR:29.79	Hits@10:50.81	Best:29.79
2025-01-03 21:52:55,780: Snapshot:1	Epoch:15	Loss:0.472	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:29.75	Hits@10:50.78	Best:29.79
2025-01-03 21:52:58,367: Snapshot:1	Epoch:16	Loss:0.46	translation_Loss:0.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.199                                                   	MRR:29.72	Hits@10:50.78	Best:29.79
2025-01-03 21:53:00,454: Snapshot:1	Epoch:17	Loss:0.449	translation_Loss:0.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.195                                                   	MRR:29.86	Hits@10:51.19	Best:29.86
2025-01-03 21:53:02,947: Snapshot:1	Epoch:18	Loss:0.441	translation_Loss:0.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:30.11	Hits@10:51.2	Best:30.11
2025-01-03 21:53:05,320: Snapshot:1	Epoch:19	Loss:0.434	translation_Loss:0.243	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:29.89	Hits@10:51.43	Best:30.11
2025-01-03 21:53:07,378: Snapshot:1	Epoch:20	Loss:0.432	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:29.97	Hits@10:51.23	Best:30.11
2025-01-03 21:53:09,760: Snapshot:1	Epoch:21	Loss:0.427	translation_Loss:0.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:30.03	Hits@10:51.43	Best:30.11
2025-01-03 21:53:12,409: Snapshot:1	Epoch:22	Loss:0.418	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:30.18	Hits@10:51.33	Best:30.18
2025-01-03 21:53:14,834: Snapshot:1	Epoch:23	Loss:0.409	translation_Loss:0.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:30.16	Hits@10:51.54	Best:30.18
2025-01-03 21:53:17,172: Snapshot:1	Epoch:24	Loss:0.41	translation_Loss:0.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:30.3	Hits@10:51.51	Best:30.3
2025-01-03 21:53:19,576: Snapshot:1	Epoch:25	Loss:0.403	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:30.4	Hits@10:51.46	Best:30.4
2025-01-03 21:53:21,823: Snapshot:1	Epoch:26	Loss:0.403	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:30.38	Hits@10:51.14	Best:30.4
2025-01-03 21:53:24,266: Snapshot:1	Epoch:27	Loss:0.395	translation_Loss:0.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:30.25	Hits@10:51.38	Best:30.4
2025-01-03 21:53:26,856: Snapshot:1	Epoch:28	Loss:0.393	translation_Loss:0.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:30.22	Hits@10:51.39	Best:30.4
2025-01-03 21:53:28,921: Snapshot:1	Epoch:29	Loss:0.396	translation_Loss:0.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:30.28	Hits@10:51.16	Best:30.4
2025-01-03 21:53:30,997: Early Stopping! Snapshot: 1 Epoch: 30 Best Results: 30.4
2025-01-03 21:53:30,997: Start to training tokens! Snapshot: 1 Epoch: 30 Loss:0.395 MRR:30.27 Best Results: 30.4
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([3, 200]), requires_grad: True
 - torch.Size([3, 200]), requires_grad: True
2025-01-03 21:53:30,998: Snapshot:1	Epoch:30	Loss:0.395	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:30.27	Hits@10:51.37	Best:30.4
2025-01-03 21:53:33,078: Snapshot:1	Epoch:31	Loss:15.352	translation_Loss:4.408	multi_layer_Loss:10.945	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.27	Hits@10:51.37	Best:30.4
2025-01-03 21:53:35,099: End of token training: 1 Epoch: 32 Loss:5.985 MRR:30.27 Best Results: 30.4
2025-01-03 21:53:35,099: Snapshot:1	Epoch:32	Loss:5.985	translation_Loss:4.401	multi_layer_Loss:1.584	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.27	Hits@10:51.37	Best:30.4
2025-01-03 21:53:35,341: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 21:53:38,936: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:1 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.259 | 0.1523 | 0.3163 | 0.3809 |  0.4546 |
|     1      | 0.296 | 0.189  | 0.3444 | 0.418  |  0.5083 |
+------------+-------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:54:10,566: Snapshot:2	Epoch:0	Loss:15.82	translation_Loss:14.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.044                                                   	MRR:14.35	Hits@10:29.05	Best:14.35
2025-01-03 21:54:19,583: Snapshot:2	Epoch:1	Loss:7.034	translation_Loss:5.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.469                                                   	MRR:19.09	Hits@10:36.29	Best:19.09
2025-01-03 21:54:30,027: Snapshot:2	Epoch:2	Loss:4.583	translation_Loss:3.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.266                                                   	MRR:20.6	Hits@10:37.73	Best:20.6
2025-01-03 21:54:40,777: Snapshot:2	Epoch:3	Loss:3.733	translation_Loss:2.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.127                                                   	MRR:21.14	Hits@10:37.9	Best:21.14
2025-01-03 21:54:50,361: Snapshot:2	Epoch:4	Loss:3.405	translation_Loss:2.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.055                                                   	MRR:21.38	Hits@10:38.06	Best:21.38
2025-01-03 21:55:01,310: Snapshot:2	Epoch:5	Loss:3.235	translation_Loss:2.209	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.026                                                   	MRR:21.45	Hits@10:38.19	Best:21.45
2025-01-03 21:55:11,816: Snapshot:2	Epoch:6	Loss:3.146	translation_Loss:2.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.006                                                   	MRR:21.59	Hits@10:38.37	Best:21.59
2025-01-03 21:55:21,068: Snapshot:2	Epoch:7	Loss:3.086	translation_Loss:2.09	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.996                                                   	MRR:21.58	Hits@10:38.4	Best:21.59
2025-01-03 21:55:30,234: Snapshot:2	Epoch:8	Loss:3.051	translation_Loss:2.059	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.992                                                   	MRR:21.56	Hits@10:38.37	Best:21.59
2025-01-03 21:55:38,994: Snapshot:2	Epoch:9	Loss:3.021	translation_Loss:2.029	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.991                                                   	MRR:21.59	Hits@10:38.36	Best:21.59
2025-01-03 21:55:49,698: Snapshot:2	Epoch:10	Loss:2.998	translation_Loss:2.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.985                                                   	MRR:21.66	Hits@10:38.33	Best:21.66
2025-01-03 21:55:59,632: Snapshot:2	Epoch:11	Loss:2.982	translation_Loss:1.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.986                                                   	MRR:21.66	Hits@10:38.48	Best:21.66
2025-01-03 21:56:08,433: Snapshot:2	Epoch:12	Loss:2.957	translation_Loss:1.977	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.979                                                   	MRR:21.56	Hits@10:38.34	Best:21.66
2025-01-03 21:56:17,671: Snapshot:2	Epoch:13	Loss:2.939	translation_Loss:1.957	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.982                                                   	MRR:21.45	Hits@10:38.56	Best:21.66
2025-01-03 21:56:28,447: Snapshot:2	Epoch:14	Loss:2.936	translation_Loss:1.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.985                                                   	MRR:21.66	Hits@10:38.36	Best:21.66
2025-01-03 21:56:37,333: Early Stopping! Snapshot: 2 Epoch: 15 Best Results: 21.66
2025-01-03 21:56:37,333: Start to training tokens! Snapshot: 2 Epoch: 15 Loss:2.929 MRR:21.54 Best Results: 21.66
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([3, 200]), requires_grad: True
 - torch.Size([3, 200]), requires_grad: True
2025-01-03 21:56:37,333: Snapshot:2	Epoch:15	Loss:2.929	translation_Loss:1.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.979                                                   	MRR:21.54	Hits@10:38.34	Best:21.66
2025-01-03 21:56:46,387: Snapshot:2	Epoch:16	Loss:29.745	translation_Loss:17.868	multi_layer_Loss:11.877	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.54	Hits@10:38.34	Best:21.66
2025-01-03 21:56:55,473: End of token training: 2 Epoch: 17 Loss:18.018 MRR:21.54 Best Results: 21.66
2025-01-03 21:56:55,473: Snapshot:2	Epoch:17	Loss:18.018	translation_Loss:17.899	multi_layer_Loss:0.119	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.54	Hits@10:38.34	Best:21.66
2025-01-03 21:56:55,708: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 21:57:03,214: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2533 | 0.1415 | 0.3117 | 0.3807 |  0.4594 |
|     1      | 0.2779 | 0.1711 | 0.3223 | 0.3937 |  0.4873 |
|     2      | 0.2167 | 0.1301 | 0.2486 | 0.3073 |  0.3866 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 21:57:38,242: Snapshot:3	Epoch:0	Loss:14.103	translation_Loss:13.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.653                                                   	MRR:16.08	Hits@10:32.36	Best:16.08
2025-01-03 21:57:50,791: Snapshot:3	Epoch:1	Loss:4.811	translation_Loss:3.624	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.187                                                   	MRR:20.37	Hits@10:38.09	Best:20.37
2025-01-03 21:58:03,757: Snapshot:3	Epoch:2	Loss:3.033	translation_Loss:1.805	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.228                                                   	MRR:21.08	Hits@10:38.89	Best:21.08
2025-01-03 21:58:15,026: Snapshot:3	Epoch:3	Loss:2.502	translation_Loss:1.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.174                                                   	MRR:21.53	Hits@10:39.56	Best:21.53
2025-01-03 21:58:27,563: Snapshot:3	Epoch:4	Loss:2.307	translation_Loss:1.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.151                                                   	MRR:21.73	Hits@10:39.78	Best:21.73
2025-01-03 21:58:38,959: Snapshot:3	Epoch:5	Loss:2.191	translation_Loss:1.056	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.134                                                   	MRR:21.76	Hits@10:39.91	Best:21.76
2025-01-03 21:58:50,098: Snapshot:3	Epoch:6	Loss:2.124	translation_Loss:1.004	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.12                                                   	MRR:21.93	Hits@10:40.08	Best:21.93
2025-01-03 21:59:03,132: Snapshot:3	Epoch:7	Loss:2.082	translation_Loss:0.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.109                                                   	MRR:21.89	Hits@10:40.13	Best:21.93
2025-01-03 21:59:16,033: Snapshot:3	Epoch:8	Loss:2.058	translation_Loss:0.954	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.104                                                   	MRR:22.04	Hits@10:40.19	Best:22.04
2025-01-03 21:59:28,904: Snapshot:3	Epoch:9	Loss:2.045	translation_Loss:0.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.106                                                   	MRR:22.01	Hits@10:40.42	Best:22.04
2025-01-03 21:59:39,827: Snapshot:3	Epoch:10	Loss:2.02	translation_Loss:0.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.104                                                   	MRR:22.0	Hits@10:40.27	Best:22.04
2025-01-03 21:59:52,583: Snapshot:3	Epoch:11	Loss:2.004	translation_Loss:0.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.098                                                   	MRR:22.07	Hits@10:40.2	Best:22.07
2025-01-03 22:00:05,544: Snapshot:3	Epoch:12	Loss:1.994	translation_Loss:0.897	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:21.93	Hits@10:40.05	Best:22.07
2025-01-03 22:00:18,540: Snapshot:3	Epoch:13	Loss:2.0	translation_Loss:0.901	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.099                                                   	MRR:22.06	Hits@10:40.27	Best:22.07
2025-01-03 22:00:29,881: Snapshot:3	Epoch:14	Loss:1.998	translation_Loss:0.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.1                                                   	MRR:21.95	Hits@10:40.09	Best:22.07
2025-01-03 22:00:42,641: Snapshot:3	Epoch:15	Loss:1.992	translation_Loss:0.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.106                                                   	MRR:22.05	Hits@10:40.11	Best:22.07
2025-01-03 22:00:53,805: Early Stopping! Snapshot: 3 Epoch: 16 Best Results: 22.07
2025-01-03 22:00:53,805: Start to training tokens! Snapshot: 3 Epoch: 16 Loss:1.994 MRR:22.02 Best Results: 22.07
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([3, 200]), requires_grad: True
 - torch.Size([3, 200]), requires_grad: True
2025-01-03 22:00:53,805: Snapshot:3	Epoch:16	Loss:1.994	translation_Loss:0.888	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.107                                                   	MRR:22.02	Hits@10:40.47	Best:22.07
2025-01-03 22:01:04,842: Snapshot:3	Epoch:17	Loss:30.137	translation_Loss:17.977	multi_layer_Loss:12.16	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.02	Hits@10:40.47	Best:22.07
2025-01-03 22:01:17,665: End of token training: 3 Epoch: 18 Loss:18.053 MRR:22.02 Best Results: 22.07
2025-01-03 22:01:17,665: Snapshot:3	Epoch:18	Loss:18.053	translation_Loss:17.992	multi_layer_Loss:0.061	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.02	Hits@10:40.47	Best:22.07
2025-01-03 22:01:17,907: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 22:01:31,418: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2539 | 0.1514 | 0.2993 | 0.3649 |  0.4489 |
|     1      | 0.2687 | 0.1693 | 0.3051 | 0.3679 |  0.4643 |
|     2      | 0.2077 | 0.1236 | 0.2348 | 0.2921 |  0.3747 |
|     3      | 0.2199 | 0.1252 | 0.257  | 0.3195 |  0.4006 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:01:48,743: Snapshot:4	Epoch:0	Loss:6.492	translation_Loss:5.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.51                                                   	MRR:9.39	Hits@10:20.77	Best:9.39
2025-01-03 22:01:54,237: Snapshot:4	Epoch:1	Loss:4.158	translation_Loss:3.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.868                                                   	MRR:15.64	Hits@10:33.36	Best:15.64
2025-01-03 22:01:58,993: Snapshot:4	Epoch:2	Loss:3.127	translation_Loss:2.229	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.899                                                   	MRR:19.33	Hits@10:35.56	Best:19.33
2025-01-03 22:02:03,470: Snapshot:4	Epoch:3	Loss:2.573	translation_Loss:1.726	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.848                                                   	MRR:21.18	Hits@10:36.84	Best:21.18
2025-01-03 22:02:08,912: Snapshot:4	Epoch:4	Loss:2.228	translation_Loss:1.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.788                                                   	MRR:21.95	Hits@10:37.35	Best:21.95
2025-01-03 22:02:13,508: Snapshot:4	Epoch:5	Loss:2.0	translation_Loss:1.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.741                                                   	MRR:22.83	Hits@10:37.8	Best:22.83
2025-01-03 22:02:18,626: Snapshot:4	Epoch:6	Loss:1.851	translation_Loss:1.15	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.701                                                   	MRR:22.85	Hits@10:37.81	Best:22.85
2025-01-03 22:02:23,639: Snapshot:4	Epoch:7	Loss:1.758	translation_Loss:1.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.677                                                   	MRR:22.7	Hits@10:37.66	Best:22.85
2025-01-03 22:02:28,740: Snapshot:4	Epoch:8	Loss:1.691	translation_Loss:1.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.653                                                   	MRR:22.76	Hits@10:37.71	Best:22.85
2025-01-03 22:02:33,302: Snapshot:4	Epoch:9	Loss:1.654	translation_Loss:1.016	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.638                                                   	MRR:23.09	Hits@10:37.71	Best:23.09
2025-01-03 22:02:38,694: Snapshot:4	Epoch:10	Loss:1.634	translation_Loss:1.006	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.628                                                   	MRR:22.91	Hits@10:38.05	Best:23.09
2025-01-03 22:02:43,299: Snapshot:4	Epoch:11	Loss:1.618	translation_Loss:1.001	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.617                                                   	MRR:22.7	Hits@10:37.63	Best:23.09
2025-01-03 22:02:48,331: Snapshot:4	Epoch:12	Loss:1.596	translation_Loss:0.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.614                                                   	MRR:23.0	Hits@10:37.5	Best:23.09
2025-01-03 22:02:53,167: Snapshot:4	Epoch:13	Loss:1.585	translation_Loss:0.978	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.606                                                   	MRR:22.79	Hits@10:37.62	Best:23.09
2025-01-03 22:02:58,178: Early Stopping! Snapshot: 4 Epoch: 14 Best Results: 23.09
2025-01-03 22:02:58,178: Start to training tokens! Snapshot: 4 Epoch: 14 Loss:1.588 MRR:22.94 Best Results: 23.09
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([3, 200]), requires_grad: True
 - torch.Size([3, 200]), requires_grad: True
2025-01-03 22:02:58,178: Snapshot:4	Epoch:14	Loss:1.588	translation_Loss:0.983	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.605                                                   	MRR:22.94	Hits@10:37.73	Best:23.09
2025-01-03 22:03:02,761: Snapshot:4	Epoch:15	Loss:20.407	translation_Loss:9.11	multi_layer_Loss:11.297	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.94	Hits@10:37.73	Best:23.09
2025-01-03 22:03:07,485: End of token training: 4 Epoch: 16 Loss:9.658 MRR:22.94 Best Results: 23.09
2025-01-03 22:03:07,485: Snapshot:4	Epoch:16	Loss:9.658	translation_Loss:9.116	multi_layer_Loss:0.542	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.94	Hits@10:37.73	Best:23.09
2025-01-03 22:03:07,719: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 22:03:22,495: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2243 | 0.127  | 0.266  | 0.3262 |  0.4074 |
|     1      | 0.2634 | 0.1673 | 0.2956 | 0.3575 |  0.4528 |
|     2      | 0.1996 | 0.1171 | 0.2251 | 0.2812 |  0.3614 |
|     3      | 0.2033 | 0.1111 | 0.2358 | 0.2994 |  0.3813 |
|     4      | 0.2303 | 0.1503 | 0.263  | 0.3144 |  0.3806 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 22:03:22,497: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1519 | 0.3157 | 0.3798 |  0.4541 |
+------------+--------+--------+--------+--------+---------+, +------------+-------+--------+--------+--------+---------+
| Snapshot:1 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.259 | 0.1523 | 0.3163 | 0.3809 |  0.4546 |
|     1      | 0.296 | 0.189  | 0.3444 | 0.418  |  0.5083 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2533 | 0.1415 | 0.3117 | 0.3807 |  0.4594 |
|     1      | 0.2779 | 0.1711 | 0.3223 | 0.3937 |  0.4873 |
|     2      | 0.2167 | 0.1301 | 0.2486 | 0.3073 |  0.3866 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2539 | 0.1514 | 0.2993 | 0.3649 |  0.4489 |
|     1      | 0.2687 | 0.1693 | 0.3051 | 0.3679 |  0.4643 |
|     2      | 0.2077 | 0.1236 | 0.2348 | 0.2921 |  0.3747 |
|     3      | 0.2199 | 0.1252 | 0.257  | 0.3195 |  0.4006 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2243 | 0.127  | 0.266  | 0.3262 |  0.4074 |
|     1      | 0.2634 | 0.1673 | 0.2956 | 0.3575 |  0.4528 |
|     2      | 0.1996 | 0.1171 | 0.2251 | 0.2812 |  0.3614 |
|     3      | 0.2033 | 0.1111 | 0.2358 | 0.2994 |  0.3813 |
|     4      | 0.2303 | 0.1503 | 0.263  | 0.3144 |  0.3806 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 22:03:22,498: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 126.65545010566711 |   0.259   |    0.152     |    0.316     |     0.454     |
|    1     | 84.80162215232849  |   0.269   |    0.162     |    0.324     |     0.469     |
|    2     | 192.82953763008118 |   0.237   |    0.139     |     0.28     |     0.424     |
|    3     | 249.80001664161682 |   0.227   |    0.134     |    0.263     |     0.407     |
|    4     | 93.64914059638977  |   0.214   |    0.125     |    0.246     |     0.385     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 22:03:22,498: Sum_Training_Time:747.7357671260834
2025-01-03 22:03:22,498: Every_Training_Time:[126.65545010566711, 84.80162215232849, 192.82953763008118, 249.80001664161682, 93.64914059638977]
2025-01-03 22:03:22,498: Forward transfer: 0.04425 Backward transfer: -0.025199999999999986
2025-01-03 22:03:54,302: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103220327/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=4, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 22:04:04,366: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 22:04:09,982: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 22:04:16,241: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.68	Best:18.94
2025-01-03 22:04:22,329: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.55	Best:22.69
2025-01-03 22:04:28,700: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:45.23	Best:24.18
2025-01-03 22:04:34,751: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:45.93	Best:24.95
2025-01-03 22:04:41,083: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.28	Hits@10:46.21	Best:25.28
2025-01-03 22:04:47,008: Snapshot:0	Epoch:7	Loss:0.8	translation_Loss:0.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.43	Best:25.6
2025-01-03 22:04:53,195: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.64	Best:25.69
2025-01-03 22:04:59,070: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.51	Best:25.71
2025-01-03 22:05:05,706: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.29	Best:25.71
2025-01-03 22:05:12,041: Snapshot:0	Epoch:11	Loss:0.4	translation_Loss:0.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.32	Best:25.75
2025-01-03 22:05:18,367: Snapshot:0	Epoch:12	Loss:0.355	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.86	Hits@10:46.27	Best:25.86
2025-01-03 22:05:24,259: Snapshot:0	Epoch:13	Loss:0.32	translation_Loss:0.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.9	Hits@10:46.12	Best:25.9
2025-01-03 22:05:30,944: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:46.06	Best:25.9
2025-01-03 22:05:36,579: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:45.92	Best:25.9
2025-01-03 22:05:43,281: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.14	Best:25.9
2025-01-03 22:05:48,972: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.01	Best:25.9
2025-01-03 22:05:55,657: Early Stopping! Snapshot: 0 Epoch: 18 Best Results: 25.9
2025-01-03 22:05:55,657: Start to training tokens! Snapshot: 0 Epoch: 18 Loss:0.223 MRR:25.65 Best Results: 25.9
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([4, 200]), requires_grad: True
 - torch.Size([4, 200]), requires_grad: True
2025-01-03 22:05:55,657: Snapshot:0	Epoch:18	Loss:0.223	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.01	Best:25.9
2025-01-03 22:06:01,938: Snapshot:0	Epoch:19	Loss:25.236	translation_Loss:11.52	multi_layer_Loss:13.716	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.01	Best:25.9
2025-01-03 22:06:08,644: End of token training: 0 Epoch: 20 Loss:11.874 MRR:25.65 Best Results: 25.9
2025-01-03 22:06:08,644: Snapshot:0	Epoch:20	Loss:11.874	translation_Loss:11.512	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.65	Hits@10:46.01	Best:25.9
2025-01-03 22:06:08,893: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 22:06:11,478: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.258 | 0.1505 | 0.3154 | 0.3762 |  0.4532 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:06:22,173: Snapshot:1	Epoch:0	Loss:4.953	translation_Loss:4.755	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.198                                                   	MRR:9.63	Hits@10:16.98	Best:9.63
2025-01-03 22:06:24,607: Snapshot:1	Epoch:1	Loss:3.036	translation_Loss:2.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.388                                                   	MRR:16.55	Hits@10:30.29	Best:16.55
2025-01-03 22:06:26,873: Snapshot:1	Epoch:2	Loss:1.956	translation_Loss:1.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.463                                                   	MRR:20.76	Hits@10:36.38	Best:20.76
2025-01-03 22:06:29,025: Snapshot:1	Epoch:3	Loss:1.387	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.433                                                   	MRR:23.4	Hits@10:40.3	Best:23.4
2025-01-03 22:06:31,522: Snapshot:1	Epoch:4	Loss:1.07	translation_Loss:0.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.365                                                   	MRR:24.42	Hits@10:43.08	Best:24.42
2025-01-03 22:06:33,922: Snapshot:1	Epoch:5	Loss:0.888	translation_Loss:0.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.309                                                   	MRR:25.55	Hits@10:45.38	Best:25.55
2025-01-03 22:06:36,032: Snapshot:1	Epoch:6	Loss:0.777	translation_Loss:0.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.275                                                   	MRR:26.21	Hits@10:46.98	Best:26.21
2025-01-03 22:06:38,549: Snapshot:1	Epoch:7	Loss:0.704	translation_Loss:0.45	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.254                                                   	MRR:27.02	Hits@10:47.89	Best:27.02
2025-01-03 22:06:41,055: Snapshot:1	Epoch:8	Loss:0.66	translation_Loss:0.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.24                                                   	MRR:27.67	Hits@10:48.86	Best:27.67
2025-01-03 22:06:43,599: Snapshot:1	Epoch:9	Loss:0.625	translation_Loss:0.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:28.17	Hits@10:49.4	Best:28.17
2025-01-03 22:06:45,718: Snapshot:1	Epoch:10	Loss:0.589	translation_Loss:0.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.22                                                   	MRR:28.57	Hits@10:49.44	Best:28.57
2025-01-03 22:06:47,812: Snapshot:1	Epoch:11	Loss:0.561	translation_Loss:0.348	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:28.76	Hits@10:49.56	Best:28.76
2025-01-03 22:06:49,923: Snapshot:1	Epoch:12	Loss:0.543	translation_Loss:0.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.207                                                   	MRR:29.06	Hits@10:49.92	Best:29.06
2025-01-03 22:06:52,027: Snapshot:1	Epoch:13	Loss:0.525	translation_Loss:0.323	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:29.19	Hits@10:50.25	Best:29.19
2025-01-03 22:06:54,550: Snapshot:1	Epoch:14	Loss:0.514	translation_Loss:0.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:29.46	Hits@10:50.41	Best:29.46
2025-01-03 22:06:57,106: Snapshot:1	Epoch:15	Loss:0.492	translation_Loss:0.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:29.51	Hits@10:50.54	Best:29.51
2025-01-03 22:06:59,256: Snapshot:1	Epoch:16	Loss:0.487	translation_Loss:0.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:29.73	Hits@10:50.78	Best:29.73
2025-01-03 22:07:01,762: Snapshot:1	Epoch:17	Loss:0.474	translation_Loss:0.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:29.87	Hits@10:50.71	Best:29.87
2025-01-03 22:07:04,050: Snapshot:1	Epoch:18	Loss:0.471	translation_Loss:0.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.182                                                   	MRR:29.96	Hits@10:50.87	Best:29.96
2025-01-03 22:07:06,457: Snapshot:1	Epoch:19	Loss:0.458	translation_Loss:0.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:29.9	Hits@10:50.91	Best:29.96
2025-01-03 22:07:08,845: Snapshot:1	Epoch:20	Loss:0.454	translation_Loss:0.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:29.91	Hits@10:50.9	Best:29.96
2025-01-03 22:07:11,037: Snapshot:1	Epoch:21	Loss:0.449	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:29.82	Hits@10:50.97	Best:29.96
2025-01-03 22:07:13,753: Snapshot:1	Epoch:22	Loss:0.441	translation_Loss:0.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:30.01	Hits@10:50.53	Best:30.01
2025-01-03 22:07:16,169: Snapshot:1	Epoch:23	Loss:0.436	translation_Loss:0.262	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:30.09	Hits@10:50.62	Best:30.09
2025-01-03 22:07:18,528: Snapshot:1	Epoch:24	Loss:0.432	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:30.08	Hits@10:50.62	Best:30.09
2025-01-03 22:07:20,879: Snapshot:1	Epoch:25	Loss:0.428	translation_Loss:0.257	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:30.07	Hits@10:50.93	Best:30.09
2025-01-03 22:07:23,140: Snapshot:1	Epoch:26	Loss:0.425	translation_Loss:0.256	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:30.03	Hits@10:50.79	Best:30.09
2025-01-03 22:07:25,176: Snapshot:1	Epoch:27	Loss:0.423	translation_Loss:0.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:30.08	Hits@10:50.93	Best:30.09
2025-01-03 22:07:27,870: Early Stopping! Snapshot: 1 Epoch: 28 Best Results: 30.09
2025-01-03 22:07:27,870: Start to training tokens! Snapshot: 1 Epoch: 28 Loss:0.423 MRR:30.05 Best Results: 30.09
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([4, 200]), requires_grad: True
 - torch.Size([4, 200]), requires_grad: True
2025-01-03 22:07:27,870: Snapshot:1	Epoch:28	Loss:0.423	translation_Loss:0.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:30.05	Hits@10:51.02	Best:30.09
2025-01-03 22:07:30,099: Snapshot:1	Epoch:29	Loss:16.249	translation_Loss:4.424	multi_layer_Loss:11.826	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.05	Hits@10:51.02	Best:30.09
2025-01-03 22:07:32,517: End of token training: 1 Epoch: 30 Loss:6.787 MRR:30.05 Best Results: 30.09
2025-01-03 22:07:32,517: Snapshot:1	Epoch:30	Loss:6.787	translation_Loss:4.43	multi_layer_Loss:2.357	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.05	Hits@10:51.02	Best:30.09
2025-01-03 22:07:32,779: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 22:07:36,730: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2589 | 0.1514 | 0.3156 | 0.379  |  0.4555 |
|     1      | 0.2946 | 0.1886 | 0.3432 | 0.4135 |  0.5057 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:08:09,427: Snapshot:2	Epoch:0	Loss:15.912	translation_Loss:14.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.138                                                   	MRR:13.97	Hits@10:28.54	Best:13.97
2025-01-03 22:08:18,296: Snapshot:2	Epoch:1	Loss:7.248	translation_Loss:5.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.461                                                   	MRR:19.0	Hits@10:35.98	Best:19.0
2025-01-03 22:08:28,783: Snapshot:2	Epoch:2	Loss:4.769	translation_Loss:3.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.208                                                   	MRR:20.47	Hits@10:37.28	Best:20.47
2025-01-03 22:08:37,790: Snapshot:2	Epoch:3	Loss:3.915	translation_Loss:2.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.06                                                   	MRR:20.92	Hits@10:37.68	Best:20.92
2025-01-03 22:08:48,263: Snapshot:2	Epoch:4	Loss:3.562	translation_Loss:2.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.995                                                   	MRR:21.2	Hits@10:37.87	Best:21.2
2025-01-03 22:08:57,918: Snapshot:2	Epoch:5	Loss:3.396	translation_Loss:2.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.961                                                   	MRR:21.14	Hits@10:37.93	Best:21.2
2025-01-03 22:09:06,862: Snapshot:2	Epoch:6	Loss:3.318	translation_Loss:2.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.947                                                   	MRR:21.36	Hits@10:38.02	Best:21.36
2025-01-03 22:09:17,574: Snapshot:2	Epoch:7	Loss:3.26	translation_Loss:2.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.939                                                   	MRR:21.32	Hits@10:37.99	Best:21.36
2025-01-03 22:09:26,924: Snapshot:2	Epoch:8	Loss:3.23	translation_Loss:2.291	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.939                                                   	MRR:21.67	Hits@10:38.1	Best:21.67
2025-01-03 22:09:35,804: Snapshot:2	Epoch:9	Loss:3.187	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.934                                                   	MRR:21.56	Hits@10:38.1	Best:21.67
2025-01-03 22:09:46,398: Snapshot:2	Epoch:10	Loss:3.157	translation_Loss:2.228	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.93                                                   	MRR:21.57	Hits@10:38.17	Best:21.67
2025-01-03 22:09:55,832: Snapshot:2	Epoch:11	Loss:3.146	translation_Loss:2.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.929                                                   	MRR:21.41	Hits@10:38.11	Best:21.67
2025-01-03 22:10:06,737: Snapshot:2	Epoch:12	Loss:3.121	translation_Loss:2.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.924                                                   	MRR:21.61	Hits@10:38.29	Best:21.67
2025-01-03 22:10:15,870: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.67
2025-01-03 22:10:15,870: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:3.113 MRR:21.35 Best Results: 21.67
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([4, 200]), requires_grad: True
 - torch.Size([4, 200]), requires_grad: True
2025-01-03 22:10:15,870: Snapshot:2	Epoch:13	Loss:3.113	translation_Loss:2.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.926                                                   	MRR:21.35	Hits@10:38.11	Best:21.67
2025-01-03 22:10:25,016: Snapshot:2	Epoch:14	Loss:31.373	translation_Loss:18.075	multi_layer_Loss:13.298	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.35	Hits@10:38.11	Best:21.67
2025-01-03 22:10:34,200: End of token training: 2 Epoch: 15 Loss:18.208 MRR:21.35 Best Results: 21.67
2025-01-03 22:10:34,200: Snapshot:2	Epoch:15	Loss:18.208	translation_Loss:18.083	multi_layer_Loss:0.125	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.35	Hits@10:38.11	Best:21.67
2025-01-03 22:10:34,436: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 22:10:41,609: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2568 | 0.147  | 0.3131 | 0.3787 |  0.4584 |
|     1      | 0.2803 | 0.1758 | 0.3262 | 0.3921 |  0.4879 |
|     2      | 0.2163 | 0.1306 | 0.2474 | 0.3035 |  0.3822 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:11:17,240: Snapshot:3	Epoch:0	Loss:14.259	translation_Loss:13.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.778                                                   	MRR:16.07	Hits@10:32.3	Best:16.07
2025-01-03 22:11:28,215: Snapshot:3	Epoch:1	Loss:5.125	translation_Loss:3.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.363                                                   	MRR:20.37	Hits@10:38.16	Best:20.37
2025-01-03 22:11:39,104: Snapshot:3	Epoch:2	Loss:3.351	translation_Loss:1.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.357                                                   	MRR:20.99	Hits@10:38.97	Best:20.99
2025-01-03 22:11:52,025: Snapshot:3	Epoch:3	Loss:2.817	translation_Loss:1.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.292                                                   	MRR:21.46	Hits@10:39.56	Best:21.46
2025-01-03 22:12:03,420: Snapshot:3	Epoch:4	Loss:2.596	translation_Loss:1.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.253                                                   	MRR:21.62	Hits@10:39.71	Best:21.62
2025-01-03 22:12:14,556: Snapshot:3	Epoch:5	Loss:2.481	translation_Loss:1.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.231                                                   	MRR:21.88	Hits@10:40.12	Best:21.88
2025-01-03 22:12:27,239: Snapshot:3	Epoch:6	Loss:2.429	translation_Loss:1.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.225                                                   	MRR:21.96	Hits@10:40.1	Best:21.96
2025-01-03 22:12:38,561: Snapshot:3	Epoch:7	Loss:2.379	translation_Loss:1.163	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.217                                                   	MRR:21.93	Hits@10:39.93	Best:21.96
2025-01-03 22:12:51,418: Snapshot:3	Epoch:8	Loss:2.35	translation_Loss:1.144	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.207                                                   	MRR:21.98	Hits@10:40.05	Best:21.98
2025-01-03 22:13:02,835: Snapshot:3	Epoch:9	Loss:2.323	translation_Loss:1.12	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.203                                                   	MRR:22.01	Hits@10:40.07	Best:22.01
2025-01-03 22:13:15,674: Snapshot:3	Epoch:10	Loss:2.304	translation_Loss:1.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.192                                                   	MRR:21.98	Hits@10:40.08	Best:22.01
2025-01-03 22:13:26,723: Snapshot:3	Epoch:11	Loss:2.296	translation_Loss:1.095	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.201                                                   	MRR:21.86	Hits@10:39.63	Best:22.01
2025-01-03 22:13:37,883: Snapshot:3	Epoch:12	Loss:2.301	translation_Loss:1.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.201                                                   	MRR:22.0	Hits@10:39.9	Best:22.01
2025-01-03 22:13:48,977: Snapshot:3	Epoch:13	Loss:2.289	translation_Loss:1.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.202                                                   	MRR:21.93	Hits@10:39.76	Best:22.01
2025-01-03 22:14:00,122: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 22.01
2025-01-03 22:14:00,123: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:2.281 MRR:21.96 Best Results: 22.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([4, 200]), requires_grad: True
 - torch.Size([4, 200]), requires_grad: True
2025-01-03 22:14:00,123: Snapshot:3	Epoch:14	Loss:2.281	translation_Loss:1.081	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.201                                                   	MRR:21.96	Hits@10:39.98	Best:22.01
2025-01-03 22:14:11,195: Snapshot:3	Epoch:15	Loss:32.216	translation_Loss:18.163	multi_layer_Loss:14.053	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.96	Hits@10:39.98	Best:22.01
2025-01-03 22:14:22,269: End of token training: 3 Epoch: 16 Loss:18.232 MRR:21.96 Best Results: 22.01
2025-01-03 22:14:22,269: Snapshot:3	Epoch:16	Loss:18.232	translation_Loss:18.162	multi_layer_Loss:0.07	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.96	Hits@10:39.98	Best:22.01
2025-01-03 22:14:22,507: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 22:14:34,816: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2599 | 0.1573 | 0.3069 | 0.3723 |  0.4527 |
|     1      | 0.2723 | 0.1744 | 0.3054 | 0.3711 |  0.4687 |
|     2      | 0.2086 | 0.1236 | 0.2355 | 0.2939 |  0.3758 |
|     3      | 0.2194 | 0.124  | 0.2565 | 0.3198 |  0.4012 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:14:52,124: Snapshot:4	Epoch:0	Loss:6.583	translation_Loss:6.015	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.568                                                   	MRR:9.42	Hits@10:20.3	Best:9.42
2025-01-03 22:14:56,815: Snapshot:4	Epoch:1	Loss:4.339	translation_Loss:3.461	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.877                                                   	MRR:15.77	Hits@10:33.19	Best:15.77
2025-01-03 22:15:01,431: Snapshot:4	Epoch:2	Loss:3.34	translation_Loss:2.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.869                                                   	MRR:19.56	Hits@10:35.58	Best:19.56
2025-01-03 22:15:06,678: Snapshot:4	Epoch:3	Loss:2.779	translation_Loss:1.969	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.809                                                   	MRR:21.35	Hits@10:36.6	Best:21.35
2025-01-03 22:15:11,623: Snapshot:4	Epoch:4	Loss:2.413	translation_Loss:1.659	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.754                                                   	MRR:22.14	Hits@10:36.83	Best:22.14
2025-01-03 22:15:16,163: Snapshot:4	Epoch:5	Loss:2.179	translation_Loss:1.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.712                                                   	MRR:22.87	Hits@10:37.61	Best:22.87
2025-01-03 22:15:21,249: Snapshot:4	Epoch:6	Loss:2.018	translation_Loss:1.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.676                                                   	MRR:22.72	Hits@10:37.41	Best:22.87
2025-01-03 22:15:26,163: Snapshot:4	Epoch:7	Loss:1.924	translation_Loss:1.276	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.648                                                   	MRR:22.53	Hits@10:37.43	Best:22.87
2025-01-03 22:15:31,372: Snapshot:4	Epoch:8	Loss:1.869	translation_Loss:1.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.631                                                   	MRR:22.67	Hits@10:37.7	Best:22.87
2025-01-03 22:15:36,111: Snapshot:4	Epoch:9	Loss:1.817	translation_Loss:1.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.614                                                   	MRR:22.69	Hits@10:37.33	Best:22.87
2025-01-03 22:15:41,574: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 22.87
2025-01-03 22:15:41,574: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:1.797 MRR:22.58 Best Results: 22.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([4, 200]), requires_grad: True
 - torch.Size([4, 200]), requires_grad: True
2025-01-03 22:15:41,575: Snapshot:4	Epoch:10	Loss:1.797	translation_Loss:1.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.606                                                   	MRR:22.58	Hits@10:37.47	Best:22.87
2025-01-03 22:15:46,724: Snapshot:4	Epoch:11	Loss:22.955	translation_Loss:9.042	multi_layer_Loss:13.913	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.58	Hits@10:37.47	Best:22.87
2025-01-03 22:15:51,844: End of token training: 4 Epoch: 12 Loss:9.782 MRR:22.58 Best Results: 22.87
2025-01-03 22:15:51,844: Snapshot:4	Epoch:12	Loss:9.782	translation_Loss:9.053	multi_layer_Loss:0.729	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.58	Hits@10:37.47	Best:22.87
2025-01-03 22:15:52,084: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 22:16:07,638: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2265 | 0.1278 | 0.2691 | 0.3315 |  0.4118 |
|     1      | 0.2634 | 0.169  | 0.2932 | 0.3605 |  0.4517 |
|     2      | 0.1986 | 0.1156 | 0.2252 | 0.2801 |  0.3613 |
|     3      | 0.2013 | 0.1089 | 0.2352 | 0.2958 |  0.3791 |
|     4      | 0.2282 | 0.1535 | 0.2527 | 0.3088 |  0.3756 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 22:16:07,641: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.258 | 0.1505 | 0.3154 | 0.3762 |  0.4532 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2589 | 0.1514 | 0.3156 | 0.379  |  0.4555 |
|     1      | 0.2946 | 0.1886 | 0.3432 | 0.4135 |  0.5057 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2568 | 0.147  | 0.3131 | 0.3787 |  0.4584 |
|     1      | 0.2803 | 0.1758 | 0.3262 | 0.3921 |  0.4879 |
|     2      | 0.2163 | 0.1306 | 0.2474 | 0.3035 |  0.3822 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2599 | 0.1573 | 0.3069 | 0.3723 |  0.4527 |
|     1      | 0.2723 | 0.1744 | 0.3054 | 0.3711 |  0.4687 |
|     2      | 0.2086 | 0.1236 | 0.2355 | 0.2939 |  0.3758 |
|     3      | 0.2194 | 0.124  | 0.2565 | 0.3198 |  0.4012 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2265 | 0.1278 | 0.2691 | 0.3315 |  0.4118 |
|     1      | 0.2634 | 0.169  | 0.2932 | 0.3605 |  0.4517 |
|     2      | 0.1986 | 0.1156 | 0.2252 | 0.2801 |  0.3613 |
|     3      | 0.2013 | 0.1089 | 0.2352 | 0.2958 |  0.3791 |
|     4      | 0.2282 | 0.1535 | 0.2527 | 0.3088 |  0.3756 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 22:16:07,641: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 134.34119510650635 |   0.258   |     0.15     |    0.315     |     0.453     |
|    1     | 79.81823515892029  |   0.268   |    0.161     |    0.323     |     0.469     |
|    2     | 173.41100931167603 |   0.238   |    0.142     |     0.28     |     0.422     |
|    3     | 215.77966618537903 |   0.229   |    0.135     |    0.264     |     0.409     |
|    4     | 74.76112127304077  |   0.213   |    0.124     |    0.245     |     0.385     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 22:16:07,641: Sum_Training_Time:678.1112270355225
2025-01-03 22:16:07,641: Every_Training_Time:[134.34119510650635, 79.81823515892029, 173.41100931167603, 215.77966618537903, 74.76112127304077]
2025-01-03 22:16:07,641: Forward transfer: 0.043925000000000006 Backward transfer: -0.024624999999999987
2025-01-03 22:16:39,471: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103221612/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=6, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 22:16:49,524: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 22:16:55,449: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 22:17:01,801: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.68	Best:18.94
2025-01-03 22:17:07,783: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.56	Best:22.69
2025-01-03 22:17:13,966: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:45.28	Best:24.18
2025-01-03 22:17:19,920: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:46.0	Best:24.99
2025-01-03 22:17:26,236: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:46.23	Best:25.31
2025-01-03 22:17:32,328: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.45	Best:25.58
2025-01-03 22:17:38,659: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.51	Best:25.69
2025-01-03 22:17:44,682: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.46	Best:25.77
2025-01-03 22:17:51,217: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.48	Best:25.77
2025-01-03 22:17:57,351: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.48	Best:25.85
2025-01-03 22:18:03,363: Snapshot:0	Epoch:12	Loss:0.355	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.89	Hits@10:46.31	Best:25.89
2025-01-03 22:18:09,039: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.16	Best:25.89
2025-01-03 22:18:15,759: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:45.97	Best:25.89
2025-01-03 22:18:21,140: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:45.93	Best:25.89
2025-01-03 22:18:27,917: Snapshot:0	Epoch:16	Loss:0.251	translation_Loss:0.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.06	Best:25.89
2025-01-03 22:18:33,636: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 25.89
2025-01-03 22:18:33,636: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.239 MRR:25.61 Best Results: 25.89
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([6, 200]), requires_grad: True
 - torch.Size([6, 200]), requires_grad: True
2025-01-03 22:18:33,636: Snapshot:0	Epoch:17	Loss:0.239	translation_Loss:0.239	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.61	Hits@10:45.94	Best:25.89
2025-01-03 22:18:39,969: Snapshot:0	Epoch:18	Loss:27.039	translation_Loss:11.506	multi_layer_Loss:15.533	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.61	Hits@10:45.94	Best:25.89
2025-01-03 22:18:45,444: End of token training: 0 Epoch: 19 Loss:11.916 MRR:25.61 Best Results: 25.89
2025-01-03 22:18:45,444: Snapshot:0	Epoch:19	Loss:11.916	translation_Loss:11.512	multi_layer_Loss:0.404	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.61	Hits@10:45.94	Best:25.89
2025-01-03 22:18:45,681: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 22:18:47,962: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2585 | 0.1509 | 0.3157 | 0.3795 |  0.4529 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:18:58,936: Snapshot:1	Epoch:0	Loss:5.006	translation_Loss:4.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.238                                                   	MRR:9.66	Hits@10:17.16	Best:9.66
2025-01-03 22:19:01,250: Snapshot:1	Epoch:1	Loss:3.136	translation_Loss:2.729	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.407                                                   	MRR:16.19	Hits@10:30.05	Best:16.19
2025-01-03 22:19:03,351: Snapshot:1	Epoch:2	Loss:2.046	translation_Loss:1.606	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.44                                                   	MRR:20.48	Hits@10:35.9	Best:20.48
2025-01-03 22:19:05,845: Snapshot:1	Epoch:3	Loss:1.483	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:23.15	Hits@10:39.88	Best:23.15
2025-01-03 22:19:08,178: Snapshot:1	Epoch:4	Loss:1.17	translation_Loss:0.831	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.34                                                   	MRR:24.43	Hits@10:43.26	Best:24.43
2025-01-03 22:19:10,700: Snapshot:1	Epoch:5	Loss:0.976	translation_Loss:0.685	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.292                                                   	MRR:25.43	Hits@10:45.74	Best:25.43
2025-01-03 22:19:12,962: Snapshot:1	Epoch:6	Loss:0.857	translation_Loss:0.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.263                                                   	MRR:26.42	Hits@10:47.23	Best:26.42
2025-01-03 22:19:15,125: Snapshot:1	Epoch:7	Loss:0.785	translation_Loss:0.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.246                                                   	MRR:27.26	Hits@10:48.35	Best:27.26
2025-01-03 22:19:17,531: Snapshot:1	Epoch:8	Loss:0.735	translation_Loss:0.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.233                                                   	MRR:27.78	Hits@10:49.32	Best:27.78
2025-01-03 22:19:19,776: Snapshot:1	Epoch:9	Loss:0.69	translation_Loss:0.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.224                                                   	MRR:28.43	Hits@10:49.62	Best:28.43
2025-01-03 22:19:22,653: Snapshot:1	Epoch:10	Loss:0.656	translation_Loss:0.442	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:28.69	Hits@10:49.68	Best:28.69
2025-01-03 22:19:24,873: Snapshot:1	Epoch:11	Loss:0.629	translation_Loss:0.422	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.207                                                   	MRR:28.94	Hits@10:50.13	Best:28.94
2025-01-03 22:19:26,981: Snapshot:1	Epoch:12	Loss:0.609	translation_Loss:0.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.201                                                   	MRR:29.38	Hits@10:50.31	Best:29.38
2025-01-03 22:19:29,135: Snapshot:1	Epoch:13	Loss:0.587	translation_Loss:0.392	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.195                                                   	MRR:29.64	Hits@10:50.48	Best:29.64
2025-01-03 22:19:31,220: Snapshot:1	Epoch:14	Loss:0.573	translation_Loss:0.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:29.79	Hits@10:50.53	Best:29.79
2025-01-03 22:19:33,657: Snapshot:1	Epoch:15	Loss:0.556	translation_Loss:0.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:29.56	Hits@10:50.29	Best:29.79
2025-01-03 22:19:36,276: Snapshot:1	Epoch:16	Loss:0.543	translation_Loss:0.359	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:29.56	Hits@10:50.59	Best:29.79
2025-01-03 22:19:38,622: Snapshot:1	Epoch:17	Loss:0.531	translation_Loss:0.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:29.6	Hits@10:50.46	Best:29.79
2025-01-03 22:19:40,849: Snapshot:1	Epoch:18	Loss:0.522	translation_Loss:0.345	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:29.81	Hits@10:50.65	Best:29.81
2025-01-03 22:19:42,938: Snapshot:1	Epoch:19	Loss:0.513	translation_Loss:0.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:29.84	Hits@10:50.75	Best:29.84
2025-01-03 22:19:45,382: Snapshot:1	Epoch:20	Loss:0.511	translation_Loss:0.339	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.172                                                   	MRR:29.9	Hits@10:50.73	Best:29.9
2025-01-03 22:19:47,590: Snapshot:1	Epoch:21	Loss:0.505	translation_Loss:0.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:29.81	Hits@10:50.63	Best:29.9
2025-01-03 22:19:50,410: Snapshot:1	Epoch:22	Loss:0.497	translation_Loss:0.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:29.91	Hits@10:50.86	Best:29.91
2025-01-03 22:19:52,624: Snapshot:1	Epoch:23	Loss:0.488	translation_Loss:0.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:30.02	Hits@10:50.98	Best:30.02
2025-01-03 22:19:55,114: Snapshot:1	Epoch:24	Loss:0.488	translation_Loss:0.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.166                                                   	MRR:30.18	Hits@10:50.97	Best:30.18
2025-01-03 22:19:57,330: Snapshot:1	Epoch:25	Loss:0.481	translation_Loss:0.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:30.17	Hits@10:51.04	Best:30.18
2025-01-03 22:19:59,472: Snapshot:1	Epoch:26	Loss:0.481	translation_Loss:0.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:30.21	Hits@10:50.91	Best:30.21
2025-01-03 22:20:01,543: Snapshot:1	Epoch:27	Loss:0.476	translation_Loss:0.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:30.21	Hits@10:50.85	Best:30.21
2025-01-03 22:20:04,310: Snapshot:1	Epoch:28	Loss:0.47	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:29.92	Hits@10:50.78	Best:30.21
2025-01-03 22:20:06,616: Snapshot:1	Epoch:29	Loss:0.474	translation_Loss:0.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:29.93	Hits@10:50.75	Best:30.21
2025-01-03 22:20:08,728: Snapshot:1	Epoch:30	Loss:0.472	translation_Loss:0.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:30.15	Hits@10:50.96	Best:30.21
2025-01-03 22:20:10,809: Early Stopping! Snapshot: 1 Epoch: 31 Best Results: 30.21
2025-01-03 22:20:10,809: Start to training tokens! Snapshot: 1 Epoch: 31 Loss:0.469 MRR:30.19 Best Results: 30.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([6, 200]), requires_grad: True
 - torch.Size([6, 200]), requires_grad: True
2025-01-03 22:20:10,810: Snapshot:1	Epoch:31	Loss:0.469	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.162                                                   	MRR:30.19	Hits@10:51.17	Best:30.21
2025-01-03 22:20:13,188: Snapshot:1	Epoch:32	Loss:16.912	translation_Loss:4.472	multi_layer_Loss:12.44	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:30.19	Hits@10:51.17	Best:30.21
2025-01-03 22:20:15,380: End of token training: 1 Epoch: 33 Loss:7.632 MRR:30.19 Best Results: 30.21
2025-01-03 22:20:15,380: Snapshot:1	Epoch:33	Loss:7.632	translation_Loss:4.477	multi_layer_Loss:3.155	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:30.19	Hits@10:51.17	Best:30.21
2025-01-03 22:20:15,618: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 22:20:19,280: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2597 | 0.1523 | 0.3175 | 0.3801 |  0.4545 |
|     1      | 0.2939 | 0.1884 | 0.342  | 0.4132 |  0.5055 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:20:51,350: Snapshot:2	Epoch:0	Loss:16.183	translation_Loss:14.924	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.259                                                   	MRR:13.84	Hits@10:28.16	Best:13.84
2025-01-03 22:21:00,558: Snapshot:2	Epoch:1	Loss:7.736	translation_Loss:6.295	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.44                                                   	MRR:18.85	Hits@10:35.7	Best:18.85
2025-01-03 22:21:10,960: Snapshot:2	Epoch:2	Loss:5.197	translation_Loss:4.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.155                                                   	MRR:20.16	Hits@10:36.92	Best:20.16
2025-01-03 22:21:20,651: Snapshot:2	Epoch:3	Loss:4.31	translation_Loss:3.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.008                                                   	MRR:20.88	Hits@10:37.31	Best:20.88
2025-01-03 22:21:29,941: Snapshot:2	Epoch:4	Loss:3.949	translation_Loss:2.999	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.949                                                   	MRR:21.02	Hits@10:37.53	Best:21.02
2025-01-03 22:21:40,374: Snapshot:2	Epoch:5	Loss:3.783	translation_Loss:2.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.918                                                   	MRR:21.08	Hits@10:37.57	Best:21.08
2025-01-03 22:21:49,782: Snapshot:2	Epoch:6	Loss:3.684	translation_Loss:2.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.908                                                   	MRR:21.25	Hits@10:37.73	Best:21.25
2025-01-03 22:22:00,516: Snapshot:2	Epoch:7	Loss:3.631	translation_Loss:2.728	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.903                                                   	MRR:21.25	Hits@10:37.45	Best:21.25
2025-01-03 22:22:09,679: Snapshot:2	Epoch:8	Loss:3.597	translation_Loss:2.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.899                                                   	MRR:21.39	Hits@10:37.8	Best:21.39
2025-01-03 22:22:20,749: Snapshot:2	Epoch:9	Loss:3.554	translation_Loss:2.656	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.898                                                   	MRR:21.24	Hits@10:37.57	Best:21.39
2025-01-03 22:22:31,415: Snapshot:2	Epoch:10	Loss:3.532	translation_Loss:2.636	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.896                                                   	MRR:21.17	Hits@10:37.7	Best:21.39
2025-01-03 22:22:40,955: Snapshot:2	Epoch:11	Loss:3.52	translation_Loss:2.622	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.897                                                   	MRR:21.17	Hits@10:37.95	Best:21.39
2025-01-03 22:22:51,327: Snapshot:2	Epoch:12	Loss:3.497	translation_Loss:2.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.896                                                   	MRR:21.09	Hits@10:37.66	Best:21.39
2025-01-03 22:23:00,983: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.39
2025-01-03 22:23:00,983: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:3.492 MRR:21.06 Best Results: 21.39
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([6, 200]), requires_grad: True
 - torch.Size([6, 200]), requires_grad: True
2025-01-03 22:23:00,984: Snapshot:2	Epoch:13	Loss:3.492	translation_Loss:2.595	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.897                                                   	MRR:21.06	Hits@10:37.59	Best:21.39
2025-01-03 22:23:10,358: Snapshot:2	Epoch:14	Loss:33.317	translation_Loss:18.325	multi_layer_Loss:14.991	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.06	Hits@10:37.59	Best:21.39
2025-01-03 22:23:20,703: End of token training: 2 Epoch: 15 Loss:18.455 MRR:21.06 Best Results: 21.39
2025-01-03 22:23:20,703: Snapshot:2	Epoch:15	Loss:18.455	translation_Loss:18.331	multi_layer_Loss:0.124	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.06	Hits@10:37.59	Best:21.39
2025-01-03 22:23:20,993: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 22:23:29,043: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2562 | 0.1458 | 0.3146 | 0.3788 |  0.4582 |
|     1      | 0.2839 | 0.1788 | 0.3327 | 0.3939 |  0.4883 |
|     2      | 0.2133 | 0.1277 | 0.2437 | 0.3011 |  0.3827 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:24:08,259: Snapshot:3	Epoch:0	Loss:14.688	translation_Loss:13.713	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.975                                                   	MRR:15.81	Hits@10:32.11	Best:15.81
2025-01-03 22:24:19,598: Snapshot:3	Epoch:1	Loss:5.732	translation_Loss:4.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.626                                                   	MRR:20.14	Hits@10:37.75	Best:20.14
2025-01-03 22:24:32,747: Snapshot:3	Epoch:2	Loss:3.921	translation_Loss:2.368	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.553                                                   	MRR:21.05	Hits@10:39.24	Best:21.05
2025-01-03 22:24:44,132: Snapshot:3	Epoch:3	Loss:3.371	translation_Loss:1.902	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.469                                                   	MRR:21.41	Hits@10:39.36	Best:21.41
2025-01-03 22:24:57,147: Snapshot:3	Epoch:4	Loss:3.133	translation_Loss:1.715	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.419                                                   	MRR:21.69	Hits@10:39.73	Best:21.69
2025-01-03 22:25:08,622: Snapshot:3	Epoch:5	Loss:3.015	translation_Loss:1.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.391                                                   	MRR:21.73	Hits@10:39.67	Best:21.73
2025-01-03 22:25:21,499: Snapshot:3	Epoch:6	Loss:2.951	translation_Loss:1.577	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.374                                                   	MRR:21.79	Hits@10:39.83	Best:21.79
2025-01-03 22:25:33,016: Snapshot:3	Epoch:7	Loss:2.898	translation_Loss:1.538	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.36                                                   	MRR:21.68	Hits@10:39.54	Best:21.79
2025-01-03 22:25:45,497: Snapshot:3	Epoch:8	Loss:2.86	translation_Loss:1.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.355                                                   	MRR:21.83	Hits@10:39.71	Best:21.83
2025-01-03 22:25:56,849: Snapshot:3	Epoch:9	Loss:2.847	translation_Loss:1.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.35                                                   	MRR:21.78	Hits@10:39.71	Best:21.83
2025-01-03 22:26:09,828: Snapshot:3	Epoch:10	Loss:2.827	translation_Loss:1.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.352                                                   	MRR:21.74	Hits@10:39.44	Best:21.83
2025-01-03 22:26:21,111: Snapshot:3	Epoch:11	Loss:2.814	translation_Loss:1.464	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.35                                                   	MRR:21.8	Hits@10:39.87	Best:21.83
2025-01-03 22:26:34,000: Snapshot:3	Epoch:12	Loss:2.814	translation_Loss:1.467	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.347                                                   	MRR:21.78	Hits@10:39.55	Best:21.83
2025-01-03 22:26:45,347: Early Stopping! Snapshot: 3 Epoch: 13 Best Results: 21.83
2025-01-03 22:26:45,347: Start to training tokens! Snapshot: 3 Epoch: 13 Loss:2.792 MRR:21.71 Best Results: 21.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([6, 200]), requires_grad: True
 - torch.Size([6, 200]), requires_grad: True
2025-01-03 22:26:45,347: Snapshot:3	Epoch:13	Loss:2.792	translation_Loss:1.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:21.71	Hits@10:39.57	Best:21.83
2025-01-03 22:26:57,847: Snapshot:3	Epoch:14	Loss:35.017	translation_Loss:18.566	multi_layer_Loss:16.452	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.71	Hits@10:39.57	Best:21.83
2025-01-03 22:27:09,079: End of token training: 3 Epoch: 15 Loss:18.64 MRR:21.71 Best Results: 21.83
2025-01-03 22:27:09,079: Snapshot:3	Epoch:15	Loss:18.64	translation_Loss:18.564	multi_layer_Loss:0.075	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.71	Hits@10:39.57	Best:21.83
2025-01-03 22:27:09,314: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 22:27:21,834: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1575 | 0.3108 | 0.3757 |  0.4552 |
|     1      | 0.2796 | 0.1774 | 0.3176 | 0.3837 |  0.4806 |
|     2      | 0.2116 | 0.1263 | 0.2404 | 0.2984 |  0.3768 |
|     3      | 0.2172 | 0.1228 | 0.2547 | 0.318  |  0.3977 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:27:38,662: Snapshot:4	Epoch:0	Loss:6.773	translation_Loss:6.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.648                                                   	MRR:9.39	Hits@10:20.22	Best:9.39
2025-01-03 22:27:43,212: Snapshot:4	Epoch:1	Loss:4.698	translation_Loss:3.836	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.862                                                   	MRR:15.22	Hits@10:32.66	Best:15.22
2025-01-03 22:27:48,399: Snapshot:4	Epoch:2	Loss:3.693	translation_Loss:2.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.819                                                   	MRR:18.97	Hits@10:35.59	Best:18.97
2025-01-03 22:27:53,871: Snapshot:4	Epoch:3	Loss:3.099	translation_Loss:2.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.757                                                   	MRR:21.24	Hits@10:36.39	Best:21.24
2025-01-03 22:27:58,696: Snapshot:4	Epoch:4	Loss:2.712	translation_Loss:2.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.704                                                   	MRR:22.31	Hits@10:36.9	Best:22.31
2025-01-03 22:28:03,849: Snapshot:4	Epoch:5	Loss:2.458	translation_Loss:1.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.664                                                   	MRR:22.25	Hits@10:37.17	Best:22.31
2025-01-03 22:28:08,807: Snapshot:4	Epoch:6	Loss:2.293	translation_Loss:1.664	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.63                                                   	MRR:22.83	Hits@10:36.92	Best:22.83
2025-01-03 22:28:13,250: Snapshot:4	Epoch:7	Loss:2.193	translation_Loss:1.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:22.51	Hits@10:36.9	Best:22.83
2025-01-03 22:28:17,652: Snapshot:4	Epoch:8	Loss:2.139	translation_Loss:1.547	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.592                                                   	MRR:22.57	Hits@10:36.82	Best:22.83
2025-01-03 22:28:23,126: Snapshot:4	Epoch:9	Loss:2.089	translation_Loss:1.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.577                                                   	MRR:22.67	Hits@10:36.69	Best:22.83
2025-01-03 22:28:27,684: Snapshot:4	Epoch:10	Loss:2.069	translation_Loss:1.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.571                                                   	MRR:22.56	Hits@10:36.41	Best:22.83
2025-01-03 22:28:32,742: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 22.83
2025-01-03 22:28:32,742: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:2.05 MRR:22.57 Best Results: 22.83
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([6, 200]), requires_grad: True
 - torch.Size([6, 200]), requires_grad: True
2025-01-03 22:28:32,742: Snapshot:4	Epoch:11	Loss:2.05	translation_Loss:1.485	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.565                                                   	MRR:22.57	Hits@10:36.52	Best:22.83
2025-01-03 22:28:37,710: Snapshot:4	Epoch:12	Loss:24.859	translation_Loss:9.392	multi_layer_Loss:15.467	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.57	Hits@10:36.52	Best:22.83
2025-01-03 22:28:42,093: End of token training: 4 Epoch: 13 Loss:10.466 MRR:22.57 Best Results: 22.83
2025-01-03 22:28:42,094: Snapshot:4	Epoch:13	Loss:10.466	translation_Loss:9.411	multi_layer_Loss:1.055	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.57	Hits@10:36.52	Best:22.83
2025-01-03 22:28:42,330: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 22:28:56,711: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2309 | 0.1302 | 0.2778 | 0.339  |  0.4167 |
|     1      | 0.2725 | 0.1722 | 0.308  | 0.376  |  0.4677 |
|     2      | 0.2037 | 0.1195 | 0.2324 | 0.2869 |  0.3654 |
|     3      | 0.2035 | 0.1118 | 0.2377 | 0.299  |  0.3794 |
|     4      | 0.2277 | 0.1553 | 0.2558 | 0.3041 |  0.3692 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 22:28:56,713: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2585 | 0.1509 | 0.3157 | 0.3795 |  0.4529 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2597 | 0.1523 | 0.3175 | 0.3801 |  0.4545 |
|     1      | 0.2939 | 0.1884 | 0.342  | 0.4132 |  0.5055 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2562 | 0.1458 | 0.3146 | 0.3788 |  0.4582 |
|     1      | 0.2839 | 0.1788 | 0.3327 | 0.3939 |  0.4883 |
|     2      | 0.2133 | 0.1277 | 0.2437 | 0.3011 |  0.3827 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1575 | 0.3108 | 0.3757 |  0.4552 |
|     1      | 0.2796 | 0.1774 | 0.3176 | 0.3837 |  0.4806 |
|     2      | 0.2116 | 0.1263 | 0.2404 | 0.2984 |  0.3768 |
|     3      | 0.2172 | 0.1228 | 0.2547 | 0.318  |  0.3977 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2309 | 0.1302 | 0.2778 | 0.339  |  0.4167 |
|     1      | 0.2725 | 0.1722 | 0.308  | 0.376  |  0.4677 |
|     2      | 0.2037 | 0.1195 | 0.2324 | 0.2869 |  0.3654 |
|     3      | 0.2035 | 0.1118 | 0.2377 | 0.299  |  0.3794 |
|     4      | 0.2277 | 0.1553 | 0.2558 | 0.3041 |  0.3692 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 22:28:56,714: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 125.97192025184631 |   0.259   |    0.151     |    0.316     |     0.453     |
|    1     | 86.02602195739746  |   0.269   |    0.162     |    0.324     |     0.468     |
|    2     | 177.6866419315338  |   0.237   |     0.14     |    0.279     |     0.422     |
|    3     | 214.78158402442932 |    0.23   |    0.136     |    0.267     |      0.41     |
|    4     | 77.77420115470886  |   0.216   |    0.127     |    0.251     |     0.387     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 22:28:56,714: Sum_Training_Time:682.2403693199158
2025-01-03 22:28:56,714: Every_Training_Time:[125.97192025184631, 86.02602195739746, 177.6866419315338, 214.78158402442932, 77.77420115470886]
2025-01-03 22:28:56,714: Forward transfer: 0.044575000000000004 Backward transfer: -0.018075
2025-01-03 22:29:28,619: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103222901/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 22:29:38,760: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 22:29:44,473: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 22:29:50,891: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.67	Best:18.93
2025-01-03 22:29:56,921: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.68	Hits@10:43.53	Best:22.68
2025-01-03 22:30:03,389: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:45.24	Best:24.23
2025-01-03 22:30:09,699: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:45.99	Best:24.95
2025-01-03 22:30:16,095: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.28	Hits@10:46.23	Best:25.28
2025-01-03 22:30:22,276: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.59	Hits@10:46.4	Best:25.59
2025-01-03 22:30:28,716: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.58	Best:25.62
2025-01-03 22:30:34,766: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.63	Best:25.81
2025-01-03 22:30:41,131: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.4	Best:25.81
2025-01-03 22:30:47,381: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.51	Best:25.81
2025-01-03 22:30:53,842: Snapshot:0	Epoch:12	Loss:0.357	translation_Loss:0.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.87	Hits@10:46.28	Best:25.87
2025-01-03 22:30:59,567: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.2	Best:25.87
2025-01-03 22:31:06,280: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.21	Best:25.87
2025-01-03 22:31:12,037: Snapshot:0	Epoch:15	Loss:0.272	translation_Loss:0.272	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:45.93	Best:25.87
2025-01-03 22:31:18,703: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.09	Best:25.87
2025-01-03 22:31:24,584: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 25.87
2025-01-03 22:31:24,584: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.237 MRR:25.71 Best Results: 25.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-03 22:31:24,585: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.07	Best:25.87
2025-01-03 22:31:30,890: Snapshot:0	Epoch:18	Loss:27.641	translation_Loss:11.524	multi_layer_Loss:16.117	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.07	Best:25.87
2025-01-03 22:31:36,384: End of token training: 0 Epoch: 19 Loss:11.987 MRR:25.71 Best Results: 25.87
2025-01-03 22:31:36,384: Snapshot:0	Epoch:19	Loss:11.987	translation_Loss:11.531	multi_layer_Loss:0.456	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.71	Hits@10:46.07	Best:25.87
2025-01-03 22:31:36,644: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 22:31:38,937: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2585 | 0.1518 | 0.3137 | 0.3794 |  0.4528 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:31:49,472: Snapshot:1	Epoch:0	Loss:5.027	translation_Loss:4.772	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.255                                                   	MRR:9.57	Hits@10:16.91	Best:9.57
2025-01-03 22:31:51,630: Snapshot:1	Epoch:1	Loss:3.169	translation_Loss:2.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.408                                                   	MRR:16.06	Hits@10:29.69	Best:16.06
2025-01-03 22:31:54,113: Snapshot:1	Epoch:2	Loss:2.081	translation_Loss:1.654	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.427                                                   	MRR:20.4	Hits@10:35.65	Best:20.4
2025-01-03 22:31:56,642: Snapshot:1	Epoch:3	Loss:1.514	translation_Loss:1.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:22.79	Hits@10:39.81	Best:22.79
2025-01-03 22:31:58,884: Snapshot:1	Epoch:4	Loss:1.199	translation_Loss:0.871	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.327                                                   	MRR:24.09	Hits@10:43.15	Best:24.09
2025-01-03 22:32:01,054: Snapshot:1	Epoch:5	Loss:1.005	translation_Loss:0.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.283                                                   	MRR:25.15	Hits@10:45.84	Best:25.15
2025-01-03 22:32:03,576: Snapshot:1	Epoch:6	Loss:0.885	translation_Loss:0.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:25.93	Hits@10:47.19	Best:25.93
2025-01-03 22:32:05,845: Snapshot:1	Epoch:7	Loss:0.809	translation_Loss:0.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:26.95	Hits@10:48.26	Best:26.95
2025-01-03 22:32:08,410: Snapshot:1	Epoch:8	Loss:0.759	translation_Loss:0.531	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:27.49	Hits@10:48.95	Best:27.49
2025-01-03 22:32:10,684: Snapshot:1	Epoch:9	Loss:0.712	translation_Loss:0.494	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.218                                                   	MRR:28.06	Hits@10:49.31	Best:28.06
2025-01-03 22:32:13,473: Snapshot:1	Epoch:10	Loss:0.677	translation_Loss:0.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:28.28	Hits@10:49.47	Best:28.28
2025-01-03 22:32:15,790: Snapshot:1	Epoch:11	Loss:0.65	translation_Loss:0.447	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.203                                                   	MRR:28.68	Hits@10:49.78	Best:28.68
2025-01-03 22:32:18,367: Snapshot:1	Epoch:12	Loss:0.629	translation_Loss:0.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:29.11	Hits@10:50.23	Best:29.11
2025-01-03 22:32:20,745: Snapshot:1	Epoch:13	Loss:0.607	translation_Loss:0.415	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.192                                                   	MRR:29.43	Hits@10:50.18	Best:29.43
2025-01-03 22:32:23,164: Snapshot:1	Epoch:14	Loss:0.592	translation_Loss:0.403	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:29.62	Hits@10:50.28	Best:29.62
2025-01-03 22:32:25,482: Snapshot:1	Epoch:15	Loss:0.574	translation_Loss:0.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:29.46	Hits@10:50.38	Best:29.62
2025-01-03 22:32:28,234: Snapshot:1	Epoch:16	Loss:0.56	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:29.44	Hits@10:50.37	Best:29.62
2025-01-03 22:32:30,436: Snapshot:1	Epoch:17	Loss:0.548	translation_Loss:0.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:29.73	Hits@10:50.59	Best:29.73
2025-01-03 22:32:32,923: Snapshot:1	Epoch:18	Loss:0.539	translation_Loss:0.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:29.76	Hits@10:50.57	Best:29.76
2025-01-03 22:32:35,374: Snapshot:1	Epoch:19	Loss:0.531	translation_Loss:0.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:29.62	Hits@10:50.65	Best:29.76
2025-01-03 22:32:37,921: Snapshot:1	Epoch:20	Loss:0.528	translation_Loss:0.361	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.63	Hits@10:50.61	Best:29.76
2025-01-03 22:32:40,131: Snapshot:1	Epoch:21	Loss:0.523	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.62	Hits@10:50.55	Best:29.76
2025-01-03 22:32:42,534: Snapshot:1	Epoch:22	Loss:0.516	translation_Loss:0.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.166                                                   	MRR:29.62	Hits@10:50.79	Best:29.76
2025-01-03 22:32:45,014: Snapshot:1	Epoch:23	Loss:0.506	translation_Loss:0.342	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:30.02	Hits@10:51.05	Best:30.02
2025-01-03 22:32:47,211: Snapshot:1	Epoch:24	Loss:0.506	translation_Loss:0.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:30.02	Hits@10:50.96	Best:30.02
2025-01-03 22:32:49,319: Snapshot:1	Epoch:25	Loss:0.5	translation_Loss:0.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.162                                                   	MRR:29.81	Hits@10:50.93	Best:30.02
2025-01-03 22:32:51,409: Snapshot:1	Epoch:26	Loss:0.499	translation_Loss:0.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.162                                                   	MRR:29.76	Hits@10:50.6	Best:30.02
2025-01-03 22:32:53,785: Snapshot:1	Epoch:27	Loss:0.491	translation_Loss:0.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.93	Hits@10:50.8	Best:30.02
2025-01-03 22:32:56,519: Early Stopping! Snapshot: 1 Epoch: 28 Best Results: 30.02
2025-01-03 22:32:56,519: Start to training tokens! Snapshot: 1 Epoch: 28 Loss:0.487 MRR:29.71 Best Results: 30.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-03 22:32:56,519: Snapshot:1	Epoch:28	Loss:0.487	translation_Loss:0.329	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:29.71	Hits@10:50.48	Best:30.02
2025-01-03 22:32:58,636: Snapshot:1	Epoch:29	Loss:16.89	translation_Loss:4.51	multi_layer_Loss:12.38	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.71	Hits@10:50.48	Best:30.02
2025-01-03 22:33:00,713: End of token training: 1 Epoch: 30 Loss:7.778 MRR:29.71 Best Results: 30.02
2025-01-03 22:33:00,713: Snapshot:1	Epoch:30	Loss:7.778	translation_Loss:4.501	multi_layer_Loss:3.277	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.71	Hits@10:50.48	Best:30.02
2025-01-03 22:33:00,998: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 22:33:04,452: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2597 | 0.1534 | 0.3155 | 0.3814 |  0.453  |
|     1      | 0.2903 | 0.1835 | 0.342  | 0.4152 |  0.5022 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:33:34,894: Snapshot:2	Epoch:0	Loss:16.271	translation_Loss:14.973	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.298                                                   	MRR:13.85	Hits@10:27.92	Best:13.85
2025-01-03 22:33:45,654: Snapshot:2	Epoch:1	Loss:7.902	translation_Loss:6.476	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.426                                                   	MRR:18.78	Hits@10:35.53	Best:18.78
2025-01-03 22:33:55,141: Snapshot:2	Epoch:2	Loss:5.329	translation_Loss:4.192	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.137                                                   	MRR:20.15	Hits@10:37.07	Best:20.15
2025-01-03 22:34:06,028: Snapshot:2	Epoch:3	Loss:4.44	translation_Loss:3.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.996                                                   	MRR:20.56	Hits@10:37.49	Best:20.56
2025-01-03 22:34:15,256: Snapshot:2	Epoch:4	Loss:4.083	translation_Loss:3.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.935                                                   	MRR:20.77	Hits@10:37.66	Best:20.77
2025-01-03 22:34:26,300: Snapshot:2	Epoch:5	Loss:3.902	translation_Loss:2.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.91                                                   	MRR:21.07	Hits@10:37.44	Best:21.07
2025-01-03 22:34:35,859: Snapshot:2	Epoch:6	Loss:3.805	translation_Loss:2.908	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.897                                                   	MRR:21.17	Hits@10:37.73	Best:21.17
2025-01-03 22:34:46,213: Snapshot:2	Epoch:7	Loss:3.755	translation_Loss:2.864	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.891                                                   	MRR:21.13	Hits@10:37.59	Best:21.17
2025-01-03 22:34:55,688: Snapshot:2	Epoch:8	Loss:3.715	translation_Loss:2.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.889                                                   	MRR:21.23	Hits@10:37.72	Best:21.23
2025-01-03 22:35:05,221: Snapshot:2	Epoch:9	Loss:3.678	translation_Loss:2.788	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.89                                                   	MRR:21.1	Hits@10:37.6	Best:21.23
2025-01-03 22:35:14,278: Snapshot:2	Epoch:10	Loss:3.641	translation_Loss:2.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.885                                                   	MRR:21.11	Hits@10:37.57	Best:21.23
2025-01-03 22:35:24,987: Snapshot:2	Epoch:11	Loss:3.639	translation_Loss:2.754	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.885                                                   	MRR:21.11	Hits@10:37.75	Best:21.23
2025-01-03 22:35:34,702: Snapshot:2	Epoch:12	Loss:3.613	translation_Loss:2.728	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.885                                                   	MRR:21.1	Hits@10:37.67	Best:21.23
2025-01-03 22:35:44,017: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.23
2025-01-03 22:35:44,017: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:3.592 MRR:21.02 Best Results: 21.23
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-03 22:35:44,017: Snapshot:2	Epoch:13	Loss:3.592	translation_Loss:2.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.887                                                   	MRR:21.02	Hits@10:37.65	Best:21.23
2025-01-03 22:35:52,936: Snapshot:2	Epoch:14	Loss:33.845	translation_Loss:18.426	multi_layer_Loss:15.419	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.02	Hits@10:37.65	Best:21.23
2025-01-03 22:36:03,868: End of token training: 2 Epoch: 15 Loss:18.555 MRR:21.02 Best Results: 21.23
2025-01-03 22:36:03,869: Snapshot:2	Epoch:15	Loss:18.555	translation_Loss:18.432	multi_layer_Loss:0.123	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.02	Hits@10:37.65	Best:21.23
2025-01-03 22:36:04,104: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 22:36:12,175: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2562 | 0.1462 | 0.3138 | 0.3794 |  0.4568 |
|     1      | 0.2802 | 0.1752 | 0.3261 | 0.3922 |  0.4929 |
|     2      | 0.2127 | 0.1274 | 0.2443 | 0.3025 |  0.3781 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:36:50,456: Snapshot:3	Epoch:0	Loss:14.818	translation_Loss:13.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.057                                                   	MRR:15.89	Hits@10:32.1	Best:15.89
2025-01-03 22:37:03,213: Snapshot:3	Epoch:1	Loss:5.954	translation_Loss:4.24	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.714                                                   	MRR:20.13	Hits@10:37.79	Best:20.13
2025-01-03 22:37:14,657: Snapshot:3	Epoch:2	Loss:4.154	translation_Loss:2.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.609                                                   	MRR:20.88	Hits@10:38.88	Best:20.88
2025-01-03 22:37:27,381: Snapshot:3	Epoch:3	Loss:3.595	translation_Loss:2.073	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.521                                                   	MRR:21.26	Hits@10:39.37	Best:21.26
2025-01-03 22:37:38,593: Snapshot:3	Epoch:4	Loss:3.349	translation_Loss:1.883	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.467                                                   	MRR:21.74	Hits@10:39.44	Best:21.74
2025-01-03 22:37:51,495: Snapshot:3	Epoch:5	Loss:3.219	translation_Loss:1.784	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.435                                                   	MRR:21.71	Hits@10:39.59	Best:21.74
2025-01-03 22:38:02,744: Snapshot:3	Epoch:6	Loss:3.147	translation_Loss:1.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.413                                                   	MRR:21.65	Hits@10:39.53	Best:21.74
2025-01-03 22:38:15,838: Snapshot:3	Epoch:7	Loss:3.098	translation_Loss:1.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.402                                                   	MRR:21.78	Hits@10:39.42	Best:21.78
2025-01-03 22:38:27,440: Snapshot:3	Epoch:8	Loss:3.068	translation_Loss:1.672	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.397                                                   	MRR:21.51	Hits@10:39.3	Best:21.78
2025-01-03 22:38:40,355: Snapshot:3	Epoch:9	Loss:3.047	translation_Loss:1.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.391                                                   	MRR:21.7	Hits@10:39.52	Best:21.78
2025-01-03 22:38:52,038: Snapshot:3	Epoch:10	Loss:3.027	translation_Loss:1.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.39                                                   	MRR:21.69	Hits@10:39.24	Best:21.78
2025-01-03 22:39:03,087: Snapshot:3	Epoch:11	Loss:3.02	translation_Loss:1.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.388                                                   	MRR:21.74	Hits@10:39.43	Best:21.78
2025-01-03 22:39:16,148: Early Stopping! Snapshot: 3 Epoch: 12 Best Results: 21.78
2025-01-03 22:39:16,148: Start to training tokens! Snapshot: 3 Epoch: 12 Loss:3.007 MRR:21.66 Best Results: 21.78
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-03 22:39:16,148: Snapshot:3	Epoch:12	Loss:3.007	translation_Loss:1.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.391                                                   	MRR:21.66	Hits@10:39.57	Best:21.78
2025-01-03 22:39:28,957: Snapshot:3	Epoch:13	Loss:35.607	translation_Loss:18.736	multi_layer_Loss:16.871	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.66	Hits@10:39.57	Best:21.78
2025-01-03 22:39:40,271: End of token training: 3 Epoch: 14 Loss:18.803 MRR:21.66 Best Results: 21.78
2025-01-03 22:39:40,271: Snapshot:3	Epoch:14	Loss:18.803	translation_Loss:18.727	multi_layer_Loss:0.076	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.66	Hits@10:39.57	Best:21.78
2025-01-03 22:39:40,510: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 22:39:53,133: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1558 | 0.3142 | 0.3789 |  0.456  |
|     1      | 0.2754 | 0.1745 | 0.3116 | 0.3766 |  0.4807 |
|     2      | 0.2082 | 0.1234 | 0.2367 | 0.2949 |  0.3754 |
|     3      | 0.2161 | 0.122  | 0.2526 | 0.3162 |  0.3959 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:40:11,359: Snapshot:4	Epoch:0	Loss:6.814	translation_Loss:6.136	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.678                                                   	MRR:9.24	Hits@10:19.43	Best:9.24
2025-01-03 22:40:16,564: Snapshot:4	Epoch:1	Loss:4.813	translation_Loss:3.96	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.853                                                   	MRR:15.16	Hits@10:31.96	Best:15.16
2025-01-03 22:40:21,733: Snapshot:4	Epoch:2	Loss:3.786	translation_Loss:2.993	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.792                                                   	MRR:18.8	Hits@10:34.5	Best:18.8
2025-01-03 22:40:27,006: Snapshot:4	Epoch:3	Loss:3.19	translation_Loss:2.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.73                                                   	MRR:20.54	Hits@10:35.58	Best:20.54
2025-01-03 22:40:31,673: Snapshot:4	Epoch:4	Loss:2.806	translation_Loss:2.127	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.679                                                   	MRR:21.75	Hits@10:35.9	Best:21.75
2025-01-03 22:40:36,565: Snapshot:4	Epoch:5	Loss:2.547	translation_Loss:1.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.642                                                   	MRR:22.21	Hits@10:36.25	Best:22.21
2025-01-03 22:40:41,754: Snapshot:4	Epoch:6	Loss:2.375	translation_Loss:1.77	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.606                                                   	MRR:22.34	Hits@10:36.08	Best:22.34
2025-01-03 22:40:46,932: Snapshot:4	Epoch:7	Loss:2.285	translation_Loss:1.7	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.585                                                   	MRR:22.17	Hits@10:36.26	Best:22.34
2025-01-03 22:40:51,881: Snapshot:4	Epoch:8	Loss:2.22	translation_Loss:1.651	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.569                                                   	MRR:22.28	Hits@10:35.71	Best:22.34
2025-01-03 22:40:56,319: Snapshot:4	Epoch:9	Loss:2.183	translation_Loss:1.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.558                                                   	MRR:22.04	Hits@10:35.81	Best:22.34
2025-01-03 22:41:01,493: Snapshot:4	Epoch:10	Loss:2.152	translation_Loss:1.601	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.551                                                   	MRR:22.12	Hits@10:36.09	Best:22.34
2025-01-03 22:41:06,975: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 22.34
2025-01-03 22:41:06,975: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:2.135 MRR:21.96 Best Results: 22.34
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-03 22:41:06,975: Snapshot:4	Epoch:11	Loss:2.135	translation_Loss:1.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.545                                                   	MRR:21.96	Hits@10:35.77	Best:22.34
2025-01-03 22:41:11,576: Snapshot:4	Epoch:12	Loss:25.27	translation_Loss:9.455	multi_layer_Loss:15.815	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.96	Hits@10:35.77	Best:22.34
2025-01-03 22:41:15,991: End of token training: 4 Epoch: 13 Loss:10.64 MRR:21.96 Best Results: 22.34
2025-01-03 22:41:15,991: Snapshot:4	Epoch:13	Loss:10.64	translation_Loss:9.448	multi_layer_Loss:1.192	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.96	Hits@10:35.77	Best:22.34
2025-01-03 22:41:16,249: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 22:41:31,400: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2325 |  0.13  | 0.2813 | 0.3438 |  0.4194 |
|     1      | 0.2693 | 0.1714 | 0.3016 | 0.3683 |  0.4709 |
|     2      | 0.1997 | 0.1159 | 0.2266 | 0.2849 |  0.3652 |
|     3      | 0.2021 | 0.1101 | 0.2362 | 0.2983 |  0.3785 |
|     4      | 0.2256 | 0.1537 | 0.2554 | 0.3016 |  0.3606 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 22:41:31,402: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2585 | 0.1518 | 0.3137 | 0.3794 |  0.4528 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2597 | 0.1534 | 0.3155 | 0.3814 |  0.453  |
|     1      | 0.2903 | 0.1835 | 0.342  | 0.4152 |  0.5022 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2562 | 0.1462 | 0.3138 | 0.3794 |  0.4568 |
|     1      | 0.2802 | 0.1752 | 0.3261 | 0.3922 |  0.4929 |
|     2      | 0.2127 | 0.1274 | 0.2443 | 0.3025 |  0.3781 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2617 | 0.1558 | 0.3142 | 0.3789 |  0.456  |
|     1      | 0.2754 | 0.1745 | 0.3116 | 0.3766 |  0.4807 |
|     2      | 0.2082 | 0.1234 | 0.2367 | 0.2949 |  0.3754 |
|     3      | 0.2161 | 0.122  | 0.2526 | 0.3162 |  0.3959 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2325 |  0.13  | 0.2813 | 0.3438 |  0.4194 |
|     1      | 0.2693 | 0.1714 | 0.3016 | 0.3683 |  0.4709 |
|     2      | 0.1997 | 0.1159 | 0.2266 | 0.2849 |  0.3652 |
|     3      | 0.2021 | 0.1101 | 0.2362 | 0.2983 |  0.3785 |
|     4      | 0.2256 | 0.1537 | 0.2554 | 0.3016 |  0.3606 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 22:41:31,403: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 127.76499652862549 |   0.259   |    0.152     |    0.314     |     0.453     |
|    1     | 80.38533568382263  |   0.268   |    0.161     |    0.323     |     0.466     |
|    2     | 175.40369939804077 |   0.236   |     0.14     |    0.279     |      0.42     |
|    3     | 203.07114696502686 |   0.228   |    0.134     |    0.265     |     0.409     |
|    4     | 80.31834888458252  |   0.215   |    0.125     |    0.249     |     0.386     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 22:41:31,403: Sum_Training_Time:666.9435274600983
2025-01-03 22:41:31,403: Every_Training_Time:[127.76499652862549, 80.38533568382263, 175.40369939804077, 203.07114696502686, 80.31834888458252]
2025-01-03 22:41:31,403: Forward transfer: 0.043550000000000005 Backward transfer: -0.018500000000000003
2025-01-03 22:42:04,987: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103224137/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 22:42:15,026: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 22:42:20,644: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 22:42:26,952: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.69	Best:18.94
2025-01-03 22:42:33,193: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.58	Best:22.69
2025-01-03 22:42:38,666: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.22	Hits@10:45.31	Best:24.22
2025-01-03 22:42:45,523: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.97	Hits@10:45.95	Best:24.97
2025-01-03 22:42:51,072: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.26	Hits@10:46.3	Best:25.26
2025-01-03 22:42:57,343: Snapshot:0	Epoch:7	Loss:0.8	translation_Loss:0.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.5	Hits@10:46.45	Best:25.5
2025-01-03 22:43:03,066: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.47	Best:25.66
2025-01-03 22:43:09,653: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.5	Best:25.82
2025-01-03 22:43:15,801: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.39	Best:25.82
2025-01-03 22:43:22,545: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.83	Hits@10:46.27	Best:25.83
2025-01-03 22:43:28,099: Snapshot:0	Epoch:12	Loss:0.355	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.28	Best:25.83
2025-01-03 22:43:34,388: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.86	Hits@10:46.02	Best:25.86
2025-01-03 22:43:40,334: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.13	Best:25.86
2025-01-03 22:43:46,679: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:45.84	Best:25.86
2025-01-03 22:43:52,541: Snapshot:0	Epoch:16	Loss:0.251	translation_Loss:0.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:45.95	Best:25.86
2025-01-03 22:43:58,949: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.02	Best:25.86
2025-01-03 22:44:05,378: Early Stopping! Snapshot: 0 Epoch: 18 Best Results: 25.86
2025-01-03 22:44:05,378: Start to training tokens! Snapshot: 0 Epoch: 18 Loss:0.222 MRR:25.67 Best Results: 25.86
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2025-01-03 22:44:05,378: Snapshot:0	Epoch:18	Loss:0.222	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:45.95	Best:25.86
2025-01-03 22:44:12,171: Snapshot:0	Epoch:19	Loss:29.022	translation_Loss:11.541	multi_layer_Loss:17.481	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:45.95	Best:25.86
2025-01-03 22:44:18,059: End of token training: 0 Epoch: 20 Loss:12.079 MRR:25.67 Best Results: 25.86
2025-01-03 22:44:18,060: Snapshot:0	Epoch:20	Loss:12.079	translation_Loss:11.535	multi_layer_Loss:0.544	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.67	Hits@10:45.95	Best:25.86
2025-01-03 22:44:18,296: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 22:44:20,631: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1522 | 0.3151 | 0.3779 |  0.4527 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:44:31,241: Snapshot:1	Epoch:0	Loss:5.072	translation_Loss:4.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.301                                                   	MRR:9.45	Hits@10:16.53	Best:9.45
2025-01-03 22:44:33,566: Snapshot:1	Epoch:1	Loss:3.233	translation_Loss:2.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.4                                                   	MRR:15.75	Hits@10:28.61	Best:15.75
2025-01-03 22:44:36,113: Snapshot:1	Epoch:2	Loss:2.149	translation_Loss:1.752	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.397                                                   	MRR:19.8	Hits@10:34.74	Best:19.8
2025-01-03 22:44:38,586: Snapshot:1	Epoch:3	Loss:1.555	translation_Loss:1.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.358                                                   	MRR:22.37	Hits@10:38.73	Best:22.37
2025-01-03 22:44:40,831: Snapshot:1	Epoch:4	Loss:1.23	translation_Loss:0.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.3                                                   	MRR:23.88	Hits@10:42.46	Best:23.88
2025-01-03 22:44:42,987: Snapshot:1	Epoch:5	Loss:1.038	translation_Loss:0.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.261                                                   	MRR:24.76	Hits@10:44.57	Best:24.76
2025-01-03 22:44:45,099: Snapshot:1	Epoch:6	Loss:0.922	translation_Loss:0.683	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:25.52	Hits@10:46.34	Best:25.52
2025-01-03 22:44:47,223: Snapshot:1	Epoch:7	Loss:0.842	translation_Loss:0.616	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.225                                                   	MRR:26.38	Hits@10:47.01	Best:26.38
2025-01-03 22:44:49,341: Snapshot:1	Epoch:8	Loss:0.79	translation_Loss:0.576	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:27.18	Hits@10:48.08	Best:27.18
2025-01-03 22:44:52,042: Snapshot:1	Epoch:9	Loss:0.748	translation_Loss:0.544	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.204                                                   	MRR:27.67	Hits@10:48.67	Best:27.67
2025-01-03 22:44:54,231: Snapshot:1	Epoch:10	Loss:0.705	translation_Loss:0.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.196                                                   	MRR:28.13	Hits@10:49.11	Best:28.13
2025-01-03 22:44:56,776: Snapshot:1	Epoch:11	Loss:0.674	translation_Loss:0.486	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:28.32	Hits@10:49.31	Best:28.32
2025-01-03 22:44:59,038: Snapshot:1	Epoch:12	Loss:0.65	translation_Loss:0.47	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:28.54	Hits@10:49.72	Best:28.54
2025-01-03 22:45:01,527: Snapshot:1	Epoch:13	Loss:0.629	translation_Loss:0.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.176                                                   	MRR:28.77	Hits@10:49.6	Best:28.77
2025-01-03 22:45:03,869: Snapshot:1	Epoch:14	Loss:0.615	translation_Loss:0.443	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.172                                                   	MRR:28.8	Hits@10:49.84	Best:28.8
2025-01-03 22:45:06,644: Snapshot:1	Epoch:15	Loss:0.589	translation_Loss:0.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:28.91	Hits@10:50.0	Best:28.91
2025-01-03 22:45:09,128: Snapshot:1	Epoch:16	Loss:0.582	translation_Loss:0.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.13	Hits@10:49.94	Best:29.13
2025-01-03 22:45:11,464: Snapshot:1	Epoch:17	Loss:0.569	translation_Loss:0.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.49	Hits@10:50.16	Best:29.49
2025-01-03 22:45:13,978: Snapshot:1	Epoch:18	Loss:0.567	translation_Loss:0.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:29.25	Hits@10:50.26	Best:29.49
2025-01-03 22:45:16,302: Snapshot:1	Epoch:19	Loss:0.552	translation_Loss:0.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:29.31	Hits@10:50.19	Best:29.49
2025-01-03 22:45:18,804: Snapshot:1	Epoch:20	Loss:0.55	translation_Loss:0.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:29.5	Hits@10:50.37	Best:29.5
2025-01-03 22:45:21,045: Snapshot:1	Epoch:21	Loss:0.544	translation_Loss:0.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:29.43	Hits@10:50.19	Best:29.5
2025-01-03 22:45:23,785: Snapshot:1	Epoch:22	Loss:0.534	translation_Loss:0.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:29.31	Hits@10:50.4	Best:29.5
2025-01-03 22:45:26,077: Snapshot:1	Epoch:23	Loss:0.529	translation_Loss:0.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:29.63	Hits@10:50.13	Best:29.63
2025-01-03 22:45:28,146: Snapshot:1	Epoch:24	Loss:0.523	translation_Loss:0.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:29.35	Hits@10:50.35	Best:29.63
2025-01-03 22:45:30,611: Snapshot:1	Epoch:25	Loss:0.519	translation_Loss:0.369	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:29.54	Hits@10:50.42	Best:29.63
2025-01-03 22:45:32,833: Snapshot:1	Epoch:26	Loss:0.518	translation_Loss:0.37	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.148                                                   	MRR:29.52	Hits@10:50.43	Best:29.63
2025-01-03 22:45:34,911: Snapshot:1	Epoch:27	Loss:0.515	translation_Loss:0.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.148                                                   	MRR:29.56	Hits@10:50.38	Best:29.63
2025-01-03 22:45:37,292: Early Stopping! Snapshot: 1 Epoch: 28 Best Results: 29.63
2025-01-03 22:45:37,293: Start to training tokens! Snapshot: 1 Epoch: 28 Loss:0.514 MRR:29.62 Best Results: 29.63
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2025-01-03 22:45:37,293: Snapshot:1	Epoch:28	Loss:0.514	translation_Loss:0.367	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.147                                                   	MRR:29.62	Hits@10:50.34	Best:29.63
2025-01-03 22:45:39,385: Snapshot:1	Epoch:29	Loss:17.579	translation_Loss:4.522	multi_layer_Loss:13.058	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.62	Hits@10:50.34	Best:29.63
2025-01-03 22:45:41,493: End of token training: 1 Epoch: 30 Loss:8.436 MRR:29.62 Best Results: 29.63
2025-01-03 22:45:41,493: Snapshot:1	Epoch:30	Loss:8.436	translation_Loss:4.527	multi_layer_Loss:3.909	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.62	Hits@10:50.34	Best:29.63
2025-01-03 22:45:41,731: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 22:45:45,384: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2607 | 0.1549 | 0.3162 |  0.38  |  0.4537 |
|     1      | 0.2931 | 0.1891 | 0.3405 | 0.4074 |  0.5032 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:46:16,280: Snapshot:2	Epoch:0	Loss:16.359	translation_Loss:14.992	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.367                                                   	MRR:13.58	Hits@10:27.85	Best:13.58
2025-01-03 22:46:26,667: Snapshot:2	Epoch:1	Loss:8.071	translation_Loss:6.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.387                                                   	MRR:18.54	Hits@10:34.88	Best:18.54
2025-01-03 22:46:35,792: Snapshot:2	Epoch:2	Loss:5.468	translation_Loss:4.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.104                                                   	MRR:19.86	Hits@10:36.22	Best:19.86
2025-01-03 22:46:44,802: Snapshot:2	Epoch:3	Loss:4.573	translation_Loss:3.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.965                                                   	MRR:20.37	Hits@10:36.77	Best:20.37
2025-01-03 22:46:55,203: Snapshot:2	Epoch:4	Loss:4.201	translation_Loss:3.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.911                                                   	MRR:20.76	Hits@10:36.82	Best:20.76
2025-01-03 22:47:04,744: Snapshot:2	Epoch:5	Loss:4.029	translation_Loss:3.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.888                                                   	MRR:20.66	Hits@10:36.89	Best:20.76
2025-01-03 22:47:13,828: Snapshot:2	Epoch:6	Loss:3.945	translation_Loss:3.07	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.876                                                   	MRR:20.81	Hits@10:36.93	Best:20.81
2025-01-03 22:47:23,217: Snapshot:2	Epoch:7	Loss:3.886	translation_Loss:3.017	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.87                                                   	MRR:20.85	Hits@10:36.91	Best:20.85
2025-01-03 22:47:32,471: Snapshot:2	Epoch:8	Loss:3.853	translation_Loss:2.98	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.873                                                   	MRR:21.11	Hits@10:36.86	Best:21.11
2025-01-03 22:47:42,894: Snapshot:2	Epoch:9	Loss:3.81	translation_Loss:2.94	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.87                                                   	MRR:20.91	Hits@10:36.94	Best:21.11
2025-01-03 22:47:52,483: Snapshot:2	Epoch:10	Loss:3.779	translation_Loss:2.912	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.867                                                   	MRR:20.98	Hits@10:36.92	Best:21.11
2025-01-03 22:48:03,264: Snapshot:2	Epoch:11	Loss:3.762	translation_Loss:2.898	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.865                                                   	MRR:20.96	Hits@10:37.12	Best:21.11
2025-01-03 22:48:12,599: Snapshot:2	Epoch:12	Loss:3.738	translation_Loss:2.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.868                                                   	MRR:21.01	Hits@10:37.1	Best:21.11
2025-01-03 22:48:23,007: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 21.11
2025-01-03 22:48:23,007: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:3.732 MRR:20.81 Best Results: 21.11
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2025-01-03 22:48:23,007: Snapshot:2	Epoch:13	Loss:3.732	translation_Loss:2.866	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.866                                                   	MRR:20.81	Hits@10:37.0	Best:21.11
2025-01-03 22:48:32,398: Snapshot:2	Epoch:14	Loss:35.455	translation_Loss:18.556	multi_layer_Loss:16.899	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.81	Hits@10:37.0	Best:21.11
2025-01-03 22:48:41,611: End of token training: 2 Epoch: 15 Loss:18.694 MRR:20.81 Best Results: 21.11
2025-01-03 22:48:41,611: Snapshot:2	Epoch:15	Loss:18.694	translation_Loss:18.563	multi_layer_Loss:0.131	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.81	Hits@10:37.0	Best:21.11
2025-01-03 22:48:41,849: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 22:48:49,082: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1477 | 0.3167 | 0.3805 |  0.4548 |
|     1      | 0.2846 | 0.1814 | 0.3279 | 0.3947 |  0.4941 |
|     2      | 0.211  | 0.1282 | 0.241  | 0.2961 |  0.3726 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:49:26,391: Snapshot:3	Epoch:0	Loss:15.075	translation_Loss:13.826	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.249                                                   	MRR:15.74	Hits@10:31.72	Best:15.74
2025-01-03 22:49:37,643: Snapshot:3	Epoch:1	Loss:6.422	translation_Loss:4.543	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.879                                                   	MRR:19.91	Hits@10:37.25	Best:19.91
2025-01-03 22:49:48,597: Snapshot:3	Epoch:2	Loss:4.603	translation_Loss:2.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.687                                                   	MRR:20.75	Hits@10:38.35	Best:20.75
2025-01-03 22:50:01,659: Snapshot:3	Epoch:3	Loss:4.017	translation_Loss:2.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.577                                                   	MRR:21.26	Hits@10:38.71	Best:21.26
2025-01-03 22:50:14,839: Snapshot:3	Epoch:4	Loss:3.755	translation_Loss:2.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.507                                                   	MRR:21.39	Hits@10:38.96	Best:21.39
2025-01-03 22:50:27,695: Snapshot:3	Epoch:5	Loss:3.627	translation_Loss:2.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.47                                                   	MRR:21.47	Hits@10:39.19	Best:21.47
2025-01-03 22:50:38,876: Snapshot:3	Epoch:6	Loss:3.571	translation_Loss:2.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.463                                                   	MRR:21.52	Hits@10:39.12	Best:21.52
2025-01-03 22:50:50,029: Snapshot:3	Epoch:7	Loss:3.513	translation_Loss:2.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.452                                                   	MRR:21.44	Hits@10:39.05	Best:21.52
2025-01-03 22:51:01,237: Snapshot:3	Epoch:8	Loss:3.477	translation_Loss:2.038	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.438                                                   	MRR:21.46	Hits@10:39.01	Best:21.52
2025-01-03 22:51:12,533: Snapshot:3	Epoch:9	Loss:3.441	translation_Loss:2.007	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.434                                                   	MRR:21.57	Hits@10:39.08	Best:21.57
2025-01-03 22:51:25,501: Snapshot:3	Epoch:10	Loss:3.426	translation_Loss:2.001	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.424                                                   	MRR:21.51	Hits@10:38.94	Best:21.57
2025-01-03 22:51:36,540: Snapshot:3	Epoch:11	Loss:3.41	translation_Loss:1.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.429                                                   	MRR:21.41	Hits@10:38.55	Best:21.57
2025-01-03 22:51:47,709: Snapshot:3	Epoch:12	Loss:3.419	translation_Loss:1.989	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.429                                                   	MRR:21.49	Hits@10:38.93	Best:21.57
2025-01-03 22:52:00,502: Snapshot:3	Epoch:13	Loss:3.399	translation_Loss:1.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.428                                                   	MRR:21.45	Hits@10:38.92	Best:21.57
2025-01-03 22:52:13,294: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 21.57
2025-01-03 22:52:13,295: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:3.389 MRR:21.56 Best Results: 21.57
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2025-01-03 22:52:13,295: Snapshot:3	Epoch:14	Loss:3.389	translation_Loss:1.961	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.427                                                   	MRR:21.56	Hits@10:38.87	Best:21.57
2025-01-03 22:52:24,675: Snapshot:3	Epoch:15	Loss:36.919	translation_Loss:19.031	multi_layer_Loss:17.888	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.56	Hits@10:38.87	Best:21.57
2025-01-03 22:52:37,451: End of token training: 3 Epoch: 16 Loss:19.108 MRR:21.56 Best Results: 21.57
2025-01-03 22:52:37,452: Snapshot:3	Epoch:16	Loss:19.108	translation_Loss:19.031	multi_layer_Loss:0.077	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.56	Hits@10:38.87	Best:21.57
2025-01-03 22:52:37,683: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 22:52:50,630: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2649 | 0.1607 | 0.3152 | 0.3812 |  0.4592 |
|     1      | 0.2826 | 0.1838 | 0.3172 | 0.3828 |  0.4804 |
|     2      | 0.2121 | 0.1286 | 0.2398 | 0.2972 |  0.3766 |
|     3      | 0.2144 | 0.1214 | 0.2516 | 0.3143 |  0.3907 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:53:08,336: Snapshot:4	Epoch:0	Loss:6.981	translation_Loss:6.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.743                                                   	MRR:9.16	Hits@10:19.46	Best:9.16
2025-01-03 22:53:13,537: Snapshot:4	Epoch:1	Loss:5.148	translation_Loss:4.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.821                                                   	MRR:14.32	Hits@10:31.47	Best:14.32
2025-01-03 22:53:18,203: Snapshot:4	Epoch:2	Loss:4.048	translation_Loss:3.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.75                                                   	MRR:18.85	Hits@10:34.32	Best:18.85
2025-01-03 22:53:23,446: Snapshot:4	Epoch:3	Loss:3.444	translation_Loss:2.761	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.683                                                   	MRR:21.11	Hits@10:35.62	Best:21.11
2025-01-03 22:53:28,507: Snapshot:4	Epoch:4	Loss:3.02	translation_Loss:2.388	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.632                                                   	MRR:21.9	Hits@10:35.77	Best:21.9
2025-01-03 22:53:33,117: Snapshot:4	Epoch:5	Loss:2.751	translation_Loss:2.159	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.591                                                   	MRR:22.5	Hits@10:36.29	Best:22.5
2025-01-03 22:53:38,219: Snapshot:4	Epoch:6	Loss:2.566	translation_Loss:2.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.555                                                   	MRR:22.45	Hits@10:35.99	Best:22.5
2025-01-03 22:53:43,182: Snapshot:4	Epoch:7	Loss:2.463	translation_Loss:1.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.532                                                   	MRR:22.58	Hits@10:35.66	Best:22.58
2025-01-03 22:53:48,228: Snapshot:4	Epoch:8	Loss:2.405	translation_Loss:1.887	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.518                                                   	MRR:22.38	Hits@10:35.8	Best:22.58
2025-01-03 22:53:52,765: Snapshot:4	Epoch:9	Loss:2.353	translation_Loss:1.849	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.505                                                   	MRR:22.31	Hits@10:35.45	Best:22.58
2025-01-03 22:53:58,097: Snapshot:4	Epoch:10	Loss:2.336	translation_Loss:1.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.502                                                   	MRR:22.26	Hits@10:35.64	Best:22.58
2025-01-03 22:54:03,041: Snapshot:4	Epoch:11	Loss:2.317	translation_Loss:1.821	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.496                                                   	MRR:22.2	Hits@10:35.54	Best:22.58
2025-01-03 22:54:08,170: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 22.58
2025-01-03 22:54:08,170: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:2.307 MRR:22.33 Best Results: 22.58
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2025-01-03 22:54:08,170: Snapshot:4	Epoch:12	Loss:2.307	translation_Loss:1.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.496                                                   	MRR:22.33	Hits@10:35.41	Best:22.58
2025-01-03 22:54:12,697: Snapshot:4	Epoch:13	Loss:26.192	translation_Loss:9.696	multi_layer_Loss:16.496	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.33	Hits@10:35.41	Best:22.58
2025-01-03 22:54:17,595: End of token training: 4 Epoch: 14 Loss:11.089 MRR:22.33 Best Results: 22.58
2025-01-03 22:54:17,595: Snapshot:4	Epoch:14	Loss:11.089	translation_Loss:9.692	multi_layer_Loss:1.397	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.33	Hits@10:35.41	Best:22.58
2025-01-03 22:54:17,863: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 22:54:32,189: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2379 | 0.1355 | 0.2851 | 0.3485 |  0.4257 |
|     1      | 0.2757 | 0.1792 | 0.3092 | 0.3713 |  0.4698 |
|     2      | 0.205  | 0.1222 | 0.2312 | 0.2891 |  0.3676 |
|     3      | 0.2035 | 0.1118 | 0.2374 | 0.3016 |  0.3795 |
|     4      | 0.223  | 0.1538 | 0.2489 | 0.2957 |  0.3573 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 22:54:32,192: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1522 | 0.3151 | 0.3779 |  0.4527 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2607 | 0.1549 | 0.3162 |  0.38  |  0.4537 |
|     1      | 0.2931 | 0.1891 | 0.3405 | 0.4074 |  0.5032 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2574 | 0.1477 | 0.3167 | 0.3805 |  0.4548 |
|     1      | 0.2846 | 0.1814 | 0.3279 | 0.3947 |  0.4941 |
|     2      | 0.211  | 0.1282 | 0.241  | 0.2961 |  0.3726 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2649 | 0.1607 | 0.3152 | 0.3812 |  0.4592 |
|     1      | 0.2826 | 0.1838 | 0.3172 | 0.3828 |  0.4804 |
|     2      | 0.2121 | 0.1286 | 0.2398 | 0.2972 |  0.3766 |
|     3      | 0.2144 | 0.1214 | 0.2516 | 0.3143 |  0.3907 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2379 | 0.1355 | 0.2851 | 0.3485 |  0.4257 |
|     1      | 0.2757 | 0.1792 | 0.3092 | 0.3713 |  0.4698 |
|     2      | 0.205  | 0.1222 | 0.2312 | 0.2891 |  0.3676 |
|     3      | 0.2035 | 0.1118 | 0.2374 | 0.3016 |  0.3795 |
|     4      | 0.223  | 0.1538 | 0.2489 | 0.2957 |  0.3573 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 22:54:32,192: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 133.07144808769226 |   0.259   |    0.152     |    0.315     |     0.453     |
|    1     | 79.71452212333679  |   0.269   |    0.164     |    0.323     |     0.467     |
|    2     | 172.4975028038025  |   0.236   |    0.142     |    0.278     |     0.416     |
|    3     | 223.48922967910767 |    0.23   |    0.137     |    0.266     |     0.408     |
|    4     | 84.53939414024353  |   0.218   |    0.129     |    0.251     |     0.388     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 22:54:32,192: Sum_Training_Time:693.3120968341827
2025-01-03 22:54:32,192: Every_Training_Time:[133.07144808769226, 79.71452212333679, 172.4975028038025, 223.48922967910767, 84.53939414024353]
2025-01-03 22:54:32,192: Forward transfer: 0.043125 Backward transfer: -0.013800000000000007
2025-01-03 22:55:04,078: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103225437/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=13, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 22:55:14,063: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 22:55:19,739: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 22:55:25,851: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.68	Best:18.93
2025-01-03 22:55:31,888: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:43.56	Best:22.67
2025-01-03 22:55:38,146: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:45.21	Best:24.24
2025-01-03 22:55:44,094: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.02	Hits@10:46.03	Best:25.02
2025-01-03 22:55:50,392: Snapshot:0	Epoch:6	Loss:1.068	translation_Loss:1.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.24	Hits@10:46.23	Best:25.24
2025-01-03 22:55:56,303: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.51	Best:25.58
2025-01-03 22:56:02,669: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.59	Hits@10:46.46	Best:25.59
2025-01-03 22:56:08,892: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.49	Best:25.69
2025-01-03 22:56:15,117: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.23	Best:25.75
2025-01-03 22:56:21,023: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.34	Best:25.75
2025-01-03 22:56:27,270: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.22	Best:25.77
2025-01-03 22:56:33,051: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.2	Best:25.84
2025-01-03 22:56:39,728: Snapshot:0	Epoch:14	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.86	Hits@10:46.05	Best:25.86
2025-01-03 22:56:45,243: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:45.94	Best:25.86
2025-01-03 22:56:51,729: Snapshot:0	Epoch:16	Loss:0.253	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.89	Best:25.86
2025-01-03 22:56:57,341: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:45.87	Best:25.86
2025-01-03 22:57:04,015: Snapshot:0	Epoch:18	Loss:0.223	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:45.81	Best:25.86
2025-01-03 22:57:09,919: Early Stopping! Snapshot: 0 Epoch: 19 Best Results: 25.86
2025-01-03 22:57:09,919: Start to training tokens! Snapshot: 0 Epoch: 19 Loss:0.219 MRR:25.54 Best Results: 25.86
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 22:57:09,919: Snapshot:0	Epoch:19	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:45.69	Best:25.86
2025-01-03 22:57:16,064: Snapshot:0	Epoch:20	Loss:29.694	translation_Loss:11.504	multi_layer_Loss:18.19	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:45.69	Best:25.86
2025-01-03 22:57:22,372: End of token training: 0 Epoch: 21 Loss:12.089 MRR:25.54 Best Results: 25.86
2025-01-03 22:57:22,372: Snapshot:0	Epoch:21	Loss:12.089	translation_Loss:11.494	multi_layer_Loss:0.596	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.54	Hits@10:45.69	Best:25.86
2025-01-03 22:57:22,601: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 22:57:25,097: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2582 | 0.1519 | 0.3137 | 0.377  |  0.4542 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:57:35,706: Snapshot:1	Epoch:0	Loss:5.111	translation_Loss:4.771	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.34                                                   	MRR:9.27	Hits@10:16.2	Best:9.27
2025-01-03 22:57:38,239: Snapshot:1	Epoch:1	Loss:3.276	translation_Loss:2.886	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:15.33	Hits@10:27.68	Best:15.33
2025-01-03 22:57:40,478: Snapshot:1	Epoch:2	Loss:2.178	translation_Loss:1.805	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.373                                                   	MRR:19.27	Hits@10:33.75	Best:19.27
2025-01-03 22:57:42,909: Snapshot:1	Epoch:3	Loss:1.592	translation_Loss:1.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.332                                                   	MRR:21.88	Hits@10:38.4	Best:21.88
2025-01-03 22:57:45,564: Snapshot:1	Epoch:4	Loss:1.256	translation_Loss:0.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.279                                                   	MRR:23.42	Hits@10:41.51	Best:23.42
2025-01-03 22:57:48,013: Snapshot:1	Epoch:5	Loss:1.067	translation_Loss:0.824	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.244                                                   	MRR:24.44	Hits@10:43.81	Best:24.44
2025-01-03 22:57:50,252: Snapshot:1	Epoch:6	Loss:0.944	translation_Loss:0.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.224                                                   	MRR:25.42	Hits@10:45.28	Best:25.42
2025-01-03 22:57:52,689: Snapshot:1	Epoch:7	Loss:0.868	translation_Loss:0.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.211                                                   	MRR:26.04	Hits@10:46.5	Best:26.04
2025-01-03 22:57:54,903: Snapshot:1	Epoch:8	Loss:0.809	translation_Loss:0.611	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.198                                                   	MRR:26.82	Hits@10:46.93	Best:26.82
2025-01-03 22:57:57,044: Snapshot:1	Epoch:9	Loss:0.756	translation_Loss:0.567	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.189                                                   	MRR:27.45	Hits@10:47.83	Best:27.45
2025-01-03 22:57:59,809: Snapshot:1	Epoch:10	Loss:0.72	translation_Loss:0.539	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.182                                                   	MRR:27.85	Hits@10:48.28	Best:27.85
2025-01-03 22:58:02,171: Snapshot:1	Epoch:11	Loss:0.691	translation_Loss:0.517	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:28.22	Hits@10:48.54	Best:28.22
2025-01-03 22:58:04,256: Snapshot:1	Epoch:12	Loss:0.658	translation_Loss:0.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:28.35	Hits@10:48.86	Best:28.35
2025-01-03 22:58:06,646: Snapshot:1	Epoch:13	Loss:0.638	translation_Loss:0.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:28.38	Hits@10:49.02	Best:28.38
2025-01-03 22:58:09,058: Snapshot:1	Epoch:14	Loss:0.624	translation_Loss:0.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.159                                                   	MRR:28.5	Hits@10:49.08	Best:28.5
2025-01-03 22:58:11,177: Snapshot:1	Epoch:15	Loss:0.607	translation_Loss:0.451	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:28.6	Hits@10:49.41	Best:28.6
2025-01-03 22:58:14,033: Snapshot:1	Epoch:16	Loss:0.597	translation_Loss:0.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:28.75	Hits@10:49.26	Best:28.75
2025-01-03 22:58:16,362: Snapshot:1	Epoch:17	Loss:0.581	translation_Loss:0.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.151                                                   	MRR:28.92	Hits@10:49.26	Best:28.92
2025-01-03 22:58:18,417: Snapshot:1	Epoch:18	Loss:0.573	translation_Loss:0.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:28.74	Hits@10:49.51	Best:28.92
2025-01-03 22:58:20,461: Snapshot:1	Epoch:19	Loss:0.562	translation_Loss:0.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.147                                                   	MRR:28.82	Hits@10:49.66	Best:28.92
2025-01-03 22:58:22,578: Snapshot:1	Epoch:20	Loss:0.554	translation_Loss:0.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.144                                                   	MRR:29.0	Hits@10:49.77	Best:29.0
2025-01-03 22:58:24,664: Snapshot:1	Epoch:21	Loss:0.555	translation_Loss:0.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:29.14	Hits@10:49.84	Best:29.14
2025-01-03 22:58:27,041: Snapshot:1	Epoch:22	Loss:0.539	translation_Loss:0.398	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.141                                                   	MRR:29.11	Hits@10:49.83	Best:29.14
2025-01-03 22:58:29,443: Snapshot:1	Epoch:23	Loss:0.539	translation_Loss:0.397	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.141                                                   	MRR:29.0	Hits@10:49.78	Best:29.14
2025-01-03 22:58:31,576: Snapshot:1	Epoch:24	Loss:0.532	translation_Loss:0.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.14                                                   	MRR:29.0	Hits@10:49.77	Best:29.14
2025-01-03 22:58:33,703: Snapshot:1	Epoch:25	Loss:0.533	translation_Loss:0.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.139                                                   	MRR:29.23	Hits@10:50.01	Best:29.23
2025-01-03 22:58:35,813: Snapshot:1	Epoch:26	Loss:0.529	translation_Loss:0.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.138                                                   	MRR:29.35	Hits@10:49.99	Best:29.35
2025-01-03 22:58:38,300: Snapshot:1	Epoch:27	Loss:0.521	translation_Loss:0.383	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.138                                                   	MRR:29.38	Hits@10:50.13	Best:29.38
2025-01-03 22:58:40,780: Snapshot:1	Epoch:28	Loss:0.525	translation_Loss:0.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.138                                                   	MRR:29.35	Hits@10:49.88	Best:29.38
2025-01-03 22:58:43,242: Snapshot:1	Epoch:29	Loss:0.517	translation_Loss:0.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.137                                                   	MRR:29.43	Hits@10:50.05	Best:29.43
2025-01-03 22:58:45,642: Snapshot:1	Epoch:30	Loss:0.513	translation_Loss:0.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.136                                                   	MRR:29.58	Hits@10:50.43	Best:29.58
2025-01-03 22:58:48,059: Snapshot:1	Epoch:31	Loss:0.519	translation_Loss:0.381	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.137                                                   	MRR:29.48	Hits@10:50.42	Best:29.58
2025-01-03 22:58:50,254: Snapshot:1	Epoch:32	Loss:0.514	translation_Loss:0.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.138                                                   	MRR:29.42	Hits@10:50.34	Best:29.58
2025-01-03 22:58:52,567: Snapshot:1	Epoch:33	Loss:0.513	translation_Loss:0.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.137                                                   	MRR:29.35	Hits@10:49.92	Best:29.58
2025-01-03 22:58:55,264: Snapshot:1	Epoch:34	Loss:0.513	translation_Loss:0.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.136                                                   	MRR:29.31	Hits@10:49.96	Best:29.58
2025-01-03 22:58:57,689: Early Stopping! Snapshot: 1 Epoch: 35 Best Results: 29.58
2025-01-03 22:58:57,689: Start to training tokens! Snapshot: 1 Epoch: 35 Loss:0.51 MRR:29.53 Best Results: 29.58
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 22:58:57,690: Snapshot:1	Epoch:35	Loss:0.51	translation_Loss:0.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.137                                                   	MRR:29.53	Hits@10:49.87	Best:29.58
2025-01-03 22:58:59,837: Snapshot:1	Epoch:36	Loss:17.937	translation_Loss:4.529	multi_layer_Loss:13.408	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.53	Hits@10:49.87	Best:29.58
2025-01-03 22:59:01,880: End of token training: 1 Epoch: 37 Loss:8.807 MRR:29.53 Best Results: 29.58
2025-01-03 22:59:01,880: Snapshot:1	Epoch:37	Loss:8.807	translation_Loss:4.524	multi_layer_Loss:4.283	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.53	Hits@10:49.87	Best:29.58
2025-01-03 22:59:02,119: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 22:59:05,798: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2596 | 0.1531 | 0.3153 | 0.3795 |  0.4535 |
|     1      | 0.2889 | 0.1838 | 0.3358 | 0.4046 |  0.4977 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 22:59:37,993: Snapshot:2	Epoch:0	Loss:16.41	translation_Loss:15.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.402                                                   	MRR:13.28	Hits@10:27.35	Best:13.28
2025-01-03 22:59:47,261: Snapshot:2	Epoch:1	Loss:8.123	translation_Loss:6.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.365                                                   	MRR:18.25	Hits@10:34.69	Best:18.25
2025-01-03 22:59:57,685: Snapshot:2	Epoch:2	Loss:5.499	translation_Loss:4.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.095                                                   	MRR:19.65	Hits@10:36.2	Best:19.65
2025-01-03 23:00:07,079: Snapshot:2	Epoch:3	Loss:4.592	translation_Loss:3.629	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.963                                                   	MRR:20.28	Hits@10:36.59	Best:20.28
2025-01-03 23:00:17,498: Snapshot:2	Epoch:4	Loss:4.243	translation_Loss:3.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.916                                                   	MRR:20.61	Hits@10:36.84	Best:20.61
2025-01-03 23:00:26,965: Snapshot:2	Epoch:5	Loss:4.073	translation_Loss:3.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.894                                                   	MRR:20.72	Hits@10:36.86	Best:20.72
2025-01-03 23:00:37,613: Snapshot:2	Epoch:6	Loss:3.968	translation_Loss:3.089	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.88                                                   	MRR:20.73	Hits@10:36.8	Best:20.73
2025-01-03 23:00:46,561: Snapshot:2	Epoch:7	Loss:3.91	translation_Loss:3.037	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.873                                                   	MRR:20.66	Hits@10:36.93	Best:20.73
2025-01-03 23:00:57,286: Snapshot:2	Epoch:8	Loss:3.872	translation_Loss:3.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.872                                                   	MRR:20.85	Hits@10:36.86	Best:20.85
2025-01-03 23:01:06,768: Snapshot:2	Epoch:9	Loss:3.826	translation_Loss:2.951	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.876                                                   	MRR:20.68	Hits@10:36.9	Best:20.85
2025-01-03 23:01:17,391: Snapshot:2	Epoch:10	Loss:3.802	translation_Loss:2.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.87                                                   	MRR:20.75	Hits@10:36.97	Best:20.85
2025-01-03 23:01:26,569: Snapshot:2	Epoch:11	Loss:3.775	translation_Loss:2.906	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.868                                                   	MRR:20.78	Hits@10:36.95	Best:20.85
2025-01-03 23:01:35,733: Snapshot:2	Epoch:12	Loss:3.773	translation_Loss:2.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.868                                                   	MRR:20.72	Hits@10:37.01	Best:20.85
2025-01-03 23:01:46,458: Early Stopping! Snapshot: 2 Epoch: 13 Best Results: 20.85
2025-01-03 23:01:46,459: Start to training tokens! Snapshot: 2 Epoch: 13 Loss:3.754 MRR:20.74 Best Results: 20.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 23:01:46,459: Snapshot:2	Epoch:13	Loss:3.754	translation_Loss:2.884	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.87                                                   	MRR:20.74	Hits@10:36.89	Best:20.85
2025-01-03 23:01:56,703: Snapshot:2	Epoch:14	Loss:36.595	translation_Loss:18.636	multi_layer_Loss:17.958	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.74	Hits@10:36.89	Best:20.85
2025-01-03 23:02:07,369: End of token training: 2 Epoch: 15 Loss:18.78 MRR:20.74 Best Results: 20.85
2025-01-03 23:02:07,370: Snapshot:2	Epoch:15	Loss:18.78	translation_Loss:18.632	multi_layer_Loss:0.148	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.74	Hits@10:36.89	Best:20.85
2025-01-03 23:02:07,640: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 23:02:16,244: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2565 | 0.1476 | 0.3143 | 0.3791 |  0.4562 |
|     1      | 0.285  | 0.1801 | 0.3303 | 0.4006 |  0.4964 |
|     2      | 0.2084 | 0.1249 | 0.238  | 0.2955 |  0.3725 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 23:02:54,003: Snapshot:3	Epoch:0	Loss:15.224	translation_Loss:13.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.391                                                   	MRR:15.56	Hits@10:31.21	Best:15.56
2025-01-03 23:03:06,609: Snapshot:3	Epoch:1	Loss:6.733	translation_Loss:4.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.953                                                   	MRR:19.81	Hits@10:36.99	Best:19.81
2025-01-03 23:03:17,849: Snapshot:3	Epoch:2	Loss:4.911	translation_Loss:3.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.705                                                   	MRR:20.67	Hits@10:38.03	Best:20.67
2025-01-03 23:03:30,389: Snapshot:3	Epoch:3	Loss:4.306	translation_Loss:2.725	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.581                                                   	MRR:21.19	Hits@10:38.65	Best:21.19
2025-01-03 23:03:41,926: Snapshot:3	Epoch:4	Loss:4.029	translation_Loss:2.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.508                                                   	MRR:21.3	Hits@10:38.51	Best:21.3
2025-01-03 23:03:52,973: Snapshot:3	Epoch:5	Loss:3.91	translation_Loss:2.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.471                                                   	MRR:21.33	Hits@10:38.72	Best:21.33
2025-01-03 23:04:04,323: Snapshot:3	Epoch:6	Loss:3.831	translation_Loss:2.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:21.18	Hits@10:38.63	Best:21.33
2025-01-03 23:04:15,479: Snapshot:3	Epoch:7	Loss:3.779	translation_Loss:2.337	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.442                                                   	MRR:21.28	Hits@10:38.52	Best:21.33
2025-01-03 23:04:28,427: Snapshot:3	Epoch:8	Loss:3.762	translation_Loss:2.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.438                                                   	MRR:21.22	Hits@10:38.53	Best:21.33
2025-01-03 23:04:39,789: Snapshot:3	Epoch:9	Loss:3.723	translation_Loss:2.289	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.434                                                   	MRR:21.19	Hits@10:38.23	Best:21.33
2025-01-03 23:04:52,219: Early Stopping! Snapshot: 3 Epoch: 10 Best Results: 21.33
2025-01-03 23:04:52,219: Start to training tokens! Snapshot: 3 Epoch: 10 Loss:3.7 MRR:21.15 Best Results: 21.33
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 23:04:52,220: Snapshot:3	Epoch:10	Loss:3.7	translation_Loss:2.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.429                                                   	MRR:21.15	Hits@10:38.49	Best:21.33
2025-01-03 23:05:03,532: Snapshot:3	Epoch:11	Loss:37.979	translation_Loss:19.234	multi_layer_Loss:18.745	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.15	Hits@10:38.49	Best:21.33
2025-01-03 23:05:14,595: End of token training: 3 Epoch: 12 Loss:19.327 MRR:21.15 Best Results: 21.33
2025-01-03 23:05:14,595: Snapshot:3	Epoch:12	Loss:19.327	translation_Loss:19.248	multi_layer_Loss:0.08	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.15	Hits@10:38.49	Best:21.33
2025-01-03 23:05:14,848: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 23:05:27,172: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2623 | 0.1575 | 0.3129 | 0.3788 |  0.4587 |
|     1      | 0.281  | 0.1793 | 0.3181 | 0.3846 |  0.4867 |
|     2      | 0.2078 | 0.1227 | 0.2376 | 0.2952 |  0.3746 |
|     3      | 0.2114 | 0.1202 | 0.2466 | 0.3072 |  0.386  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 23:05:44,052: Snapshot:4	Epoch:0	Loss:7.078	translation_Loss:6.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.794                                                   	MRR:8.87	Hits@10:18.66	Best:8.87
2025-01-03 23:05:48,614: Snapshot:4	Epoch:1	Loss:5.313	translation_Loss:4.498	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.815                                                   	MRR:14.84	Hits@10:30.1	Best:14.84
2025-01-03 23:05:53,685: Snapshot:4	Epoch:2	Loss:4.197	translation_Loss:3.471	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.725                                                   	MRR:18.89	Hits@10:33.33	Best:18.89
2025-01-03 23:05:59,009: Snapshot:4	Epoch:3	Loss:3.597	translation_Loss:2.945	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.653                                                   	MRR:20.8	Hits@10:34.87	Best:20.8
2025-01-03 23:06:04,209: Snapshot:4	Epoch:4	Loss:3.174	translation_Loss:2.573	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.601                                                   	MRR:21.67	Hits@10:35.13	Best:21.67
2025-01-03 23:06:09,429: Snapshot:4	Epoch:5	Loss:2.906	translation_Loss:2.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.56                                                   	MRR:22.13	Hits@10:35.02	Best:22.13
2025-01-03 23:06:14,171: Snapshot:4	Epoch:6	Loss:2.711	translation_Loss:2.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.527                                                   	MRR:22.28	Hits@10:35.24	Best:22.28
2025-01-03 23:06:19,278: Snapshot:4	Epoch:7	Loss:2.604	translation_Loss:2.1	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.504                                                   	MRR:22.0	Hits@10:34.64	Best:22.28
2025-01-03 23:06:24,251: Snapshot:4	Epoch:8	Loss:2.546	translation_Loss:2.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.489                                                   	MRR:22.04	Hits@10:35.09	Best:22.28
2025-01-03 23:06:29,357: Snapshot:4	Epoch:9	Loss:2.512	translation_Loss:2.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.482                                                   	MRR:21.95	Hits@10:34.54	Best:22.28
2025-01-03 23:06:34,431: Snapshot:4	Epoch:10	Loss:2.473	translation_Loss:2.001	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.472                                                   	MRR:21.78	Hits@10:34.7	Best:22.28
2025-01-03 23:06:39,331: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 22.28
2025-01-03 23:06:39,331: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:2.469 MRR:22.01 Best Results: 22.28
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 23:06:39,331: Snapshot:4	Epoch:11	Loss:2.469	translation_Loss:2.0	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.469                                                   	MRR:22.01	Hits@10:34.77	Best:22.28
2025-01-03 23:06:43,728: Snapshot:4	Epoch:12	Loss:26.765	translation_Loss:9.751	multi_layer_Loss:17.014	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.01	Hits@10:34.77	Best:22.28
2025-01-03 23:06:48,796: End of token training: 4 Epoch: 13 Loss:11.384 MRR:22.01 Best Results: 22.28
2025-01-03 23:06:48,797: Snapshot:4	Epoch:13	Loss:11.384	translation_Loss:9.75	multi_layer_Loss:1.634	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.01	Hits@10:34.77	Best:22.28
2025-01-03 23:06:49,048: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 23:07:04,905: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2375 | 0.1354 | 0.2838 | 0.3478 |  0.428  |
|     1      | 0.2731 | 0.1721 | 0.3109 | 0.3768 |  0.4771 |
|     2      | 0.1997 | 0.1153 | 0.2285 | 0.2859 |  0.3668 |
|     3      | 0.1994 | 0.1097 | 0.2312 | 0.2924 |  0.3739 |
|     4      | 0.2227 | 0.1544 | 0.2501 | 0.2949 |  0.3542 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 23:07:04,907: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2582 | 0.1519 | 0.3137 | 0.377  |  0.4542 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2596 | 0.1531 | 0.3153 | 0.3795 |  0.4535 |
|     1      | 0.2889 | 0.1838 | 0.3358 | 0.4046 |  0.4977 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2565 | 0.1476 | 0.3143 | 0.3791 |  0.4562 |
|     1      | 0.285  | 0.1801 | 0.3303 | 0.4006 |  0.4964 |
|     2      | 0.2084 | 0.1249 | 0.238  | 0.2955 |  0.3725 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2623 | 0.1575 | 0.3129 | 0.3788 |  0.4587 |
|     1      | 0.281  | 0.1793 | 0.3181 | 0.3846 |  0.4867 |
|     2      | 0.2078 | 0.1227 | 0.2376 | 0.2952 |  0.3746 |
|     3      | 0.2114 | 0.1202 | 0.2466 | 0.3072 |  0.386  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2375 | 0.1354 | 0.2838 | 0.3478 |  0.428  |
|     1      | 0.2731 | 0.1721 | 0.3109 | 0.3768 |  0.4771 |
|     2      | 0.1997 | 0.1153 | 0.2285 | 0.2859 |  0.3668 |
|     3      | 0.1994 | 0.1097 | 0.2312 | 0.2924 |  0.3739 |
|     4      | 0.2227 | 0.1544 | 0.2501 | 0.2949 |  0.3542 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 23:07:04,908: Report Result:
+----------+-------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time       | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+-------------------+-----------+--------------+--------------+---------------+
|    0     | 138.2935004234314 |   0.258   |    0.152     |    0.314     |     0.454     |
|    1     |  95.3120973110199 |   0.267   |    0.161     |    0.321     |     0.465     |
|    2     | 177.8005726337433 |   0.235   |     0.14     |    0.276     |     0.417     |
|    3     | 173.3360345363617 |   0.226   |    0.134     |    0.263     |     0.406     |
|    4     | 79.11192679405212 |   0.215   |    0.126     |    0.248     |     0.386     |
+----------+-------------------+-----------+--------------+--------------+---------------+
2025-01-03 23:07:04,908: Sum_Training_Time:663.8541316986084
2025-01-03 23:07:04,908: Every_Training_Time:[138.2935004234314, 95.3120973110199, 177.8005726337433, 173.3360345363617, 79.11192679405212]
2025-01-03 23:07:04,908: Forward transfer: 0.042825 Backward transfer: -0.0143
