2025-01-03 19:31:21,527: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103193048/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=1, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:31:30,670: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 19:31:36,220: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 19:31:41,712: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.68	Best:18.94
2025-01-03 19:31:47,586: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.68	Hits@10:43.6	Best:22.68
2025-01-03 19:31:53,043: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:45.28	Best:24.2
2025-01-03 19:31:59,020: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.95	Hits@10:46.0	Best:24.95
2025-01-03 19:32:04,540: Snapshot:0	Epoch:6	Loss:1.068	translation_Loss:1.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.3	Hits@10:46.21	Best:25.3
2025-01-03 19:32:10,329: Snapshot:0	Epoch:7	Loss:0.8	translation_Loss:0.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:46.57	Best:25.55
2025-01-03 19:32:15,751: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.45	Best:25.67
2025-01-03 19:32:21,662: Snapshot:0	Epoch:9	Loss:0.536	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.44	Best:25.7
2025-01-03 19:32:27,193: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:46.34	Best:25.7
2025-01-03 19:32:33,030: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.32	Best:25.73
2025-01-03 19:32:38,454: Snapshot:0	Epoch:12	Loss:0.354	translation_Loss:0.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.3	Best:25.74
2025-01-03 19:32:43,982: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.69	Hits@10:46.28	Best:25.74
2025-01-03 19:32:49,704: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.1	Best:25.79
2025-01-03 19:32:55,167: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.07	Best:25.79
2025-01-03 19:33:00,991: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.25	Best:25.79
2025-01-03 19:33:06,442: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:46.15	Best:25.79
2025-01-03 19:33:12,187: Snapshot:0	Epoch:18	Loss:0.222	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.51	Hits@10:45.86	Best:25.79
2025-01-03 19:33:17,702: Early Stopping! Snapshot: 0 Epoch: 19 Best Results: 25.79
2025-01-03 19:33:17,703: Start to training tokens! Snapshot: 0 Epoch: 19 Loss:0.219 MRR:25.45 Best Results: 25.79
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: True
 - torch.Size([1, 200]), requires_grad: True
2025-01-03 19:33:17,703: Snapshot:0	Epoch:19	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.45	Hits@10:45.8	Best:25.79
2025-01-03 19:33:23,953: Snapshot:0	Epoch:20	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.45	Hits@10:45.8	Best:25.79
2025-01-03 19:33:29,374: End of token training: 0 Epoch: 21 Loss:nan MRR:25.45 Best Results: 25.79
2025-01-03 19:33:29,374: Snapshot:0	Epoch:21	Loss:nan	translation_Loss:nan	multi_layer_Loss:nan	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.45	Hits@10:45.8	Best:25.79
2025-01-03 19:33:29,625: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:33:31,909: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1507 | 0.3136 | 0.3784 |  0.4534 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:33:42,068: Snapshot:1	Epoch:0	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 19:33:44,169: Snapshot:1	Epoch:1	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 19:33:46,335: Snapshot:1	Epoch:2	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 19:33:48,367: Snapshot:1	Epoch:3	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 19:33:50,828: Snapshot:1	Epoch:4	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 19:33:52,987: Early Stopping! Snapshot: 1 Epoch: 5 Best Results: 0.3
2025-01-03 19:33:52,988: Start to training tokens! Snapshot: 1 Epoch: 5 Loss:nan MRR:0.3 Best Results: 0.3
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([1, 200]), requires_grad: False
 - torch.Size([1, 200]), requires_grad: False
2025-01-03 19:33:52,988: Snapshot:1	Epoch:5	Loss:nan	translation_Loss:nan	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:nan                                                   	MRR:0.3	Hits@10:1.11	Best:0.3
2025-01-03 19:34:24,143: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103193357/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:34:33,310: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 19:34:38,706: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 19:34:44,020: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.68	Best:18.94
2025-01-03 19:34:49,831: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.7	Hits@10:43.58	Best:22.7
2025-01-03 19:34:55,283: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.21	Hits@10:45.35	Best:24.21
2025-01-03 19:35:01,010: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:45.99	Best:24.98
2025-01-03 19:35:06,625: Snapshot:0	Epoch:6	Loss:1.068	translation_Loss:1.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.26	Hits@10:46.11	Best:25.26
2025-01-03 19:35:12,443: Snapshot:0	Epoch:7	Loss:0.798	translation_Loss:0.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.52	Hits@10:46.4	Best:25.52
2025-01-03 19:35:17,894: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.39	Best:25.6
2025-01-03 19:35:23,575: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.48	Best:25.64
2025-01-03 19:35:28,956: Snapshot:0	Epoch:10	Loss:0.453	translation_Loss:0.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.4	Best:25.71
2025-01-03 19:35:34,819: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.43	Best:25.81
2025-01-03 19:35:40,339: Snapshot:0	Epoch:12	Loss:0.355	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.86	Hits@10:46.31	Best:25.86
2025-01-03 19:35:45,851: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.25	Best:25.86
2025-01-03 19:35:51,707: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.28	Best:25.86
2025-01-03 19:35:57,170: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:45.95	Best:25.86
2025-01-03 19:36:02,843: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.13	Best:25.86
2025-01-03 19:36:08,321: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 25.86
2025-01-03 19:36:08,321: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.238 MRR:25.6 Best Results: 25.86
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2025-01-03 19:36:08,321: Snapshot:0	Epoch:17	Loss:0.238	translation_Loss:0.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.09	Best:25.86
2025-01-03 19:36:14,475: Snapshot:0	Epoch:18	Loss:21.158	translation_Loss:11.534	multi_layer_Loss:9.624	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.09	Best:25.86
2025-01-03 19:36:19,927: End of token training: 0 Epoch: 19 Loss:11.9 MRR:25.6 Best Results: 25.86
2025-01-03 19:36:19,927: Snapshot:0	Epoch:19	Loss:11.9	translation_Loss:11.542	multi_layer_Loss:0.358	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.6	Hits@10:46.09	Best:25.86
2025-01-03 19:36:20,165: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:36:22,511: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2582 | 0.1514 | 0.3145 | 0.3795 |  0.453  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:36:33,023: Snapshot:1	Epoch:0	Loss:4.898	translation_Loss:4.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.147                                                   	MRR:9.83	Hits@10:17.43	Best:9.83
2025-01-03 19:36:35,251: Snapshot:1	Epoch:1	Loss:2.89	translation_Loss:2.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.32                                                   	MRR:16.88	Hits@10:31.1	Best:16.88
2025-01-03 19:36:37,358: Snapshot:1	Epoch:2	Loss:1.797	translation_Loss:1.353	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:21.2	Hits@10:37.46	Best:21.2
2025-01-03 19:36:39,464: Snapshot:1	Epoch:3	Loss:1.268	translation_Loss:0.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.46                                                   	MRR:24.07	Hits@10:41.34	Best:24.07
2025-01-03 19:36:41,605: Snapshot:1	Epoch:4	Loss:0.971	translation_Loss:0.559	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.412                                                   	MRR:25.15	Hits@10:43.25	Best:25.15
2025-01-03 19:36:43,784: Snapshot:1	Epoch:5	Loss:0.785	translation_Loss:0.434	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.352                                                   	MRR:25.9	Hits@10:45.01	Best:25.9
2025-01-03 19:36:46,004: Snapshot:1	Epoch:6	Loss:0.672	translation_Loss:0.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.307                                                   	MRR:26.59	Hits@10:46.67	Best:26.59
2025-01-03 19:36:48,120: Snapshot:1	Epoch:7	Loss:0.607	translation_Loss:0.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.28                                                   	MRR:27.38	Hits@10:48.04	Best:27.38
2025-01-03 19:36:50,222: Snapshot:1	Epoch:8	Loss:0.565	translation_Loss:0.305	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.26                                                   	MRR:27.87	Hits@10:49.37	Best:27.87
2025-01-03 19:36:52,301: Snapshot:1	Epoch:9	Loss:0.525	translation_Loss:0.277	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:28.44	Hits@10:49.66	Best:28.44
2025-01-03 19:36:54,859: Snapshot:1	Epoch:10	Loss:0.495	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.238                                                   	MRR:28.81	Hits@10:50.05	Best:28.81
2025-01-03 19:36:56,993: Snapshot:1	Epoch:11	Loss:0.473	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:28.85	Hits@10:50.13	Best:28.85
2025-01-03 19:36:59,187: Snapshot:1	Epoch:12	Loss:0.461	translation_Loss:0.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.223                                                   	MRR:29.2	Hits@10:50.56	Best:29.2
2025-01-03 19:37:01,413: Snapshot:1	Epoch:13	Loss:0.441	translation_Loss:0.224	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.217                                                   	MRR:29.47	Hits@10:50.61	Best:29.47
2025-01-03 19:37:03,573: Snapshot:1	Epoch:14	Loss:0.431	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.212                                                   	MRR:30.01	Hits@10:50.62	Best:30.01
2025-01-03 19:37:05,686: Snapshot:1	Epoch:15	Loss:0.418	translation_Loss:0.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.207                                                   	MRR:29.89	Hits@10:51.02	Best:30.01
2025-01-03 19:37:07,998: Snapshot:1	Epoch:16	Loss:0.408	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.204                                                   	MRR:29.88	Hits@10:50.74	Best:30.01
2025-01-03 19:37:10,004: Snapshot:1	Epoch:17	Loss:0.399	translation_Loss:0.199	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.2                                                   	MRR:30.0	Hits@10:51.19	Best:30.01
2025-01-03 19:37:12,131: Snapshot:1	Epoch:18	Loss:0.389	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.198                                                   	MRR:29.98	Hits@10:51.07	Best:30.01
2025-01-03 19:37:14,139: Early Stopping! Snapshot: 1 Epoch: 19 Best Results: 30.01
2025-01-03 19:37:14,139: Start to training tokens! Snapshot: 1 Epoch: 19 Loss:0.383 MRR:29.9 Best Results: 30.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: False
 - torch.Size([2, 200]), requires_grad: False
2025-01-03 19:37:14,139: Snapshot:1	Epoch:19	Loss:0.383	translation_Loss:0.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.194                                                   	MRR:29.9	Hits@10:51.22	Best:30.01
2025-01-03 19:37:45,870: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103193718/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=3, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:37:55,089: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 19:38:00,509: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 19:38:05,853: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.68	Best:18.94
2025-01-03 19:38:11,702: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.58	Best:22.69
2025-01-03 19:38:17,191: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.19	Hits@10:45.24	Best:24.19
2025-01-03 19:38:23,010: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.01	Hits@10:45.93	Best:25.01
2025-01-03 19:38:28,456: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.28	Hits@10:46.18	Best:25.28
2025-01-03 19:38:34,384: Snapshot:0	Epoch:7	Loss:0.8	translation_Loss:0.8	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:46.56	Best:25.54
2025-01-03 19:38:39,772: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.65	Best:25.7
2025-01-03 19:38:45,638: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.47	Best:25.71
2025-01-03 19:38:51,094: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.3	Best:25.77
2025-01-03 19:38:56,908: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.44	Best:25.85
2025-01-03 19:39:02,392: Snapshot:0	Epoch:12	Loss:0.355	translation_Loss:0.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.42	Best:25.85
2025-01-03 19:39:07,811: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.78	Hits@10:46.24	Best:25.85
2025-01-03 19:39:13,457: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.87	Hits@10:46.19	Best:25.87
2025-01-03 19:39:18,878: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:45.87	Best:25.87
2025-01-03 19:39:24,584: Snapshot:0	Epoch:16	Loss:0.253	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.85	Hits@10:46.03	Best:25.87
2025-01-03 19:39:30,002: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.06	Best:25.87
2025-01-03 19:39:35,742: Snapshot:0	Epoch:18	Loss:0.223	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.54	Hits@10:45.74	Best:25.87
2025-01-03 19:39:41,287: Early Stopping! Snapshot: 0 Epoch: 19 Best Results: 25.87
2025-01-03 19:39:41,287: Start to training tokens! Snapshot: 0 Epoch: 19 Loss:0.219 MRR:25.52 Best Results: 25.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([3, 200]), requires_grad: True
 - torch.Size([3, 200]), requires_grad: True
2025-01-03 19:39:41,287: Snapshot:0	Epoch:19	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.52	Hits@10:45.83	Best:25.87
2025-01-03 19:39:47,604: Snapshot:0	Epoch:20	Loss:23.332	translation_Loss:11.514	multi_layer_Loss:11.818	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.52	Hits@10:45.83	Best:25.87
2025-01-03 19:39:52,904: End of token training: 0 Epoch: 21 Loss:11.867 MRR:25.52 Best Results: 25.87
2025-01-03 19:39:52,904: Snapshot:0	Epoch:21	Loss:11.867	translation_Loss:11.501	multi_layer_Loss:0.366	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.52	Hits@10:45.83	Best:25.87
2025-01-03 19:39:53,173: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:39:55,522: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2569 |  0.15  | 0.3139 | 0.3768 |  0.4518 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:40:05,833: Snapshot:1	Epoch:0	Loss:4.923	translation_Loss:4.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:9.62	Hits@10:17.16	Best:9.62
2025-01-03 19:40:07,956: Snapshot:1	Epoch:1	Loss:2.959	translation_Loss:2.597	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.361                                                   	MRR:16.63	Hits@10:30.63	Best:16.63
2025-01-03 19:40:10,144: Snapshot:1	Epoch:2	Loss:1.87	translation_Loss:1.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.463                                                   	MRR:20.98	Hits@10:36.69	Best:20.98
2025-01-03 19:40:12,166: Snapshot:1	Epoch:3	Loss:1.326	translation_Loss:0.878	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.448                                                   	MRR:23.71	Hits@10:40.72	Best:23.71
2025-01-03 19:40:14,585: Snapshot:1	Epoch:4	Loss:1.008	translation_Loss:0.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.383                                                   	MRR:24.83	Hits@10:43.14	Best:24.83
2025-01-03 19:40:16,700: Snapshot:1	Epoch:5	Loss:0.829	translation_Loss:0.507	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:25.61	Hits@10:45.07	Best:25.61
2025-01-03 19:40:18,915: Snapshot:1	Epoch:6	Loss:0.719	translation_Loss:0.437	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.282                                                   	MRR:26.62	Hits@10:46.59	Best:26.62
2025-01-03 19:40:21,126: Snapshot:1	Epoch:7	Loss:0.652	translation_Loss:0.394	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:27.17	Hits@10:47.8	Best:27.17
2025-01-03 19:40:23,325: Snapshot:1	Epoch:8	Loss:0.604	translation_Loss:0.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.242                                                   	MRR:27.53	Hits@10:48.59	Best:27.53
2025-01-03 19:40:25,400: Snapshot:1	Epoch:9	Loss:0.565	translation_Loss:0.334	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:28.11	Hits@10:49.09	Best:28.11
2025-01-03 19:40:27,903: Snapshot:1	Epoch:10	Loss:0.538	translation_Loss:0.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.222                                                   	MRR:28.58	Hits@10:49.58	Best:28.58
2025-01-03 19:40:30,014: Snapshot:1	Epoch:11	Loss:0.517	translation_Loss:0.303	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.214                                                   	MRR:28.81	Hits@10:49.76	Best:28.81
2025-01-03 19:40:32,115: Snapshot:1	Epoch:12	Loss:0.492	translation_Loss:0.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.207                                                   	MRR:29.11	Hits@10:49.76	Best:29.11
2025-01-03 19:40:34,188: Snapshot:1	Epoch:13	Loss:0.476	translation_Loss:0.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.202                                                   	MRR:29.01	Hits@10:50.07	Best:29.11
2025-01-03 19:40:36,251: Snapshot:1	Epoch:14	Loss:0.467	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:29.14	Hits@10:50.18	Best:29.14
2025-01-03 19:40:38,306: Snapshot:1	Epoch:15	Loss:0.453	translation_Loss:0.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.194                                                   	MRR:29.42	Hits@10:50.34	Best:29.42
2025-01-03 19:40:40,753: Snapshot:1	Epoch:16	Loss:0.443	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.191                                                   	MRR:29.54	Hits@10:50.46	Best:29.54
2025-01-03 19:40:42,974: Snapshot:1	Epoch:17	Loss:0.431	translation_Loss:0.242	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.189                                                   	MRR:29.63	Hits@10:50.26	Best:29.63
2025-01-03 19:40:45,083: Snapshot:1	Epoch:18	Loss:0.423	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:29.55	Hits@10:50.35	Best:29.63
2025-01-03 19:40:47,211: Snapshot:1	Epoch:19	Loss:0.413	translation_Loss:0.232	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.181                                                   	MRR:29.43	Hits@10:50.49	Best:29.63
2025-01-03 19:40:49,297: Snapshot:1	Epoch:20	Loss:0.408	translation_Loss:0.23	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:29.53	Hits@10:50.57	Best:29.63
2025-01-03 19:40:51,495: Snapshot:1	Epoch:21	Loss:0.409	translation_Loss:0.231	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:29.75	Hits@10:50.99	Best:29.75
2025-01-03 19:40:53,912: Snapshot:1	Epoch:22	Loss:0.396	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.177                                                   	MRR:29.73	Hits@10:50.71	Best:29.75
2025-01-03 19:40:55,905: Snapshot:1	Epoch:23	Loss:0.392	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.173                                                   	MRR:29.66	Hits@10:50.5	Best:29.75
2025-01-03 19:40:58,091: Snapshot:1	Epoch:24	Loss:0.388	translation_Loss:0.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:29.77	Hits@10:50.67	Best:29.77
2025-01-03 19:41:00,126: Snapshot:1	Epoch:25	Loss:0.389	translation_Loss:0.217	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.172                                                   	MRR:29.8	Hits@10:50.31	Best:29.8
2025-01-03 19:41:02,326: Snapshot:1	Epoch:26	Loss:0.384	translation_Loss:0.214	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.17                                                   	MRR:29.93	Hits@10:50.96	Best:29.93
2025-01-03 19:41:04,450: Snapshot:1	Epoch:27	Loss:0.379	translation_Loss:0.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:29.85	Hits@10:50.97	Best:29.93
2025-01-03 19:41:06,828: Snapshot:1	Epoch:28	Loss:0.381	translation_Loss:0.213	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:29.57	Hits@10:50.95	Best:29.93
2025-01-03 19:41:08,905: Snapshot:1	Epoch:29	Loss:0.376	translation_Loss:0.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:29.7	Hits@10:51.04	Best:29.93
2025-01-03 19:41:11,073: Snapshot:1	Epoch:30	Loss:0.372	translation_Loss:0.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:30.0	Hits@10:51.11	Best:30.0
2025-01-03 19:41:13,181: Snapshot:1	Epoch:31	Loss:0.373	translation_Loss:0.208	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:30.12	Hits@10:51.01	Best:30.12
2025-01-03 19:41:15,265: Snapshot:1	Epoch:32	Loss:0.371	translation_Loss:0.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.81	Hits@10:50.87	Best:30.12
2025-01-03 19:41:17,417: Snapshot:1	Epoch:33	Loss:0.37	translation_Loss:0.206	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.87	Hits@10:50.87	Best:30.12
2025-01-03 19:41:19,831: Snapshot:1	Epoch:34	Loss:0.37	translation_Loss:0.204	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.166                                                   	MRR:29.82	Hits@10:51.04	Best:30.12
2025-01-03 19:41:21,936: Snapshot:1	Epoch:35	Loss:0.367	translation_Loss:0.201	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:29.86	Hits@10:50.93	Best:30.12
2025-01-03 19:41:23,985: Early Stopping! Snapshot: 1 Epoch: 36 Best Results: 30.12
2025-01-03 19:41:23,985: Start to training tokens! Snapshot: 1 Epoch: 36 Loss:0.364 MRR:29.83 Best Results: 30.12
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([3, 200]), requires_grad: False
 - torch.Size([3, 200]), requires_grad: False
2025-01-03 19:41:23,985: Snapshot:1	Epoch:36	Loss:0.364	translation_Loss:0.2	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.83	Hits@10:50.79	Best:30.12
2025-01-03 19:41:55,321: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103194128/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=4, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:42:04,535: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 19:42:10,044: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 19:42:15,536: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.67	Best:18.93
2025-01-03 19:42:21,429: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:43.54	Best:22.67
2025-01-03 19:42:27,051: Snapshot:0	Epoch:4	Loss:2.463	translation_Loss:2.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.23	Hits@10:45.21	Best:24.23
2025-01-03 19:42:32,901: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:45.99	Best:24.99
2025-01-03 19:42:38,400: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.25	Hits@10:46.1	Best:25.25
2025-01-03 19:42:44,333: Snapshot:0	Epoch:7	Loss:0.798	translation_Loss:0.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.45	Best:25.58
2025-01-03 19:42:49,859: Snapshot:0	Epoch:8	Loss:0.632	translation_Loss:0.632	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.58	Best:25.67
2025-01-03 19:42:55,700: Snapshot:0	Epoch:9	Loss:0.536	translation_Loss:0.536	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.5	Best:25.67
2025-01-03 19:43:01,247: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.46	Best:25.8
2025-01-03 19:43:07,202: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.36	Best:25.8
2025-01-03 19:43:12,629: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.39	Best:25.8
2025-01-03 19:43:18,129: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.26	Best:25.8
2025-01-03 19:43:23,871: Snapshot:0	Epoch:14	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.04	Best:25.8
2025-01-03 19:43:29,245: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.05	Best:25.84
2025-01-03 19:43:35,012: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.16	Best:25.84
2025-01-03 19:43:40,610: Snapshot:0	Epoch:17	Loss:0.237	translation_Loss:0.237	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.0	Best:25.84
2025-01-03 19:43:46,362: Snapshot:0	Epoch:18	Loss:0.222	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.03	Best:25.84
2025-01-03 19:43:51,745: Snapshot:0	Epoch:19	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:45.77	Best:25.84
2025-01-03 19:43:57,515: Early Stopping! Snapshot: 0 Epoch: 20 Best Results: 25.84
2025-01-03 19:43:57,515: Start to training tokens! Snapshot: 0 Epoch: 20 Loss:0.196 MRR:25.4 Best Results: 25.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([4, 200]), requires_grad: True
 - torch.Size([4, 200]), requires_grad: True
2025-01-03 19:43:57,515: Snapshot:0	Epoch:20	Loss:0.196	translation_Loss:0.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.4	Hits@10:45.67	Best:25.84
2025-01-03 19:44:03,469: Snapshot:0	Epoch:21	Loss:25.212	translation_Loss:11.496	multi_layer_Loss:13.716	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.4	Hits@10:45.67	Best:25.84
2025-01-03 19:44:08,842: End of token training: 0 Epoch: 22 Loss:11.855 MRR:25.4 Best Results: 25.84
2025-01-03 19:44:08,843: Snapshot:0	Epoch:22	Loss:11.855	translation_Loss:11.493	multi_layer_Loss:0.362	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.4	Hits@10:45.67	Best:25.84
2025-01-03 19:44:09,082: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:44:11,587: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.1512 | 0.3136 | 0.3766 |  0.4519 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:44:21,723: Snapshot:1	Epoch:0	Loss:4.938	translation_Loss:4.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.197                                                   	MRR:9.65	Hits@10:16.67	Best:9.65
2025-01-03 19:44:23,871: Snapshot:1	Epoch:1	Loss:3.015	translation_Loss:2.63	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.385                                                   	MRR:16.47	Hits@10:30.18	Best:16.47
2025-01-03 19:44:26,049: Snapshot:1	Epoch:2	Loss:1.926	translation_Loss:1.468	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.458                                                   	MRR:20.68	Hits@10:36.36	Best:20.68
2025-01-03 19:44:28,139: Snapshot:1	Epoch:3	Loss:1.359	translation_Loss:0.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.427                                                   	MRR:23.28	Hits@10:40.02	Best:23.28
2025-01-03 19:44:30,198: Snapshot:1	Epoch:4	Loss:1.044	translation_Loss:0.686	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.358                                                   	MRR:24.17	Hits@10:42.66	Best:24.17
2025-01-03 19:44:32,248: Snapshot:1	Epoch:5	Loss:0.863	translation_Loss:0.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.301                                                   	MRR:25.37	Hits@10:44.9	Best:25.37
2025-01-03 19:44:34,307: Snapshot:1	Epoch:6	Loss:0.754	translation_Loss:0.487	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.267                                                   	MRR:26.33	Hits@10:46.74	Best:26.33
2025-01-03 19:44:36,779: Snapshot:1	Epoch:7	Loss:0.682	translation_Loss:0.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.246                                                   	MRR:26.75	Hits@10:47.94	Best:26.75
2025-01-03 19:44:38,902: Snapshot:1	Epoch:8	Loss:0.633	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.231                                                   	MRR:27.52	Hits@10:48.5	Best:27.52
2025-01-03 19:44:41,027: Snapshot:1	Epoch:9	Loss:0.596	translation_Loss:0.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.221                                                   	MRR:28.22	Hits@10:48.9	Best:28.22
2025-01-03 19:44:43,099: Snapshot:1	Epoch:10	Loss:0.57	translation_Loss:0.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.213                                                   	MRR:28.37	Hits@10:49.08	Best:28.37
2025-01-03 19:44:45,238: Snapshot:1	Epoch:11	Loss:0.548	translation_Loss:0.343	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.205                                                   	MRR:28.7	Hits@10:49.47	Best:28.7
2025-01-03 19:44:47,353: Snapshot:1	Epoch:12	Loss:0.521	translation_Loss:0.322	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.199                                                   	MRR:28.79	Hits@10:49.63	Best:28.79
2025-01-03 19:44:49,778: Snapshot:1	Epoch:13	Loss:0.509	translation_Loss:0.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.193                                                   	MRR:29.04	Hits@10:49.81	Best:29.04
2025-01-03 19:44:51,916: Snapshot:1	Epoch:14	Loss:0.492	translation_Loss:0.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:29.2	Hits@10:49.78	Best:29.2
2025-01-03 19:44:54,080: Snapshot:1	Epoch:15	Loss:0.48	translation_Loss:0.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.184                                                   	MRR:29.38	Hits@10:50.05	Best:29.38
2025-01-03 19:44:56,114: Snapshot:1	Epoch:16	Loss:0.465	translation_Loss:0.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.181                                                   	MRR:29.26	Hits@10:50.36	Best:29.38
2025-01-03 19:44:58,176: Snapshot:1	Epoch:17	Loss:0.458	translation_Loss:0.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:29.4	Hits@10:50.36	Best:29.4
2025-01-03 19:45:00,246: Snapshot:1	Epoch:18	Loss:0.448	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:29.52	Hits@10:50.5	Best:29.52
2025-01-03 19:45:02,688: Snapshot:1	Epoch:19	Loss:0.439	translation_Loss:0.265	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:29.73	Hits@10:50.49	Best:29.73
2025-01-03 19:45:04,853: Snapshot:1	Epoch:20	Loss:0.433	translation_Loss:0.263	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.169                                                   	MRR:29.99	Hits@10:50.62	Best:29.99
2025-01-03 19:45:06,982: Snapshot:1	Epoch:21	Loss:0.425	translation_Loss:0.258	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.98	Hits@10:50.41	Best:29.99
2025-01-03 19:45:08,996: Snapshot:1	Epoch:22	Loss:0.421	translation_Loss:0.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.68	Hits@10:50.34	Best:29.99
2025-01-03 19:45:11,098: Snapshot:1	Epoch:23	Loss:0.419	translation_Loss:0.254	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:29.83	Hits@10:50.31	Best:29.99
2025-01-03 19:45:13,253: Snapshot:1	Epoch:24	Loss:0.412	translation_Loss:0.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:29.72	Hits@10:50.46	Best:29.99
2025-01-03 19:45:15,700: Early Stopping! Snapshot: 1 Epoch: 25 Best Results: 29.99
2025-01-03 19:45:15,700: Start to training tokens! Snapshot: 1 Epoch: 25 Loss:0.41 MRR:29.85 Best Results: 29.99
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([4, 200]), requires_grad: False
 - torch.Size([4, 200]), requires_grad: False
2025-01-03 19:45:15,701: Snapshot:1	Epoch:25	Loss:0.41	translation_Loss:0.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:29.85	Hits@10:50.41	Best:29.99
2025-01-03 19:45:47,262: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103194520/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=6, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:45:56,467: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.0	Best:7.3
2025-01-03 19:46:02,027: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.05	Best:12.4
2025-01-03 19:46:07,447: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.7	Best:18.94
2025-01-03 19:46:13,375: Snapshot:0	Epoch:3	Loss:4.124	translation_Loss:4.124	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.67	Hits@10:43.59	Best:22.67
2025-01-03 19:46:18,960: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.24	Hits@10:45.37	Best:24.24
2025-01-03 19:46:24,833: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:45.93	Best:24.99
2025-01-03 19:46:30,366: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.21	Hits@10:46.27	Best:25.21
2025-01-03 19:46:36,351: Snapshot:0	Epoch:7	Loss:0.798	translation_Loss:0.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:46.56	Best:25.53
2025-01-03 19:46:41,890: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.43	Best:25.56
2025-01-03 19:46:47,780: Snapshot:0	Epoch:9	Loss:0.534	translation_Loss:0.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.4	Best:25.63
2025-01-03 19:46:53,378: Snapshot:0	Epoch:10	Loss:0.455	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.33	Best:25.64
2025-01-03 19:46:59,281: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.27	Best:25.75
2025-01-03 19:47:04,851: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.23	Best:25.75
2025-01-03 19:47:10,392: Snapshot:0	Epoch:13	Loss:0.317	translation_Loss:0.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.15	Best:25.75
2025-01-03 19:47:16,331: Snapshot:0	Epoch:14	Loss:0.294	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.05	Best:25.79
2025-01-03 19:47:21,873: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:45.88	Best:25.79
2025-01-03 19:47:27,828: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:45.88	Best:25.79
2025-01-03 19:47:33,196: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.61	Hits@10:45.89	Best:25.79
2025-01-03 19:47:38,931: Snapshot:0	Epoch:18	Loss:0.222	translation_Loss:0.222	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:45.71	Best:25.79
2025-01-03 19:47:44,420: Early Stopping! Snapshot: 0 Epoch: 19 Best Results: 25.79
2025-01-03 19:47:44,420: Start to training tokens! Snapshot: 0 Epoch: 19 Loss:0.219 MRR:25.55 Best Results: 25.79
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([6, 200]), requires_grad: True
 - torch.Size([6, 200]), requires_grad: True
2025-01-03 19:47:44,420: Snapshot:0	Epoch:19	Loss:0.219	translation_Loss:0.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:45.68	Best:25.79
2025-01-03 19:47:50,735: Snapshot:0	Epoch:20	Loss:27.054	translation_Loss:11.521	multi_layer_Loss:15.533	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.55	Hits@10:45.68	Best:25.79
2025-01-03 19:47:56,147: End of token training: 0 Epoch: 21 Loss:11.911 MRR:25.55 Best Results: 25.79
2025-01-03 19:47:56,147: Snapshot:0	Epoch:21	Loss:11.911	translation_Loss:11.507	multi_layer_Loss:0.404	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.55	Hits@10:45.68	Best:25.79
2025-01-03 19:47:56,383: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:47:58,653: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2569 | 0.1492 | 0.3135 | 0.3795 |  0.4525 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:48:08,839: Snapshot:1	Epoch:0	Loss:4.994	translation_Loss:4.758	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.237                                                   	MRR:9.51	Hits@10:16.79	Best:9.51
2025-01-03 19:48:11,070: Snapshot:1	Epoch:1	Loss:3.111	translation_Loss:2.707	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.405                                                   	MRR:16.18	Hits@10:29.72	Best:16.18
2025-01-03 19:48:13,186: Snapshot:1	Epoch:2	Loss:2.017	translation_Loss:1.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.437                                                   	MRR:20.45	Hits@10:35.7	Best:20.45
2025-01-03 19:48:15,420: Snapshot:1	Epoch:3	Loss:1.453	translation_Loss:1.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:22.88	Hits@10:39.7	Best:22.88
2025-01-03 19:48:17,821: Snapshot:1	Epoch:4	Loss:1.126	translation_Loss:0.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.333                                                   	MRR:24.16	Hits@10:42.78	Best:24.16
2025-01-03 19:48:20,012: Snapshot:1	Epoch:5	Loss:0.944	translation_Loss:0.659	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.286                                                   	MRR:25.24	Hits@10:45.1	Best:25.24
2025-01-03 19:48:22,129: Snapshot:1	Epoch:6	Loss:0.829	translation_Loss:0.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.258                                                   	MRR:26.24	Hits@10:46.67	Best:26.24
2025-01-03 19:48:24,365: Snapshot:1	Epoch:7	Loss:0.758	translation_Loss:0.519	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.239                                                   	MRR:26.87	Hits@10:47.77	Best:26.87
2025-01-03 19:48:26,556: Snapshot:1	Epoch:8	Loss:0.705	translation_Loss:0.48	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.224                                                   	MRR:27.41	Hits@10:48.48	Best:27.41
2025-01-03 19:48:28,667: Snapshot:1	Epoch:9	Loss:0.661	translation_Loss:0.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.216                                                   	MRR:28.01	Hits@10:48.73	Best:28.01
2025-01-03 19:48:31,220: Snapshot:1	Epoch:10	Loss:0.63	translation_Loss:0.422	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.208                                                   	MRR:28.58	Hits@10:49.41	Best:28.58
2025-01-03 19:48:33,300: Snapshot:1	Epoch:11	Loss:0.606	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.199                                                   	MRR:28.88	Hits@10:49.56	Best:28.88
2025-01-03 19:48:35,458: Snapshot:1	Epoch:12	Loss:0.577	translation_Loss:0.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.192                                                   	MRR:29.04	Hits@10:49.51	Best:29.04
2025-01-03 19:48:37,579: Snapshot:1	Epoch:13	Loss:0.559	translation_Loss:0.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.187                                                   	MRR:29.07	Hits@10:49.86	Best:29.07
2025-01-03 19:48:39,747: Snapshot:1	Epoch:14	Loss:0.547	translation_Loss:0.363	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.183                                                   	MRR:29.1	Hits@10:49.96	Best:29.1
2025-01-03 19:48:41,945: Snapshot:1	Epoch:15	Loss:0.531	translation_Loss:0.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.178                                                   	MRR:29.28	Hits@10:50.06	Best:29.28
2025-01-03 19:48:44,460: Snapshot:1	Epoch:16	Loss:0.52	translation_Loss:0.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:29.36	Hits@10:49.98	Best:29.36
2025-01-03 19:48:46,594: Snapshot:1	Epoch:17	Loss:0.508	translation_Loss:0.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:29.43	Hits@10:50.26	Best:29.43
2025-01-03 19:48:48,706: Snapshot:1	Epoch:18	Loss:0.498	translation_Loss:0.327	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.172                                                   	MRR:29.46	Hits@10:50.05	Best:29.46
2025-01-03 19:48:50,796: Snapshot:1	Epoch:19	Loss:0.487	translation_Loss:0.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.48	Hits@10:50.13	Best:29.48
2025-01-03 19:48:52,944: Snapshot:1	Epoch:20	Loss:0.483	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.165                                                   	MRR:29.69	Hits@10:50.63	Best:29.69
2025-01-03 19:48:55,216: Snapshot:1	Epoch:21	Loss:0.482	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:29.81	Hits@10:50.34	Best:29.81
2025-01-03 19:48:57,749: Snapshot:1	Epoch:22	Loss:0.467	translation_Loss:0.306	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.72	Hits@10:50.18	Best:29.81
2025-01-03 19:48:59,808: Snapshot:1	Epoch:23	Loss:0.467	translation_Loss:0.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.159                                                   	MRR:29.54	Hits@10:50.3	Best:29.81
2025-01-03 19:49:01,933: Snapshot:1	Epoch:24	Loss:0.462	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.16                                                   	MRR:29.79	Hits@10:50.42	Best:29.81
2025-01-03 19:49:03,967: Snapshot:1	Epoch:25	Loss:0.461	translation_Loss:0.302	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.159                                                   	MRR:29.81	Hits@10:50.49	Best:29.81
2025-01-03 19:49:06,035: Snapshot:1	Epoch:26	Loss:0.458	translation_Loss:0.301	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:29.85	Hits@10:50.61	Best:29.85
2025-01-03 19:49:08,204: Snapshot:1	Epoch:27	Loss:0.451	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:29.67	Hits@10:50.61	Best:29.85
2025-01-03 19:49:10,600: Snapshot:1	Epoch:28	Loss:0.454	translation_Loss:0.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:29.54	Hits@10:50.4	Best:29.85
2025-01-03 19:49:12,613: Snapshot:1	Epoch:29	Loss:0.448	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:29.62	Hits@10:50.59	Best:29.85
2025-01-03 19:49:14,760: Snapshot:1	Epoch:30	Loss:0.444	translation_Loss:0.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:29.77	Hits@10:50.53	Best:29.85
2025-01-03 19:49:16,827: Early Stopping! Snapshot: 1 Epoch: 31 Best Results: 29.85
2025-01-03 19:49:16,827: Start to training tokens! Snapshot: 1 Epoch: 31 Loss:0.446 MRR:29.79 Best Results: 29.85
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([6, 200]), requires_grad: False
 - torch.Size([6, 200]), requires_grad: False
2025-01-03 19:49:16,827: Snapshot:1	Epoch:31	Loss:0.446	translation_Loss:0.294	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:29.79	Hits@10:50.57	Best:29.85
2025-01-03 19:49:48,298: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103194921/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:49:57,500: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 19:50:03,082: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 19:50:08,778: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.93	Hits@10:39.68	Best:18.93
2025-01-03 19:50:14,624: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.66	Hits@10:43.54	Best:22.66
2025-01-03 19:50:20,145: Snapshot:0	Epoch:4	Loss:2.463	translation_Loss:2.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.2	Hits@10:45.25	Best:24.2
2025-01-03 19:50:26,025: Snapshot:0	Epoch:5	Loss:1.562	translation_Loss:1.562	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:46.01	Best:24.99
2025-01-03 19:50:31,557: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.28	Hits@10:46.36	Best:25.28
2025-01-03 19:50:37,480: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.47	Best:25.6
2025-01-03 19:50:42,960: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.38	Best:25.66
2025-01-03 19:50:48,894: Snapshot:0	Epoch:9	Loss:0.534	translation_Loss:0.534	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.6	Best:25.75
2025-01-03 19:50:54,465: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.28	Best:25.75
2025-01-03 19:51:00,288: Snapshot:0	Epoch:11	Loss:0.401	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.29	Best:25.77
2025-01-03 19:51:05,668: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.74	Hits@10:46.22	Best:25.77
2025-01-03 19:51:11,019: Snapshot:0	Epoch:13	Loss:0.318	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.73	Hits@10:46.2	Best:25.77
2025-01-03 19:51:16,883: Snapshot:0	Epoch:14	Loss:0.292	translation_Loss:0.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.26	Best:25.77
2025-01-03 19:51:22,308: Snapshot:0	Epoch:15	Loss:0.271	translation_Loss:0.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.03	Best:25.77
2025-01-03 19:51:28,019: Snapshot:0	Epoch:16	Loss:0.253	translation_Loss:0.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.81	Hits@10:46.09	Best:25.81
2025-01-03 19:51:33,535: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:45.98	Best:25.81
2025-01-03 19:51:39,433: Snapshot:0	Epoch:18	Loss:0.223	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:45.84	Best:25.81
2025-01-03 19:51:44,906: Snapshot:0	Epoch:19	Loss:0.22	translation_Loss:0.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.51	Hits@10:45.75	Best:25.81
2025-01-03 19:51:50,682: Snapshot:0	Epoch:20	Loss:0.196	translation_Loss:0.196	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.46	Hits@10:45.83	Best:25.81
2025-01-03 19:51:56,134: Early Stopping! Snapshot: 0 Epoch: 21 Best Results: 25.81
2025-01-03 19:51:56,134: Start to training tokens! Snapshot: 0 Epoch: 21 Loss:0.195 MRR:25.24 Best Results: 25.81
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-03 19:51:56,135: Snapshot:0	Epoch:21	Loss:0.195	translation_Loss:0.195	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.24	Hits@10:45.67	Best:25.81
2025-01-03 19:52:02,102: Snapshot:0	Epoch:22	Loss:27.645	translation_Loss:11.528	multi_layer_Loss:16.117	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.24	Hits@10:45.67	Best:25.81
2025-01-03 19:52:07,960: End of token training: 0 Epoch: 23 Loss:12.002 MRR:25.24 Best Results: 25.81
2025-01-03 19:52:07,960: Snapshot:0	Epoch:23	Loss:12.002	translation_Loss:11.546	multi_layer_Loss:0.456	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.24	Hits@10:45.67	Best:25.81
2025-01-03 19:52:08,210: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:52:10,639: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2562 | 0.1489 | 0.3139 | 0.3774 |  0.4521 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:52:20,709: Snapshot:1	Epoch:0	Loss:5.001	translation_Loss:4.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.253                                                   	MRR:9.49	Hits@10:16.6	Best:9.49
2025-01-03 19:52:22,796: Snapshot:1	Epoch:1	Loss:3.101	translation_Loss:2.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.402                                                   	MRR:16.09	Hits@10:29.34	Best:16.09
2025-01-03 19:52:25,333: Snapshot:1	Epoch:2	Loss:2.019	translation_Loss:1.599	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.42                                                   	MRR:20.03	Hits@10:35.19	Best:20.03
2025-01-03 19:52:27,572: Snapshot:1	Epoch:3	Loss:1.441	translation_Loss:1.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.379                                                   	MRR:22.57	Hits@10:39.39	Best:22.57
2025-01-03 19:52:29,775: Snapshot:1	Epoch:4	Loss:1.126	translation_Loss:0.81	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.316                                                   	MRR:24.05	Hits@10:42.73	Best:24.05
2025-01-03 19:52:31,848: Snapshot:1	Epoch:5	Loss:0.945	translation_Loss:0.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.27                                                   	MRR:25.18	Hits@10:45.1	Best:25.18
2025-01-03 19:52:33,917: Snapshot:1	Epoch:6	Loss:0.833	translation_Loss:0.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.245                                                   	MRR:26.17	Hits@10:46.68	Best:26.17
2025-01-03 19:52:36,027: Snapshot:1	Epoch:7	Loss:0.751	translation_Loss:0.523	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.228                                                   	MRR:26.81	Hits@10:47.26	Best:26.81
2025-01-03 19:52:38,473: Snapshot:1	Epoch:8	Loss:0.703	translation_Loss:0.488	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.215                                                   	MRR:27.34	Hits@10:48.07	Best:27.34
2025-01-03 19:52:40,542: Snapshot:1	Epoch:9	Loss:0.663	translation_Loss:0.457	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.205                                                   	MRR:27.64	Hits@10:48.32	Best:27.64
2025-01-03 19:52:42,636: Snapshot:1	Epoch:10	Loss:0.623	translation_Loss:0.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.196                                                   	MRR:28.09	Hits@10:48.66	Best:28.09
2025-01-03 19:52:44,838: Snapshot:1	Epoch:11	Loss:0.599	translation_Loss:0.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.188                                                   	MRR:28.32	Hits@10:49.03	Best:28.32
2025-01-03 19:52:47,036: Snapshot:1	Epoch:12	Loss:0.578	translation_Loss:0.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.182                                                   	MRR:28.65	Hits@10:49.53	Best:28.65
2025-01-03 19:52:49,083: Snapshot:1	Epoch:13	Loss:0.564	translation_Loss:0.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.18                                                   	MRR:29.06	Hits@10:49.7	Best:29.06
2025-01-03 19:52:51,557: Snapshot:1	Epoch:14	Loss:0.548	translation_Loss:0.373	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.175                                                   	MRR:29.09	Hits@10:49.51	Best:29.09
2025-01-03 19:52:53,768: Snapshot:1	Epoch:15	Loss:0.527	translation_Loss:0.36	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.167                                                   	MRR:29.15	Hits@10:49.68	Best:29.15
2025-01-03 19:52:55,924: Snapshot:1	Epoch:16	Loss:0.513	translation_Loss:0.349	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.34	Hits@10:49.84	Best:29.34
2025-01-03 19:52:58,045: Snapshot:1	Epoch:17	Loss:0.505	translation_Loss:0.344	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.41	Hits@10:49.86	Best:29.41
2025-01-03 19:53:00,236: Snapshot:1	Epoch:18	Loss:0.495	translation_Loss:0.336	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.16                                                   	MRR:29.56	Hits@10:49.8	Best:29.56
2025-01-03 19:53:02,401: Snapshot:1	Epoch:19	Loss:0.488	translation_Loss:0.331	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.157                                                   	MRR:29.59	Hits@10:50.02	Best:29.59
2025-01-03 19:53:04,456: Snapshot:1	Epoch:20	Loss:0.479	translation_Loss:0.325	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:29.56	Hits@10:50.34	Best:29.59
2025-01-03 19:53:06,794: Snapshot:1	Epoch:21	Loss:0.476	translation_Loss:0.324	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:29.53	Hits@10:50.3	Best:29.59
2025-01-03 19:53:08,931: Snapshot:1	Epoch:22	Loss:0.465	translation_Loss:0.315	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.15                                                   	MRR:29.39	Hits@10:50.12	Best:29.59
2025-01-03 19:53:11,062: Snapshot:1	Epoch:23	Loss:0.468	translation_Loss:0.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:29.41	Hits@10:50.02	Best:29.59
2025-01-03 19:53:13,180: Early Stopping! Snapshot: 1 Epoch: 24 Best Results: 29.59
2025-01-03 19:53:13,180: Start to training tokens! Snapshot: 1 Epoch: 24 Loss:0.46 MRR:29.46 Best Results: 29.59
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: False
 - torch.Size([7, 200]), requires_grad: False
2025-01-03 19:53:13,180: Snapshot:1	Epoch:24	Loss:0.46	translation_Loss:0.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:29.46	Hits@10:50.25	Best:29.59
2025-01-03 19:53:46,298: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103195319/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:53:55,575: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 19:54:01,215: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 19:54:06,722: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.68	Best:18.94
2025-01-03 19:54:12,709: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.69	Hits@10:43.55	Best:22.69
2025-01-03 19:54:18,200: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.18	Hits@10:45.29	Best:24.18
2025-01-03 19:54:24,038: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:45.9	Best:25.0
2025-01-03 19:54:29,615: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.28	Hits@10:46.21	Best:25.28
2025-01-03 19:54:35,590: Snapshot:0	Epoch:7	Loss:0.798	translation_Loss:0.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.46	Best:25.56
2025-01-03 19:54:41,082: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.52	Best:25.7
2025-01-03 19:54:46,984: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.53	Best:25.76
2025-01-03 19:54:52,436: Snapshot:0	Epoch:10	Loss:0.456	translation_Loss:0.456	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.31	Best:25.76
2025-01-03 19:54:58,377: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.92	Hits@10:46.3	Best:25.92
2025-01-03 19:55:03,949: Snapshot:0	Epoch:12	Loss:0.357	translation_Loss:0.357	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.97	Hits@10:46.33	Best:25.97
2025-01-03 19:55:09,616: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.12	Best:25.97
2025-01-03 19:55:15,609: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.79	Hits@10:46.08	Best:25.97
2025-01-03 19:55:21,050: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.04	Best:25.97
2025-01-03 19:55:26,842: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.72	Hits@10:46.14	Best:25.97
2025-01-03 19:55:32,365: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 25.97
2025-01-03 19:55:32,366: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.236 MRR:25.56 Best Results: 25.97
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2025-01-03 19:55:32,366: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:45.99	Best:25.97
2025-01-03 19:55:38,809: Snapshot:0	Epoch:18	Loss:28.998	translation_Loss:11.516	multi_layer_Loss:17.481	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:45.99	Best:25.97
2025-01-03 19:55:44,366: End of token training: 0 Epoch: 19 Loss:12.067 MRR:25.56 Best Results: 25.97
2025-01-03 19:55:44,366: Snapshot:0	Epoch:19	Loss:12.067	translation_Loss:11.523	multi_layer_Loss:0.544	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.56	Hits@10:45.99	Best:25.97
2025-01-03 19:55:44,602: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:55:47,195: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.1516 | 0.3147 | 0.3784 |  0.4541 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:55:57,748: Snapshot:1	Epoch:0	Loss:5.079	translation_Loss:4.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.301                                                   	MRR:9.5	Hits@10:16.56	Best:9.5
2025-01-03 19:55:59,884: Snapshot:1	Epoch:1	Loss:3.243	translation_Loss:2.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.401                                                   	MRR:15.68	Hits@10:28.78	Best:15.68
2025-01-03 19:56:02,023: Snapshot:1	Epoch:2	Loss:2.152	translation_Loss:1.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:19.79	Hits@10:34.91	Best:19.79
2025-01-03 19:56:04,345: Snapshot:1	Epoch:3	Loss:1.578	translation_Loss:1.219	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.359                                                   	MRR:22.39	Hits@10:39.22	Best:22.39
2025-01-03 19:56:06,546: Snapshot:1	Epoch:4	Loss:1.258	translation_Loss:0.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.303                                                   	MRR:23.93	Hits@10:42.97	Best:23.93
2025-01-03 19:56:08,741: Snapshot:1	Epoch:5	Loss:1.06	translation_Loss:0.798	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.262                                                   	MRR:24.91	Hits@10:45.17	Best:24.91
2025-01-03 19:56:10,928: Snapshot:1	Epoch:6	Loss:0.937	translation_Loss:0.696	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.241                                                   	MRR:25.8	Hits@10:46.5	Best:25.8
2025-01-03 19:56:13,038: Snapshot:1	Epoch:7	Loss:0.86	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.229                                                   	MRR:26.62	Hits@10:47.89	Best:26.62
2025-01-03 19:56:15,288: Snapshot:1	Epoch:8	Loss:0.808	translation_Loss:0.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.215                                                   	MRR:27.24	Hits@10:48.5	Best:27.24
2025-01-03 19:56:17,476: Snapshot:1	Epoch:9	Loss:0.756	translation_Loss:0.551	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.206                                                   	MRR:27.72	Hits@10:49.03	Best:27.72
2025-01-03 19:56:20,116: Snapshot:1	Epoch:10	Loss:0.72	translation_Loss:0.524	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.196                                                   	MRR:28.12	Hits@10:49.49	Best:28.12
2025-01-03 19:56:22,280: Snapshot:1	Epoch:11	Loss:0.69	translation_Loss:0.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.189                                                   	MRR:28.58	Hits@10:49.7	Best:28.58
2025-01-03 19:56:24,511: Snapshot:1	Epoch:12	Loss:0.668	translation_Loss:0.483	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.185                                                   	MRR:28.84	Hits@10:49.93	Best:28.84
2025-01-03 19:56:26,671: Snapshot:1	Epoch:13	Loss:0.645	translation_Loss:0.466	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:29.03	Hits@10:49.95	Best:29.03
2025-01-03 19:56:28,811: Snapshot:1	Epoch:14	Loss:0.629	translation_Loss:0.455	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:29.25	Hits@10:50.28	Best:29.25
2025-01-03 19:56:30,959: Snapshot:1	Epoch:15	Loss:0.61	translation_Loss:0.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.171                                                   	MRR:29.36	Hits@10:50.35	Best:29.36
2025-01-03 19:56:33,360: Snapshot:1	Epoch:16	Loss:0.596	translation_Loss:0.429	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:29.24	Hits@10:50.55	Best:29.36
2025-01-03 19:56:35,531: Snapshot:1	Epoch:17	Loss:0.584	translation_Loss:0.42	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.27	Hits@10:50.32	Best:29.36
2025-01-03 19:56:37,617: Snapshot:1	Epoch:18	Loss:0.575	translation_Loss:0.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.162                                                   	MRR:29.4	Hits@10:50.32	Best:29.4
2025-01-03 19:56:39,724: Snapshot:1	Epoch:19	Loss:0.567	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.161                                                   	MRR:29.37	Hits@10:50.73	Best:29.4
2025-01-03 19:56:41,881: Snapshot:1	Epoch:20	Loss:0.563	translation_Loss:0.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:29.35	Hits@10:50.67	Best:29.4
2025-01-03 19:56:43,983: Snapshot:1	Epoch:21	Loss:0.559	translation_Loss:0.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:29.22	Hits@10:50.42	Best:29.4
2025-01-03 19:56:46,404: Snapshot:1	Epoch:22	Loss:0.551	translation_Loss:0.395	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.156                                                   	MRR:29.29	Hits@10:50.75	Best:29.4
2025-01-03 19:56:48,454: Early Stopping! Snapshot: 1 Epoch: 23 Best Results: 29.4
2025-01-03 19:56:48,454: Start to training tokens! Snapshot: 1 Epoch: 23 Loss:0.54 MRR:29.34 Best Results: 29.4
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: False
 - torch.Size([10, 200]), requires_grad: False
2025-01-03 19:56:48,454: Snapshot:1	Epoch:23	Loss:0.54	translation_Loss:0.385	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:29.34	Hits@10:50.73	Best:29.4
2025-01-03 19:57:19,771: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250103195653/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[10000.0, 5000.0, 1000.0, 15000.0], token_num=13, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-03 19:57:28,889: Snapshot:0	Epoch:0	Loss:15.399	translation_Loss:15.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.3	Hits@10:18.01	Best:7.3
2025-01-03 19:57:34,487: Snapshot:0	Epoch:1	Loss:10.741	translation_Loss:10.741	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.4	Hits@10:31.06	Best:12.4
2025-01-03 19:57:40,079: Snapshot:0	Epoch:2	Loss:7.011	translation_Loss:7.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.94	Hits@10:39.69	Best:18.94
2025-01-03 19:57:45,932: Snapshot:0	Epoch:3	Loss:4.123	translation_Loss:4.123	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.7	Hits@10:43.56	Best:22.7
2025-01-03 19:57:51,335: Snapshot:0	Epoch:4	Loss:2.462	translation_Loss:2.462	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.26	Hits@10:45.24	Best:24.26
2025-01-03 19:57:57,229: Snapshot:0	Epoch:5	Loss:1.561	translation_Loss:1.561	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.0	Hits@10:46.03	Best:25.0
2025-01-03 19:58:02,778: Snapshot:0	Epoch:6	Loss:1.067	translation_Loss:1.067	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.32	Hits@10:46.13	Best:25.32
2025-01-03 19:58:08,636: Snapshot:0	Epoch:7	Loss:0.799	translation_Loss:0.799	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.47	Best:25.67
2025-01-03 19:58:14,179: Snapshot:0	Epoch:8	Loss:0.631	translation_Loss:0.631	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.65	Hits@10:46.47	Best:25.67
2025-01-03 19:58:20,032: Snapshot:0	Epoch:9	Loss:0.535	translation_Loss:0.535	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.51	Best:25.67
2025-01-03 19:58:25,550: Snapshot:0	Epoch:10	Loss:0.454	translation_Loss:0.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:46.42	Best:25.67
2025-01-03 19:58:31,398: Snapshot:0	Epoch:11	Loss:0.402	translation_Loss:0.402	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.34	Best:25.8
2025-01-03 19:58:36,913: Snapshot:0	Epoch:12	Loss:0.356	translation_Loss:0.356	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.97	Hits@10:46.25	Best:25.97
2025-01-03 19:58:42,417: Snapshot:0	Epoch:13	Loss:0.319	translation_Loss:0.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.92	Hits@10:46.26	Best:25.97
2025-01-03 19:58:48,372: Snapshot:0	Epoch:14	Loss:0.293	translation_Loss:0.293	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.93	Hits@10:46.3	Best:25.97
2025-01-03 19:58:53,910: Snapshot:0	Epoch:15	Loss:0.27	translation_Loss:0.27	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.05	Best:25.97
2025-01-03 19:58:59,728: Snapshot:0	Epoch:16	Loss:0.251	translation_Loss:0.251	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.08	Best:25.97
2025-01-03 19:59:05,285: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 25.97
2025-01-03 19:59:05,285: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.236 MRR:25.63 Best Results: 25.97
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 19:59:05,285: Snapshot:0	Epoch:17	Loss:0.236	translation_Loss:0.236	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:45.94	Best:25.97
2025-01-03 19:59:11,620: Snapshot:0	Epoch:18	Loss:29.697	translation_Loss:11.507	multi_layer_Loss:18.19	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.63	Hits@10:45.94	Best:25.97
2025-01-03 19:59:17,032: End of token training: 0 Epoch: 19 Loss:12.11 MRR:25.63 Best Results: 25.97
2025-01-03 19:59:17,032: Snapshot:0	Epoch:19	Loss:12.11	translation_Loss:11.514	multi_layer_Loss:0.596	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.63	Hits@10:45.94	Best:25.97
2025-01-03 19:59:17,317: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2025-01-03 19:59:19,841: 
+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.259 | 0.152  | 0.3155 | 0.3792 |  0.4536 |
+------------+-------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 19:59:30,292: Snapshot:1	Epoch:0	Loss:5.124	translation_Loss:4.783	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:9.45	Hits@10:16.51	Best:9.45
2025-01-03 19:59:32,472: Snapshot:1	Epoch:1	Loss:3.302	translation_Loss:2.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.392                                                   	MRR:15.42	Hits@10:28.05	Best:15.42
2025-01-03 19:59:34,563: Snapshot:1	Epoch:2	Loss:2.207	translation_Loss:1.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.375                                                   	MRR:19.39	Hits@10:34.12	Best:19.39
2025-01-03 19:59:36,727: Snapshot:1	Epoch:3	Loss:1.624	translation_Loss:1.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.336                                                   	MRR:22.07	Hits@10:38.73	Best:22.07
2025-01-03 19:59:38,901: Snapshot:1	Epoch:4	Loss:1.301	translation_Loss:1.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.283                                                   	MRR:23.5	Hits@10:42.22	Best:23.5
2025-01-03 19:59:41,094: Snapshot:1	Epoch:5	Loss:1.102	translation_Loss:0.855	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.247                                                   	MRR:24.71	Hits@10:44.46	Best:24.71
2025-01-03 19:59:43,296: Snapshot:1	Epoch:6	Loss:0.975	translation_Loss:0.747	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.227                                                   	MRR:25.65	Hits@10:46.05	Best:25.65
2025-01-03 19:59:45,420: Snapshot:1	Epoch:7	Loss:0.896	translation_Loss:0.681	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.215                                                   	MRR:26.5	Hits@10:46.95	Best:26.5
2025-01-03 19:59:47,506: Snapshot:1	Epoch:8	Loss:0.841	translation_Loss:0.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.204                                                   	MRR:26.95	Hits@10:47.78	Best:26.95
2025-01-03 19:59:49,688: Snapshot:1	Epoch:9	Loss:0.788	translation_Loss:0.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.194                                                   	MRR:27.53	Hits@10:48.47	Best:27.53
2025-01-03 19:59:52,237: Snapshot:1	Epoch:10	Loss:0.748	translation_Loss:0.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.186                                                   	MRR:27.95	Hits@10:48.77	Best:27.95
2025-01-03 19:59:54,354: Snapshot:1	Epoch:11	Loss:0.718	translation_Loss:0.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.179                                                   	MRR:28.39	Hits@10:49.33	Best:28.39
2025-01-03 19:59:56,448: Snapshot:1	Epoch:12	Loss:0.695	translation_Loss:0.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.174                                                   	MRR:28.78	Hits@10:49.41	Best:28.78
2025-01-03 19:59:58,560: Snapshot:1	Epoch:13	Loss:0.669	translation_Loss:0.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.168                                                   	MRR:28.96	Hits@10:49.52	Best:28.96
2025-01-03 20:00:00,642: Snapshot:1	Epoch:14	Loss:0.653	translation_Loss:0.489	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.164                                                   	MRR:29.13	Hits@10:49.74	Best:29.13
2025-01-03 20:00:02,755: Snapshot:1	Epoch:15	Loss:0.634	translation_Loss:0.474	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.16                                                   	MRR:28.97	Hits@10:49.62	Best:29.13
2025-01-03 20:00:05,292: Snapshot:1	Epoch:16	Loss:0.62	translation_Loss:0.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.158                                                   	MRR:29.17	Hits@10:50.13	Best:29.17
2025-01-03 20:00:07,415: Snapshot:1	Epoch:17	Loss:0.608	translation_Loss:0.453	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.155                                                   	MRR:29.16	Hits@10:50.1	Best:29.17
2025-01-03 20:00:09,592: Snapshot:1	Epoch:18	Loss:0.598	translation_Loss:0.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.154                                                   	MRR:29.18	Hits@10:49.87	Best:29.18
2025-01-03 20:00:11,715: Snapshot:1	Epoch:19	Loss:0.59	translation_Loss:0.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.152                                                   	MRR:29.35	Hits@10:50.24	Best:29.35
2025-01-03 20:00:13,774: Snapshot:1	Epoch:20	Loss:0.587	translation_Loss:0.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:29.19	Hits@10:50.24	Best:29.35
2025-01-03 20:00:15,945: Snapshot:1	Epoch:21	Loss:0.581	translation_Loss:0.431	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.149                                                   	MRR:29.17	Hits@10:50.32	Best:29.35
2025-01-03 20:00:18,320: Snapshot:1	Epoch:22	Loss:0.573	translation_Loss:0.425	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.147                                                   	MRR:29.3	Hits@10:50.53	Best:29.35
2025-01-03 20:00:20,481: Snapshot:1	Epoch:23	Loss:0.563	translation_Loss:0.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.147                                                   	MRR:29.54	Hits@10:50.78	Best:29.54
2025-01-03 20:00:22,593: Snapshot:1	Epoch:24	Loss:0.562	translation_Loss:0.417	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.145                                                   	MRR:29.59	Hits@10:50.53	Best:29.59
2025-01-03 20:00:24,700: Snapshot:1	Epoch:25	Loss:0.555	translation_Loss:0.412	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:29.44	Hits@10:50.49	Best:29.59
2025-01-03 20:00:26,724: Snapshot:1	Epoch:26	Loss:0.554	translation_Loss:0.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:29.49	Hits@10:50.32	Best:29.59
2025-01-03 20:00:28,843: Snapshot:1	Epoch:27	Loss:0.549	translation_Loss:0.406	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.143                                                   	MRR:29.59	Hits@10:50.54	Best:29.59
2025-01-03 20:00:31,360: Snapshot:1	Epoch:28	Loss:0.546	translation_Loss:0.401	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.145                                                   	MRR:29.51	Hits@10:50.59	Best:29.59
2025-01-03 20:00:33,521: Early Stopping! Snapshot: 1 Epoch: 29 Best Results: 29.59
2025-01-03 20:00:33,521: Start to training tokens! Snapshot: 1 Epoch: 29 Loss:0.549 MRR:29.57 Best Results: 29.59
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 20:00:33,521: Snapshot:1	Epoch:29	Loss:0.549	translation_Loss:0.405	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.144                                                   	MRR:29.57	Hits@10:50.57	Best:29.59
2025-01-03 20:00:35,656: Snapshot:1	Epoch:30	Loss:18.104	translation_Loss:4.541	multi_layer_Loss:13.564	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.57	Hits@10:50.57	Best:29.59
2025-01-03 20:00:37,797: End of token training: 1 Epoch: 31 Loss:8.931 MRR:29.57 Best Results: 29.59
2025-01-03 20:00:37,797: Snapshot:1	Epoch:31	Loss:8.931	translation_Loss:4.541	multi_layer_Loss:4.389	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.57	Hits@10:50.57	Best:29.59
2025-01-03 20:00:38,038: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2025-01-03 20:00:41,703: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2603 | 0.1533 | 0.3187 | 0.3804 |  0.4531 |
|     1      | 0.2907 | 0.1857 | 0.339  | 0.4107 |  0.5002 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 20:01:12,374: Snapshot:2	Epoch:0	Loss:16.737	translation_Loss:15.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.418                                                   	MRR:13.13	Hits@10:27.06	Best:13.13
2025-01-03 20:01:21,581: Snapshot:2	Epoch:1	Loss:8.51	translation_Loss:7.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.399                                                   	MRR:18.21	Hits@10:34.74	Best:18.21
2025-01-03 20:01:30,553: Snapshot:2	Epoch:2	Loss:5.846	translation_Loss:4.703	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.143                                                   	MRR:19.97	Hits@10:36.33	Best:19.97
2025-01-03 20:01:39,726: Snapshot:2	Epoch:3	Loss:4.856	translation_Loss:3.849	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.007                                                   	MRR:20.51	Hits@10:36.84	Best:20.51
2025-01-03 20:01:48,786: Snapshot:2	Epoch:4	Loss:4.464	translation_Loss:3.512	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.952                                                   	MRR:20.68	Hits@10:36.85	Best:20.68
2025-01-03 20:01:58,061: Snapshot:2	Epoch:5	Loss:4.275	translation_Loss:3.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.925                                                   	MRR:20.83	Hits@10:37.07	Best:20.83
2025-01-03 20:02:07,530: Snapshot:2	Epoch:6	Loss:4.164	translation_Loss:3.255	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.908                                                   	MRR:20.88	Hits@10:37.13	Best:20.88
2025-01-03 20:02:16,512: Snapshot:2	Epoch:7	Loss:4.108	translation_Loss:3.202	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.906                                                   	MRR:20.87	Hits@10:37.14	Best:20.88
2025-01-03 20:02:25,768: Snapshot:2	Epoch:8	Loss:4.06	translation_Loss:3.157	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.903                                                   	MRR:20.92	Hits@10:37.13	Best:20.92
2025-01-03 20:02:35,146: Snapshot:2	Epoch:9	Loss:4.015	translation_Loss:3.112	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.903                                                   	MRR:20.92	Hits@10:37.16	Best:20.92
2025-01-03 20:02:44,602: Snapshot:2	Epoch:10	Loss:4.002	translation_Loss:3.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.9                                                   	MRR:20.98	Hits@10:37.21	Best:20.98
2025-01-03 20:02:53,699: Snapshot:2	Epoch:11	Loss:3.974	translation_Loss:3.075	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.9                                                   	MRR:20.84	Hits@10:36.99	Best:20.98
2025-01-03 20:03:03,156: Snapshot:2	Epoch:12	Loss:3.951	translation_Loss:3.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.9                                                   	MRR:20.74	Hits@10:36.99	Best:20.98
2025-01-03 20:03:12,495: Snapshot:2	Epoch:13	Loss:3.94	translation_Loss:3.04	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.9                                                   	MRR:20.79	Hits@10:37.18	Best:20.98
2025-01-03 20:03:21,454: Snapshot:2	Epoch:14	Loss:3.931	translation_Loss:3.031	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.9                                                   	MRR:20.68	Hits@10:37.17	Best:20.98
2025-01-03 20:03:30,649: Early Stopping! Snapshot: 2 Epoch: 15 Best Results: 20.98
2025-01-03 20:03:30,650: Start to training tokens! Snapshot: 2 Epoch: 15 Loss:3.922 MRR:20.79 Best Results: 20.98
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 20:03:30,650: Snapshot:2	Epoch:15	Loss:3.922	translation_Loss:3.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.898                                                   	MRR:20.79	Hits@10:37.0	Best:20.98
2025-01-03 20:03:40,035: Snapshot:2	Epoch:16	Loss:37.272	translation_Loss:18.598	multi_layer_Loss:18.674	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.79	Hits@10:37.0	Best:20.98
2025-01-03 20:03:48,989: End of token training: 2 Epoch: 17 Loss:18.745 MRR:20.79 Best Results: 20.98
2025-01-03 20:03:48,989: Snapshot:2	Epoch:17	Loss:18.745	translation_Loss:18.594	multi_layer_Loss:0.151	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.79	Hits@10:37.0	Best:20.98
2025-01-03 20:03:49,195: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2025-01-03 20:03:56,728: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2586 | 0.1496 | 0.3162 | 0.3807 |  0.4561 |
|     1      | 0.2833 | 0.1787 | 0.3269 | 0.3987 |  0.4951 |
|     2      | 0.2098 | 0.1265 | 0.2392 | 0.2957 |  0.3738 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 20:04:32,459: Snapshot:3	Epoch:0	Loss:15.529	translation_Loss:14.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.398                                                   	MRR:15.51	Hits@10:31.82	Best:15.51
2025-01-03 20:04:43,541: Snapshot:3	Epoch:1	Loss:6.999	translation_Loss:4.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.017                                                   	MRR:19.88	Hits@10:37.25	Best:19.88
2025-01-03 20:04:54,930: Snapshot:3	Epoch:2	Loss:5.155	translation_Loss:3.365	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.791                                                   	MRR:20.8	Hits@10:38.43	Best:20.8
2025-01-03 20:05:06,362: Snapshot:3	Epoch:3	Loss:4.52	translation_Loss:2.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.657                                                   	MRR:21.09	Hits@10:38.65	Best:21.09
2025-01-03 20:05:17,458: Snapshot:3	Epoch:4	Loss:4.224	translation_Loss:2.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.575                                                   	MRR:21.26	Hits@10:38.74	Best:21.26
2025-01-03 20:05:28,857: Snapshot:3	Epoch:5	Loss:4.086	translation_Loss:2.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.534                                                   	MRR:21.31	Hits@10:38.78	Best:21.31
2025-01-03 20:05:40,145: Snapshot:3	Epoch:6	Loss:3.985	translation_Loss:2.475	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.509                                                   	MRR:21.31	Hits@10:38.63	Best:21.31
2025-01-03 20:05:51,499: Snapshot:3	Epoch:7	Loss:3.929	translation_Loss:2.438	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.492                                                   	MRR:21.22	Hits@10:38.55	Best:21.31
2025-01-03 20:06:02,752: Snapshot:3	Epoch:8	Loss:3.888	translation_Loss:2.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.489                                                   	MRR:21.38	Hits@10:38.66	Best:21.38
2025-01-03 20:06:13,928: Snapshot:3	Epoch:9	Loss:3.87	translation_Loss:2.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.488                                                   	MRR:21.4	Hits@10:38.77	Best:21.4
2025-01-03 20:06:25,184: Snapshot:3	Epoch:10	Loss:3.846	translation_Loss:2.366	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.481                                                   	MRR:21.28	Hits@10:38.65	Best:21.4
2025-01-03 20:06:36,455: Snapshot:3	Epoch:11	Loss:3.826	translation_Loss:2.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.475                                                   	MRR:21.38	Hits@10:38.72	Best:21.4
2025-01-03 20:06:47,625: Snapshot:3	Epoch:12	Loss:3.818	translation_Loss:2.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.471                                                   	MRR:21.28	Hits@10:38.81	Best:21.4
2025-01-03 20:06:58,785: Snapshot:3	Epoch:13	Loss:3.795	translation_Loss:2.32	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.475                                                   	MRR:21.13	Hits@10:38.51	Best:21.4
2025-01-03 20:07:09,952: Early Stopping! Snapshot: 3 Epoch: 14 Best Results: 21.4
2025-01-03 20:07:09,952: Start to training tokens! Snapshot: 3 Epoch: 14 Loss:3.789 MRR:21.24 Best Results: 21.4
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 20:07:09,952: Snapshot:3	Epoch:14	Loss:3.789	translation_Loss:2.317	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.472                                                   	MRR:21.24	Hits@10:38.61	Best:21.4
2025-01-03 20:07:20,821: Snapshot:3	Epoch:15	Loss:37.681	translation_Loss:19.176	multi_layer_Loss:18.505	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.24	Hits@10:38.61	Best:21.4
2025-01-03 20:07:31,929: End of token training: 3 Epoch: 16 Loss:19.277 MRR:21.24 Best Results: 21.4
2025-01-03 20:07:31,929: Snapshot:3	Epoch:16	Loss:19.277	translation_Loss:19.198	multi_layer_Loss:0.079	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.24	Hits@10:38.61	Best:21.4
2025-01-03 20:07:32,151: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2025-01-03 20:07:44,402: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.262  | 0.1554 | 0.3159 | 0.3804 |  0.4578 |
|     1      | 0.2795 | 0.1781 | 0.3188 | 0.3808 |  0.4802 |
|     2      | 0.2056 | 0.121  | 0.2336 | 0.2921 |  0.3739 |
|     3      | 0.2121 | 0.1191 | 0.2493 | 0.3109 |  0.3885 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-03 20:08:01,082: Snapshot:4	Epoch:0	Loss:7.009	translation_Loss:6.22	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.789                                                   	MRR:9.36	Hits@10:20.06	Best:9.36
2025-01-03 20:08:06,036: Snapshot:4	Epoch:1	Loss:5.287	translation_Loss:4.482	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.805                                                   	MRR:15.02	Hits@10:31.3	Best:15.02
2025-01-03 20:08:10,654: Snapshot:4	Epoch:2	Loss:4.153	translation_Loss:3.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.722                                                   	MRR:18.46	Hits@10:34.42	Best:18.46
2025-01-03 20:08:15,204: Snapshot:4	Epoch:3	Loss:3.549	translation_Loss:2.895	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.654                                                   	MRR:20.84	Hits@10:35.07	Best:20.84
2025-01-03 20:08:20,046: Snapshot:4	Epoch:4	Loss:3.144	translation_Loss:2.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:21.8	Hits@10:35.7	Best:21.8
2025-01-03 20:08:24,546: Snapshot:4	Epoch:5	Loss:2.877	translation_Loss:2.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.563                                                   	MRR:21.91	Hits@10:35.4	Best:21.91
2025-01-03 20:08:29,082: Snapshot:4	Epoch:6	Loss:2.715	translation_Loss:2.186	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.529                                                   	MRR:22.18	Hits@10:35.46	Best:22.18
2025-01-03 20:08:33,726: Snapshot:4	Epoch:7	Loss:2.619	translation_Loss:2.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.508                                                   	MRR:22.1	Hits@10:35.36	Best:22.18
2025-01-03 20:08:38,462: Snapshot:4	Epoch:8	Loss:2.576	translation_Loss:2.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.494                                                   	MRR:21.97	Hits@10:35.23	Best:22.18
2025-01-03 20:08:42,973: Snapshot:4	Epoch:9	Loss:2.534	translation_Loss:2.051	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.483                                                   	MRR:22.14	Hits@10:35.19	Best:22.18
2025-01-03 20:08:47,576: Snapshot:4	Epoch:10	Loss:2.506	translation_Loss:2.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.476                                                   	MRR:21.98	Hits@10:35.07	Best:22.18
2025-01-03 20:08:52,300: Early Stopping! Snapshot: 4 Epoch: 11 Best Results: 22.18
2025-01-03 20:08:52,300: Start to training tokens! Snapshot: 4 Epoch: 11 Loss:2.491 MRR:22.02 Best Results: 22.18
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([13, 200]), requires_grad: True
 - torch.Size([13, 200]), requires_grad: True
2025-01-03 20:08:52,301: Snapshot:4	Epoch:11	Loss:2.491	translation_Loss:2.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.473                                                   	MRR:22.02	Hits@10:34.94	Best:22.18
2025-01-03 20:08:56,857: Snapshot:4	Epoch:12	Loss:26.52	translation_Loss:9.776	multi_layer_Loss:16.744	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.02	Hits@10:34.94	Best:22.18
2025-01-03 20:09:01,323: End of token training: 4 Epoch: 13 Loss:11.332 MRR:22.02 Best Results: 22.18
2025-01-03 20:09:01,324: Snapshot:4	Epoch:13	Loss:11.332	translation_Loss:9.774	multi_layer_Loss:1.558	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.02	Hits@10:34.94	Best:22.18
2025-01-03 20:09:01,638: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2025-01-03 20:09:16,751: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2365 | 0.1327 | 0.2844 | 0.3489 |  0.4279 |
|     1      | 0.2725 | 0.1719 | 0.3098 | 0.3754 |  0.4717 |
|     2      | 0.1987 | 0.1148 | 0.2261 | 0.2841 |  0.3664 |
|     3      | 0.2004 | 0.1086 | 0.2351 | 0.2964 |  0.3756 |
|     4      | 0.2195 | 0.1491 | 0.2491 | 0.2955 |  0.3553 |
+------------+--------+--------+--------+--------+---------+
2025-01-03 20:09:16,753: Final Result:
[+------------+-------+--------+--------+--------+---------+
| Snapshot:0 |  MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+-------+--------+--------+--------+---------+
|     0      | 0.259 | 0.152  | 0.3155 | 0.3792 |  0.4536 |
+------------+-------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2603 | 0.1533 | 0.3187 | 0.3804 |  0.4531 |
|     1      | 0.2907 | 0.1857 | 0.339  | 0.4107 |  0.5002 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2586 | 0.1496 | 0.3162 | 0.3807 |  0.4561 |
|     1      | 0.2833 | 0.1787 | 0.3269 | 0.3987 |  0.4951 |
|     2      | 0.2098 | 0.1265 | 0.2392 | 0.2957 |  0.3738 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.262  | 0.1554 | 0.3159 | 0.3804 |  0.4578 |
|     1      | 0.2795 | 0.1781 | 0.3188 | 0.3808 |  0.4802 |
|     2      | 0.2056 | 0.121  | 0.2336 | 0.2921 |  0.3739 |
|     3      | 0.2121 | 0.1191 | 0.2493 | 0.3109 |  0.3885 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2365 | 0.1327 | 0.2844 | 0.3489 |  0.4279 |
|     1      | 0.2725 | 0.1719 | 0.3098 | 0.3754 |  0.4717 |
|     2      | 0.1987 | 0.1148 | 0.2261 | 0.2841 |  0.3664 |
|     3      | 0.2004 | 0.1086 | 0.2351 | 0.2964 |  0.3756 |
|     4      | 0.2195 | 0.1491 | 0.2491 | 0.2955 |  0.3553 |
+------------+--------+--------+--------+--------+---------+]
2025-01-03 20:09:16,754: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 117.26036524772644 |   0.259   |    0.152     |    0.316     |     0.454     |
|    1     | 76.55382466316223  |   0.268   |    0.162     |    0.324     |     0.466     |
|    2     | 183.5141899585724  |   0.236   |    0.141     |    0.277     |     0.417     |
|    3     | 210.3738248348236  |   0.226   |    0.132     |    0.264     |     0.406     |
|    4     |  74.4332664012909  |   0.214   |    0.124     |    0.249     |     0.387     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-03 20:09:16,754: Sum_Training_Time:662.1354711055756
2025-01-03 20:09:16,754: Every_Training_Time:[117.26036524772644, 76.55382466316223, 183.5141899585724, 210.3738248348236, 74.4332664012909]
2025-01-03 20:09:16,754: Forward transfer: 0.043325 Backward transfer: -0.015875000000000007
