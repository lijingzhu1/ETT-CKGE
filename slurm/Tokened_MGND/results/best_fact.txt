python main.py -dataset FACT -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 3 -multi_layer_weight 1 -token_distillation_weight 1000 10000 10000 10000
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2024-12-29 19:05:01,668: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229190429/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[1000.0, 10000.0, 10000.0, 10000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 19:05:10,736: Snapshot:0	Epoch:0	Loss:17.024	translation_Loss:17.024	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.23	Hits@10:13.26	Best:6.23
2024-12-29 19:05:16,970: Snapshot:0	Epoch:1	Loss:11.963	translation_Loss:11.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:9.68	Hits@10:22.6	Best:9.68
2024-12-29 19:05:22,752: Snapshot:0	Epoch:2	Loss:8.603	translation_Loss:8.603	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.9	Hits@10:28.89	Best:12.9
2024-12-29 19:05:28,922: Snapshot:0	Epoch:3	Loss:6.158	translation_Loss:6.158	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.77	Hits@10:33.1	Best:15.77
2024-12-29 19:05:34,701: Snapshot:0	Epoch:4	Loss:4.364	translation_Loss:4.364	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.34	Hits@10:35.94	Best:18.34
2024-12-29 19:05:40,428: Snapshot:0	Epoch:5	Loss:3.084	translation_Loss:3.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.44	Hits@10:37.73	Best:20.44
2024-12-29 19:05:46,550: Snapshot:0	Epoch:6	Loss:2.175	translation_Loss:2.175	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.92	Hits@10:38.69	Best:21.92
2024-12-29 19:05:52,325: Snapshot:0	Epoch:7	Loss:1.541	translation_Loss:1.541	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.01	Hits@10:39.45	Best:23.01
2024-12-29 19:05:58,475: Snapshot:0	Epoch:8	Loss:1.097	translation_Loss:1.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.75	Hits@10:39.79	Best:23.75
2024-12-29 19:06:04,293: Snapshot:0	Epoch:9	Loss:0.816	translation_Loss:0.816	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:40.05	Best:24.14
2024-12-29 19:06:10,044: Snapshot:0	Epoch:10	Loss:0.625	translation_Loss:0.625	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:40.17	Best:24.36
2024-12-29 19:06:16,274: Snapshot:0	Epoch:11	Loss:0.495	translation_Loss:0.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.42	Hits@10:40.21	Best:24.42
2024-12-29 19:06:22,023: Snapshot:0	Epoch:12	Loss:0.41	translation_Loss:0.41	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.55	Hits@10:40.28	Best:24.55
2024-12-29 19:06:28,160: Snapshot:0	Epoch:13	Loss:0.352	translation_Loss:0.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.73	Hits@10:40.42	Best:24.73
2024-12-29 19:06:33,916: Snapshot:0	Epoch:14	Loss:0.311	translation_Loss:0.311	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:40.54	Best:24.76
2024-12-29 19:06:39,665: Snapshot:0	Epoch:15	Loss:0.274	translation_Loss:0.274	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.66	Hits@10:40.35	Best:24.76
2024-12-29 19:06:45,768: Snapshot:0	Epoch:16	Loss:0.244	translation_Loss:0.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.74	Hits@10:40.31	Best:24.76
2024-12-29 19:06:51,544: Snapshot:0	Epoch:17	Loss:0.223	translation_Loss:0.223	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.78	Hits@10:40.48	Best:24.78
2024-12-29 19:06:57,639: Snapshot:0	Epoch:18	Loss:0.207	translation_Loss:0.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.76	Hits@10:40.4	Best:24.78
2024-12-29 19:07:03,344: Snapshot:0	Epoch:19	Loss:0.191	translation_Loss:0.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.7	Hits@10:40.33	Best:24.78
2024-12-29 19:07:09,503: Early Stopping! Snapshot: 0 Epoch: 20 Best Results: 24.78
2024-12-29 19:07:09,503: Start to training tokens! Snapshot: 0 Epoch: 20 Loss:0.18 MRR:24.71 Best Results: 24.78
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 19:07:09,504: Snapshot:0	Epoch:20	Loss:0.18	translation_Loss:0.18	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:40.24	Best:24.78
2024-12-29 19:07:15,791: Snapshot:0	Epoch:21	Loss:27.639	translation_Loss:12.308	multi_layer_Loss:15.33	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:40.24	Best:24.78
2024-12-29 19:07:21,502: End of token training: 0 Epoch: 22 Loss:12.604 MRR:24.71 Best Results: 24.78
2024-12-29 19:07:21,503: Snapshot:0	Epoch:22	Loss:12.604	translation_Loss:12.295	multi_layer_Loss:0.309	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:24.71	Hits@10:40.24	Best:24.78
2024-12-29 19:07:21,798: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 19:07:24,311: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1552 | 0.2821 | 0.3336 |  0.394  |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 19:07:47,071: Snapshot:1	Epoch:0	Loss:7.528	translation_Loss:7.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:20.66	Hits@10:34.48	Best:20.66
2024-12-29 19:07:53,291: Snapshot:1	Epoch:1	Loss:4.171	translation_Loss:3.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.987                                                   	MRR:22.36	Hits@10:37.13	Best:22.36
2024-12-29 19:07:59,496: Snapshot:1	Epoch:2	Loss:3.043	translation_Loss:1.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.246                                                   	MRR:23.0	Hits@10:38.07	Best:23.0
2024-12-29 19:08:05,756: Snapshot:1	Epoch:3	Loss:2.663	translation_Loss:1.33	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.333                                                   	MRR:23.17	Hits@10:38.41	Best:23.17
2024-12-29 19:08:11,977: Snapshot:1	Epoch:4	Loss:2.531	translation_Loss:1.154	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.377                                                   	MRR:23.34	Hits@10:38.54	Best:23.34
2024-12-29 19:08:18,170: Snapshot:1	Epoch:5	Loss:2.474	translation_Loss:1.077	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.397                                                   	MRR:23.28	Hits@10:38.46	Best:23.34
2024-12-29 19:08:24,681: Snapshot:1	Epoch:6	Loss:2.448	translation_Loss:1.033	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.415                                                   	MRR:23.25	Hits@10:38.54	Best:23.34
2024-12-29 19:08:30,840: Early Stopping! Snapshot: 1 Epoch: 7 Best Results: 23.34
2024-12-29 19:08:30,841: Start to training tokens! Snapshot: 1 Epoch: 7 Loss:2.435 MRR:23.24 Best Results: 23.34
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 19:08:30,841: Snapshot:1	Epoch:7	Loss:2.435	translation_Loss:1.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.425                                                   	MRR:23.24	Hits@10:38.52	Best:23.34
2024-12-29 19:08:36,992: Snapshot:1	Epoch:8	Loss:28.217	translation_Loss:13.418	multi_layer_Loss:14.799	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.24	Hits@10:38.52	Best:23.34
2024-12-29 19:08:43,547: End of token training: 1 Epoch: 9 Loss:13.704 MRR:23.24 Best Results: 23.34
2024-12-29 19:08:43,547: Snapshot:1	Epoch:9	Loss:13.704	translation_Loss:13.396	multi_layer_Loss:0.308	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:23.24	Hits@10:38.52	Best:23.34
2024-12-29 19:08:43,814: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 19:08:49,405: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2537 | 0.1647 | 0.2939 | 0.351  |  0.4213 |
|     1      | 0.2347 | 0.1515 | 0.2744 | 0.325  |  0.3873 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 19:09:12,094: Snapshot:2	Epoch:0	Loss:5.237	translation_Loss:4.097	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.14                                                   	MRR:21.38	Hits@10:37.4	Best:21.38
2024-12-29 19:09:18,472: Snapshot:2	Epoch:1	Loss:4.033	translation_Loss:2.793	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.24                                                   	MRR:21.76	Hits@10:37.53	Best:21.76
2024-12-29 19:09:25,002: Snapshot:2	Epoch:2	Loss:3.746	translation_Loss:2.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.094                                                   	MRR:21.94	Hits@10:37.82	Best:21.94
2024-12-29 19:09:31,610: Snapshot:2	Epoch:3	Loss:3.642	translation_Loss:2.532	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.111                                                   	MRR:21.97	Hits@10:37.73	Best:21.97
2024-12-29 19:09:37,910: Snapshot:2	Epoch:4	Loss:3.623	translation_Loss:2.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.097                                                   	MRR:21.9	Hits@10:37.82	Best:21.97
2024-12-29 19:09:44,235: Snapshot:2	Epoch:5	Loss:3.612	translation_Loss:2.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.113                                                   	MRR:21.98	Hits@10:37.76	Best:21.98
2024-12-29 19:09:50,998: Snapshot:2	Epoch:6	Loss:3.601	translation_Loss:2.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.105                                                   	MRR:21.93	Hits@10:37.9	Best:21.98
2024-12-29 19:09:57,340: Snapshot:2	Epoch:7	Loss:3.616	translation_Loss:2.496	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.12                                                   	MRR:22.06	Hits@10:37.87	Best:22.06
2024-12-29 19:10:03,698: Snapshot:2	Epoch:8	Loss:3.598	translation_Loss:2.492	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.106                                                   	MRR:21.92	Hits@10:37.73	Best:22.06
2024-12-29 19:10:10,429: Snapshot:2	Epoch:9	Loss:3.61	translation_Loss:2.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.113                                                   	MRR:21.98	Hits@10:37.86	Best:22.06
2024-12-29 19:10:16,840: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 22.06
2024-12-29 19:10:16,841: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:3.601 MRR:22.02 Best Results: 22.06
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 19:10:16,841: Snapshot:2	Epoch:10	Loss:3.601	translation_Loss:2.491	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.11                                                   	MRR:22.02	Hits@10:37.81	Best:22.06
2024-12-29 19:10:23,505: Snapshot:2	Epoch:11	Loss:29.843	translation_Loss:14.357	multi_layer_Loss:15.486	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.02	Hits@10:37.81	Best:22.06
2024-12-29 19:10:29,790: End of token training: 2 Epoch: 12 Loss:14.68 MRR:22.02 Best Results: 22.06
2024-12-29 19:10:29,791: Snapshot:2	Epoch:12	Loss:14.68	translation_Loss:14.354	multi_layer_Loss:0.326	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.02	Hits@10:37.81	Best:22.06
2024-12-29 19:10:30,044: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 19:10:38,512: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1632 | 0.2919 | 0.3481 |  0.4208 |
|     1      | 0.2381 | 0.1535 | 0.2765 | 0.3298 |  0.397  |
|     2      | 0.2204 | 0.137  | 0.2555 | 0.3071 |  0.3805 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 19:11:01,714: Snapshot:3	Epoch:0	Loss:3.498	translation_Loss:2.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.995                                                   	MRR:19.58	Hits@10:37.54	Best:19.58
2024-12-29 19:11:08,119: Snapshot:3	Epoch:1	Loss:2.674	translation_Loss:1.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.046                                                   	MRR:19.76	Hits@10:37.52	Best:19.76
2024-12-29 19:11:14,570: Snapshot:3	Epoch:2	Loss:2.589	translation_Loss:1.641	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.948                                                   	MRR:19.86	Hits@10:37.46	Best:19.86
2024-12-29 19:11:21,014: Snapshot:3	Epoch:3	Loss:2.533	translation_Loss:1.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.961                                                   	MRR:19.91	Hits@10:37.71	Best:19.91
2024-12-29 19:11:27,367: Snapshot:3	Epoch:4	Loss:2.536	translation_Loss:1.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.947                                                   	MRR:19.86	Hits@10:37.55	Best:19.91
2024-12-29 19:11:33,781: Snapshot:3	Epoch:5	Loss:2.546	translation_Loss:1.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.967                                                   	MRR:19.89	Hits@10:37.82	Best:19.91
2024-12-29 19:11:40,539: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 19.91
2024-12-29 19:11:40,540: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:2.536 MRR:19.86 Best Results: 19.91
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 19:11:40,540: Snapshot:3	Epoch:6	Loss:2.536	translation_Loss:1.569	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.966                                                   	MRR:19.86	Hits@10:37.64	Best:19.91
2024-12-29 19:11:46,847: Snapshot:3	Epoch:7	Loss:28.433	translation_Loss:13.75	multi_layer_Loss:14.684	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.86	Hits@10:37.64	Best:19.91
2024-12-29 19:11:53,190: End of token training: 3 Epoch: 8 Loss:14.072 MRR:19.86 Best Results: 19.91
2024-12-29 19:11:53,191: Snapshot:3	Epoch:8	Loss:14.072	translation_Loss:13.762	multi_layer_Loss:0.31	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.86	Hits@10:37.64	Best:19.91
2024-12-29 19:11:53,443: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 19:12:05,218: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2446 | 0.1563 | 0.2826 | 0.3387 |  0.4133 |
|     1      | 0.2331 | 0.1491 | 0.2697 | 0.3227 |  0.392  |
|     2      | 0.2179 | 0.1304 | 0.2536 | 0.3109 |  0.3877 |
|     3      | 0.2018 | 0.1123 | 0.2347 | 0.2972 |  0.3797 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 19:12:28,140: Snapshot:4	Epoch:0	Loss:2.067	translation_Loss:1.418	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.649                                                   	MRR:20.53	Hits@10:44.91	Best:20.53
2024-12-29 19:12:34,667: Snapshot:4	Epoch:1	Loss:1.21	translation_Loss:0.637	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.573                                                   	MRR:20.76	Hits@10:43.82	Best:20.76
2024-12-29 19:12:41,093: Snapshot:4	Epoch:2	Loss:1.075	translation_Loss:0.549	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.526                                                   	MRR:20.82	Hits@10:44.23	Best:20.82
2024-12-29 19:12:47,552: Snapshot:4	Epoch:3	Loss:1.023	translation_Loss:0.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.519                                                   	MRR:21.0	Hits@10:44.31	Best:21.0
2024-12-29 19:12:53,946: Snapshot:4	Epoch:4	Loss:1.027	translation_Loss:0.506	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.521                                                   	MRR:20.92	Hits@10:44.12	Best:21.0
2024-12-29 19:13:00,698: Snapshot:4	Epoch:5	Loss:1.016	translation_Loss:0.495	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.522                                                   	MRR:20.81	Hits@10:44.16	Best:21.0
2024-12-29 19:13:07,126: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 21.0
2024-12-29 19:13:07,126: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:1.023 MRR:20.82 Best Results: 21.0
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 19:13:07,127: Snapshot:4	Epoch:6	Loss:1.023	translation_Loss:0.502	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.52                                                   	MRR:20.82	Hits@10:44.32	Best:21.0
2024-12-29 19:13:13,525: Snapshot:4	Epoch:7	Loss:26.246	translation_Loss:11.476	multi_layer_Loss:14.77	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.82	Hits@10:44.32	Best:21.0
2024-12-29 19:13:20,461: End of token training: 4 Epoch: 8 Loss:11.798 MRR:20.82 Best Results: 21.0
2024-12-29 19:13:20,462: Snapshot:4	Epoch:8	Loss:11.798	translation_Loss:11.481	multi_layer_Loss:0.317	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.82	Hits@10:44.32	Best:21.0
2024-12-29 19:13:20,728: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 19:13:34,981: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.237  | 0.1516 | 0.2716 | 0.3256 |  0.4007 |
|     1      | 0.2236 | 0.1417 | 0.2553 | 0.3088 |  0.3809 |
|     2      | 0.2102 | 0.1261 | 0.2412 | 0.2985 |  0.3769 |
|     3      | 0.1966 | 0.1069 | 0.2242 |  0.29  |  0.3799 |
|     4      | 0.2106 | 0.1011 | 0.2418 | 0.3261 |  0.4403 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 19:13:34,983: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1552 | 0.2821 | 0.3336 |  0.394  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2537 | 0.1647 | 0.2939 | 0.351  |  0.4213 |
|     1      | 0.2347 | 0.1515 | 0.2744 | 0.325  |  0.3873 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1632 | 0.2919 | 0.3481 |  0.4208 |
|     1      | 0.2381 | 0.1535 | 0.2765 | 0.3298 |  0.397  |
|     2      | 0.2204 | 0.137  | 0.2555 | 0.3071 |  0.3805 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2446 | 0.1563 | 0.2826 | 0.3387 |  0.4133 |
|     1      | 0.2331 | 0.1491 | 0.2697 | 0.3227 |  0.392  |
|     2      | 0.2179 | 0.1304 | 0.2536 | 0.3109 |  0.3877 |
|     3      | 0.2018 | 0.1123 | 0.2347 | 0.2972 |  0.3797 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.237  | 0.1516 | 0.2716 | 0.3256 |  0.4007 |
|     1      | 0.2236 | 0.1417 | 0.2553 | 0.3088 |  0.3809 |
|     2      | 0.2102 | 0.1261 | 0.2412 | 0.2985 |  0.3769 |
|     3      | 0.1966 | 0.1069 | 0.2242 |  0.29  |  0.3799 |
|     4      | 0.2106 | 0.1011 | 0.2418 | 0.3261 |  0.4403 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 19:13:34,984: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 139.83392786979675 |   0.241   |    0.155     |    0.282     |     0.394     |
|    1     | 76.15245246887207  |   0.244   |    0.158     |    0.284     |     0.404     |
|    2     | 97.40648603439331  |   0.237   |    0.151     |    0.275     |     0.399     |
|    3     | 71.69060921669006  |   0.224   |    0.137     |     0.26     |     0.393     |
|    4     | 72.14582848548889  |   0.216   |    0.125     |    0.247     |     0.396     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 19:13:34,984: Sum_Training_Time:457.2293040752411
2024-12-29 19:13:34,985: Every_Training_Time:[139.83392786979675, 76.15245246887207, 97.40648603439331, 71.69060921669006, 72.14582848548889]
2024-12-29 19:13:34,985: Forward transfer: 0.17485 Backward transfer: -0.007550000000000008
______________________________________________________________________________________________________
[lijing@p0316 IncDE]$ python main.py -dataset FACT -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 3 -multi_layer_weight 1 -token_distillation_weight 400 9000 10000 10000
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2024-12-29 20:51:04,429: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229205032/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[400.0, 9000.0, 10000.0, 10000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 20:51:13,560: Snapshot:0 Epoch:0 Loss:17.024 translation_Loss:17.024 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:6.23  Hits@10:13.26 Best:6.23
2024-12-29 20:51:19,874: Snapshot:0 Epoch:1 Loss:11.963 translation_Loss:11.963 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:9.68  Hits@10:22.61 Best:9.68
2024-12-29 20:51:25,677: Snapshot:0 Epoch:2 Loss:8.603  translation_Loss:8.603  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:12.9  Hits@10:28.88 Best:12.9
2024-12-29 20:51:31,988: Snapshot:0 Epoch:3 Loss:6.158  translation_Loss:6.158  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:15.78 Hits@10:33.12 Best:15.78
2024-12-29 20:51:37,895: Snapshot:0 Epoch:4 Loss:4.364  translation_Loss:4.364  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:18.34 Hits@10:35.98 Best:18.34
2024-12-29 20:51:43,907: Snapshot:0 Epoch:5 Loss:3.085  translation_Loss:3.085  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:20.44 Hits@10:37.74 Best:20.44
2024-12-29 20:51:50,108: Snapshot:0 Epoch:6 Loss:2.175  translation_Loss:2.175  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:21.96 Hits@10:38.78 Best:21.96
2024-12-29 20:51:55,911: Snapshot:0 Epoch:7 Loss:1.541  translation_Loss:1.541  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:22.95 Hits@10:39.48 Best:22.95
2024-12-29 20:52:02,104: Snapshot:0 Epoch:8 Loss:1.096  translation_Loss:1.096  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:23.74 Hits@10:39.84 Best:23.74
2024-12-29 20:52:07,965: Snapshot:0 Epoch:9 Loss:0.815  translation_Loss:0.815  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.13 Hits@10:40.02 Best:24.13
2024-12-29 20:52:13,803: Snapshot:0 Epoch:10  Loss:0.628  translation_Loss:0.628  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.34 Hits@10:40.15 Best:24.34
2024-12-29 20:52:20,050: Snapshot:0 Epoch:11  Loss:0.496  translation_Loss:0.496  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.36 Hits@10:40.2  Best:24.36
2024-12-29 20:52:25,946: Snapshot:0 Epoch:12  Loss:0.41 translation_Loss:0.41 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.49 Hits@10:40.31 Best:24.49
2024-12-29 20:52:32,138: Snapshot:0 Epoch:13  Loss:0.35 translation_Loss:0.35 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.7  Hits@10:40.23 Best:24.7
2024-12-29 20:52:37,965: Snapshot:0 Epoch:14  Loss:0.312  translation_Loss:0.312  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.65 Hits@10:40.25 Best:24.7
2024-12-29 20:52:43,723: Snapshot:0 Epoch:15  Loss:0.275  translation_Loss:0.275  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.59 Hits@10:40.45 Best:24.7
2024-12-29 20:52:49,895: Snapshot:0 Epoch:16  Loss:0.245  translation_Loss:0.245  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.71 Hits@10:40.39 Best:24.71
2024-12-29 20:52:55,731: Snapshot:0 Epoch:17  Loss:0.223  translation_Loss:0.223  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.81 Hits@10:40.43 Best:24.81
2024-12-29 20:53:01,924: Snapshot:0 Epoch:18  Loss:0.207  translation_Loss:0.207  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.69 Hits@10:40.38 Best:24.81
2024-12-29 20:53:07,759: Snapshot:0 Epoch:19  Loss:0.192  translation_Loss:0.192  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.78 Hits@10:40.29 Best:24.81
2024-12-29 20:53:14,013: Early Stopping! Snapshot: 0 Epoch: 20 Best Results: 24.81
2024-12-29 20:53:14,014: Start to training tokens! Snapshot: 0 Epoch: 20 Loss:0.18 MRR:24.79 Best Results: 24.81
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 20:53:14,014: Snapshot:0 Epoch:20  Loss:0.18 translation_Loss:0.18 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.79 Hits@10:40.36 Best:24.81
2024-12-29 20:53:20,291: Snapshot:0 Epoch:21  Loss:27.63  translation_Loss:12.3 multi_layer_Loss:15.33  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.79 Hits@10:40.36 Best:24.81
2024-12-29 20:53:26,063: End of token training: 0 Epoch: 22 Loss:12.599 MRR:24.79 Best Results: 24.81
2024-12-29 20:53:26,064: Snapshot:0 Epoch:22  Loss:12.599 translation_Loss:12.29  multi_layer_Loss:0.309  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:24.79 Hits@10:40.36 Best:24.81
2024-12-29 20:53:26,373: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 20:53:28,921: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2409 | 0.1555 | 0.2837 | 0.331  |  0.3931 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 20:53:51,898: Snapshot:1 Epoch:0 Loss:7.319  translation_Loss:7.068  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.252                                                    MRR:20.85 Hits@10:34.75 Best:20.85
2024-12-29 20:53:58,255: Snapshot:1 Epoch:1 Loss:3.562  translation_Loss:2.993  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.569                                                    MRR:22.58 Hits@10:37.32 Best:22.58
2024-12-29 20:54:04,551: Snapshot:1 Epoch:2 Loss:2.189  translation_Loss:1.426  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.763                                                    MRR:23.24 Hits@10:38.42 Best:23.24
2024-12-29 20:54:10,938: Snapshot:1 Epoch:3 Loss:1.723  translation_Loss:0.883  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.841                                                    MRR:23.35 Hits@10:38.59 Best:23.35
2024-12-29 20:54:17,287: Snapshot:1 Epoch:4 Loss:1.564  translation_Loss:0.698  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.866                                                    MRR:23.43 Hits@10:38.62 Best:23.43
2024-12-29 20:54:23,556: Snapshot:1 Epoch:5 Loss:1.496  translation_Loss:0.62 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.876                                                    MRR:23.35 Hits@10:38.62 Best:23.43
2024-12-29 20:54:30,167: Snapshot:1 Epoch:6 Loss:1.466  translation_Loss:0.582  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.884                                                    MRR:23.37 Hits@10:38.52 Best:23.43
2024-12-29 20:54:36,437: Early Stopping! Snapshot: 1 Epoch: 7 Best Results: 23.43
2024-12-29 20:54:36,437: Start to training tokens! Snapshot: 1 Epoch: 7 Loss:1.449 MRR:23.4 Best Results: 23.43
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 20:54:36,438: Snapshot:1 Epoch:7 Loss:1.449  translation_Loss:0.553  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.896                                                    MRR:23.4  Hits@10:38.65 Best:23.43
2024-12-29 20:54:42,708: Snapshot:1 Epoch:8 Loss:27.917 translation_Loss:13.118 multi_layer_Loss:14.799 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:23.4  Hits@10:38.65 Best:23.43
2024-12-29 20:54:49,374: End of token training: 1 Epoch: 9 Loss:13.405 MRR:23.4 Best Results: 23.43
2024-12-29 20:54:49,374: Snapshot:1 Epoch:9 Loss:13.405 translation_Loss:13.097 multi_layer_Loss:0.308  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:23.4  Hits@10:38.65 Best:23.43
2024-12-29 20:54:49,597: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 20:54:55,721: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2448 | 0.1566 | 0.2838 | 0.3393 |  0.4131 |
|     1      | 0.2358 | 0.1535 | 0.2747 | 0.3244 |  0.3855 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 20:55:18,863: Snapshot:2 Epoch:0 Loss:4.896  translation_Loss:3.819  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.077                                                    MRR:21.79 Hits@10:37.67 Best:21.79
2024-12-29 20:55:25,555: Snapshot:2 Epoch:1 Loss:3.668  translation_Loss:2.463  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.205                                                    MRR:22.06 Hits@10:37.85 Best:22.06
2024-12-29 20:55:32,013: Snapshot:2 Epoch:2 Loss:3.415  translation_Loss:2.347  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.069                                                    MRR:22.12 Hits@10:38.24 Best:22.12
2024-12-29 20:55:38,487: Snapshot:2 Epoch:3 Loss:3.305  translation_Loss:2.22 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.085                                                    MRR:22.22 Hits@10:38.11 Best:22.22
2024-12-29 20:55:44,937: Snapshot:2 Epoch:4 Loss:3.291  translation_Loss:2.219  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.072                                                    MRR:22.24 Hits@10:38.27 Best:22.24
2024-12-29 20:55:51,451: Snapshot:2 Epoch:5 Loss:3.277  translation_Loss:2.194  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.083                                                    MRR:22.21 Hits@10:38.1  Best:22.24
2024-12-29 20:55:58,252: Snapshot:2 Epoch:6 Loss:3.27 translation_Loss:2.189  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.08                                                     MRR:22.24 Hits@10:38.17 Best:22.24
2024-12-29 20:56:04,660: Snapshot:2 Epoch:7 Loss:3.283  translation_Loss:2.194  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.088                                                    MRR:22.26 Hits@10:38.04 Best:22.26
2024-12-29 20:56:11,048: Snapshot:2 Epoch:8 Loss:3.27 translation_Loss:2.184  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.086                                                    MRR:22.23 Hits@10:38.21 Best:22.26
2024-12-29 20:56:17,840: Snapshot:2 Epoch:9 Loss:3.278  translation_Loss:2.193  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.085                                                    MRR:22.23 Hits@10:38.14 Best:22.26
2024-12-29 20:56:24,197: Early Stopping! Snapshot: 2 Epoch: 10 Best Results: 22.26
2024-12-29 20:56:24,197: Start to training tokens! Snapshot: 2 Epoch: 10 Loss:3.267 MRR:22.26 Best Results: 22.26
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 20:56:24,198: Snapshot:2 Epoch:10  Loss:3.267  translation_Loss:2.182  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.085                                                    MRR:22.26 Hits@10:38.05 Best:22.26
2024-12-29 20:56:30,948: Snapshot:2 Epoch:11  Loss:29.77  translation_Loss:14.284 multi_layer_Loss:15.486 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:22.26 Hits@10:38.05 Best:22.26
2024-12-29 20:56:37,321: End of token training: 2 Epoch: 12 Loss:14.608 MRR:22.26 Best Results: 22.26
2024-12-29 20:56:37,321: Snapshot:2 Epoch:12  Loss:14.608 translation_Loss:14.282 multi_layer_Loss:0.326  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:22.26 Hits@10:38.05 Best:22.26
2024-12-29 20:56:37,579: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 20:56:45,708: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2443 | 0.1563 | 0.281  | 0.3391 |  0.4141 |
|     1      | 0.2393 | 0.1543 | 0.2767 | 0.3315 |  0.3985 |
|     2      | 0.2227 | 0.1381 | 0.259  | 0.3116 |  0.3827 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 20:57:09,012: Snapshot:3 Epoch:0 Loss:3.222  translation_Loss:2.277  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.945                                                    MRR:19.71 Hits@10:37.41 Best:19.71
2024-12-29 20:57:15,673: Snapshot:3 Epoch:1 Loss:2.454  translation_Loss:1.479  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.975                                                    MRR:19.82 Hits@10:37.42 Best:19.82
2024-12-29 20:57:22,159: Snapshot:3 Epoch:2 Loss:2.361  translation_Loss:1.482  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.88                                                     MRR:19.85 Hits@10:37.45 Best:19.85
2024-12-29 20:57:28,665: Snapshot:3 Epoch:3 Loss:2.318  translation_Loss:1.42 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.898                                                    MRR:19.99 Hits@10:37.78 Best:19.99
2024-12-29 20:57:35,133: Snapshot:3 Epoch:4 Loss:2.318  translation_Loss:1.436  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.882                                                    MRR:19.94 Hits@10:37.65 Best:19.99
2024-12-29 20:57:41,819: Snapshot:3 Epoch:5 Loss:2.33 translation_Loss:1.425  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.905                                                    MRR:20.01 Hits@10:37.96 Best:20.01
2024-12-29 20:57:48,783: Snapshot:3 Epoch:6 Loss:2.316  translation_Loss:1.416  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.9                                                    MRR:19.83 Hits@10:37.73 Best:20.01
2024-12-29 20:57:55,222: Snapshot:3 Epoch:7 Loss:2.326  translation_Loss:1.424  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.902                                                    MRR:19.99 Hits@10:37.47 Best:20.01
2024-12-29 20:58:01,628: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 20.01
2024-12-29 20:58:01,628: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:2.328 MRR:19.95 Best Results: 20.01
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 20:58:01,628: Snapshot:3 Epoch:8 Loss:2.328  translation_Loss:1.426  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.902                                                    MRR:19.95 Hits@10:37.87 Best:20.01
2024-12-29 20:58:08,389: Snapshot:3 Epoch:9 Loss:28.384 translation_Loss:13.7 multi_layer_Loss:14.684 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:19.95 Hits@10:37.87 Best:20.01
2024-12-29 20:58:14,781: End of token training: 3 Epoch: 10 Loss:14.039 MRR:19.95 Best Results: 20.01
2024-12-29 20:58:14,781: Snapshot:3 Epoch:10  Loss:14.039 translation_Loss:13.728 multi_layer_Loss:0.31 MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:19.95 Hits@10:37.87 Best:20.01
2024-12-29 20:58:15,039: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 20:58:26,235: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2381 | 0.1504 | 0.2743 | 0.3306 |  0.4062 |
|     1      | 0.2347 | 0.1506 | 0.2717 | 0.3247 |  0.3956 |
|     2      | 0.2214 | 0.1348 | 0.2571 | 0.3143 |  0.3895 |
|     3      | 0.2034 | 0.1147 | 0.2339 | 0.2944 |  0.3784 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 20:58:49,479: Snapshot:4 Epoch:0 Loss:1.93 translation_Loss:1.323  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.607                                                    MRR:20.39 Hits@10:44.52 Best:20.39
2024-12-29 20:58:55,953: Snapshot:4 Epoch:1 Loss:1.115  translation_Loss:0.582  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.534                                                    MRR:20.66 Hits@10:43.44 Best:20.66
2024-12-29 20:59:02,438: Snapshot:4 Epoch:2 Loss:0.982  translation_Loss:0.504  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.477                                                    MRR:20.94 Hits@10:43.58 Best:20.94
2024-12-29 20:59:08,962: Snapshot:4 Epoch:3 Loss:0.938  translation_Loss:0.46 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.478                                                    MRR:20.83 Hits@10:43.64 Best:20.94
2024-12-29 20:59:15,487: Snapshot:4 Epoch:4 Loss:0.933  translation_Loss:0.461  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.472                                                    MRR:21.02 Hits@10:44.03 Best:21.02
2024-12-29 20:59:21,956: Snapshot:4 Epoch:5 Loss:0.936  translation_Loss:0.456  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.48                                                     MRR:20.71 Hits@10:43.56 Best:21.02
2024-12-29 20:59:28,860: Snapshot:4 Epoch:6 Loss:0.939  translation_Loss:0.459  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.481                                                    MRR:20.83 Hits@10:43.92 Best:21.02
2024-12-29 20:59:35,283: Early Stopping! Snapshot: 4 Epoch: 7 Best Results: 21.02
2024-12-29 20:59:35,284: Start to training tokens! Snapshot: 4 Epoch: 7 Loss:0.945 MRR:21.02 Best Results: 21.02
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 20:59:35,284: Snapshot:4 Epoch:7 Loss:0.945  translation_Loss:0.457  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.488                                                    MRR:21.02 Hits@10:43.8  Best:21.02
2024-12-29 20:59:42,155: Snapshot:4 Epoch:8 Loss:26.293 translation_Loss:11.523 multi_layer_Loss:14.77  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:21.02 Hits@10:43.8  Best:21.02
2024-12-29 20:59:48,587: End of token training: 4 Epoch: 9 Loss:11.831 MRR:21.02 Best Results: 21.02
2024-12-29 20:59:48,587: Snapshot:4 Epoch:9 Loss:11.831 translation_Loss:11.514 multi_layer_Loss:0.317  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:21.02 Hits@10:43.8  Best:21.02
2024-12-29 20:59:48,843: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 21:00:03,377: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2309 | 0.1453 | 0.2646 | 0.3212 |  0.3975 |
|     1      | 0.2256 | 0.143  | 0.2597 | 0.3125 |  0.3844 |
|     2      | 0.2136 | 0.1281 | 0.2473 | 0.3038 |  0.3804 |
|     3      | 0.1991 | 0.1097 | 0.2277 | 0.2907 |  0.3819 |
|     4      | 0.2127 | 0.1035 | 0.2458 | 0.3277 |  0.4389 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 21:00:03,380: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2409 | 0.1555 | 0.2837 | 0.331  |  0.3931 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2448 | 0.1566 | 0.2838 | 0.3393 |  0.4131 |
|     1      | 0.2358 | 0.1535 | 0.2747 | 0.3244 |  0.3855 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2443 | 0.1563 | 0.281  | 0.3391 |  0.4141 |
|     1      | 0.2393 | 0.1543 | 0.2767 | 0.3315 |  0.3985 |
|     2      | 0.2227 | 0.1381 | 0.259  | 0.3116 |  0.3827 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2381 | 0.1504 | 0.2743 | 0.3306 |  0.4062 |
|     1      | 0.2347 | 0.1506 | 0.2717 | 0.3247 |  0.3956 |
|     2      | 0.2214 | 0.1348 | 0.2571 | 0.3143 |  0.3895 |
|     3      | 0.2034 | 0.1147 | 0.2339 | 0.2944 |  0.3784 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2309 | 0.1453 | 0.2646 | 0.3212 |  0.3975 |
|     1      | 0.2256 | 0.143  | 0.2597 | 0.3125 |  0.3844 |
|     2      | 0.2136 | 0.1281 | 0.2473 | 0.3038 |  0.3804 |
|     3      | 0.1991 | 0.1097 | 0.2277 | 0.2907 |  0.3819 |
|     4      | 0.2127 | 0.1035 | 0.2458 | 0.3277 |  0.4389 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 21:00:03,381: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 141.63414525985718 |   0.241   |    0.155     |    0.284     |     0.393     |
|    1     | 77.37405490875244  |    0.24   |    0.155     |    0.279     |     0.399     |
|    2     |  98.603750705719   |   0.235   |     0.15     |    0.272     |     0.398     |
|    3     |  86.0707676410675  |   0.224   |    0.138     |    0.259     |     0.392     |
|    4     | 79.03711533546448  |   0.216   |    0.126     |    0.249     |     0.397     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 21:00:03,382: Sum_Training_Time:482.7198338508606
2024-12-29 21:00:03,382: Every_Training_Time:[141.63414525985718, 77.37405490875244, 98.603750705719, 86.0707676410675, 79.03711533546448]
2024-12-29 21:00:03,382: Forward transfer: 0.1762 Backward transfer: -0.008400000000000005
[lijing@p0316 IncDE]$ python main.py -dataset FACT -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 3 -multi_layer_weight 1 -token_distillation_weight 500 15000 10000 10000
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2024-12-29 21:01:25,987: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20241229210054/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[500.0, 15000.0, 10000.0, 10000.0], token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-29 21:01:35,144: Snapshot:0 Epoch:0 Loss:17.024 translation_Loss:17.024 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:6.23  Hits@10:13.26 Best:6.23
2024-12-29 21:01:41,420: Snapshot:0 Epoch:1 Loss:11.963 translation_Loss:11.963 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:9.68  Hits@10:22.6  Best:9.68
2024-12-29 21:01:47,309: Snapshot:0 Epoch:2 Loss:8.603  translation_Loss:8.603  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:12.89 Hits@10:28.89 Best:12.89
2024-12-29 21:01:53,544: Snapshot:0 Epoch:3 Loss:6.158  translation_Loss:6.158  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:15.76 Hits@10:33.14 Best:15.76
2024-12-29 21:01:59,372: Snapshot:0 Epoch:4 Loss:4.364  translation_Loss:4.364  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:18.37 Hits@10:36.0  Best:18.37
2024-12-29 21:02:05,211: Snapshot:0 Epoch:5 Loss:3.085  translation_Loss:3.085  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:20.47 Hits@10:37.75 Best:20.47
2024-12-29 21:02:11,480: Snapshot:0 Epoch:6 Loss:2.174  translation_Loss:2.174  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:21.96 Hits@10:38.79 Best:21.96
2024-12-29 21:02:17,353: Snapshot:0 Epoch:7 Loss:1.541  translation_Loss:1.541  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:22.97 Hits@10:39.42 Best:22.97
2024-12-29 21:02:23,567: Snapshot:0 Epoch:8 Loss:1.096  translation_Loss:1.096  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:23.69 Hits@10:39.9  Best:23.69
2024-12-29 21:02:29,434: Snapshot:0 Epoch:9 Loss:0.813  translation_Loss:0.813  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.14 Hits@10:39.9  Best:24.14
2024-12-29 21:02:35,330: Snapshot:0 Epoch:10  Loss:0.628  translation_Loss:0.628  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.28 Hits@10:40.22 Best:24.28
2024-12-29 21:02:41,689: Snapshot:0 Epoch:11  Loss:0.496  translation_Loss:0.496  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.43 Hits@10:40.27 Best:24.43
2024-12-29 21:02:47,549: Snapshot:0 Epoch:12  Loss:0.41 translation_Loss:0.41 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.48 Hits@10:40.4  Best:24.48
2024-12-29 21:02:53,770: Snapshot:0 Epoch:13  Loss:0.35 translation_Loss:0.35 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.67 Hits@10:40.44 Best:24.67
2024-12-29 21:02:59,595: Snapshot:0 Epoch:14  Loss:0.31 translation_Loss:0.31 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.66 Hits@10:40.43 Best:24.67
2024-12-29 21:03:05,404: Snapshot:0 Epoch:15  Loss:0.272  translation_Loss:0.272  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.67 Hits@10:40.3  Best:24.67
2024-12-29 21:03:11,752: Snapshot:0 Epoch:16  Loss:0.245  translation_Loss:0.245  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.68 Hits@10:40.36 Best:24.68
2024-12-29 21:03:17,681: Snapshot:0 Epoch:17  Loss:0.224  translation_Loss:0.224  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.77 Hits@10:40.46 Best:24.77
2024-12-29 21:03:23,879: Snapshot:0 Epoch:18  Loss:0.207  translation_Loss:0.207  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.74 Hits@10:40.13 Best:24.77
2024-12-29 21:03:29,699: Snapshot:0 Epoch:19  Loss:0.19 translation_Loss:0.19 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.67 Hits@10:40.09 Best:24.77
2024-12-29 21:03:36,160: Early Stopping! Snapshot: 0 Epoch: 20 Best Results: 24.77
2024-12-29 21:03:36,160: Start to training tokens! Snapshot: 0 Epoch: 20 Loss:0.179 MRR:24.69 Best Results: 24.77
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 21:03:36,161: Snapshot:0 Epoch:20  Loss:0.179  translation_Loss:0.179  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.69 Hits@10:40.18 Best:24.77
2024-12-29 21:03:42,735: Snapshot:0 Epoch:21  Loss:27.635 translation_Loss:12.305 multi_layer_Loss:15.33  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.69 Hits@10:40.18 Best:24.77
2024-12-29 21:03:48,651: End of token training: 0 Epoch: 22 Loss:12.601 MRR:24.69 Best Results: 24.77
2024-12-29 21:03:48,651: Snapshot:0 Epoch:22  Loss:12.601 translation_Loss:12.293 multi_layer_Loss:0.309  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:24.69 Hits@10:40.18 Best:24.77
2024-12-29 21:03:48,929: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2024-12-29 21:03:51,387: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2401 | 0.1551 | 0.2825 | 0.3306 |  0.3936 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 21:04:14,547: Snapshot:1 Epoch:0 Loss:7.364  translation_Loss:7.074  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.291                                                    MRR:20.8  Hits@10:34.65 Best:20.8
2024-12-29 21:04:20,822: Snapshot:1 Epoch:1 Loss:3.683  translation_Loss:3.028  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.655                                                    MRR:22.56 Hits@10:37.26 Best:22.56
2024-12-29 21:04:27,204: Snapshot:1 Epoch:2 Loss:2.36 translation_Loss:1.486  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.874                                                    MRR:23.2  Hits@10:38.43 Best:23.2
2024-12-29 21:04:33,574: Snapshot:1 Epoch:3 Loss:1.909  translation_Loss:0.955  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.955                                                    MRR:23.36 Hits@10:38.54 Best:23.36
2024-12-29 21:04:39,922: Snapshot:1 Epoch:4 Loss:1.759  translation_Loss:0.777  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.982                                                    MRR:23.32 Hits@10:38.59 Best:23.36
2024-12-29 21:04:46,332: Snapshot:1 Epoch:5 Loss:1.691  translation_Loss:0.696  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.996                                                    MRR:23.41 Hits@10:38.59 Best:23.41
2024-12-29 21:04:52,989: Snapshot:1 Epoch:6 Loss:1.666  translation_Loss:0.658  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.008                                                    MRR:23.35 Hits@10:38.47 Best:23.41
2024-12-29 21:04:59,232: Snapshot:1 Epoch:7 Loss:1.645  translation_Loss:0.626  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.02                                                     MRR:23.36 Hits@10:38.64 Best:23.41
2024-12-29 21:05:05,648: Snapshot:1 Epoch:8 Loss:1.647  translation_Loss:0.619  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.028                                                    MRR:23.49 Hits@10:38.82 Best:23.49
2024-12-29 21:05:12,495: Snapshot:1 Epoch:9 Loss:1.637  translation_Loss:0.598  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.039                                                    MRR:23.4  Hits@10:38.54 Best:23.49
2024-12-29 21:05:18,770: Snapshot:1 Epoch:10  Loss:1.631  translation_Loss:0.585  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.046                                                    MRR:23.23 Hits@10:38.31 Best:23.49
2024-12-29 21:05:24,991: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 23.49
2024-12-29 21:05:24,992: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:1.643 MRR:23.32 Best Results: 23.49
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 21:05:24,992: Snapshot:1 Epoch:11  Loss:1.643  translation_Loss:0.593  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.051                                                    MRR:23.32 Hits@10:38.46 Best:23.49
2024-12-29 21:05:31,613: Snapshot:1 Epoch:12  Loss:27.866 translation_Loss:13.067 multi_layer_Loss:14.799 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:23.32 Hits@10:38.46 Best:23.49
2024-12-29 21:05:37,827: End of token training: 1 Epoch: 13 Loss:13.371 MRR:23.32 Best Results: 23.49
2024-12-29 21:05:37,828: Snapshot:1 Epoch:13  Loss:13.371 translation_Loss:13.062 multi_layer_Loss:0.308  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:23.32 Hits@10:38.46 Best:23.49
2024-12-29 21:05:38,079: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2024-12-29 21:05:43,830: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2452 | 0.1555 | 0.2853 | 0.3407 |  0.4151 |
|     1      | 0.2346 | 0.1521 | 0.2732 | 0.3219 |  0.3858 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 21:06:06,823: Snapshot:2 Epoch:0 Loss:4.973  translation_Loss:3.795  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.178                                                    MRR:21.6  Hits@10:37.04 Best:21.6
2024-12-29 21:06:13,326: Snapshot:2 Epoch:1 Loss:4.103  translation_Loss:3.04 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.063                                                    MRR:22.05 Hits@10:37.53 Best:22.05
2024-12-29 21:06:19,896: Snapshot:2 Epoch:2 Loss:3.676  translation_Loss:2.743  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.933                                                    MRR:22.16 Hits@10:37.71 Best:22.16
2024-12-29 21:06:26,423: Snapshot:2 Epoch:3 Loss:3.6  translation_Loss:2.658  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.943                                                    MRR:22.06 Hits@10:37.66 Best:22.16
2024-12-29 21:06:32,865: Snapshot:2 Epoch:4 Loss:3.577  translation_Loss:2.652  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.924                                                    MRR:22.1  Hits@10:37.65 Best:22.16
2024-12-29 21:06:39,757: Early Stopping! Snapshot: 2 Epoch: 5 Best Results: 22.16
2024-12-29 21:06:39,758: Start to training tokens! Snapshot: 2 Epoch: 5 Loss:3.569 MRR:22.12 Best Results: 22.16
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 21:06:39,758: Snapshot:2 Epoch:5 Loss:3.569  translation_Loss:2.634  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.935                                                    MRR:22.12 Hits@10:37.67 Best:22.16
2024-12-29 21:06:46,105: Snapshot:2 Epoch:6 Loss:29.864 translation_Loss:14.378 multi_layer_Loss:15.486 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:22.12 Hits@10:37.67 Best:22.16
2024-12-29 21:06:52,531: End of token training: 2 Epoch: 7 Loss:14.701 MRR:22.12 Best Results: 22.16
2024-12-29 21:06:52,532: Snapshot:2 Epoch:7 Loss:14.701 translation_Loss:14.375 multi_layer_Loss:0.326  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:22.12 Hits@10:37.67 Best:22.16
2024-12-29 21:06:52,790: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2024-12-29 21:07:01,224: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2461 | 0.1564 | 0.2855 | 0.3435 |  0.4162 |
|     1      | 0.2396 | 0.1565 | 0.2761 | 0.3301 |  0.3947 |
|     2      | 0.2203 | 0.1387 | 0.254  | 0.3058 |  0.3757 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 21:07:24,587: Snapshot:3 Epoch:0 Loss:3.444  translation_Loss:2.449  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.994                                                    MRR:19.71 Hits@10:37.27 Best:19.71
2024-12-29 21:07:31,199: Snapshot:3 Epoch:1 Loss:2.608  translation_Loss:1.564  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.044                                                    MRR:19.8  Hits@10:37.2  Best:19.8
2024-12-29 21:07:37,822: Snapshot:3 Epoch:2 Loss:2.525  translation_Loss:1.584  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.942                                                    MRR:19.91 Hits@10:37.44 Best:19.91
2024-12-29 21:07:44,317: Snapshot:3 Epoch:3 Loss:2.486  translation_Loss:1.527  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.959                                                    MRR:19.85 Hits@10:37.46 Best:19.91
2024-12-29 21:07:50,764: Snapshot:3 Epoch:4 Loss:2.48 translation_Loss:1.523  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.957                                                    MRR:19.91 Hits@10:37.38 Best:19.91
2024-12-29 21:07:57,717: Snapshot:3 Epoch:5 Loss:2.486  translation_Loss:1.52 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.966                                                    MRR:19.97 Hits@10:37.55 Best:19.97
2024-12-29 21:08:04,146: Snapshot:3 Epoch:6 Loss:2.491  translation_Loss:1.524  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.967                                                    MRR:19.91 Hits@10:37.54 Best:19.97
2024-12-29 21:08:11,165: Snapshot:3 Epoch:7 Loss:2.492  translation_Loss:1.525  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.967                                                    MRR:19.91 Hits@10:37.42 Best:19.97
2024-12-29 21:08:17,643: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 19.97
2024-12-29 21:08:17,643: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:2.483 MRR:19.85 Best Results: 19.97
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 21:08:17,643: Snapshot:3 Epoch:8 Loss:2.483  translation_Loss:1.519  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.964                                                    MRR:19.85 Hits@10:37.35 Best:19.97
2024-12-29 21:08:24,133: Snapshot:3 Epoch:9 Loss:28.375 translation_Loss:13.691 multi_layer_Loss:14.684 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:19.85 Hits@10:37.35 Best:19.97
2024-12-29 21:08:30,934: End of token training: 3 Epoch: 10 Loss:14.001 MRR:19.85 Best Results: 19.97
2024-12-29 21:08:30,934: Snapshot:3 Epoch:10  Loss:14.001 translation_Loss:13.69  multi_layer_Loss:0.31 MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:19.85 Hits@10:37.35 Best:19.97
2024-12-29 21:08:31,219: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2024-12-29 21:08:42,731: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      |  0.24  | 0.1516 | 0.2771 | 0.3351 |  0.4083 |
|     1      | 0.2345 | 0.1512 | 0.2704 | 0.3244 |  0.3934 |
|     2      | 0.2188 | 0.1346 | 0.252  | 0.3077 |  0.3841 |
|     3      | 0.2021 | 0.1135 | 0.2343 | 0.2962 |  0.3777 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-29 21:09:05,646: Snapshot:4 Epoch:0 Loss:2.017  translation_Loss:1.378  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.639                                                    MRR:20.76 Hits@10:44.99 Best:20.76
2024-12-29 21:09:12,206: Snapshot:4 Epoch:1 Loss:1.172  translation_Loss:0.612  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.56                                                     MRR:20.5  Hits@10:43.36 Best:20.76
2024-12-29 21:09:18,799: Snapshot:4 Epoch:2 Loss:1.033  translation_Loss:0.525  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.508                                                    MRR:20.92 Hits@10:44.16 Best:20.92
2024-12-29 21:09:25,721: Snapshot:4 Epoch:3 Loss:0.997  translation_Loss:0.493  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.504                                                    MRR:20.95 Hits@10:43.97 Best:20.95
2024-12-29 21:09:32,321: Snapshot:4 Epoch:4 Loss:0.994  translation_Loss:0.485  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.509                                                    MRR:20.97 Hits@10:43.89 Best:20.97
2024-12-29 21:09:39,354: Snapshot:4 Epoch:5 Loss:0.99 translation_Loss:0.478  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.511                                                    MRR:20.92 Hits@10:43.74 Best:20.97
2024-12-29 21:09:45,866: Snapshot:4 Epoch:6 Loss:0.992  translation_Loss:0.479  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.513                                                    MRR:21.06 Hits@10:43.85 Best:21.06
2024-12-29 21:09:52,420: Snapshot:4 Epoch:7 Loss:0.991  translation_Loss:0.482  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.508                                                    MRR:20.91 Hits@10:44.04 Best:21.06
2024-12-29 21:09:59,255: Snapshot:4 Epoch:8 Loss:0.994  translation_Loss:0.481  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.513                                                    MRR:20.85 Hits@10:44.11 Best:21.06
2024-12-29 21:10:05,827: Snapshot:4 Epoch:9 Loss:0.994  translation_Loss:0.477  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.516                                                    MRR:21.2  Hits@10:44.07 Best:21.2
2024-12-29 21:10:12,839: Snapshot:4 Epoch:10  Loss:0.994  translation_Loss:0.476  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.518                                                    MRR:21.04 Hits@10:43.79 Best:21.2
2024-12-29 21:10:19,312: Snapshot:4 Epoch:11  Loss:0.989  translation_Loss:0.475  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.515                                                    MRR:20.78 Hits@10:43.86 Best:21.2
2024-12-29 21:10:25,941: Early Stopping! Snapshot: 4 Epoch: 12 Best Results: 21.2
2024-12-29 21:10:25,942: Start to training tokens! Snapshot: 4 Epoch: 12 Loss:0.997 MRR:21.11 Best Results: 21.2
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-29 21:10:25,942: Snapshot:4 Epoch:12  Loss:0.997  translation_Loss:0.481  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.516                                                    MRR:21.11 Hits@10:43.89 Best:21.2
2024-12-29 21:10:32,790: Snapshot:4 Epoch:13  Loss:26.188 translation_Loss:11.418 multi_layer_Loss:14.77  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:21.11 Hits@10:43.89 Best:21.2
2024-12-29 21:10:39,310: End of token training: 4 Epoch: 14 Loss:11.74 MRR:21.11 Best Results: 21.2
2024-12-29 21:10:39,310: Snapshot:4 Epoch:14  Loss:11.74  translation_Loss:11.423 multi_layer_Loss:0.317  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:21.11 Hits@10:43.89 Best:21.2
2024-12-29 21:10:39,604: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2024-12-29 21:10:54,412: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2332 | 0.1466 | 0.2677 | 0.3236 |  0.3987 |
|     1      | 0.2252 | 0.1434 | 0.2565 | 0.3118 |  0.3832 |
|     2      | 0.2116 | 0.1293 | 0.2406 | 0.2968 |  0.3778 |
|     3      | 0.1983 | 0.1091 | 0.2272 | 0.2906 |  0.3819 |
|     4      | 0.2143 | 0.105  | 0.2494 | 0.3279 |  0.4391 |
+------------+--------+--------+--------+--------+---------+
2024-12-29 21:10:54,415: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2401 | 0.1551 | 0.2825 | 0.3306 |  0.3936 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2452 | 0.1555 | 0.2853 | 0.3407 |  0.4151 |
|     1      | 0.2346 | 0.1521 | 0.2732 | 0.3219 |  0.3858 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2461 | 0.1564 | 0.2855 | 0.3435 |  0.4162 |
|     1      | 0.2396 | 0.1565 | 0.2761 | 0.3301 |  0.3947 |
|     2      | 0.2203 | 0.1387 | 0.254  | 0.3058 |  0.3757 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      |  0.24  | 0.1516 | 0.2771 | 0.3351 |  0.4083 |
|     1      | 0.2345 | 0.1512 | 0.2704 | 0.3244 |  0.3934 |
|     2      | 0.2188 | 0.1346 | 0.252  | 0.3077 |  0.3841 |
|     3      | 0.2021 | 0.1135 | 0.2343 | 0.2962 |  0.3777 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2332 | 0.1466 | 0.2677 | 0.3236 |  0.3987 |
|     1      | 0.2252 | 0.1434 | 0.2565 | 0.3118 |  0.3832 |
|     2      | 0.2116 | 0.1293 | 0.2406 | 0.2968 |  0.3778 |
|     3      | 0.1983 | 0.1091 | 0.2272 | 0.2906 |  0.3819 |
|     4      | 0.2143 | 0.105  | 0.2494 | 0.3279 |  0.4391 |
+------------+--------+--------+--------+--------+---------+]
2024-12-29 21:10:54,416: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 142.66303753852844 |    0.24   |    0.155     |    0.282     |     0.394     |
|    1     | 103.34264898300171 |    0.24   |    0.154     |    0.279     |      0.4      |
|    2     | 65.71902370452881  |   0.235   |    0.151     |    0.272     |     0.396     |
|    3     | 86.42924284934998  |   0.224   |    0.138     |    0.258     |     0.391     |
|    4     | 113.52243542671204 |   0.217   |    0.127     |    0.248     |     0.396     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-29 21:10:54,416: Sum_Training_Time:511.676388502121
2024-12-29 21:10:54,416: Every_Training_Time:[142.66303753852844, 103.34264898300171, 65.71902370452881, 86.42924284934998, 113.52243542671204]
2024-12-29 21:10:54,416: Forward transfer: 0.175125 Backward transfer: -0.007199999999999998
[lijing@p0313 IncDE]$ python main.py -dataset FACT -lifelong_name double_tokened -using_token_distillation_loss True -use_multi_layers False -without_multi_layers True -use_two_stage False -batch_size 3072 -learning_rate 0.001 -patience 5 -multi_layer_weight 1 -token_distillation_weight 500 10000 8000 15000 -token_num 7
/users/PCS0256/lijing/.local/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
2025-01-02 14:20:41,335: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250102142009/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=5, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[500.0, 10000.0, 8000.0, 15000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-02 14:20:50,323: Snapshot:0 Epoch:0 Loss:17.024 translation_Loss:17.024 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:6.23  Hits@10:13.26 Best:6.23
2025-01-02 14:20:56,493: Snapshot:0 Epoch:1 Loss:11.963 translation_Loss:11.963 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:9.69  Hits@10:22.61 Best:9.69
2025-01-02 14:21:02,300: Snapshot:0 Epoch:2 Loss:8.603  translation_Loss:8.603  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:12.9  Hits@10:28.89 Best:12.9
2025-01-02 14:21:08,443: Snapshot:0 Epoch:3 Loss:6.158  translation_Loss:6.158  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:15.77 Hits@10:33.11 Best:15.77
2025-01-02 14:21:14,212: Snapshot:0 Epoch:4 Loss:4.364  translation_Loss:4.364  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:18.36 Hits@10:35.93 Best:18.36
2025-01-02 14:21:20,024: Snapshot:0 Epoch:5 Loss:3.086  translation_Loss:3.086  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:20.45 Hits@10:37.67 Best:20.45
2025-01-02 14:21:26,158: Snapshot:0 Epoch:6 Loss:2.175  translation_Loss:2.175  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:21.94 Hits@10:38.83 Best:21.94
2025-01-02 14:21:31,918: Snapshot:0 Epoch:7 Loss:1.541  translation_Loss:1.541  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:23.01 Hits@10:39.55 Best:23.01
2025-01-02 14:21:38,086: Snapshot:0 Epoch:8 Loss:1.095  translation_Loss:1.095  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:23.75 Hits@10:39.82 Best:23.75
2025-01-02 14:21:43,913: Snapshot:0 Epoch:9 Loss:0.818  translation_Loss:0.818  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.19 Hits@10:40.11 Best:24.19
2025-01-02 14:21:49,693: Snapshot:0 Epoch:10  Loss:0.628  translation_Loss:0.628  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.26 Hits@10:40.19 Best:24.26
2025-01-02 14:21:55,896: Snapshot:0 Epoch:11  Loss:0.496  translation_Loss:0.496  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.45 Hits@10:40.17 Best:24.45
2025-01-02 14:22:01,695: Snapshot:0 Epoch:12  Loss:0.411  translation_Loss:0.411  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.55 Hits@10:40.24 Best:24.55
2025-01-02 14:22:07,816: Snapshot:0 Epoch:13  Loss:0.352  translation_Loss:0.352  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.68 Hits@10:40.4  Best:24.68
2025-01-02 14:22:13,564: Snapshot:0 Epoch:14  Loss:0.309  translation_Loss:0.309  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.66 Hits@10:40.42 Best:24.68
2025-01-02 14:22:19,287: Snapshot:0 Epoch:15  Loss:0.274  translation_Loss:0.274  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.6  Hits@10:40.41 Best:24.68
2025-01-02 14:22:25,416: Snapshot:0 Epoch:16  Loss:0.244  translation_Loss:0.244  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.67 Hits@10:40.42 Best:24.68
2025-01-02 14:22:31,155: Snapshot:0 Epoch:17  Loss:0.222  translation_Loss:0.222  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.66 Hits@10:40.37 Best:24.68
2025-01-02 14:22:37,285: Snapshot:0 Epoch:18  Loss:0.209  translation_Loss:0.209  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.73 Hits@10:40.39 Best:24.73
2025-01-02 14:22:43,066: Snapshot:0 Epoch:19  Loss:0.193  translation_Loss:0.193  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.62 Hits@10:40.24 Best:24.73
2025-01-02 14:22:49,194: Snapshot:0 Epoch:20  Loss:0.182  translation_Loss:0.182  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.73 Hits@10:40.48 Best:24.73
2025-01-02 14:22:54,907: Snapshot:0 Epoch:21  Loss:0.167  translation_Loss:0.167  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.71 Hits@10:40.38 Best:24.73
2025-01-02 14:23:00,747: Snapshot:0 Epoch:22  Loss:0.157  translation_Loss:0.157  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.67 Hits@10:40.28 Best:24.73
2025-01-02 14:23:06,885: Early Stopping! Snapshot: 0 Epoch: 23 Best Results: 24.73
2025-01-02 14:23:06,886: Start to training tokens! Snapshot: 0 Epoch: 23 Loss:0.152 MRR:24.7 Best Results: 24.73
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-02 14:23:06,886: Snapshot:0 Epoch:23  Loss:0.152  translation_Loss:0.152  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.7  Hits@10:40.21 Best:24.73
2025-01-02 14:23:13,117: Snapshot:0 Epoch:24  Loss:28.262 translation_Loss:12.307 multi_layer_Loss:15.955 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:24.7  Hits@10:40.21 Best:24.73
2025-01-02 14:23:19,238: End of token training: 0 Epoch: 25 Loss:12.669 MRR:24.7 Best Results: 24.73
2025-01-02 14:23:19,238: Snapshot:0 Epoch:25  Loss:12.669 translation_Loss:12.323 multi_layer_Loss:0.346  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:24.7  Hits@10:40.21 Best:24.73
2025-01-02 14:23:19,480: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2025-01-02 14:23:21,622: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2397 | 0.1548 | 0.2809 | 0.331  |  0.3923 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 14:23:44,292: Snapshot:1 Epoch:0 Loss:7.358  translation_Loss:7.0  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.358                                                    MRR:20.63 Hits@10:34.72 Best:20.63
2025-01-02 14:23:50,485: Snapshot:1 Epoch:1 Loss:3.851  translation_Loss:3.047  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.803                                                    MRR:22.41 Hits@10:37.17 Best:22.41
2025-01-02 14:23:56,781: Snapshot:1 Epoch:2 Loss:2.634  translation_Loss:1.586  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.048                                                    MRR:23.13 Hits@10:38.23 Best:23.13
2025-01-02 14:24:03,045: Snapshot:1 Epoch:3 Loss:2.236  translation_Loss:1.106  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.13                                                     MRR:23.34 Hits@10:38.65 Best:23.34
2025-01-02 14:24:09,632: Snapshot:1 Epoch:4 Loss:2.094  translation_Loss:0.931  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.163                                                    MRR:23.49 Hits@10:38.75 Best:23.49
2025-01-02 14:24:15,873: Snapshot:1 Epoch:5 Loss:2.021  translation_Loss:0.841  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.181                                                    MRR:23.48 Hits@10:38.62 Best:23.49
2025-01-02 14:24:22,038: Snapshot:1 Epoch:6 Loss:1.999  translation_Loss:0.803  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.197                                                    MRR:23.39 Hits@10:38.45 Best:23.49
2025-01-02 14:24:28,701: Snapshot:1 Epoch:7 Loss:1.981  translation_Loss:0.774  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.207                                                    MRR:23.57 Hits@10:38.66 Best:23.57
2025-01-02 14:24:34,958: Snapshot:1 Epoch:8 Loss:1.969  translation_Loss:0.755  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.214                                                    MRR:23.5  Hits@10:38.62 Best:23.57
2025-01-02 14:24:41,194: Snapshot:1 Epoch:9 Loss:1.975  translation_Loss:0.75 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.225                                                    MRR:23.46 Hits@10:38.5  Best:23.57
2025-01-02 14:24:47,762: Snapshot:1 Epoch:10  Loss:1.971  translation_Loss:0.737  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.234                                                    MRR:23.5  Hits@10:38.57 Best:23.57
2025-01-02 14:24:53,964: Snapshot:1 Epoch:11  Loss:1.967  translation_Loss:0.726  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.24                                                     MRR:23.35 Hits@10:38.68 Best:23.57
2025-01-02 14:25:00,586: Early Stopping! Snapshot: 1 Epoch: 12 Best Results: 23.57
2025-01-02 14:25:00,587: Start to training tokens! Snapshot: 1 Epoch: 12 Loss:1.966 MRR:23.48 Best Results: 23.57
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-02 14:25:00,587: Snapshot:1 Epoch:12  Loss:1.966  translation_Loss:0.722  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.244                                                    MRR:23.48 Hits@10:38.6  Best:23.57
2025-01-02 14:25:06,752: Snapshot:1 Epoch:13  Loss:28.753 translation_Loss:13.177 multi_layer_Loss:15.576 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:23.48 Hits@10:38.6  Best:23.57
2025-01-02 14:25:12,884: End of token training: 1 Epoch: 14 Loss:13.514 MRR:23.48 Best Results: 23.57
2025-01-02 14:25:12,884: Snapshot:1 Epoch:14  Loss:13.514 translation_Loss:13.171 multi_layer_Loss:0.342  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:23.48 Hits@10:38.6  Best:23.57
2025-01-02 14:25:13,131: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2025-01-02 14:25:18,709: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2506 | 0.1619 | 0.2904 | 0.3464 |  0.417  |
|     1      | 0.2357 | 0.1532 | 0.274  | 0.3248 |  0.3867 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 14:25:41,474: Snapshot:2 Epoch:0 Loss:5.03 translation_Loss:3.857  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.173                                                    MRR:21.57 Hits@10:37.17 Best:21.57
2025-01-02 14:25:47,881: Snapshot:2 Epoch:1 Loss:4.116  translation_Loss:3.025  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.091                                                    MRR:21.92 Hits@10:37.4  Best:21.92
2025-01-02 14:25:54,264: Snapshot:2 Epoch:2 Loss:3.716  translation_Loss:2.762  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.954                                                    MRR:22.04 Hits@10:37.63 Best:22.04
2025-01-02 14:26:00,807: Snapshot:2 Epoch:3 Loss:3.632  translation_Loss:2.665  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.967                                                    MRR:22.18 Hits@10:37.65 Best:22.18
2025-01-02 14:26:07,132: Snapshot:2 Epoch:4 Loss:3.618  translation_Loss:2.668  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.95                                                     MRR:22.1  Hits@10:37.7  Best:22.18
2025-01-02 14:26:13,480: Snapshot:2 Epoch:5 Loss:3.616  translation_Loss:2.65 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.966                                                    MRR:22.14 Hits@10:37.59 Best:22.18
2025-01-02 14:26:20,144: Snapshot:2 Epoch:6 Loss:3.593  translation_Loss:2.636  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.957                                                    MRR:22.12 Hits@10:37.69 Best:22.18
2025-01-02 14:26:26,449: Snapshot:2 Epoch:7 Loss:3.598  translation_Loss:2.635  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.962                                                    MRR:22.05 Hits@10:37.57 Best:22.18
2025-01-02 14:26:32,773: Early Stopping! Snapshot: 2 Epoch: 8 Best Results: 22.18
2025-01-02 14:26:32,773: Start to training tokens! Snapshot: 2 Epoch: 8 Loss:3.591 MRR:22.07 Best Results: 22.18
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-02 14:26:32,773: Snapshot:2 Epoch:8 Loss:3.591  translation_Loss:2.631  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.961                                                    MRR:22.07 Hits@10:37.77 Best:22.18
2025-01-02 14:26:39,424: Snapshot:2 Epoch:9 Loss:30.84  translation_Loss:14.408 multi_layer_Loss:16.432 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:22.07 Hits@10:37.77 Best:22.18
2025-01-02 14:26:45,753: End of token training: 2 Epoch: 10 Loss:14.764 MRR:22.07 Best Results: 22.18
2025-01-02 14:26:45,753: Snapshot:2 Epoch:10  Loss:14.764 translation_Loss:14.404 multi_layer_Loss:0.361  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:22.07 Hits@10:37.77 Best:22.18
2025-01-02 14:26:46,002: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2025-01-02 14:26:54,482: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2502 | 0.1614 | 0.289  | 0.3465 |  0.4173 |
|     1      | 0.2397 | 0.1567 | 0.2755 | 0.3292 |  0.3958 |
|     2      | 0.2208 | 0.1381 | 0.2548 | 0.3087 |  0.3781 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 14:27:17,203: Snapshot:3 Epoch:0 Loss:3.495  translation_Loss:2.475  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.02                                                     MRR:19.65 Hits@10:37.17 Best:19.65
2025-01-02 14:27:23,670: Snapshot:3 Epoch:1 Loss:2.734  translation_Loss:1.697  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:1.037                                                    MRR:19.61 Hits@10:37.09 Best:19.65
2025-01-02 14:27:30,110: Snapshot:3 Epoch:2 Loss:2.619  translation_Loss:1.687  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.933                                                    MRR:19.75 Hits@10:37.47 Best:19.75
2025-01-02 14:27:36,668: Snapshot:3 Epoch:3 Loss:2.58 translation_Loss:1.632  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.948                                                    MRR:19.86 Hits@10:37.11 Best:19.86
2025-01-02 14:27:43,061: Snapshot:3 Epoch:4 Loss:2.59 translation_Loss:1.648  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.943                                                    MRR:19.79 Hits@10:37.39 Best:19.86
2025-01-02 14:27:49,794: Snapshot:3 Epoch:5 Loss:2.576  translation_Loss:1.615  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.962                                                    MRR:19.81 Hits@10:37.07 Best:19.86
2025-01-02 14:27:56,221: Snapshot:3 Epoch:6 Loss:2.589  translation_Loss:1.639  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.949                                                    MRR:19.95 Hits@10:37.36 Best:19.95
2025-01-02 14:28:02,652: Snapshot:3 Epoch:7 Loss:2.59 translation_Loss:1.629  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.961                                                    MRR:19.74 Hits@10:37.27 Best:19.95
2025-01-02 14:28:09,498: Snapshot:3 Epoch:8 Loss:2.59 translation_Loss:1.639  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.951                                                    MRR:19.77 Hits@10:37.19 Best:19.95
2025-01-02 14:28:15,990: Snapshot:3 Epoch:9 Loss:2.588  translation_Loss:1.626  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.962                                                    MRR:19.79 Hits@10:37.48 Best:19.95
2025-01-02 14:28:22,707: Snapshot:3 Epoch:10  Loss:2.594  translation_Loss:1.637  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.956                                                    MRR:19.81 Hits@10:37.32 Best:19.95
2025-01-02 14:28:29,073: Early Stopping! Snapshot: 3 Epoch: 11 Best Results: 19.95
2025-01-02 14:28:29,073: Start to training tokens! Snapshot: 3 Epoch: 11 Loss:2.583 MRR:19.73 Best Results: 19.95
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-02 14:28:29,074: Snapshot:3 Epoch:11  Loss:2.583  translation_Loss:1.628  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.955                                                    MRR:19.73 Hits@10:37.3  Best:19.95
2025-01-02 14:28:35,797: Snapshot:3 Epoch:12  Loss:30.507 translation_Loss:13.774 multi_layer_Loss:16.733 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:19.73 Hits@10:37.3  Best:19.95
2025-01-02 14:28:42,179: End of token training: 3 Epoch: 13 Loss:14.128 MRR:19.73 Best Results: 19.95
2025-01-02 14:28:42,180: Snapshot:3 Epoch:13  Loss:14.128 translation_Loss:13.78  multi_layer_Loss:0.347  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:19.73 Hits@10:37.3  Best:19.95
2025-01-02 14:28:42,429: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2025-01-02 14:28:53,565: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2444 | 0.1563 | 0.2816 | 0.3391 |  0.4116 |
|     1      | 0.2353 | 0.1518 | 0.2709 | 0.3254 |  0.3929 |
|     2      | 0.2201 | 0.1351 | 0.2538 | 0.3108 |  0.3867 |
|     3      | 0.2006 | 0.1128 | 0.2315 | 0.2925 |  0.3753 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-02 14:29:16,776: Snapshot:4 Epoch:0 Loss:2.299  translation_Loss:1.498  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.801                                                    MRR:19.19 Hits@10:42.08 Best:19.19
2025-01-02 14:29:23,237: Snapshot:4 Epoch:1 Loss:1.557  translation_Loss:0.911  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.646                                                    MRR:19.9  Hits@10:42.4  Best:19.9
2025-01-02 14:29:29,661: Snapshot:4 Epoch:2 Loss:1.338  translation_Loss:0.751  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.587                                                    MRR:20.34 Hits@10:42.41 Best:20.34
2025-01-02 14:29:36,198: Snapshot:4 Epoch:3 Loss:1.314  translation_Loss:0.732  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.581                                                    MRR:20.18 Hits@10:42.6  Best:20.34
2025-01-02 14:29:42,634: Snapshot:4 Epoch:4 Loss:1.314  translation_Loss:0.732  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.582                                                    MRR:20.15 Hits@10:42.37 Best:20.34
2025-01-02 14:29:49,145: Snapshot:4 Epoch:5 Loss:1.303  translation_Loss:0.722  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.581                                                    MRR:20.36 Hits@10:42.49 Best:20.36
2025-01-02 14:29:55,561: Snapshot:4 Epoch:6 Loss:1.311  translation_Loss:0.728  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.583                                                    MRR:20.19 Hits@10:42.52 Best:20.36
2025-01-02 14:30:02,380: Snapshot:4 Epoch:7 Loss:1.305  translation_Loss:0.711  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.594                                                    MRR:20.32 Hits@10:42.68 Best:20.36
2025-01-02 14:30:09,137: Snapshot:4 Epoch:8 Loss:1.296  translation_Loss:0.717  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.579                                                    MRR:20.35 Hits@10:42.19 Best:20.36
2025-01-02 14:30:15,976: Snapshot:4 Epoch:9 Loss:1.302  translation_Loss:0.717  multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.585                                                    MRR:20.16 Hits@10:42.23 Best:20.36
2025-01-02 14:30:22,353: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 20.36
2025-01-02 14:30:22,353: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:1.304 MRR:20.24 Best Results: 20.36
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([7, 200]), requires_grad: True
 - torch.Size([7, 200]), requires_grad: True
2025-01-02 14:30:22,353: Snapshot:4 Epoch:10  Loss:1.304  translation_Loss:0.72 multi_layer_Loss:0.0  MAE_Loss:0.0  decompose_Loss:0.583                                                    MRR:20.24 Hits@10:42.31 Best:20.36
2025-01-02 14:30:28,819: Snapshot:4 Epoch:11  Loss:27.672 translation_Loss:11.698 multi_layer_Loss:15.975 MAE_Loss:0.0  decompose_Loss:0.0                                                    MRR:20.24 Hits@10:42.31 Best:20.36
2025-01-02 14:30:35,664: End of token training: 4 Epoch: 12 Loss:12.03 MRR:20.24 Best Results: 20.36
2025-01-02 14:30:35,665: Snapshot:4 Epoch:12  Loss:12.03  translation_Loss:11.679 multi_layer_Loss:0.351  MAE_Loss:0.0  decompose_Loss:0.0                                                            MRR:20.24 Hits@10:42.31 Best:20.36
2025-01-02 14:30:35,910: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2025-01-02 14:30:50,179: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1545 | 0.2759 | 0.3308 |  0.4056 |
|     1      | 0.2305 | 0.1485 | 0.2638 | 0.3184 |  0.387  |
|     2      | 0.2152 | 0.1321 | 0.2455 | 0.303  |  0.3804 |
|     3      | 0.199  | 0.1113 | 0.2252 | 0.2892 |  0.3793 |
|     4      | 0.2042 | 0.0988 | 0.2347 | 0.3155 |  0.4216 |
+------------+--------+--------+--------+--------+---------+
2025-01-02 14:30:50,182: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2397 | 0.1548 | 0.2809 | 0.331  |  0.3923 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2506 | 0.1619 | 0.2904 | 0.3464 |  0.417  |
|     1      | 0.2357 | 0.1532 | 0.274  | 0.3248 |  0.3867 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2502 | 0.1614 | 0.289  | 0.3465 |  0.4173 |
|     1      | 0.2397 | 0.1567 | 0.2755 | 0.3292 |  0.3958 |
|     2      | 0.2208 | 0.1381 | 0.2548 | 0.3087 |  0.3781 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2444 | 0.1563 | 0.2816 | 0.3391 |  0.4116 |
|     1      | 0.2353 | 0.1518 | 0.2709 | 0.3254 |  0.3929 |
|     2      | 0.2201 | 0.1351 | 0.2538 | 0.3108 |  0.3867 |
|     3      | 0.2006 | 0.1128 | 0.2315 | 0.2925 |  0.3753 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2407 | 0.1545 | 0.2759 | 0.3308 |  0.4056 |
|     1      | 0.2305 | 0.1485 | 0.2638 | 0.3184 |  0.387  |
|     2      | 0.2152 | 0.1321 | 0.2455 | 0.303  |  0.3804 |
|     3      | 0.199  | 0.1113 | 0.2252 | 0.2892 |  0.3793 |
|     4      | 0.2042 | 0.0988 | 0.2347 | 0.3155 |  0.4216 |
+------------+--------+--------+--------+--------+---------+]
2025-01-02 14:30:50,183: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 157.9028136730194  |    0.24   |    0.155     |    0.281     |     0.392     |
|    1     | 108.19873809814453 |   0.243   |    0.158     |    0.282     |     0.402     |
|    2     | 84.06878685951233  |   0.237   |    0.152     |    0.273     |     0.397     |
|    3     | 104.68650102615356 |   0.225   |    0.139     |    0.259     |     0.392     |
|    4     | 99.06954002380371  |   0.218   |    0.129     |    0.249     |     0.395     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-02 14:30:50,183: Sum_Training_Time:553.9263796806335
2025-01-02 14:30:50,183: Every_Training_Time:[157.9028136730194, 108.19873809814453, 84.06878685951233, 104.68650102615356, 99.06954002380371]
2025-01-02 14:30:50,183: Forward transfer: 0.17527500000000001 Backward transfer: -0.0028499999999999914
2025-01-04 19:06:08,532: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250104190532/FACT', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[500.0, 10000.0, 8000.0, 15000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-04 19:06:17,794: Snapshot:0 Epoch:0 Loss:17.024 translation_Loss:17.024 token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:6.23  Hits@10:13.26 Best:6.23
2025-01-04 19:06:24,339: Snapshot:0 Epoch:1 Loss:11.963 translation_Loss:11.963 token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:9.68  Hits@10:22.61 Best:9.68
2025-01-04 19:06:30,235: Snapshot:0 Epoch:2 Loss:8.603  translation_Loss:8.603  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:12.91 Hits@10:28.87 Best:12.91
2025-01-04 19:06:36,539: Snapshot:0 Epoch:3 Loss:6.158  translation_Loss:6.158  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:15.77 Hits@10:33.1  Best:15.77
2025-01-04 19:06:42,348: Snapshot:0 Epoch:4 Loss:4.364  translation_Loss:4.364  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:18.34 Hits@10:36.01 Best:18.34
2025-01-04 19:06:48,164: Snapshot:0 Epoch:5 Loss:3.085  translation_Loss:3.085  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:20.45 Hits@10:37.75 Best:20.45
2025-01-04 19:06:54,509: Snapshot:0 Epoch:6 Loss:2.175  translation_Loss:2.175  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:21.94 Hits@10:38.69 Best:21.94
2025-01-04 19:07:00,439: Snapshot:0 Epoch:7 Loss:1.541  translation_Loss:1.541  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:23.01 Hits@10:39.41 Best:23.01
2025-01-04 19:07:06,759: Snapshot:0 Epoch:8 Loss:1.096  translation_Loss:1.096  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:23.68 Hits@10:39.86 Best:23.68
2025-01-04 19:07:12,609: Snapshot:0 Epoch:9 Loss:0.814  translation_Loss:0.814  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.08 Hits@10:40.18 Best:24.08
2025-01-04 19:07:18,488: Snapshot:0 Epoch:10  Loss:0.626  translation_Loss:0.626  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.34 Hits@10:40.26 Best:24.34
2025-01-04 19:07:24,721: Snapshot:0 Epoch:11  Loss:0.494  translation_Loss:0.494  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.4  Hits@10:40.27 Best:24.4
2025-01-04 19:07:30,528: Snapshot:0 Epoch:12  Loss:0.41 translation_Loss:0.41 token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.52 Hits@10:40.38 Best:24.52
2025-01-04 19:07:36,794: Snapshot:0 Epoch:13  Loss:0.351  translation_Loss:0.351  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.71 Hits@10:40.38 Best:24.71
2025-01-04 19:07:42,621: Snapshot:0 Epoch:14  Loss:0.311  translation_Loss:0.311  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.78 Hits@10:40.4  Best:24.78
2025-01-04 19:07:48,513: Snapshot:0 Epoch:15  Loss:0.272  translation_Loss:0.272  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.71 Hits@10:40.3  Best:24.78
2025-01-04 19:07:54,774: Snapshot:0 Epoch:16  Loss:0.243  translation_Loss:0.243  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.79 Hits@10:40.43 Best:24.79
2025-01-04 19:08:00,639: Snapshot:0 Epoch:17  Loss:0.224  translation_Loss:0.224  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.77 Hits@10:40.37 Best:24.79
2025-01-04 19:08:06,860: Snapshot:0 Epoch:18  Loss:0.208  translation_Loss:0.208  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.75 Hits@10:40.23 Best:24.79
2025-01-04 19:08:12,776: Snapshot:0 Epoch:19  Loss:0.192  translation_Loss:0.192  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.8  Hits@10:40.22 Best:24.8
2025-01-04 19:08:19,015: Snapshot:0 Epoch:20  Loss:0.18 translation_Loss:0.18 token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.78 Hits@10:40.27 Best:24.8
2025-01-04 19:08:24,803: Snapshot:0 Epoch:21  Loss:0.167  translation_Loss:0.167  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.67 Hits@10:40.44 Best:24.8
2025-01-04 19:08:30,698: Early Stopping! Snapshot: 0 Epoch: 22 Best Results: 24.8
2025-01-04 19:08:30,698: Start to training tokens! Snapshot: 0 Epoch: 22 Loss:0.156 MRR:24.67 Best Results: 24.8
Token added to optimizer, embeddings excluded successfully.
2025-01-04 19:08:30,699: Snapshot:0 Epoch:22  Loss:0.156  translation_Loss:0.156  token_training_loss:0.0 distillation_Loss:0.0                                                     MRR:24.67 Hits@10:40.13 Best:24.8
2025-01-04 19:08:37,421: Snapshot:0 Epoch:23  Loss:28.268 translation_Loss:12.314 token_training_loss:15.955  distillation_Loss:0.0                                                     MRR:24.67 Hits@10:40.13 Best:24.8
2025-01-04 19:08:43,334: End of token training: 0 Epoch: 24 Loss:12.646 MRR:24.67 Best Results: 24.8
2025-01-04 19:08:43,334: Snapshot:0 Epoch:24  Loss:12.646 translation_Loss:12.3 token_training_loss:0.346 distillation_Loss:0.0                                                             MRR:24.67 Hits@10:40.13 Best:24.8
2025-01-04 19:08:43,590: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2025-01-04 19:08:46,044: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2398 | 0.1553 | 0.2796 | 0.3304 |  0.3922 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-04 19:08:58,588: Snapshot:1 Epoch:0 Loss:7.307  translation_Loss:6.949  token_training_loss:0.0 distillation_Loss:0.357                                                     MRR:20.7  Hits@10:34.6  Best:20.7
2025-01-04 19:09:05,234: Snapshot:1 Epoch:1 Loss:3.815  translation_Loss:3.016  token_training_loss:0.0 distillation_Loss:0.8                                                     MRR:22.44 Hits@10:37.14 Best:22.44
2025-01-04 19:09:11,515: Snapshot:1 Epoch:2 Loss:2.602  translation_Loss:1.563  token_training_loss:0.0 distillation_Loss:1.039                                                     MRR:23.23 Hits@10:38.21 Best:23.23
2025-01-04 19:09:18,350: Snapshot:1 Epoch:3 Loss:2.199  translation_Loss:1.084  token_training_loss:0.0 distillation_Loss:1.115                                                     MRR:23.31 Hits@10:38.61 Best:23.31
2025-01-04 19:09:24,583: Snapshot:1 Epoch:4 Loss:2.064  translation_Loss:0.912  token_training_loss:0.0 distillation_Loss:1.152                                                     MRR:23.39 Hits@10:38.57 Best:23.39
2025-01-04 19:09:30,826: Snapshot:1 Epoch:5 Loss:2.012  translation_Loss:0.843  token_training_loss:0.0 distillation_Loss:1.169                                                     MRR:23.52 Hits@10:38.84 Best:23.52
2025-01-04 19:09:37,501: Snapshot:1 Epoch:6 Loss:1.978  translation_Loss:0.797  token_training_loss:0.0 distillation_Loss:1.182                                                     MRR:23.41 Hits@10:38.65 Best:23.52
2025-01-04 19:09:43,711: Snapshot:1 Epoch:7 Loss:1.961  translation_Loss:0.768  token_training_loss:0.0 distillation_Loss:1.193                                                     MRR:23.28 Hits@10:38.61 Best:23.52
2025-01-04 19:09:50,328: Early Stopping! Snapshot: 1 Epoch: 8 Best Results: 23.52
2025-01-04 19:09:50,328: Start to training tokens! Snapshot: 1 Epoch: 8 Loss:1.961 MRR:23.36 Best Results: 23.52
Token added to optimizer, embeddings excluded successfully.
2025-01-04 19:09:50,328: Snapshot:1 Epoch:8 Loss:1.961  translation_Loss:0.758  token_training_loss:0.0 distillation_Loss:1.203                                                     MRR:23.36 Hits@10:38.58 Best:23.52
2025-01-04 19:09:56,585: Snapshot:1 Epoch:9 Loss:28.845 translation_Loss:13.27  token_training_loss:15.576  distillation_Loss:0.0                                                     MRR:23.36 Hits@10:38.58 Best:23.52
2025-01-04 19:10:02,960: End of token training: 1 Epoch: 10 Loss:13.616 MRR:23.36 Best Results: 23.52
2025-01-04 19:10:02,960: Snapshot:1 Epoch:10  Loss:13.616 translation_Loss:13.273 token_training_loss:0.342 distillation_Loss:0.0                                                             MRR:23.36 Hits@10:38.58 Best:23.52
2025-01-04 19:10:03,220: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2025-01-04 19:10:08,905: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2496 | 0.1614 | 0.2888 | 0.3444 |  0.4175 |
|     1      | 0.2355 | 0.1534 | 0.2731 | 0.3245 |  0.3851 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-04 19:10:21,592: Snapshot:2 Epoch:0 Loss:5.065  translation_Loss:3.886  token_training_loss:0.0 distillation_Loss:1.18                                                    MRR:21.43 Hits@10:36.94 Best:21.43
2025-01-04 19:10:28,398: Snapshot:2 Epoch:1 Loss:4.143  translation_Loss:3.045  token_training_loss:0.0 distillation_Loss:1.098                                                     MRR:21.86 Hits@10:37.3  Best:21.86
2025-01-04 19:10:34,914: Snapshot:2 Epoch:2 Loss:3.748  translation_Loss:2.784  token_training_loss:0.0 distillation_Loss:0.964                                                     MRR:22.01 Hits@10:37.56 Best:22.01
2025-01-04 19:10:41,773: Snapshot:2 Epoch:3 Loss:3.663  translation_Loss:2.687  token_training_loss:0.0 distillation_Loss:0.976                                                     MRR:22.01 Hits@10:37.62 Best:22.01
2025-01-04 19:10:48,208: Snapshot:2 Epoch:4 Loss:3.632  translation_Loss:2.675  token_training_loss:0.0 distillation_Loss:0.956                                                     MRR:22.04 Hits@10:37.51 Best:22.04
2025-01-04 19:10:54,592: Snapshot:2 Epoch:5 Loss:3.623  translation_Loss:2.652  token_training_loss:0.0 distillation_Loss:0.972                                                     MRR:22.03 Hits@10:37.56 Best:22.04
2025-01-04 19:11:01,410: Snapshot:2 Epoch:6 Loss:3.618  translation_Loss:2.652  token_training_loss:0.0 distillation_Loss:0.966                                                     MRR:21.93 Hits@10:37.55 Best:22.04
2025-01-04 19:11:07,760: Early Stopping! Snapshot: 2 Epoch: 7 Best Results: 22.04
2025-01-04 19:11:07,764: Start to training tokens! Snapshot: 2 Epoch: 7 Loss:3.627 MRR:21.92 Best Results: 22.04
Token added to optimizer, embeddings excluded successfully.
2025-01-04 19:11:07,764: Snapshot:2 Epoch:7 Loss:3.627  translation_Loss:2.653  token_training_loss:0.0 distillation_Loss:0.973                                                     MRR:21.92 Hits@10:37.43 Best:22.04
2025-01-04 19:11:14,444: Snapshot:2 Epoch:8 Loss:30.857 translation_Loss:14.425 token_training_loss:16.432  distillation_Loss:0.0                                                     MRR:21.92 Hits@10:37.43 Best:22.04
2025-01-04 19:11:20,915: End of token training: 2 Epoch: 9 Loss:14.782 MRR:21.92 Best Results: 22.04
2025-01-04 19:11:20,916: Snapshot:2 Epoch:9 Loss:14.782 translation_Loss:14.421 token_training_loss:0.361 distillation_Loss:0.0                                                             MRR:21.92 Hits@10:37.43 Best:22.04
2025-01-04 19:11:21,196: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2025-01-04 19:11:29,411: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2494 | 0.1605 | 0.2887 | 0.3452 |  0.418  |
|     1      | 0.2391 | 0.156  | 0.2759 | 0.3296 |  0.3953 |
|     2      | 0.2207 | 0.1382 | 0.256  | 0.3082 |  0.3768 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-04 19:11:42,375: Snapshot:3 Epoch:0 Loss:3.49 translation_Loss:2.471  token_training_loss:0.0 distillation_Loss:1.019                                                     MRR:19.56 Hits@10:37.21 Best:19.56
2025-01-04 19:11:49,192: Snapshot:3 Epoch:1 Loss:2.722  translation_Loss:1.691  token_training_loss:0.0 distillation_Loss:1.031                                                     MRR:19.64 Hits@10:36.93 Best:19.64
2025-01-04 19:11:55,769: Snapshot:3 Epoch:2 Loss:2.608  translation_Loss:1.679  token_training_loss:0.0 distillation_Loss:0.929                                                     MRR:19.79 Hits@10:37.32 Best:19.79
2025-01-04 19:12:02,620: Snapshot:3 Epoch:3 Loss:2.57 translation_Loss:1.625  token_training_loss:0.0 distillation_Loss:0.945                                                     MRR:19.78 Hits@10:37.13 Best:19.79
2025-01-04 19:12:09,043: Snapshot:3 Epoch:4 Loss:2.573  translation_Loss:1.639  token_training_loss:0.0 distillation_Loss:0.934                                                     MRR:19.74 Hits@10:37.15 Best:19.79
2025-01-04 19:12:15,536: Early Stopping! Snapshot: 3 Epoch: 5 Best Results: 19.79
2025-01-04 19:12:15,536: Start to training tokens! Snapshot: 3 Epoch: 5 Loss:2.57 MRR:19.79 Best Results: 19.79
Token added to optimizer, embeddings excluded successfully.
2025-01-04 19:12:15,536: Snapshot:3 Epoch:5 Loss:2.57 translation_Loss:1.625  token_training_loss:0.0 distillation_Loss:0.946                                                     MRR:19.79 Hits@10:37.14 Best:19.79
2025-01-04 19:12:22,371: Snapshot:3 Epoch:6 Loss:30.498 translation_Loss:13.766 token_training_loss:16.733  distillation_Loss:0.0                                                     MRR:19.79 Hits@10:37.14 Best:19.79
2025-01-04 19:12:28,784: End of token training: 3 Epoch: 7 Loss:14.128 MRR:19.79 Best Results: 19.79
2025-01-04 19:12:28,784: Snapshot:3 Epoch:7 Loss:14.128 translation_Loss:13.78  token_training_loss:0.347 distillation_Loss:0.0                                                             MRR:19.79 Hits@10:37.14 Best:19.79
2025-01-04 19:12:29,009: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2025-01-04 19:12:40,484: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1553 | 0.2786 | 0.3363 |  0.4125 |
|     1      | 0.2352 | 0.1524 | 0.2699 | 0.3246 |  0.3926 |
|     2      | 0.2195 | 0.1351 | 0.2532 | 0.3093 |  0.3853 |
|     3      | 0.2009 | 0.1119 | 0.2343 | 0.2943 |  0.375  |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-04 19:12:53,256: Snapshot:4 Epoch:0 Loss:2.295  translation_Loss:1.496  token_training_loss:0.0 distillation_Loss:0.799                                                     MRR:19.23 Hits@10:41.7  Best:19.23
2025-01-04 19:13:00,196: Snapshot:4 Epoch:1 Loss:1.558  translation_Loss:0.905  token_training_loss:0.0 distillation_Loss:0.653                                                     MRR:20.05 Hits@10:42.08 Best:20.05
2025-01-04 19:13:06,636: Snapshot:4 Epoch:2 Loss:1.351  translation_Loss:0.762  token_training_loss:0.0 distillation_Loss:0.589                                                     MRR:20.14 Hits@10:42.05 Best:20.14
2025-01-04 19:13:13,625: Snapshot:4 Epoch:3 Loss:1.325  translation_Loss:0.734  token_training_loss:0.0 distillation_Loss:0.591                                                     MRR:20.3  Hits@10:42.63 Best:20.3
2025-01-04 19:13:20,059: Snapshot:4 Epoch:4 Loss:1.316  translation_Loss:0.731  token_training_loss:0.0 distillation_Loss:0.584                                                     MRR:20.2  Hits@10:42.06 Best:20.3
2025-01-04 19:13:26,456: Snapshot:4 Epoch:5 Loss:1.313  translation_Loss:0.722  token_training_loss:0.0 distillation_Loss:0.591                                                     MRR:20.08 Hits@10:42.23 Best:20.3
2025-01-04 19:13:33,242: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 20.3
2025-01-04 19:13:33,243: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:1.312 MRR:20.29 Best Results: 20.3
Token added to optimizer, embeddings excluded successfully.
2025-01-04 19:13:33,243: Snapshot:4 Epoch:6 Loss:1.312  translation_Loss:0.724  token_training_loss:0.0 distillation_Loss:0.588                                                     MRR:20.29 Hits@10:42.1  Best:20.3
2025-01-04 19:13:39,772: Snapshot:4 Epoch:7 Loss:27.681 translation_Loss:11.706 token_training_loss:15.975  distillation_Loss:0.0                                                     MRR:20.29 Hits@10:42.1  Best:20.3
2025-01-04 19:13:46,594: End of token training: 4 Epoch: 8 Loss:12.069 MRR:20.29 Best Results: 20.3
2025-01-04 19:13:46,594: Snapshot:4 Epoch:8 Loss:12.069 translation_Loss:11.718 token_training_loss:0.351 distillation_Loss:0.0                                                             MRR:20.29 Hits@10:42.1  Best:20.3
2025-01-04 19:13:46,862: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2025-01-04 19:14:01,040: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1538 | 0.2742 | 0.3303 |  0.4067 |
|     1      | 0.2306 | 0.148  | 0.2653 | 0.3183 |  0.3886 |
|     2      | 0.2161 | 0.1329 | 0.2461 | 0.3025 |  0.3814 |
|     3      | 0.2001 | 0.1109 | 0.2293 | 0.2928 |  0.3794 |
|     4      | 0.2034 | 0.0969 | 0.2336 | 0.3155 |  0.4272 |
+------------+--------+--------+--------+--------+---------+
2025-01-04 19:14:01,042: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2398 | 0.1553 | 0.2796 | 0.3304 |  0.3922 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2496 | 0.1614 | 0.2888 | 0.3444 |  0.4175 |
|     1      | 0.2355 | 0.1534 | 0.2731 | 0.3245 |  0.3851 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2494 | 0.1605 | 0.2887 | 0.3452 |  0.418  |
|     1      | 0.2391 | 0.156  | 0.2759 | 0.3296 |  0.3953 |
|     2      | 0.2207 | 0.1382 | 0.256  | 0.3082 |  0.3768 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.243  | 0.1553 | 0.2786 | 0.3363 |  0.4125 |
|     1      | 0.2352 | 0.1524 | 0.2699 | 0.3246 |  0.3926 |
|     2      | 0.2195 | 0.1351 | 0.2532 | 0.3093 |  0.3853 |
|     3      | 0.2009 | 0.1119 | 0.2343 | 0.2943 |  0.375  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2396 | 0.1538 | 0.2742 | 0.3303 |  0.4067 |
|     1      | 0.2306 | 0.148  | 0.2653 | 0.3183 |  0.3886 |
|     2      | 0.2161 | 0.1329 | 0.2461 | 0.3025 |  0.3814 |
|     3      | 0.2001 | 0.1109 | 0.2293 | 0.2928 |  0.3794 |
|     4      | 0.2034 | 0.0969 | 0.2336 | 0.3155 |  0.4272 |
+------------+--------+--------+--------+--------+---------+]
2025-01-04 19:14:01,043: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 154.80146169662476 |    0.24   |    0.155     |     0.28     |     0.392     |
|    1     | 74.13615012168884  |   0.243   |    0.157     |    0.281     |     0.401     |
|    2     | 68.97984957695007  |   0.236   |    0.152     |    0.274     |     0.397     |
|    3     | 56.11144685745239  |   0.225   |    0.139     |    0.259     |     0.391     |
|    4     | 63.08298659324646  |   0.218   |    0.129     |     0.25     |     0.397     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-04 19:14:01,043: Sum_Training_Time:417.1118948459625
2025-01-04 19:14:01,043: Every_Training_Time:[154.80146169662476, 74.13615012168884, 68.97984957695007, 56.11144685745239, 63.08298659324646]
2025-01-04 19:14:01,043: Forward transfer: 0.175275 Backward transfer: -0.0026250000000000023