2025-01-06 22:23:30,399: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106222313/FACT', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=1111, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[1000.0, 10000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:23:41,268: Snapshot:0	Epoch:0	Loss:17.018	translation_Loss:17.018	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.36	Hits@10:13.57	Best:6.36
2025-01-06 22:23:48,986: Snapshot:0	Epoch:1	Loss:11.931	translation_Loss:11.931	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.86	Hits@10:23.24	Best:9.86
2025-01-06 22:23:56,230: Snapshot:0	Epoch:2	Loss:8.593	translation_Loss:8.593	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:13.05	Hits@10:29.27	Best:13.05
2025-01-06 22:24:03,921: Snapshot:0	Epoch:3	Loss:6.15	translation_Loss:6.15	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.83	Hits@10:33.33	Best:15.83
2025-01-06 22:24:11,185: Snapshot:0	Epoch:4	Loss:4.359	translation_Loss:4.359	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.35	Hits@10:36.14	Best:18.35
2025-01-06 22:24:18,466: Snapshot:0	Epoch:5	Loss:3.101	translation_Loss:3.101	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:20.39	Hits@10:37.77	Best:20.39
2025-01-06 22:24:26,112: Snapshot:0	Epoch:6	Loss:2.206	translation_Loss:2.206	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:21.93	Hits@10:38.84	Best:21.93
2025-01-06 22:24:33,359: Snapshot:0	Epoch:7	Loss:1.563	translation_Loss:1.563	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:22.95	Hits@10:39.4	Best:22.95
2025-01-06 22:24:40,978: Snapshot:0	Epoch:8	Loss:1.124	translation_Loss:1.124	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.64	Hits@10:39.67	Best:23.64
2025-01-06 22:24:48,270: Snapshot:0	Epoch:9	Loss:0.841	translation_Loss:0.841	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.08	Hits@10:39.98	Best:24.08
2025-01-06 22:24:55,486: Snapshot:0	Epoch:10	Loss:0.643	translation_Loss:0.643	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.25	Hits@10:40.06	Best:24.25
2025-01-06 22:25:03,174: Snapshot:0	Epoch:11	Loss:0.52	translation_Loss:0.52	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.44	Hits@10:40.02	Best:24.44
2025-01-06 22:25:10,524: Snapshot:0	Epoch:12	Loss:0.423	translation_Loss:0.423	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.59	Hits@10:40.18	Best:24.59
2025-01-06 22:25:18,172: Snapshot:0	Epoch:13	Loss:0.358	translation_Loss:0.358	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.73	Hits@10:40.21	Best:24.73
2025-01-06 22:25:25,396: Snapshot:0	Epoch:14	Loss:0.315	translation_Loss:0.315	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.7	Hits@10:40.08	Best:24.73
2025-01-06 22:25:32,620: Snapshot:0	Epoch:15	Loss:0.278	translation_Loss:0.278	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.74	Hits@10:40.08	Best:24.74
2025-01-06 22:25:40,239: Snapshot:0	Epoch:16	Loss:0.252	translation_Loss:0.252	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.81	Hits@10:40.12	Best:24.81
2025-01-06 22:25:47,554: Snapshot:0	Epoch:17	Loss:0.231	translation_Loss:0.231	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.83	Hits@10:40.11	Best:24.83
2025-01-06 22:25:55,208: Snapshot:0	Epoch:18	Loss:0.21	translation_Loss:0.21	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.79	Hits@10:40.23	Best:24.83
2025-01-06 22:26:02,399: Snapshot:0	Epoch:19	Loss:0.196	translation_Loss:0.196	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.73	Hits@10:40.25	Best:24.83
2025-01-06 22:26:09,612: Early Stopping! Snapshot: 0 Epoch: 20 Best Results: 24.83
2025-01-06 22:26:09,612: Start to training tokens! Snapshot: 0 Epoch: 20 Loss:0.181 MRR:24.7 Best Results: 24.83
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:26:09,613: Snapshot:0	Epoch:20	Loss:0.181	translation_Loss:0.181	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.7	Hits@10:40.31	Best:24.83
2025-01-06 22:26:17,876: Snapshot:0	Epoch:21	Loss:28.264	translation_Loss:12.316	token_training_loss:15.948	distillation_Loss:0.0                                                   	MRR:24.7	Hits@10:40.31	Best:24.83
2025-01-06 22:26:25,124: End of token training: 0 Epoch: 22 Loss:12.661 MRR:24.7 Best Results: 24.83
2025-01-06 22:26:25,124: Snapshot:0	Epoch:22	Loss:12.661	translation_Loss:12.308	token_training_loss:0.354	distillation_Loss:0.0                                                           	MRR:24.7	Hits@10:40.31	Best:24.83
2025-01-06 22:26:25,382: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2025-01-06 22:26:28,335: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2395 | 0.155  | 0.2811 | 0.3294 |  0.3907 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,102,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,200,200
Trainable params: 2,800
Non-trainable params: 2,197,400
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:26:43,392: Snapshot:1	Epoch:0	Loss:7.608	translation_Loss:7.069	token_training_loss:0.0	distillation_Loss:0.538                                                   	MRR:20.57	Hits@10:34.49	Best:20.57
2025-01-06 22:26:51,289: Snapshot:1	Epoch:1	Loss:4.484	translation_Loss:3.324	token_training_loss:0.0	distillation_Loss:1.16                                                   	MRR:22.14	Hits@10:36.63	Best:22.14
2025-01-06 22:26:59,168: Snapshot:1	Epoch:2	Loss:3.481	translation_Loss:2.083	token_training_loss:0.0	distillation_Loss:1.398                                                   	MRR:22.63	Hits@10:37.37	Best:22.63
2025-01-06 22:27:07,431: Snapshot:1	Epoch:3	Loss:3.12	translation_Loss:1.64	token_training_loss:0.0	distillation_Loss:1.479                                                   	MRR:22.93	Hits@10:37.88	Best:22.93
2025-01-06 22:27:15,349: Snapshot:1	Epoch:4	Loss:3.005	translation_Loss:1.485	token_training_loss:0.0	distillation_Loss:1.52                                                   	MRR:23.02	Hits@10:37.91	Best:23.02
2025-01-06 22:27:23,771: Snapshot:1	Epoch:5	Loss:2.954	translation_Loss:1.406	token_training_loss:0.0	distillation_Loss:1.548                                                   	MRR:23.04	Hits@10:38.07	Best:23.04
2025-01-06 22:27:31,650: Snapshot:1	Epoch:6	Loss:2.932	translation_Loss:1.369	token_training_loss:0.0	distillation_Loss:1.563                                                   	MRR:23.14	Hits@10:38.03	Best:23.14
2025-01-06 22:27:39,455: Snapshot:1	Epoch:7	Loss:2.904	translation_Loss:1.327	token_training_loss:0.0	distillation_Loss:1.577                                                   	MRR:23.13	Hits@10:38.1	Best:23.14
2025-01-06 22:27:47,658: Snapshot:1	Epoch:8	Loss:2.907	translation_Loss:1.32	token_training_loss:0.0	distillation_Loss:1.588                                                   	MRR:23.03	Hits@10:38.03	Best:23.14
2025-01-06 22:27:55,479: Early Stopping! Snapshot: 1 Epoch: 9 Best Results: 23.14
2025-01-06 22:27:55,479: Start to training tokens! Snapshot: 1 Epoch: 9 Loss:2.9 MRR:23.1 Best Results: 23.14
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:27:55,480: Snapshot:1	Epoch:9	Loss:2.9	translation_Loss:1.309	token_training_loss:0.0	distillation_Loss:1.592                                                   	MRR:23.1	Hits@10:38.12	Best:23.14
2025-01-06 22:28:03,162: Snapshot:1	Epoch:10	Loss:29.531	translation_Loss:13.507	token_training_loss:16.024	distillation_Loss:0.0                                                   	MRR:23.1	Hits@10:38.12	Best:23.14
2025-01-06 22:28:11,255: End of token training: 1 Epoch: 11 Loss:13.834 MRR:23.1 Best Results: 23.14
2025-01-06 22:28:11,256: Snapshot:1	Epoch:11	Loss:13.834	translation_Loss:13.488	token_training_loss:0.347	distillation_Loss:0.0                                                           	MRR:23.1	Hits@10:38.12	Best:23.14
2025-01-06 22:28:11,517: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2025-01-06 22:28:18,025: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2535 | 0.1659 | 0.2934 | 0.3472 |  0.4172 |
|     1      | 0.2316 | 0.149  | 0.2697 | 0.3202 |  0.3835 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,555,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,653,400
Trainable params: 2,800
Non-trainable params: 2,650,600
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:28:33,219: Snapshot:2	Epoch:0	Loss:5.422	translation_Loss:4.208	token_training_loss:0.0	distillation_Loss:1.214                                                   	MRR:21.34	Hits@10:36.71	Best:21.34
2025-01-06 22:28:41,270: Snapshot:2	Epoch:1	Loss:4.469	translation_Loss:3.332	token_training_loss:0.0	distillation_Loss:1.138                                                   	MRR:21.61	Hits@10:36.95	Best:21.61
2025-01-06 22:28:49,759: Snapshot:2	Epoch:2	Loss:4.054	translation_Loss:3.053	token_training_loss:0.0	distillation_Loss:1.001                                                   	MRR:21.8	Hits@10:37.18	Best:21.8
2025-01-06 22:28:57,760: Snapshot:2	Epoch:3	Loss:3.984	translation_Loss:2.97	token_training_loss:0.0	distillation_Loss:1.014                                                   	MRR:21.83	Hits@10:37.32	Best:21.83
2025-01-06 22:29:06,103: Snapshot:2	Epoch:4	Loss:3.964	translation_Loss:2.971	token_training_loss:0.0	distillation_Loss:0.993                                                   	MRR:21.81	Hits@10:37.4	Best:21.83
2025-01-06 22:29:14,094: Snapshot:2	Epoch:5	Loss:3.949	translation_Loss:2.937	token_training_loss:0.0	distillation_Loss:1.012                                                   	MRR:21.83	Hits@10:37.28	Best:21.83
2025-01-06 22:29:22,547: Snapshot:2	Epoch:6	Loss:3.922	translation_Loss:2.924	token_training_loss:0.0	distillation_Loss:0.998                                                   	MRR:21.87	Hits@10:37.23	Best:21.87
2025-01-06 22:29:30,565: Snapshot:2	Epoch:7	Loss:3.932	translation_Loss:2.924	token_training_loss:0.0	distillation_Loss:1.007                                                   	MRR:21.84	Hits@10:37.26	Best:21.87
2025-01-06 22:29:38,545: Snapshot:2	Epoch:8	Loss:3.928	translation_Loss:2.931	token_training_loss:0.0	distillation_Loss:0.996                                                   	MRR:21.86	Hits@10:37.21	Best:21.87
2025-01-06 22:29:47,002: Early Stopping! Snapshot: 2 Epoch: 9 Best Results: 21.87
2025-01-06 22:29:47,002: Start to training tokens! Snapshot: 2 Epoch: 9 Loss:3.934 MRR:21.84 Best Results: 21.87
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:29:47,003: Snapshot:2	Epoch:9	Loss:3.934	translation_Loss:2.922	token_training_loss:0.0	distillation_Loss:1.012                                                   	MRR:21.84	Hits@10:37.28	Best:21.87
2025-01-06 22:29:54,889: Snapshot:2	Epoch:10	Loss:30.27	translation_Loss:14.434	token_training_loss:15.836	distillation_Loss:0.0                                                   	MRR:21.84	Hits@10:37.28	Best:21.87
2025-01-06 22:30:02,751: End of token training: 2 Epoch: 11 Loss:14.768 MRR:21.84 Best Results: 21.87
2025-01-06 22:30:02,752: Snapshot:2	Epoch:11	Loss:14.768	translation_Loss:14.423	token_training_loss:0.345	distillation_Loss:0.0                                                           	MRR:21.84	Hits@10:37.28	Best:21.87
2025-01-06 22:30:03,026: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2025-01-06 22:30:12,972: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1642 | 0.291  | 0.3454 |  0.4182 |
|     1      | 0.2353 | 0.1524 | 0.2712 | 0.3247 |  0.3911 |
|     2      | 0.218  | 0.1349 | 0.2543 | 0.3055 |  0.374  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,717,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,814,800
Trainable params: 2,800
Non-trainable params: 2,812,000
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:30:28,567: Snapshot:3	Epoch:0	Loss:3.868	translation_Loss:2.752	token_training_loss:0.0	distillation_Loss:1.116                                                   	MRR:19.5	Hits@10:37.03	Best:19.5
2025-01-06 22:30:36,671: Snapshot:3	Epoch:1	Loss:3.181	translation_Loss:2.119	token_training_loss:0.0	distillation_Loss:1.062                                                   	MRR:19.71	Hits@10:37.07	Best:19.71
2025-01-06 22:30:44,883: Snapshot:3	Epoch:2	Loss:3.019	translation_Loss:2.063	token_training_loss:0.0	distillation_Loss:0.955                                                   	MRR:19.73	Hits@10:37.28	Best:19.73
2025-01-06 22:30:53,456: Snapshot:3	Epoch:3	Loss:2.976	translation_Loss:2.01	token_training_loss:0.0	distillation_Loss:0.966                                                   	MRR:19.84	Hits@10:37.13	Best:19.84
2025-01-06 22:31:01,507: Snapshot:3	Epoch:4	Loss:2.992	translation_Loss:2.034	token_training_loss:0.0	distillation_Loss:0.958                                                   	MRR:19.75	Hits@10:37.35	Best:19.84
2025-01-06 22:31:10,003: Snapshot:3	Epoch:5	Loss:2.992	translation_Loss:2.02	token_training_loss:0.0	distillation_Loss:0.972                                                   	MRR:19.72	Hits@10:37.27	Best:19.84
2025-01-06 22:31:18,082: Early Stopping! Snapshot: 3 Epoch: 6 Best Results: 19.84
2025-01-06 22:31:18,082: Start to training tokens! Snapshot: 3 Epoch: 6 Loss:2.982 MRR:19.8 Best Results: 19.84
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:31:18,082: Snapshot:3	Epoch:6	Loss:2.982	translation_Loss:2.013	token_training_loss:0.0	distillation_Loss:0.969                                                   	MRR:19.8	Hits@10:37.21	Best:19.84
2025-01-06 22:31:25,983: Snapshot:3	Epoch:7	Loss:30.317	translation_Loss:13.901	token_training_loss:16.416	distillation_Loss:0.0                                                   	MRR:19.8	Hits@10:37.21	Best:19.84
2025-01-06 22:31:34,314: End of token training: 3 Epoch: 8 Loss:14.238 MRR:19.8 Best Results: 19.84
2025-01-06 22:31:34,315: Snapshot:3	Epoch:8	Loss:14.238	translation_Loss:13.884	token_training_loss:0.354	distillation_Loss:0.0                                                           	MRR:19.8	Hits@10:37.21	Best:19.84
2025-01-06 22:31:34,574: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2025-01-06 22:31:47,854: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2474 | 0.1597 | 0.2855 | 0.3399 |  0.4127 |
|     1      | 0.232  | 0.1486 | 0.2677 | 0.3212 |  0.391  |
|     2      | 0.2183 | 0.1336 | 0.2521 | 0.3097 |  0.3849 |
|     3      | 0.1992 | 0.112  | 0.2306 | 0.2902 |  0.3709 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,778,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,876,400
Trainable params: 2,800
Non-trainable params: 2,873,600
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:32:03,248: Snapshot:4	Epoch:0	Loss:2.388	translation_Loss:1.603	token_training_loss:0.0	distillation_Loss:0.785                                                   	MRR:20.36	Hits@10:44.03	Best:20.36
2025-01-06 22:32:11,466: Snapshot:4	Epoch:1	Loss:1.529	translation_Loss:0.856	token_training_loss:0.0	distillation_Loss:0.673                                                   	MRR:20.66	Hits@10:43.15	Best:20.66
2025-01-06 22:32:19,687: Snapshot:4	Epoch:2	Loss:1.351	translation_Loss:0.737	token_training_loss:0.0	distillation_Loss:0.614                                                   	MRR:20.63	Hits@10:43.06	Best:20.66
2025-01-06 22:32:28,223: Snapshot:4	Epoch:3	Loss:1.325	translation_Loss:0.703	token_training_loss:0.0	distillation_Loss:0.621                                                   	MRR:20.69	Hits@10:43.2	Best:20.69
2025-01-06 22:32:36,411: Snapshot:4	Epoch:4	Loss:1.318	translation_Loss:0.703	token_training_loss:0.0	distillation_Loss:0.615                                                   	MRR:20.87	Hits@10:43.39	Best:20.87
2025-01-06 22:32:44,917: Snapshot:4	Epoch:5	Loss:1.325	translation_Loss:0.704	token_training_loss:0.0	distillation_Loss:0.62                                                   	MRR:20.96	Hits@10:43.51	Best:20.96
2025-01-06 22:32:53,102: Snapshot:4	Epoch:6	Loss:1.319	translation_Loss:0.694	token_training_loss:0.0	distillation_Loss:0.625                                                   	MRR:20.77	Hits@10:43.33	Best:20.96
2025-01-06 22:33:01,270: Snapshot:4	Epoch:7	Loss:1.312	translation_Loss:0.697	token_training_loss:0.0	distillation_Loss:0.615                                                   	MRR:21.0	Hits@10:43.33	Best:21.0
2025-01-06 22:33:09,790: Snapshot:4	Epoch:8	Loss:1.312	translation_Loss:0.69	token_training_loss:0.0	distillation_Loss:0.622                                                   	MRR:20.96	Hits@10:43.39	Best:21.0
2025-01-06 22:33:17,889: Snapshot:4	Epoch:9	Loss:1.313	translation_Loss:0.693	token_training_loss:0.0	distillation_Loss:0.619                                                   	MRR:20.89	Hits@10:43.34	Best:21.0
2025-01-06 22:33:26,425: Early Stopping! Snapshot: 4 Epoch: 10 Best Results: 21.0
2025-01-06 22:33:26,425: Start to training tokens! Snapshot: 4 Epoch: 10 Loss:1.319 MRR:20.86 Best Results: 21.0
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:33:26,426: Snapshot:4	Epoch:10	Loss:1.319	translation_Loss:0.694	token_training_loss:0.0	distillation_Loss:0.625                                                   	MRR:20.86	Hits@10:43.44	Best:21.0
2025-01-06 22:33:34,400: Snapshot:4	Epoch:11	Loss:27.368	translation_Loss:11.428	token_training_loss:15.941	distillation_Loss:0.0                                                   	MRR:20.86	Hits@10:43.44	Best:21.0
2025-01-06 22:33:42,351: End of token training: 4 Epoch: 12 Loss:11.803 MRR:20.86 Best Results: 21.0
2025-01-06 22:33:42,352: Snapshot:4	Epoch:12	Loss:11.803	translation_Loss:11.453	token_training_loss:0.349	distillation_Loss:0.0                                                           	MRR:20.86	Hits@10:43.44	Best:21.0
2025-01-06 22:33:42,680: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2025-01-06 22:34:00,322: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2398 | 0.1533 | 0.2758 | 0.3296 |  0.4044 |
|     1      | 0.2226 | 0.1407 | 0.2559 | 0.3083 |  0.3797 |
|     2      | 0.2099 | 0.1253 | 0.2419 | 0.2992 |  0.3774 |
|     3      | 0.1943 | 0.106  | 0.2225 | 0.2848 |  0.3734 |
|     4      | 0.2096 | 0.1003 | 0.2443 | 0.325  |  0.4334 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,800
Trainable params: 2,800
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:34:00,324: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2395 | 0.155  | 0.2811 | 0.3294 |  0.3907 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2535 | 0.1659 | 0.2934 | 0.3472 |  0.4172 |
|     1      | 0.2316 | 0.149  | 0.2697 | 0.3202 |  0.3835 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2522 | 0.1642 | 0.291  | 0.3454 |  0.4182 |
|     1      | 0.2353 | 0.1524 | 0.2712 | 0.3247 |  0.3911 |
|     2      | 0.218  | 0.1349 | 0.2543 | 0.3055 |  0.374  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2474 | 0.1597 | 0.2855 | 0.3399 |  0.4127 |
|     1      | 0.232  | 0.1486 | 0.2677 | 0.3212 |  0.391  |
|     2      | 0.2183 | 0.1336 | 0.2521 | 0.3097 |  0.3849 |
|     3      | 0.1992 | 0.112  | 0.2306 | 0.2902 |  0.3709 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2398 | 0.1533 | 0.2758 | 0.3296 |  0.4044 |
|     1      | 0.2226 | 0.1407 | 0.2559 | 0.3083 |  0.3797 |
|     2      | 0.2099 | 0.1253 | 0.2419 | 0.2992 |  0.3774 |
|     3      | 0.1943 | 0.106  | 0.2225 | 0.2848 |  0.3734 |
|     4      | 0.2096 | 0.1003 | 0.2443 | 0.325  |  0.4334 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:34:00,325: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 174.7249298095703  |   0.239   |    0.155     |    0.281     |     0.391     |
|    1     | 99.64213371276855  |   0.243   |    0.157     |    0.282     |      0.4      |
|    2     | 101.22884941101074 |   0.235   |     0.15     |    0.272     |     0.394     |
|    3     | 77.51088285446167  |   0.224   |    0.138     |    0.259     |      0.39     |
|    4     | 111.20368266105652 |   0.215   |    0.125     |    0.248     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:34:00,325: Sum_Training_Time:564.3104784488678
2025-01-06 22:34:00,325: Every_Training_Time:[174.7249298095703, 99.64213371276855, 101.22884941101074, 77.51088285446167, 111.20368266105652]
2025-01-06 22:34:00,325: Forward transfer: 0.17405 Backward transfer: -0.005424999999999992
2025-01-06 22:34:22,012: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106223405/FACT', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=2222, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[1000.0, 10000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:34:32,858: Snapshot:0	Epoch:0	Loss:17.026	translation_Loss:17.026	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.15	Hits@10:12.86	Best:6.15
2025-01-06 22:34:40,548: Snapshot:0	Epoch:1	Loss:11.984	translation_Loss:11.984	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.71	Hits@10:22.79	Best:9.71
2025-01-06 22:34:47,856: Snapshot:0	Epoch:2	Loss:8.664	translation_Loss:8.664	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:12.96	Hits@10:29.18	Best:12.96
2025-01-06 22:34:55,545: Snapshot:0	Epoch:3	Loss:6.189	translation_Loss:6.189	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.74	Hits@10:33.29	Best:15.74
2025-01-06 22:35:02,800: Snapshot:0	Epoch:4	Loss:4.39	translation_Loss:4.39	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.29	Hits@10:36.0	Best:18.29
2025-01-06 22:35:10,192: Snapshot:0	Epoch:5	Loss:3.114	translation_Loss:3.114	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:20.36	Hits@10:37.96	Best:20.36
2025-01-06 22:35:17,866: Snapshot:0	Epoch:6	Loss:2.228	translation_Loss:2.228	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:21.73	Hits@10:38.92	Best:21.73
2025-01-06 22:35:25,181: Snapshot:0	Epoch:7	Loss:1.579	translation_Loss:1.579	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:22.88	Hits@10:39.67	Best:22.88
2025-01-06 22:35:32,839: Snapshot:0	Epoch:8	Loss:1.133	translation_Loss:1.133	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.56	Hits@10:40.08	Best:23.56
2025-01-06 22:35:40,090: Snapshot:0	Epoch:9	Loss:0.837	translation_Loss:0.837	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.1	Hits@10:40.28	Best:24.1
2025-01-06 22:35:47,362: Snapshot:0	Epoch:10	Loss:0.648	translation_Loss:0.648	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.32	Hits@10:40.48	Best:24.32
2025-01-06 22:35:55,083: Snapshot:0	Epoch:11	Loss:0.516	translation_Loss:0.516	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.44	Hits@10:40.47	Best:24.44
2025-01-06 22:36:02,317: Snapshot:0	Epoch:12	Loss:0.428	translation_Loss:0.428	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.69	Hits@10:40.62	Best:24.69
2025-01-06 22:36:09,974: Snapshot:0	Epoch:13	Loss:0.36	translation_Loss:0.36	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.66	Hits@10:40.61	Best:24.69
2025-01-06 22:36:17,278: Snapshot:0	Epoch:14	Loss:0.314	translation_Loss:0.314	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.87	Hits@10:40.6	Best:24.87
2025-01-06 22:36:24,486: Snapshot:0	Epoch:15	Loss:0.282	translation_Loss:0.282	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.87	Hits@10:40.64	Best:24.87
2025-01-06 22:36:32,073: Snapshot:0	Epoch:16	Loss:0.254	translation_Loss:0.254	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.87	Hits@10:40.5	Best:24.87
2025-01-06 22:36:39,299: Early Stopping! Snapshot: 0 Epoch: 17 Best Results: 24.87
2025-01-06 22:36:39,299: Start to training tokens! Snapshot: 0 Epoch: 17 Loss:0.228 MRR:24.86 Best Results: 24.87
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:36:39,299: Snapshot:0	Epoch:17	Loss:0.228	translation_Loss:0.228	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.86	Hits@10:40.56	Best:24.87
2025-01-06 22:36:47,568: Snapshot:0	Epoch:18	Loss:28.191	translation_Loss:12.311	token_training_loss:15.88	distillation_Loss:0.0                                                   	MRR:24.86	Hits@10:40.56	Best:24.87
2025-01-06 22:36:54,831: End of token training: 0 Epoch: 19 Loss:12.654 MRR:24.86 Best Results: 24.87
2025-01-06 22:36:54,832: Snapshot:0	Epoch:19	Loss:12.654	translation_Loss:12.313	token_training_loss:0.341	distillation_Loss:0.0                                                           	MRR:24.86	Hits@10:40.56	Best:24.87
2025-01-06 22:36:55,101: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2025-01-06 22:36:58,101: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1562 | 0.2843 | 0.3318 |  0.3954 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,102,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,200,200
Trainable params: 2,800
Non-trainable params: 2,197,400
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:37:12,865: Snapshot:1	Epoch:0	Loss:7.927	translation_Loss:7.383	token_training_loss:0.0	distillation_Loss:0.544                                                   	MRR:20.82	Hits@10:34.89	Best:20.82
2025-01-06 22:37:20,885: Snapshot:1	Epoch:1	Loss:4.726	translation_Loss:3.538	token_training_loss:0.0	distillation_Loss:1.188                                                   	MRR:22.43	Hits@10:37.14	Best:22.43
2025-01-06 22:37:29,152: Snapshot:1	Epoch:2	Loss:3.675	translation_Loss:2.214	token_training_loss:0.0	distillation_Loss:1.461                                                   	MRR:23.1	Hits@10:38.07	Best:23.1
2025-01-06 22:37:37,020: Snapshot:1	Epoch:3	Loss:3.308	translation_Loss:1.748	token_training_loss:0.0	distillation_Loss:1.56                                                   	MRR:23.25	Hits@10:38.43	Best:23.25
2025-01-06 22:37:45,285: Snapshot:1	Epoch:4	Loss:3.164	translation_Loss:1.558	token_training_loss:0.0	distillation_Loss:1.606                                                   	MRR:23.34	Hits@10:38.44	Best:23.34
2025-01-06 22:37:53,131: Snapshot:1	Epoch:5	Loss:3.114	translation_Loss:1.481	token_training_loss:0.0	distillation_Loss:1.633                                                   	MRR:23.27	Hits@10:38.59	Best:23.34
2025-01-06 22:38:00,953: Snapshot:1	Epoch:6	Loss:3.09	translation_Loss:1.434	token_training_loss:0.0	distillation_Loss:1.656                                                   	MRR:23.33	Hits@10:38.71	Best:23.34
2025-01-06 22:38:09,270: Snapshot:1	Epoch:7	Loss:3.061	translation_Loss:1.386	token_training_loss:0.0	distillation_Loss:1.675                                                   	MRR:23.43	Hits@10:38.65	Best:23.43
2025-01-06 22:38:17,128: Snapshot:1	Epoch:8	Loss:3.063	translation_Loss:1.379	token_training_loss:0.0	distillation_Loss:1.684                                                   	MRR:23.38	Hits@10:38.59	Best:23.43
2025-01-06 22:38:25,360: Snapshot:1	Epoch:9	Loss:3.061	translation_Loss:1.368	token_training_loss:0.0	distillation_Loss:1.693                                                   	MRR:23.37	Hits@10:38.5	Best:23.43
2025-01-06 22:38:33,208: Snapshot:1	Epoch:10	Loss:3.04	translation_Loss:1.344	token_training_loss:0.0	distillation_Loss:1.696                                                   	MRR:23.45	Hits@10:38.64	Best:23.45
2025-01-06 22:38:41,402: Snapshot:1	Epoch:11	Loss:3.054	translation_Loss:1.343	token_training_loss:0.0	distillation_Loss:1.711                                                   	MRR:23.37	Hits@10:38.3	Best:23.45
2025-01-06 22:38:49,251: Snapshot:1	Epoch:12	Loss:3.038	translation_Loss:1.325	token_training_loss:0.0	distillation_Loss:1.712                                                   	MRR:23.28	Hits@10:38.59	Best:23.45
2025-01-06 22:38:57,084: Early Stopping! Snapshot: 1 Epoch: 13 Best Results: 23.45
2025-01-06 22:38:57,084: Start to training tokens! Snapshot: 1 Epoch: 13 Loss:3.031 MRR:23.25 Best Results: 23.45
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:38:57,084: Snapshot:1	Epoch:13	Loss:3.031	translation_Loss:1.325	token_training_loss:0.0	distillation_Loss:1.706                                                   	MRR:23.25	Hits@10:38.49	Best:23.45
2025-01-06 22:39:05,220: Snapshot:1	Epoch:14	Loss:29.761	translation_Loss:13.467	token_training_loss:16.294	distillation_Loss:0.0                                                   	MRR:23.25	Hits@10:38.49	Best:23.45
2025-01-06 22:39:12,950: End of token training: 1 Epoch: 15 Loss:13.823 MRR:23.25 Best Results: 23.45
2025-01-06 22:39:12,951: Snapshot:1	Epoch:15	Loss:13.823	translation_Loss:13.458	token_training_loss:0.365	distillation_Loss:0.0                                                           	MRR:23.25	Hits@10:38.49	Best:23.45
2025-01-06 22:39:13,254: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2025-01-06 22:39:19,600: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2542 | 0.1644 | 0.2955 | 0.3524 |  0.4241 |
|     1      | 0.2323 | 0.1491 |  0.27  | 0.3204 |  0.3865 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,555,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,653,400
Trainable params: 2,800
Non-trainable params: 2,650,600
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:39:35,004: Snapshot:2	Epoch:0	Loss:5.62	translation_Loss:4.383	token_training_loss:0.0	distillation_Loss:1.237                                                   	MRR:21.52	Hits@10:37.11	Best:21.52
2025-01-06 22:39:43,055: Snapshot:2	Epoch:1	Loss:4.639	translation_Loss:3.482	token_training_loss:0.0	distillation_Loss:1.156                                                   	MRR:21.73	Hits@10:37.65	Best:21.73
2025-01-06 22:39:51,158: Snapshot:2	Epoch:2	Loss:4.246	translation_Loss:3.227	token_training_loss:0.0	distillation_Loss:1.019                                                   	MRR:21.94	Hits@10:37.69	Best:21.94
2025-01-06 22:39:59,563: Snapshot:2	Epoch:3	Loss:4.156	translation_Loss:3.127	token_training_loss:0.0	distillation_Loss:1.028                                                   	MRR:21.92	Hits@10:37.7	Best:21.94
2025-01-06 22:40:07,718: Snapshot:2	Epoch:4	Loss:4.124	translation_Loss:3.112	token_training_loss:0.0	distillation_Loss:1.012                                                   	MRR:21.87	Hits@10:37.75	Best:21.94
2025-01-06 22:40:16,145: Early Stopping! Snapshot: 2 Epoch: 5 Best Results: 21.94
2025-01-06 22:40:16,146: Start to training tokens! Snapshot: 2 Epoch: 5 Loss:4.132 MRR:21.91 Best Results: 21.94
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:40:16,146: Snapshot:2	Epoch:5	Loss:4.132	translation_Loss:3.101	token_training_loss:0.0	distillation_Loss:1.031                                                   	MRR:21.91	Hits@10:37.68	Best:21.94
2025-01-06 22:40:24,097: Snapshot:2	Epoch:6	Loss:31.269	translation_Loss:14.467	token_training_loss:16.802	distillation_Loss:0.0                                                   	MRR:21.91	Hits@10:37.68	Best:21.94
2025-01-06 22:40:31,990: End of token training: 2 Epoch: 7 Loss:14.81 MRR:21.91 Best Results: 21.94
2025-01-06 22:40:31,990: Snapshot:2	Epoch:7	Loss:14.81	translation_Loss:14.454	token_training_loss:0.356	distillation_Loss:0.0                                                           	MRR:21.91	Hits@10:37.68	Best:21.94
2025-01-06 22:40:32,311: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2025-01-06 22:40:42,237: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2534 | 0.1634 | 0.2952 |  0.35  |  0.4225 |
|     1      | 0.235  | 0.1509 | 0.2731 | 0.323  |  0.3917 |
|     2      | 0.2163 | 0.1328 | 0.2529 | 0.3036 |  0.374  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,717,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,814,800
Trainable params: 2,800
Non-trainable params: 2,812,000
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:40:57,497: Snapshot:3	Epoch:0	Loss:4.142	translation_Loss:2.98	token_training_loss:0.0	distillation_Loss:1.162                                                   	MRR:19.59	Hits@10:37.32	Best:19.59
2025-01-06 22:41:05,619: Snapshot:3	Epoch:1	Loss:3.42	translation_Loss:2.291	token_training_loss:0.0	distillation_Loss:1.129                                                   	MRR:19.97	Hits@10:37.36	Best:19.97
2025-01-06 22:41:13,661: Snapshot:3	Epoch:2	Loss:3.246	translation_Loss:2.235	token_training_loss:0.0	distillation_Loss:1.011                                                   	MRR:19.74	Hits@10:37.38	Best:19.97
2025-01-06 22:41:22,247: Snapshot:3	Epoch:3	Loss:3.217	translation_Loss:2.189	token_training_loss:0.0	distillation_Loss:1.028                                                   	MRR:19.86	Hits@10:37.5	Best:19.97
2025-01-06 22:41:30,298: Early Stopping! Snapshot: 3 Epoch: 4 Best Results: 19.97
2025-01-06 22:41:30,298: Start to training tokens! Snapshot: 3 Epoch: 4 Loss:3.209 MRR:19.88 Best Results: 19.97
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:41:30,299: Snapshot:3	Epoch:4	Loss:3.209	translation_Loss:2.194	token_training_loss:0.0	distillation_Loss:1.016                                                   	MRR:19.88	Hits@10:37.61	Best:19.97
2025-01-06 22:41:38,689: Snapshot:3	Epoch:5	Loss:29.947	translation_Loss:13.994	token_training_loss:15.953	distillation_Loss:0.0                                                   	MRR:19.88	Hits@10:37.61	Best:19.97
2025-01-06 22:41:46,643: End of token training: 3 Epoch: 6 Loss:14.332 MRR:19.88 Best Results: 19.97
2025-01-06 22:41:46,644: Snapshot:3	Epoch:6	Loss:14.332	translation_Loss:13.982	token_training_loss:0.35	distillation_Loss:0.0                                                           	MRR:19.88	Hits@10:37.61	Best:19.97
2025-01-06 22:41:46,925: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2025-01-06 22:42:00,010: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2477 | 0.1597 | 0.286  | 0.3422 |  0.4154 |
|     1      | 0.2322 | 0.1483 | 0.2674 | 0.321  |  0.3909 |
|     2      | 0.2161 | 0.1297 | 0.2511 | 0.3108 |  0.3833 |
|     3      | 0.2005 | 0.1124 | 0.2311 | 0.295  |  0.3769 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,778,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,876,400
Trainable params: 2,800
Non-trainable params: 2,873,600
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:42:15,619: Snapshot:4	Epoch:0	Loss:2.574	translation_Loss:1.745	token_training_loss:0.0	distillation_Loss:0.829                                                   	MRR:20.8	Hits@10:45.11	Best:20.8
2025-01-06 22:42:23,793: Snapshot:4	Epoch:1	Loss:1.669	translation_Loss:0.926	token_training_loss:0.0	distillation_Loss:0.743                                                   	MRR:20.88	Hits@10:44.02	Best:20.88
2025-01-06 22:42:31,941: Snapshot:4	Epoch:2	Loss:1.503	translation_Loss:0.827	token_training_loss:0.0	distillation_Loss:0.675                                                   	MRR:21.18	Hits@10:44.6	Best:21.18
2025-01-06 22:42:40,469: Snapshot:4	Epoch:3	Loss:1.466	translation_Loss:0.787	token_training_loss:0.0	distillation_Loss:0.679                                                   	MRR:21.11	Hits@10:44.77	Best:21.18
2025-01-06 22:42:48,597: Snapshot:4	Epoch:4	Loss:1.459	translation_Loss:0.78	token_training_loss:0.0	distillation_Loss:0.679                                                   	MRR:21.18	Hits@10:44.52	Best:21.18
2025-01-06 22:42:56,695: Early Stopping! Snapshot: 4 Epoch: 5 Best Results: 21.18
2025-01-06 22:42:56,696: Start to training tokens! Snapshot: 4 Epoch: 5 Loss:1.465 MRR:21.03 Best Results: 21.18
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:42:56,696: Snapshot:4	Epoch:5	Loss:1.465	translation_Loss:0.78	token_training_loss:0.0	distillation_Loss:0.686                                                   	MRR:21.03	Hits@10:44.37	Best:21.18
2025-01-06 22:43:05,138: Snapshot:4	Epoch:6	Loss:28.292	translation_Loss:11.829	token_training_loss:16.463	distillation_Loss:0.0                                                   	MRR:21.03	Hits@10:44.37	Best:21.18
2025-01-06 22:43:13,110: End of token training: 4 Epoch: 7 Loss:12.175 MRR:21.03 Best Results: 21.18
2025-01-06 22:43:13,110: Snapshot:4	Epoch:7	Loss:12.175	translation_Loss:11.818	token_training_loss:0.357	distillation_Loss:0.0                                                           	MRR:21.03	Hits@10:44.37	Best:21.18
2025-01-06 22:43:13,373: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2025-01-06 22:43:30,591: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1554 | 0.2765 | 0.3312 |  0.4051 |
|     1      | 0.2235 | 0.1413 | 0.2546 | 0.3083 |  0.3816 |
|     2      | 0.2092 | 0.1242 | 0.2405 | 0.2983 |  0.3771 |
|     3      | 0.1973 | 0.1077 | 0.2262 | 0.2893 |  0.3817 |
|     4      | 0.2129 | 0.102  | 0.2472 | 0.3341 |  0.4441 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,800
Trainable params: 2,800
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:43:30,594: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2415 | 0.1562 | 0.2843 | 0.3318 |  0.3954 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2542 | 0.1644 | 0.2955 | 0.3524 |  0.4241 |
|     1      | 0.2323 | 0.1491 |  0.27  | 0.3204 |  0.3865 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2534 | 0.1634 | 0.2952 |  0.35  |  0.4225 |
|     1      | 0.235  | 0.1509 | 0.2731 | 0.323  |  0.3917 |
|     2      | 0.2163 | 0.1328 | 0.2529 | 0.3036 |  0.374  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2477 | 0.1597 | 0.286  | 0.3422 |  0.4154 |
|     1      | 0.2322 | 0.1483 | 0.2674 | 0.321  |  0.3909 |
|     2      | 0.2161 | 0.1297 | 0.2511 | 0.3108 |  0.3833 |
|     3      | 0.2005 | 0.1124 | 0.2311 | 0.295  |  0.3769 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2412 | 0.1554 | 0.2765 | 0.3312 |  0.4051 |
|     1      | 0.2235 | 0.1413 | 0.2546 | 0.3083 |  0.3816 |
|     2      | 0.2092 | 0.1242 | 0.2405 | 0.2983 |  0.3771 |
|     3      | 0.1973 | 0.1077 | 0.2262 | 0.2893 |  0.3817 |
|     4      | 0.2129 | 0.102  | 0.2472 | 0.3341 |  0.4441 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:43:30,594: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 152.81859755516052 |   0.241   |    0.156     |    0.284     |     0.395     |
|    1     | 131.55803608894348 |   0.243   |    0.157     |    0.283     |     0.405     |
|    2     |  68.9437882900238  |   0.235   |    0.149     |    0.274     |     0.396     |
|    3     | 60.87807750701904  |   0.224   |    0.138     |    0.259     |     0.392     |
|    4     | 69.55363917350769  |   0.217   |    0.126     |    0.249     |     0.398     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:43:30,595: Sum_Training_Time:483.75213861465454
2025-01-06 22:43:30,595: Every_Training_Time:[152.81859755516052, 131.55803608894348, 68.9437882900238, 60.87807750701904, 69.55363917350769]
2025-01-06 22:43:30,595: Forward transfer: 0.1749 Backward transfer: -0.00485
2025-01-06 22:43:52,230: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106224335/FACT', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3333, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[1000.0, 10000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:44:03,125: Snapshot:0	Epoch:0	Loss:17.027	translation_Loss:17.027	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.27	Hits@10:13.28	Best:6.27
2025-01-06 22:44:10,815: Snapshot:0	Epoch:1	Loss:11.966	translation_Loss:11.966	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.65	Hits@10:22.74	Best:9.65
2025-01-06 22:44:18,300: Snapshot:0	Epoch:2	Loss:8.641	translation_Loss:8.641	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:12.95	Hits@10:29.29	Best:12.95
2025-01-06 22:44:26,088: Snapshot:0	Epoch:3	Loss:6.197	translation_Loss:6.197	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.84	Hits@10:33.44	Best:15.84
2025-01-06 22:44:33,343: Snapshot:0	Epoch:4	Loss:4.418	translation_Loss:4.418	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.35	Hits@10:36.34	Best:18.35
2025-01-06 22:44:40,605: Snapshot:0	Epoch:5	Loss:3.152	translation_Loss:3.152	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:20.4	Hits@10:37.98	Best:20.4
2025-01-06 22:44:48,283: Snapshot:0	Epoch:6	Loss:2.248	translation_Loss:2.248	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:21.84	Hits@10:38.86	Best:21.84
2025-01-06 22:44:55,576: Snapshot:0	Epoch:7	Loss:1.592	translation_Loss:1.592	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:22.95	Hits@10:39.52	Best:22.95
2025-01-06 22:45:03,236: Snapshot:0	Epoch:8	Loss:1.154	translation_Loss:1.154	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.7	Hits@10:39.93	Best:23.7
2025-01-06 22:45:10,579: Snapshot:0	Epoch:9	Loss:0.855	translation_Loss:0.855	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.13	Hits@10:40.11	Best:24.13
2025-01-06 22:45:17,906: Snapshot:0	Epoch:10	Loss:0.663	translation_Loss:0.663	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.43	Hits@10:40.26	Best:24.43
2025-01-06 22:45:25,671: Snapshot:0	Epoch:11	Loss:0.528	translation_Loss:0.528	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.59	Hits@10:40.25	Best:24.59
2025-01-06 22:45:32,922: Snapshot:0	Epoch:12	Loss:0.433	translation_Loss:0.433	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.76	Hits@10:40.41	Best:24.76
2025-01-06 22:45:40,546: Snapshot:0	Epoch:13	Loss:0.366	translation_Loss:0.366	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.92	Hits@10:40.33	Best:24.92
2025-01-06 22:45:47,824: Snapshot:0	Epoch:14	Loss:0.317	translation_Loss:0.317	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.91	Hits@10:40.31	Best:24.92
2025-01-06 22:45:55,070: Snapshot:0	Epoch:15	Loss:0.281	translation_Loss:0.281	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.89	Hits@10:40.33	Best:24.92
2025-01-06 22:46:02,657: Early Stopping! Snapshot: 0 Epoch: 16 Best Results: 24.92
2025-01-06 22:46:02,657: Start to training tokens! Snapshot: 0 Epoch: 16 Loss:0.254 MRR:24.88 Best Results: 24.92
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:46:02,658: Snapshot:0	Epoch:16	Loss:0.254	translation_Loss:0.254	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.88	Hits@10:40.37	Best:24.92
2025-01-06 22:46:10,484: Snapshot:0	Epoch:17	Loss:29.387	translation_Loss:12.233	token_training_loss:17.154	distillation_Loss:0.0                                                   	MRR:24.88	Hits@10:40.37	Best:24.92
2025-01-06 22:46:18,307: End of token training: 0 Epoch: 18 Loss:12.607 MRR:24.88 Best Results: 24.92
2025-01-06 22:46:18,307: Snapshot:0	Epoch:18	Loss:12.607	translation_Loss:12.234	token_training_loss:0.373	distillation_Loss:0.0                                                           	MRR:24.88	Hits@10:40.37	Best:24.92
2025-01-06 22:46:18,563: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2025-01-06 22:46:21,155: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2376 | 0.1519 | 0.2788 | 0.3298 |  0.395  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,102,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,200,200
Trainable params: 2,800
Non-trainable params: 2,197,400
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:46:36,277: Snapshot:1	Epoch:0	Loss:8.075	translation_Loss:7.527	token_training_loss:0.0	distillation_Loss:0.548                                                   	MRR:20.48	Hits@10:34.68	Best:20.48
2025-01-06 22:46:44,150: Snapshot:1	Epoch:1	Loss:4.828	translation_Loss:3.626	token_training_loss:0.0	distillation_Loss:1.202                                                   	MRR:22.1	Hits@10:36.88	Best:22.1
2025-01-06 22:46:52,132: Snapshot:1	Epoch:2	Loss:3.765	translation_Loss:2.28	token_training_loss:0.0	distillation_Loss:1.484                                                   	MRR:22.73	Hits@10:37.92	Best:22.73
2025-01-06 22:47:00,478: Snapshot:1	Epoch:3	Loss:3.393	translation_Loss:1.806	token_training_loss:0.0	distillation_Loss:1.587                                                   	MRR:23.11	Hits@10:38.25	Best:23.11
2025-01-06 22:47:08,418: Snapshot:1	Epoch:4	Loss:3.251	translation_Loss:1.607	token_training_loss:0.0	distillation_Loss:1.644                                                   	MRR:23.12	Hits@10:38.46	Best:23.12
2025-01-06 22:47:16,651: Snapshot:1	Epoch:5	Loss:3.207	translation_Loss:1.533	token_training_loss:0.0	distillation_Loss:1.675                                                   	MRR:23.1	Hits@10:38.55	Best:23.12
2025-01-06 22:47:24,572: Snapshot:1	Epoch:6	Loss:3.168	translation_Loss:1.465	token_training_loss:0.0	distillation_Loss:1.702                                                   	MRR:23.05	Hits@10:38.48	Best:23.12
2025-01-06 22:47:32,449: Snapshot:1	Epoch:7	Loss:3.151	translation_Loss:1.439	token_training_loss:0.0	distillation_Loss:1.712                                                   	MRR:23.15	Hits@10:38.49	Best:23.15
2025-01-06 22:47:40,750: Snapshot:1	Epoch:8	Loss:3.143	translation_Loss:1.419	token_training_loss:0.0	distillation_Loss:1.724                                                   	MRR:23.07	Hits@10:38.35	Best:23.15
2025-01-06 22:47:48,699: Snapshot:1	Epoch:9	Loss:3.129	translation_Loss:1.392	token_training_loss:0.0	distillation_Loss:1.737                                                   	MRR:23.27	Hits@10:38.45	Best:23.27
2025-01-06 22:47:56,943: Snapshot:1	Epoch:10	Loss:3.118	translation_Loss:1.379	token_training_loss:0.0	distillation_Loss:1.738                                                   	MRR:23.21	Hits@10:38.64	Best:23.27
2025-01-06 22:48:04,772: Snapshot:1	Epoch:11	Loss:3.124	translation_Loss:1.376	token_training_loss:0.0	distillation_Loss:1.748                                                   	MRR:23.12	Hits@10:38.61	Best:23.27
2025-01-06 22:48:12,605: Early Stopping! Snapshot: 1 Epoch: 12 Best Results: 23.27
2025-01-06 22:48:12,605: Start to training tokens! Snapshot: 1 Epoch: 12 Loss:3.115 MRR:23.27 Best Results: 23.27
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:48:12,605: Snapshot:1	Epoch:12	Loss:3.115	translation_Loss:1.362	token_training_loss:0.0	distillation_Loss:1.754                                                   	MRR:23.27	Hits@10:38.38	Best:23.27
2025-01-06 22:48:20,726: Snapshot:1	Epoch:13	Loss:29.484	translation_Loss:13.327	token_training_loss:16.158	distillation_Loss:0.0                                                   	MRR:23.27	Hits@10:38.38	Best:23.27
2025-01-06 22:48:28,431: End of token training: 1 Epoch: 14 Loss:13.661 MRR:23.27 Best Results: 23.27
2025-01-06 22:48:28,431: Snapshot:1	Epoch:14	Loss:13.661	translation_Loss:13.324	token_training_loss:0.337	distillation_Loss:0.0                                                           	MRR:23.27	Hits@10:38.38	Best:23.27
2025-01-06 22:48:28,701: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2025-01-06 22:48:35,237: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2481 | 0.1577 | 0.2891 | 0.3464 |  0.4176 |
|     1      | 0.2314 | 0.1474 | 0.272  | 0.321  |  0.3856 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,555,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,653,400
Trainable params: 2,800
Non-trainable params: 2,650,600
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:48:50,520: Snapshot:2	Epoch:0	Loss:5.701	translation_Loss:4.449	token_training_loss:0.0	distillation_Loss:1.252                                                   	MRR:21.36	Hits@10:37.1	Best:21.36
2025-01-06 22:48:58,611: Snapshot:2	Epoch:1	Loss:4.729	translation_Loss:3.549	token_training_loss:0.0	distillation_Loss:1.18                                                   	MRR:21.64	Hits@10:37.28	Best:21.64
2025-01-06 22:49:06,754: Snapshot:2	Epoch:2	Loss:4.332	translation_Loss:3.293	token_training_loss:0.0	distillation_Loss:1.039                                                   	MRR:21.83	Hits@10:37.55	Best:21.83
2025-01-06 22:49:15,167: Snapshot:2	Epoch:3	Loss:4.243	translation_Loss:3.195	token_training_loss:0.0	distillation_Loss:1.048                                                   	MRR:21.78	Hits@10:37.56	Best:21.83
2025-01-06 22:49:23,224: Snapshot:2	Epoch:4	Loss:4.203	translation_Loss:3.171	token_training_loss:0.0	distillation_Loss:1.032                                                   	MRR:21.72	Hits@10:37.47	Best:21.83
2025-01-06 22:49:31,568: Early Stopping! Snapshot: 2 Epoch: 5 Best Results: 21.83
2025-01-06 22:49:31,568: Start to training tokens! Snapshot: 2 Epoch: 5 Loss:4.211 MRR:21.82 Best Results: 21.83
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:49:31,568: Snapshot:2	Epoch:5	Loss:4.211	translation_Loss:3.163	token_training_loss:0.0	distillation_Loss:1.047                                                   	MRR:21.82	Hits@10:37.62	Best:21.83
2025-01-06 22:49:39,443: Snapshot:2	Epoch:6	Loss:31.045	translation_Loss:14.358	token_training_loss:16.687	distillation_Loss:0.0                                                   	MRR:21.82	Hits@10:37.62	Best:21.83
2025-01-06 22:49:47,361: End of token training: 2 Epoch: 7 Loss:14.685 MRR:21.82 Best Results: 21.83
2025-01-06 22:49:47,362: Snapshot:2	Epoch:7	Loss:14.685	translation_Loss:14.337	token_training_loss:0.348	distillation_Loss:0.0                                                           	MRR:21.82	Hits@10:37.62	Best:21.83
2025-01-06 22:49:47,639: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2025-01-06 22:49:57,625: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1583 | 0.2874 | 0.343  |  0.4172 |
|     1      | 0.2346 |  0.15  | 0.2723 | 0.3261 |  0.3946 |
|     2      | 0.2158 | 0.1329 | 0.2493 | 0.304  |  0.3737 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,717,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,814,800
Trainable params: 2,800
Non-trainable params: 2,812,000
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:50:13,149: Snapshot:3	Epoch:0	Loss:4.198	translation_Loss:3.024	token_training_loss:0.0	distillation_Loss:1.174                                                   	MRR:19.55	Hits@10:37.05	Best:19.55
2025-01-06 22:50:21,369: Snapshot:3	Epoch:1	Loss:3.462	translation_Loss:2.31	token_training_loss:0.0	distillation_Loss:1.151                                                   	MRR:19.58	Hits@10:37.14	Best:19.58
2025-01-06 22:50:29,566: Snapshot:3	Epoch:2	Loss:3.287	translation_Loss:2.268	token_training_loss:0.0	distillation_Loss:1.02                                                   	MRR:19.72	Hits@10:37.23	Best:19.72
2025-01-06 22:50:38,023: Snapshot:3	Epoch:3	Loss:3.269	translation_Loss:2.218	token_training_loss:0.0	distillation_Loss:1.051                                                   	MRR:19.68	Hits@10:37.09	Best:19.72
2025-01-06 22:50:46,138: Snapshot:3	Epoch:4	Loss:3.247	translation_Loss:2.219	token_training_loss:0.0	distillation_Loss:1.029                                                   	MRR:19.76	Hits@10:37.24	Best:19.76
2025-01-06 22:50:54,694: Snapshot:3	Epoch:5	Loss:3.252	translation_Loss:2.203	token_training_loss:0.0	distillation_Loss:1.049                                                   	MRR:19.78	Hits@10:37.15	Best:19.78
2025-01-06 22:51:02,804: Snapshot:3	Epoch:6	Loss:3.263	translation_Loss:2.22	token_training_loss:0.0	distillation_Loss:1.043                                                   	MRR:19.71	Hits@10:37.14	Best:19.78
2025-01-06 22:51:10,914: Snapshot:3	Epoch:7	Loss:3.266	translation_Loss:2.215	token_training_loss:0.0	distillation_Loss:1.051                                                   	MRR:19.67	Hits@10:37.18	Best:19.78
2025-01-06 22:51:19,477: Early Stopping! Snapshot: 3 Epoch: 8 Best Results: 19.78
2025-01-06 22:51:19,477: Start to training tokens! Snapshot: 3 Epoch: 8 Loss:3.264 MRR:19.73 Best Results: 19.78
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:51:19,477: Snapshot:3	Epoch:8	Loss:3.264	translation_Loss:2.215	token_training_loss:0.0	distillation_Loss:1.049                                                   	MRR:19.73	Hits@10:37.24	Best:19.78
2025-01-06 22:51:27,447: Snapshot:3	Epoch:9	Loss:29.578	translation_Loss:13.76	token_training_loss:15.818	distillation_Loss:0.0                                                   	MRR:19.73	Hits@10:37.24	Best:19.78
2025-01-06 22:51:35,875: End of token training: 3 Epoch: 10 Loss:14.104 MRR:19.73 Best Results: 19.78
2025-01-06 22:51:35,875: Snapshot:3	Epoch:10	Loss:14.104	translation_Loss:13.766	token_training_loss:0.338	distillation_Loss:0.0                                                           	MRR:19.73	Hits@10:37.24	Best:19.78
2025-01-06 22:51:36,164: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2025-01-06 22:51:49,932: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.242  | 0.1527 | 0.2792 | 0.3366 |  0.4132 |
|     1      | 0.2297 | 0.1447 | 0.2667 | 0.3201 |  0.392  |
|     2      | 0.2143 | 0.1293 | 0.2484 | 0.3043 |  0.3809 |
|     3      | 0.2005 | 0.1129 | 0.2326 | 0.2895 |  0.3725 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,778,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,876,400
Trainable params: 2,800
Non-trainable params: 2,873,600
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:52:05,433: Snapshot:4	Epoch:0	Loss:2.547	translation_Loss:1.723	token_training_loss:0.0	distillation_Loss:0.824                                                   	MRR:20.69	Hits@10:44.68	Best:20.69
2025-01-06 22:52:13,628: Snapshot:4	Epoch:1	Loss:1.659	translation_Loss:0.926	token_training_loss:0.0	distillation_Loss:0.733                                                   	MRR:21.08	Hits@10:44.21	Best:21.08
2025-01-06 22:52:21,905: Snapshot:4	Epoch:2	Loss:1.473	translation_Loss:0.811	token_training_loss:0.0	distillation_Loss:0.662                                                   	MRR:21.18	Hits@10:44.06	Best:21.18
2025-01-06 22:52:30,586: Snapshot:4	Epoch:3	Loss:1.442	translation_Loss:0.776	token_training_loss:0.0	distillation_Loss:0.666                                                   	MRR:21.4	Hits@10:44.34	Best:21.4
2025-01-06 22:52:38,702: Snapshot:4	Epoch:4	Loss:1.433	translation_Loss:0.77	token_training_loss:0.0	distillation_Loss:0.663                                                   	MRR:21.23	Hits@10:44.32	Best:21.4
2025-01-06 22:52:46,792: Snapshot:4	Epoch:5	Loss:1.432	translation_Loss:0.765	token_training_loss:0.0	distillation_Loss:0.666                                                   	MRR:21.33	Hits@10:44.61	Best:21.4
2025-01-06 22:52:55,247: Early Stopping! Snapshot: 4 Epoch: 6 Best Results: 21.4
2025-01-06 22:52:55,247: Start to training tokens! Snapshot: 4 Epoch: 6 Loss:1.423 MRR:21.12 Best Results: 21.4
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:52:55,248: Snapshot:4	Epoch:6	Loss:1.423	translation_Loss:0.76	token_training_loss:0.0	distillation_Loss:0.662                                                   	MRR:21.12	Hits@10:44.32	Best:21.4
2025-01-06 22:53:03,229: Snapshot:4	Epoch:7	Loss:27.045	translation_Loss:11.475	token_training_loss:15.57	distillation_Loss:0.0                                                   	MRR:21.12	Hits@10:44.32	Best:21.4
2025-01-06 22:53:11,705: End of token training: 4 Epoch: 8 Loss:11.801 MRR:21.12 Best Results: 21.4
2025-01-06 22:53:11,719: Snapshot:4	Epoch:8	Loss:11.801	translation_Loss:11.467	token_training_loss:0.334	distillation_Loss:0.0                                                           	MRR:21.12	Hits@10:44.32	Best:21.4
2025-01-06 22:53:11,981: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2025-01-06 22:53:29,185: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2367 |  0.15  | 0.2728 | 0.3271 |  0.4026 |
|     1      | 0.223  | 0.1401 | 0.2576 | 0.3079 |  0.3814 |
|     2      | 0.2088 | 0.1259 | 0.2388 | 0.2942 |  0.3734 |
|     3      | 0.1962 | 0.1079 | 0.2254 | 0.2874 |  0.3767 |
|     4      | 0.2133 | 0.1044 | 0.2472 | 0.329  |   0.44  |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,800
Trainable params: 2,800
Non-trainable params: 3,003,000
=================================================================
2025-01-06 22:53:29,188: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2376 | 0.1519 | 0.2788 | 0.3298 |  0.395  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2481 | 0.1577 | 0.2891 | 0.3464 |  0.4176 |
|     1      | 0.2314 | 0.1474 | 0.272  | 0.321  |  0.3856 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2478 | 0.1583 | 0.2874 | 0.343  |  0.4172 |
|     1      | 0.2346 |  0.15  | 0.2723 | 0.3261 |  0.3946 |
|     2      | 0.2158 | 0.1329 | 0.2493 | 0.304  |  0.3737 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.242  | 0.1527 | 0.2792 | 0.3366 |  0.4132 |
|     1      | 0.2297 | 0.1447 | 0.2667 | 0.3201 |  0.392  |
|     2      | 0.2143 | 0.1293 | 0.2484 | 0.3043 |  0.3809 |
|     3      | 0.2005 | 0.1129 | 0.2326 | 0.2895 |  0.3725 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2367 |  0.15  | 0.2728 | 0.3271 |  0.4026 |
|     1      | 0.223  | 0.1401 | 0.2576 | 0.3079 |  0.3814 |
|     2      | 0.2088 | 0.1259 | 0.2388 | 0.2942 |  0.3734 |
|     3      | 0.1962 | 0.1079 | 0.2254 | 0.2874 |  0.3767 |
|     4      | 0.2133 | 0.1044 | 0.2472 | 0.329  |   0.44  |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 22:53:29,188: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 146.07658576965332 |   0.238   |    0.152     |    0.279     |     0.395     |
|    1     | 123.6829252243042  |    0.24   |    0.153     |    0.281     |     0.402     |
|    2     | 68.94519090652466  |   0.233   |    0.147     |     0.27     |     0.395     |
|    3     | 94.71042251586914  |   0.222   |    0.135     |    0.257     |      0.39     |
|    4     | 78.16188263893127  |   0.216   |    0.126     |    0.248     |     0.395     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 22:53:29,188: Sum_Training_Time:511.5770070552826
2025-01-06 22:53:29,188: Every_Training_Time:[146.07658576965332, 123.6829252243042, 68.94519090652466, 94.71042251586914, 78.16188263893127]
2025-01-06 22:53:29,188: Forward transfer: 0.17445000000000002 Backward transfer: -0.005149999999999995
2025-01-06 22:53:51,240: Namespace(batch_size='3072', contrast_loss_weight=0.1, data_path='./data/FACT/', dataset='FACT', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_training=True, gpu=0, l2=0.0, learning_rate=0.001, lifelong_name='double_tokened', log_path='./logs/20250106225334/FACT', logger=<RootLogger root (INFO)>, margin=8.0, multi_distill_num=3, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=4444, record=False, reply_loss_weight=0.1, save_path='./checkpoint/FACT', score_distill_weight=1, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=[1000.0, 10000.0, 10000.0, 10000.0], token_num=7, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2025-01-06 22:54:02,141: Snapshot:0	Epoch:0	Loss:17.021	translation_Loss:17.021	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:6.31	Hits@10:13.37	Best:6.31
2025-01-06 22:54:09,884: Snapshot:0	Epoch:1	Loss:11.999	translation_Loss:11.999	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:9.71	Hits@10:22.64	Best:9.71
2025-01-06 22:54:17,193: Snapshot:0	Epoch:2	Loss:8.696	translation_Loss:8.696	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:12.88	Hits@10:28.86	Best:12.88
2025-01-06 22:54:24,987: Snapshot:0	Epoch:3	Loss:6.24	translation_Loss:6.24	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:15.71	Hits@10:33.2	Best:15.71
2025-01-06 22:54:32,282: Snapshot:0	Epoch:4	Loss:4.439	translation_Loss:4.439	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:18.21	Hits@10:35.87	Best:18.21
2025-01-06 22:54:39,621: Snapshot:0	Epoch:5	Loss:3.148	translation_Loss:3.148	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:20.29	Hits@10:37.6	Best:20.29
2025-01-06 22:54:47,307: Snapshot:0	Epoch:6	Loss:2.226	translation_Loss:2.226	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:21.8	Hits@10:38.8	Best:21.8
2025-01-06 22:54:54,613: Snapshot:0	Epoch:7	Loss:1.591	translation_Loss:1.591	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:22.86	Hits@10:39.4	Best:22.86
2025-01-06 22:55:02,290: Snapshot:0	Epoch:8	Loss:1.134	translation_Loss:1.134	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:23.6	Hits@10:39.77	Best:23.6
2025-01-06 22:55:09,742: Snapshot:0	Epoch:9	Loss:0.833	translation_Loss:0.833	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.16	Hits@10:40.0	Best:24.16
2025-01-06 22:55:17,084: Snapshot:0	Epoch:10	Loss:0.645	translation_Loss:0.645	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.38	Hits@10:40.17	Best:24.38
2025-01-06 22:55:24,896: Snapshot:0	Epoch:11	Loss:0.507	translation_Loss:0.507	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.54	Hits@10:40.13	Best:24.54
2025-01-06 22:55:32,226: Snapshot:0	Epoch:12	Loss:0.42	translation_Loss:0.42	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.71	Hits@10:40.23	Best:24.71
2025-01-06 22:55:39,970: Snapshot:0	Epoch:13	Loss:0.353	translation_Loss:0.353	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.86	Hits@10:40.32	Best:24.86
2025-01-06 22:55:47,499: Snapshot:0	Epoch:14	Loss:0.309	translation_Loss:0.309	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.87	Hits@10:40.31	Best:24.87
2025-01-06 22:55:54,935: Snapshot:0	Epoch:15	Loss:0.277	translation_Loss:0.277	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.84	Hits@10:40.36	Best:24.87
2025-01-06 22:56:02,641: Snapshot:0	Epoch:16	Loss:0.249	translation_Loss:0.249	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.88	Hits@10:40.25	Best:24.88
2025-01-06 22:56:09,985: Snapshot:0	Epoch:17	Loss:0.224	translation_Loss:0.224	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.89	Hits@10:40.27	Best:24.89
2025-01-06 22:56:17,722: Snapshot:0	Epoch:18	Loss:0.211	translation_Loss:0.211	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.93	Hits@10:40.25	Best:24.93
2025-01-06 22:56:25,108: Snapshot:0	Epoch:19	Loss:0.193	translation_Loss:0.193	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:25.03	Hits@10:40.31	Best:25.03
2025-01-06 22:56:32,382: Snapshot:0	Epoch:20	Loss:0.179	translation_Loss:0.179	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.95	Hits@10:40.18	Best:25.03
2025-01-06 22:56:40,068: Snapshot:0	Epoch:21	Loss:0.167	translation_Loss:0.167	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.84	Hits@10:40.24	Best:25.03
2025-01-06 22:56:47,320: Early Stopping! Snapshot: 0 Epoch: 22 Best Results: 25.03
2025-01-06 22:56:47,320: Start to training tokens! Snapshot: 0 Epoch: 22 Loss:0.16 MRR:24.94 Best Results: 25.03
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:56:47,321: Snapshot:0	Epoch:22	Loss:0.16	translation_Loss:0.16	token_training_loss:0.0	distillation_Loss:0.0                                                   	MRR:24.94	Hits@10:40.25	Best:25.03
2025-01-06 22:56:55,550: Snapshot:0	Epoch:23	Loss:27.658	translation_Loss:12.205	token_training_loss:15.453	distillation_Loss:0.0                                                   	MRR:24.94	Hits@10:40.25	Best:25.03
2025-01-06 22:57:02,852: End of token training: 0 Epoch: 24 Loss:12.512 MRR:24.94 Best Results: 25.03
2025-01-06 22:57:02,852: Snapshot:0	Epoch:24	Loss:12.512	translation_Loss:12.193	token_training_loss:0.319	distillation_Loss:0.0                                                           	MRR:24.94	Hits@10:40.25	Best:25.03
2025-01-06 22:57:03,119: => loading checkpoint './checkpoint/FACT/0model_best.tar'
2025-01-06 22:57:06,009: 
+------------+------+--------+--------+--------+---------+
| Snapshot:0 | MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+------+--------+--------+--------+---------+
|     0      | 0.24 | 0.1562 |  0.28  | 0.3306 |  0.3912 |
+------------+------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,102,600)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,200,200
Trainable params: 2,800
Non-trainable params: 2,197,400
=================================================================
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:57:20,879: Snapshot:1	Epoch:0	Loss:7.477	translation_Loss:6.945	token_training_loss:0.0	distillation_Loss:0.533                                                   	MRR:20.52	Hits@10:34.51	Best:20.52
2025-01-06 22:57:29,207: Snapshot:1	Epoch:1	Loss:4.372	translation_Loss:3.229	token_training_loss:0.0	distillation_Loss:1.143                                                   	MRR:22.07	Hits@10:36.57	Best:22.07
2025-01-06 22:57:37,176: Snapshot:1	Epoch:2	Loss:3.384	translation_Loss:2.013	token_training_loss:0.0	distillation_Loss:1.371                                                   	MRR:22.68	Hits@10:37.48	Best:22.68
2025-01-06 22:57:45,144: Snapshot:1	Epoch:3	Loss:3.044	translation_Loss:1.597	token_training_loss:0.0	distillation_Loss:1.447                                                   	MRR:22.83	Hits@10:37.87	Best:22.83
2025-01-06 22:57:53,586: Snapshot:1	Epoch:4	Loss:2.927	translation_Loss:1.445	token_training_loss:0.0	distillation_Loss:1.481                                                   	MRR:22.87	Hits@10:37.96	Best:22.87
2025-01-06 22:58:01,514: Snapshot:1	Epoch:5	Loss:2.885	translation_Loss:1.378	token_training_loss:0.0	distillation_Loss:1.507                                                   	MRR:22.97	Hits@10:37.96	Best:22.97
2025-01-06 22:58:09,838: Snapshot:1	Epoch:6	Loss:2.859	translation_Loss:1.338	token_training_loss:0.0	distillation_Loss:1.521                                                   	MRR:22.92	Hits@10:37.96	Best:22.97
2025-01-06 22:58:17,785: Snapshot:1	Epoch:7	Loss:2.855	translation_Loss:1.322	token_training_loss:0.0	distillation_Loss:1.533                                                   	MRR:23.07	Hits@10:37.89	Best:23.07
2025-01-06 22:58:25,741: Snapshot:1	Epoch:8	Loss:2.848	translation_Loss:1.3	token_training_loss:0.0	distillation_Loss:1.548                                                   	MRR:23.13	Hits@10:37.96	Best:23.13
2025-01-06 22:58:34,004: Snapshot:1	Epoch:9	Loss:2.835	translation_Loss:1.281	token_training_loss:0.0	distillation_Loss:1.554                                                   	MRR:23.03	Hits@10:38.02	Best:23.13
2025-01-06 22:58:41,860: Snapshot:1	Epoch:10	Loss:2.829	translation_Loss:1.274	token_training_loss:0.0	distillation_Loss:1.555                                                   	MRR:22.94	Hits@10:37.83	Best:23.13
2025-01-06 22:58:50,153: Early Stopping! Snapshot: 1 Epoch: 11 Best Results: 23.13
2025-01-06 22:58:50,153: Start to training tokens! Snapshot: 1 Epoch: 11 Loss:2.824 MRR:22.96 Best Results: 23.13
Token added to optimizer, embeddings excluded successfully.
2025-01-06 22:58:50,153: Snapshot:1	Epoch:11	Loss:2.824	translation_Loss:1.266	token_training_loss:0.0	distillation_Loss:1.558                                                   	MRR:22.96	Hits@10:37.95	Best:23.13
2025-01-06 22:58:57,922: Snapshot:1	Epoch:12	Loss:30.36	translation_Loss:13.474	token_training_loss:16.886	distillation_Loss:0.0                                                   	MRR:22.96	Hits@10:37.95	Best:23.13
2025-01-06 22:59:05,684: End of token training: 1 Epoch: 13 Loss:13.86 MRR:22.96 Best Results: 23.13
2025-01-06 22:59:05,685: Snapshot:1	Epoch:13	Loss:13.86	translation_Loss:13.486	token_training_loss:0.374	distillation_Loss:0.0                                                           	MRR:22.96	Hits@10:37.95	Best:23.13
2025-01-06 22:59:06,010: => loading checkpoint './checkpoint/FACT/1model_best.tar'
2025-01-06 22:59:12,228: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2531 | 0.1642 | 0.2933 | 0.3494 |  0.4159 |
|     1      | 0.2311 | 0.1487 | 0.2686 | 0.3204 |  0.3839 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,555,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,653,400
Trainable params: 2,800
Non-trainable params: 2,650,600
=================================================================
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 22:59:27,782: Snapshot:2	Epoch:0	Loss:5.259	translation_Loss:4.082	token_training_loss:0.0	distillation_Loss:1.178                                                   	MRR:21.22	Hits@10:36.61	Best:21.22
2025-01-06 22:59:36,007: Snapshot:2	Epoch:1	Loss:4.336	translation_Loss:3.238	token_training_loss:0.0	distillation_Loss:1.098                                                   	MRR:21.62	Hits@10:36.99	Best:21.62
2025-01-06 22:59:44,186: Snapshot:2	Epoch:2	Loss:3.931	translation_Loss:2.969	token_training_loss:0.0	distillation_Loss:0.962                                                   	MRR:21.72	Hits@10:37.23	Best:21.72
2025-01-06 22:59:52,692: Snapshot:2	Epoch:3	Loss:3.846	translation_Loss:2.871	token_training_loss:0.0	distillation_Loss:0.975                                                   	MRR:21.87	Hits@10:37.3	Best:21.87
2025-01-06 23:00:00,732: Snapshot:2	Epoch:4	Loss:3.828	translation_Loss:2.868	token_training_loss:0.0	distillation_Loss:0.96                                                   	MRR:21.7	Hits@10:37.32	Best:21.87
2025-01-06 23:00:09,044: Snapshot:2	Epoch:5	Loss:3.806	translation_Loss:2.842	token_training_loss:0.0	distillation_Loss:0.964                                                   	MRR:21.71	Hits@10:37.31	Best:21.87
2025-01-06 23:00:17,543: Early Stopping! Snapshot: 2 Epoch: 6 Best Results: 21.87
2025-01-06 23:00:17,543: Start to training tokens! Snapshot: 2 Epoch: 6 Loss:3.81 MRR:21.85 Best Results: 21.87
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:00:17,543: Snapshot:2	Epoch:6	Loss:3.81	translation_Loss:2.853	token_training_loss:0.0	distillation_Loss:0.957                                                   	MRR:21.85	Hits@10:37.26	Best:21.87
2025-01-06 23:00:25,435: Snapshot:2	Epoch:7	Loss:30.692	translation_Loss:14.357	token_training_loss:16.335	distillation_Loss:0.0                                                   	MRR:21.85	Hits@10:37.26	Best:21.87
2025-01-06 23:00:33,792: End of token training: 2 Epoch: 8 Loss:14.715 MRR:21.85 Best Results: 21.87
2025-01-06 23:00:33,793: Snapshot:2	Epoch:8	Loss:14.715	translation_Loss:14.358	token_training_loss:0.357	distillation_Loss:0.0                                                           	MRR:21.85	Hits@10:37.26	Best:21.87
2025-01-06 23:00:34,051: => loading checkpoint './checkpoint/FACT/2model_best.tar'
2025-01-06 23:00:43,731: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2533 | 0.165  | 0.2924 | 0.3465 |  0.416  |
|     1      | 0.2351 | 0.152  | 0.2717 | 0.3241 |  0.3924 |
|     2      | 0.217  | 0.1365 | 0.2487 | 0.3031 |  0.3728 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,717,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,814,800
Trainable params: 2,800
Non-trainable params: 2,812,000
=================================================================
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:00:58,786: Snapshot:3	Epoch:0	Loss:3.77	translation_Loss:2.676	token_training_loss:0.0	distillation_Loss:1.093                                                   	MRR:19.39	Hits@10:36.67	Best:19.39
2025-01-06 23:01:07,457: Snapshot:3	Epoch:1	Loss:3.102	translation_Loss:2.067	token_training_loss:0.0	distillation_Loss:1.036                                                   	MRR:19.67	Hits@10:36.73	Best:19.67
2025-01-06 23:01:15,619: Snapshot:3	Epoch:2	Loss:2.938	translation_Loss:2.013	token_training_loss:0.0	distillation_Loss:0.925                                                   	MRR:19.7	Hits@10:36.8	Best:19.7
2025-01-06 23:01:24,407: Snapshot:3	Epoch:3	Loss:2.91	translation_Loss:1.965	token_training_loss:0.0	distillation_Loss:0.944                                                   	MRR:19.65	Hits@10:36.79	Best:19.7
2025-01-06 23:01:32,528: Snapshot:3	Epoch:4	Loss:2.915	translation_Loss:1.973	token_training_loss:0.0	distillation_Loss:0.941                                                   	MRR:19.64	Hits@10:36.69	Best:19.7
2025-01-06 23:01:41,022: Early Stopping! Snapshot: 3 Epoch: 5 Best Results: 19.7
2025-01-06 23:01:41,023: Start to training tokens! Snapshot: 3 Epoch: 5 Loss:2.908 MRR:19.68 Best Results: 19.7
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:01:41,023: Snapshot:3	Epoch:5	Loss:2.908	translation_Loss:1.96	token_training_loss:0.0	distillation_Loss:0.948                                                   	MRR:19.68	Hits@10:36.64	Best:19.7
2025-01-06 23:01:48,993: Snapshot:3	Epoch:6	Loss:30.781	translation_Loss:13.804	token_training_loss:16.977	distillation_Loss:0.0                                                   	MRR:19.68	Hits@10:36.64	Best:19.7
2025-01-06 23:01:56,991: End of token training: 3 Epoch: 7 Loss:14.159 MRR:19.68 Best Results: 19.7
2025-01-06 23:01:56,991: Snapshot:3	Epoch:7	Loss:14.159	translation_Loss:13.793	token_training_loss:0.366	distillation_Loss:0.0                                                           	MRR:19.68	Hits@10:36.64	Best:19.7
2025-01-06 23:01:57,253: => loading checkpoint './checkpoint/FACT/3model_best.tar'
2025-01-06 23:02:10,661: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2465 | 0.1578 | 0.2852 | 0.3398 |  0.412  |
|     1      | 0.2316 | 0.1473 | 0.2677 | 0.322  |  0.3904 |
|     2      | 0.2159 | 0.1314 | 0.2494 | 0.3075 |  0.3813 |
|     3      | 0.1989 | 0.1116 | 0.2324 | 0.2915 |  0.3692 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,778,800)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 2,876,400
Trainable params: 2,800
Non-trainable params: 2,873,600
=================================================================
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2025-01-06 23:02:26,193: Snapshot:4	Epoch:0	Loss:2.389	translation_Loss:1.613	token_training_loss:0.0	distillation_Loss:0.776                                                   	MRR:20.3	Hits@10:43.28	Best:20.3
2025-01-06 23:02:34,403: Snapshot:4	Epoch:1	Loss:1.529	translation_Loss:0.851	token_training_loss:0.0	distillation_Loss:0.677                                                   	MRR:20.59	Hits@10:43.03	Best:20.59
2025-01-06 23:02:42,665: Snapshot:4	Epoch:2	Loss:1.349	translation_Loss:0.729	token_training_loss:0.0	distillation_Loss:0.62                                                   	MRR:20.82	Hits@10:43.14	Best:20.82
2025-01-06 23:02:51,404: Snapshot:4	Epoch:3	Loss:1.324	translation_Loss:0.698	token_training_loss:0.0	distillation_Loss:0.626                                                   	MRR:21.01	Hits@10:43.31	Best:21.01
2025-01-06 23:02:59,584: Snapshot:4	Epoch:4	Loss:1.31	translation_Loss:0.69	token_training_loss:0.0	distillation_Loss:0.62                                                   	MRR:20.93	Hits@10:43.17	Best:21.01
2025-01-06 23:03:08,111: Snapshot:4	Epoch:5	Loss:1.31	translation_Loss:0.693	token_training_loss:0.0	distillation_Loss:0.617                                                   	MRR:20.94	Hits@10:43.27	Best:21.01
2025-01-06 23:03:16,316: Snapshot:4	Epoch:6	Loss:1.313	translation_Loss:0.693	token_training_loss:0.0	distillation_Loss:0.62                                                   	MRR:21.11	Hits@10:43.84	Best:21.11
2025-01-06 23:03:24,529: Snapshot:4	Epoch:7	Loss:1.316	translation_Loss:0.687	token_training_loss:0.0	distillation_Loss:0.629                                                   	MRR:20.73	Hits@10:43.3	Best:21.11
2025-01-06 23:03:33,049: Snapshot:4	Epoch:8	Loss:1.315	translation_Loss:0.686	token_training_loss:0.0	distillation_Loss:0.629                                                   	MRR:20.76	Hits@10:43.19	Best:21.11
2025-01-06 23:03:41,188: Early Stopping! Snapshot: 4 Epoch: 9 Best Results: 21.11
2025-01-06 23:03:41,188: Start to training tokens! Snapshot: 4 Epoch: 9 Loss:1.313 MRR:20.88 Best Results: 21.11
Token added to optimizer, embeddings excluded successfully.
2025-01-06 23:03:41,188: Snapshot:4	Epoch:9	Loss:1.313	translation_Loss:0.687	token_training_loss:0.0	distillation_Loss:0.626                                                   	MRR:20.88	Hits@10:43.37	Best:21.11
2025-01-06 23:03:49,667: Snapshot:4	Epoch:10	Loss:26.846	translation_Loss:11.462	token_training_loss:15.384	distillation_Loss:0.0                                                   	MRR:20.88	Hits@10:43.37	Best:21.11
2025-01-06 23:03:57,684: End of token training: 4 Epoch: 11 Loss:11.794 MRR:20.88 Best Results: 21.11
2025-01-06 23:03:57,685: Snapshot:4	Epoch:11	Loss:11.794	translation_Loss:11.457	token_training_loss:0.337	distillation_Loss:0.0                                                           	MRR:20.88	Hits@10:43.37	Best:21.11
2025-01-06 23:03:58,005: => loading checkpoint './checkpoint/FACT/4model_best.tar'
2025-01-06 23:04:14,973: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2419 | 0.1557 | 0.2783 | 0.331  |  0.4045 |
|     1      | 0.2247 | 0.143  | 0.2577 | 0.3087 |  0.382  |
|     2      | 0.2093 | 0.1261 | 0.2416 | 0.2969 |  0.3744 |
|     3      | 0.196  | 0.1081 | 0.2252 | 0.287  |  0.3744 |
|     4      | 0.2114 | 0.1028 | 0.2442 | 0.3274 |  0.4352 |
+------------+--------+--------+--------+--------+---------+
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
TransE                                   2,800
├─Embedding: 1-1                         (2,908,200)
├─Embedding: 1-2                         (94,800)
├─MarginRankingLoss: 1-3                 --
├─MSELoss: 1-4                           --
├─HuberLoss: 1-5                         --
=================================================================
Total params: 3,005,800
Trainable params: 2,800
Non-trainable params: 3,003,000
=================================================================
2025-01-06 23:04:14,975: Final Result:
[+------------+------+--------+--------+--------+---------+
| Snapshot:0 | MRR  | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+------+--------+--------+--------+---------+
|     0      | 0.24 | 0.1562 |  0.28  | 0.3306 |  0.3912 |
+------------+------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2531 | 0.1642 | 0.2933 | 0.3494 |  0.4159 |
|     1      | 0.2311 | 0.1487 | 0.2686 | 0.3204 |  0.3839 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2533 | 0.165  | 0.2924 | 0.3465 |  0.416  |
|     1      | 0.2351 | 0.152  | 0.2717 | 0.3241 |  0.3924 |
|     2      | 0.217  | 0.1365 | 0.2487 | 0.3031 |  0.3728 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2465 | 0.1578 | 0.2852 | 0.3398 |  0.412  |
|     1      | 0.2316 | 0.1473 | 0.2677 | 0.322  |  0.3904 |
|     2      | 0.2159 | 0.1314 | 0.2494 | 0.3075 |  0.3813 |
|     3      | 0.1989 | 0.1116 | 0.2324 | 0.2915 |  0.3692 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2419 | 0.1557 | 0.2783 | 0.331  |  0.4045 |
|     1      | 0.2247 | 0.143  | 0.2577 | 0.3087 |  0.382  |
|     2      | 0.2093 | 0.1261 | 0.2416 | 0.2969 |  0.3744 |
|     3      | 0.196  | 0.1081 | 0.2252 | 0.287  |  0.3744 |
|     4      | 0.2114 | 0.1028 | 0.2442 | 0.3274 |  0.4352 |
+------------+--------+--------+--------+--------+---------+]
2025-01-06 23:04:14,976: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 191.61123204231262 |    0.24   |    0.156     |     0.28     |     0.391     |
|    1     | 116.39446258544922 |   0.242   |    0.156     |    0.281     |      0.4      |
|    2     | 77.81527161598206  |   0.235   |    0.151     |    0.271     |     0.394     |
|    3     | 69.68224477767944  |   0.223   |    0.137     |    0.259     |     0.388     |
|    4     | 103.43040227890015 |   0.217   |    0.127     |    0.249     |     0.394     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2025-01-06 23:04:14,976: Sum_Training_Time:558.9336133003235
2025-01-06 23:04:14,976: Every_Training_Time:[191.61123204231262, 116.39446258544922, 77.81527161598206, 69.68224477767944, 103.43040227890015]
2025-01-06 23:04:14,976: Forward transfer: 0.174375 Backward transfer: -0.0037749999999999867
