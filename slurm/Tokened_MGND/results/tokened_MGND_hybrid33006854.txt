2024-12-27 02:53:25,057: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227025252/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=50000.0, token_num=5, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 02:53:34,437: Snapshot:0	Epoch:0	Loss:48.314	translation_Loss:48.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.3	Hits@10:2.38	Best:1.3
2024-12-27 02:53:40,161: Snapshot:0	Epoch:1	Loss:45.002	translation_Loss:45.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.79	Hits@10:7.92	Best:3.79
2024-12-27 02:53:45,917: Snapshot:0	Epoch:2	Loss:41.936	translation_Loss:41.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:12.67	Best:5.6
2024-12-27 02:53:51,755: Snapshot:0	Epoch:3	Loss:39.022	translation_Loss:39.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.85	Hits@10:16.11	Best:6.85
2024-12-27 02:53:57,480: Snapshot:0	Epoch:4	Loss:36.216	translation_Loss:36.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.82	Hits@10:18.96	Best:7.82
2024-12-27 02:54:03,329: Snapshot:0	Epoch:5	Loss:33.472	translation_Loss:33.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.69	Best:8.89
2024-12-27 02:54:09,559: Snapshot:0	Epoch:6	Loss:30.69	translation_Loss:30.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:25.15	Best:10.29
2024-12-27 02:54:15,379: Snapshot:0	Epoch:7	Loss:27.959	translation_Loss:27.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.01	Hits@10:28.74	Best:12.01
2024-12-27 02:54:21,260: Snapshot:0	Epoch:8	Loss:25.173	translation_Loss:25.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.68	Hits@10:31.87	Best:13.68
2024-12-27 02:54:27,099: Snapshot:0	Epoch:9	Loss:22.449	translation_Loss:22.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.25	Hits@10:34.31	Best:15.25
2024-12-27 02:54:32,912: Snapshot:0	Epoch:10	Loss:19.787	translation_Loss:19.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.96	Hits@10:36.32	Best:16.96
2024-12-27 02:54:38,714: Snapshot:0	Epoch:11	Loss:17.285	translation_Loss:17.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.51	Hits@10:38.17	Best:18.51
2024-12-27 02:54:44,512: Snapshot:0	Epoch:12	Loss:14.981	translation_Loss:14.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.76	Hits@10:39.62	Best:19.76
2024-12-27 02:54:50,323: Snapshot:0	Epoch:13	Loss:13.018	translation_Loss:13.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.9	Hits@10:40.74	Best:20.9
2024-12-27 02:54:56,160: Snapshot:0	Epoch:14	Loss:11.354	translation_Loss:11.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.82	Hits@10:41.73	Best:21.82
2024-12-27 02:55:01,977: Snapshot:0	Epoch:15	Loss:9.881	translation_Loss:9.881	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.58	Hits@10:42.5	Best:22.58
2024-12-27 02:55:07,903: Snapshot:0	Epoch:16	Loss:8.647	translation_Loss:8.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:43.08	Best:23.1
2024-12-27 02:55:14,222: Snapshot:0	Epoch:17	Loss:7.591	translation_Loss:7.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.47	Hits@10:43.65	Best:23.47
2024-12-27 02:55:20,058: Snapshot:0	Epoch:18	Loss:6.694	translation_Loss:6.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.89	Hits@10:44.06	Best:23.89
2024-12-27 02:55:25,880: Snapshot:0	Epoch:19	Loss:5.931	translation_Loss:5.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:44.43	Best:24.14
2024-12-27 02:55:31,715: Snapshot:0	Epoch:20	Loss:5.225	translation_Loss:5.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.33	Hits@10:44.63	Best:24.33
2024-12-27 02:55:37,534: Snapshot:0	Epoch:21	Loss:4.674	translation_Loss:4.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:44.97	Best:24.5
2024-12-27 02:55:43,429: Snapshot:0	Epoch:22	Loss:4.193	translation_Loss:4.193	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.64	Hits@10:45.1	Best:24.64
2024-12-27 02:55:49,259: Snapshot:0	Epoch:23	Loss:3.781	translation_Loss:3.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:45.34	Best:24.71
2024-12-27 02:55:55,090: Snapshot:0	Epoch:24	Loss:3.458	translation_Loss:3.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.89	Hits@10:45.41	Best:24.89
2024-12-27 02:56:00,936: Snapshot:0	Epoch:25	Loss:3.106	translation_Loss:3.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:45.63	Best:24.98
2024-12-27 02:56:06,771: Snapshot:0	Epoch:26	Loss:2.85	translation_Loss:2.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.07	Hits@10:45.68	Best:25.07
2024-12-27 02:56:12,644: Snapshot:0	Epoch:27	Loss:2.619	translation_Loss:2.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.15	Hits@10:45.74	Best:25.15
2024-12-27 02:56:18,512: Snapshot:0	Epoch:28	Loss:2.413	translation_Loss:2.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.23	Hits@10:45.8	Best:25.23
2024-12-27 02:56:24,788: Snapshot:0	Epoch:29	Loss:2.253	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.37	Hits@10:45.91	Best:25.37
2024-12-27 02:56:30,619: Snapshot:0	Epoch:30	Loss:2.102	translation_Loss:2.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.39	Hits@10:45.97	Best:25.39
2024-12-27 02:56:36,461: Snapshot:0	Epoch:31	Loss:1.939	translation_Loss:1.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.45	Hits@10:46.1	Best:25.45
2024-12-27 02:56:42,276: Snapshot:0	Epoch:32	Loss:1.819	translation_Loss:1.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.48	Hits@10:46.11	Best:25.48
2024-12-27 02:56:48,120: Snapshot:0	Epoch:33	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.18	Best:25.58
2024-12-27 02:56:53,932: Snapshot:0	Epoch:34	Loss:1.62	translation_Loss:1.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.56	Hits@10:46.08	Best:25.58
2024-12-27 02:56:59,737: Snapshot:0	Epoch:35	Loss:1.527	translation_Loss:1.527	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.03	Best:25.58
2024-12-27 02:57:05,576: Snapshot:0	Epoch:36	Loss:1.444	translation_Loss:1.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:46.17	Best:25.68
2024-12-27 02:57:11,435: Snapshot:0	Epoch:37	Loss:1.371	translation_Loss:1.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.2	Best:25.71
2024-12-27 02:57:17,313: Snapshot:0	Epoch:38	Loss:1.299	translation_Loss:1.299	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.8	Hits@10:46.17	Best:25.8
2024-12-27 02:57:23,165: Snapshot:0	Epoch:39	Loss:1.247	translation_Loss:1.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.82	Hits@10:46.15	Best:25.82
2024-12-27 02:57:29,016: Snapshot:0	Epoch:40	Loss:1.188	translation_Loss:1.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.25	Best:25.84
2024-12-27 02:57:34,765: Snapshot:0	Epoch:41	Loss:1.132	translation_Loss:1.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.13	Best:25.84
2024-12-27 02:57:40,919: Snapshot:0	Epoch:42	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.2	Best:25.84
2024-12-27 02:57:46,610: Early Stopping! Snapshot: 0 Epoch: 43 Best Results: 25.84
2024-12-27 02:57:46,610: Start to training tokens! Snapshot: 0 Epoch: 43 Loss:1.041 MRR:25.77 Best Results: 25.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 02:57:46,610: Snapshot:0	Epoch:43	Loss:1.041	translation_Loss:1.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.17	Best:25.84
2024-12-27 02:57:52,895: Snapshot:0	Epoch:44	Loss:130.41	translation_Loss:35.018	multi_layer_Loss:95.391	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.17	Best:25.84
2024-12-27 02:57:58,707: End of token training: 0 Epoch: 45 Loss:61.174 MRR:25.77 Best Results: 25.84
2024-12-27 02:57:58,707: Snapshot:0	Epoch:45	Loss:61.174	translation_Loss:35.024	multi_layer_Loss:26.15	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.77	Hits@10:46.17	Best:25.84
2024-12-27 02:57:58,954: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-27 02:58:01,394: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2577 | 0.1504 | 0.3154 | 0.379  |  0.4523 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 02:58:12,060: Snapshot:1	Epoch:0	Loss:15.093	translation_Loss:14.997	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.096                                                   	MRR:6.3	Hits@10:10.51	Best:6.3
2024-12-27 02:58:14,316: Snapshot:1	Epoch:1	Loss:13.76	translation_Loss:13.54	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.22                                                   	MRR:7.0	Hits@10:11.8	Best:7.0
2024-12-27 02:58:16,584: Snapshot:1	Epoch:2	Loss:12.662	translation_Loss:12.321	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.341                                                   	MRR:7.9	Hits@10:13.46	Best:7.9
2024-12-27 02:58:18,832: Snapshot:1	Epoch:3	Loss:11.631	translation_Loss:11.203	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.428                                                   	MRR:9.23	Hits@10:16.55	Best:9.23
2024-12-27 02:58:21,104: Snapshot:1	Epoch:4	Loss:10.61	translation_Loss:10.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.492                                                   	MRR:10.78	Hits@10:19.89	Best:10.78
2024-12-27 02:58:23,319: Snapshot:1	Epoch:5	Loss:9.603	translation_Loss:9.061	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.542                                                   	MRR:12.43	Hits@10:23.46	Best:12.43
2024-12-27 02:58:25,562: Snapshot:1	Epoch:6	Loss:8.662	translation_Loss:8.08	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.581                                                   	MRR:13.97	Hits@10:26.22	Best:13.97
2024-12-27 02:58:27,788: Snapshot:1	Epoch:7	Loss:7.828	translation_Loss:7.221	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.607                                                   	MRR:15.31	Hits@10:28.36	Best:15.31
2024-12-27 02:58:30,017: Snapshot:1	Epoch:8	Loss:7.124	translation_Loss:6.501	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.623                                                   	MRR:16.65	Hits@10:30.32	Best:16.65
2024-12-27 02:58:32,280: Snapshot:1	Epoch:9	Loss:6.515	translation_Loss:5.885	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.63                                                   	MRR:17.79	Hits@10:32.41	Best:17.79
2024-12-27 02:58:34,521: Snapshot:1	Epoch:10	Loss:6.005	translation_Loss:5.374	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.631                                                   	MRR:18.85	Hits@10:34.12	Best:18.85
2024-12-27 02:58:36,771: Snapshot:1	Epoch:11	Loss:5.583	translation_Loss:4.955	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.627                                                   	MRR:19.71	Hits@10:35.93	Best:19.71
2024-12-27 02:58:39,023: Snapshot:1	Epoch:12	Loss:5.183	translation_Loss:4.563	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.62                                                   	MRR:20.51	Hits@10:37.35	Best:20.51
2024-12-27 02:58:41,251: Snapshot:1	Epoch:13	Loss:4.889	translation_Loss:4.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.611                                                   	MRR:21.11	Hits@10:38.74	Best:21.11
2024-12-27 02:58:43,490: Snapshot:1	Epoch:14	Loss:4.615	translation_Loss:4.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.603                                                   	MRR:21.79	Hits@10:39.82	Best:21.79
2024-12-27 02:58:45,744: Snapshot:1	Epoch:15	Loss:4.384	translation_Loss:3.79	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.594                                                   	MRR:22.21	Hits@10:40.85	Best:22.21
2024-12-27 02:58:47,996: Snapshot:1	Epoch:16	Loss:4.172	translation_Loss:3.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.585                                                   	MRR:22.62	Hits@10:41.68	Best:22.62
2024-12-27 02:58:50,240: Snapshot:1	Epoch:17	Loss:3.991	translation_Loss:3.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.577                                                   	MRR:23.0	Hits@10:42.4	Best:23.0
2024-12-27 02:58:52,491: Snapshot:1	Epoch:18	Loss:3.837	translation_Loss:3.271	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.567                                                   	MRR:23.45	Hits@10:42.89	Best:23.45
2024-12-27 02:58:55,127: Snapshot:1	Epoch:19	Loss:3.696	translation_Loss:3.138	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.557                                                   	MRR:23.86	Hits@10:43.49	Best:23.86
2024-12-27 02:58:57,419: Snapshot:1	Epoch:20	Loss:3.563	translation_Loss:3.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.55                                                   	MRR:24.2	Hits@10:44.04	Best:24.2
2024-12-27 02:58:59,667: Snapshot:1	Epoch:21	Loss:3.459	translation_Loss:2.916	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.543                                                   	MRR:24.63	Hits@10:44.59	Best:24.63
2024-12-27 02:59:01,918: Snapshot:1	Epoch:22	Loss:3.348	translation_Loss:2.812	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.536                                                   	MRR:24.99	Hits@10:45.04	Best:24.99
2024-12-27 02:59:04,185: Snapshot:1	Epoch:23	Loss:3.252	translation_Loss:2.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.529                                                   	MRR:25.37	Hits@10:45.5	Best:25.37
2024-12-27 02:59:06,459: Snapshot:1	Epoch:24	Loss:3.166	translation_Loss:2.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.521                                                   	MRR:25.65	Hits@10:45.89	Best:25.65
2024-12-27 02:59:08,729: Snapshot:1	Epoch:25	Loss:3.092	translation_Loss:2.579	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.512                                                   	MRR:25.85	Hits@10:46.08	Best:25.85
2024-12-27 02:59:10,955: Snapshot:1	Epoch:26	Loss:3.02	translation_Loss:2.515	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.505                                                   	MRR:26.14	Hits@10:46.51	Best:26.14
2024-12-27 02:59:13,236: Snapshot:1	Epoch:27	Loss:2.943	translation_Loss:2.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.498                                                   	MRR:26.34	Hits@10:46.67	Best:26.34
2024-12-27 02:59:15,488: Snapshot:1	Epoch:28	Loss:2.886	translation_Loss:2.396	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.49                                                   	MRR:26.55	Hits@10:46.98	Best:26.55
2024-12-27 02:59:17,751: Snapshot:1	Epoch:29	Loss:2.839	translation_Loss:2.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.484                                                   	MRR:26.73	Hits@10:47.34	Best:26.73
2024-12-27 02:59:20,003: Snapshot:1	Epoch:30	Loss:2.786	translation_Loss:2.307	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.479                                                   	MRR:26.91	Hits@10:47.43	Best:26.91
2024-12-27 02:59:22,231: Snapshot:1	Epoch:31	Loss:2.725	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.472                                                   	MRR:27.16	Hits@10:47.65	Best:27.16
2024-12-27 02:59:24,470: Snapshot:1	Epoch:32	Loss:2.682	translation_Loss:2.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.466                                                   	MRR:27.3	Hits@10:47.82	Best:27.3
2024-12-27 02:59:26,752: Snapshot:1	Epoch:33	Loss:2.639	translation_Loss:2.179	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.46                                                   	MRR:27.42	Hits@10:48.0	Best:27.42
2024-12-27 02:59:28,996: Snapshot:1	Epoch:34	Loss:2.612	translation_Loss:2.156	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.456                                                   	MRR:27.51	Hits@10:48.17	Best:27.51
2024-12-27 02:59:31,281: Snapshot:1	Epoch:35	Loss:2.568	translation_Loss:2.118	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.45                                                   	MRR:27.52	Hits@10:48.17	Best:27.52
2024-12-27 02:59:33,519: Snapshot:1	Epoch:36	Loss:2.529	translation_Loss:2.084	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.444                                                   	MRR:27.57	Hits@10:48.37	Best:27.57
2024-12-27 02:59:35,764: Snapshot:1	Epoch:37	Loss:2.499	translation_Loss:2.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.439                                                   	MRR:27.69	Hits@10:48.35	Best:27.69
2024-12-27 02:59:37,996: Snapshot:1	Epoch:38	Loss:2.462	translation_Loss:2.028	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.434                                                   	MRR:27.83	Hits@10:48.38	Best:27.83
2024-12-27 02:59:40,209: Snapshot:1	Epoch:39	Loss:2.446	translation_Loss:2.016	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:27.91	Hits@10:48.56	Best:27.91
2024-12-27 02:59:42,445: Snapshot:1	Epoch:40	Loss:2.409	translation_Loss:1.982	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.427                                                   	MRR:27.95	Hits@10:48.67	Best:27.95
2024-12-27 02:59:44,714: Snapshot:1	Epoch:41	Loss:2.387	translation_Loss:1.964	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.423                                                   	MRR:28.01	Hits@10:48.68	Best:28.01
2024-12-27 02:59:46,981: Snapshot:1	Epoch:42	Loss:2.361	translation_Loss:1.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.419                                                   	MRR:28.12	Hits@10:48.85	Best:28.12
2024-12-27 02:59:49,253: Snapshot:1	Epoch:43	Loss:2.325	translation_Loss:1.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.414                                                   	MRR:28.2	Hits@10:48.91	Best:28.2
2024-12-27 02:59:51,499: Snapshot:1	Epoch:44	Loss:2.306	translation_Loss:1.893	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.412                                                   	MRR:28.28	Hits@10:49.11	Best:28.28
2024-12-27 02:59:53,695: Snapshot:1	Epoch:45	Loss:2.285	translation_Loss:1.877	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.409                                                   	MRR:28.33	Hits@10:49.22	Best:28.33
2024-12-27 02:59:55,912: Snapshot:1	Epoch:46	Loss:2.263	translation_Loss:1.859	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.405                                                   	MRR:28.41	Hits@10:49.28	Best:28.41
2024-12-27 02:59:58,174: Snapshot:1	Epoch:47	Loss:2.244	translation_Loss:1.842	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.402                                                   	MRR:28.43	Hits@10:49.39	Best:28.43
2024-12-27 03:00:00,422: Snapshot:1	Epoch:48	Loss:2.227	translation_Loss:1.828	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.399                                                   	MRR:28.47	Hits@10:49.37	Best:28.47
2024-12-27 03:00:02,675: Snapshot:1	Epoch:49	Loss:2.216	translation_Loss:1.82	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:28.42	Hits@10:49.41	Best:28.47
2024-12-27 03:00:04,959: Snapshot:1	Epoch:50	Loss:2.186	translation_Loss:1.792	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.394                                                   	MRR:28.49	Hits@10:49.49	Best:28.49
2024-12-27 03:00:07,247: Snapshot:1	Epoch:51	Loss:2.178	translation_Loss:1.786	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.392                                                   	MRR:28.57	Hits@10:49.53	Best:28.57
2024-12-27 03:00:09,990: Snapshot:1	Epoch:52	Loss:2.161	translation_Loss:1.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.388                                                   	MRR:28.6	Hits@10:49.65	Best:28.6
2024-12-27 03:00:12,206: Snapshot:1	Epoch:53	Loss:2.138	translation_Loss:1.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.385                                                   	MRR:28.77	Hits@10:49.59	Best:28.77
2024-12-27 03:00:14,433: Snapshot:1	Epoch:54	Loss:2.134	translation_Loss:1.749	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.384                                                   	MRR:28.76	Hits@10:49.77	Best:28.77
2024-12-27 03:00:16,638: Snapshot:1	Epoch:55	Loss:2.114	translation_Loss:1.733	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.381                                                   	MRR:28.86	Hits@10:49.75	Best:28.86
2024-12-27 03:00:18,896: Snapshot:1	Epoch:56	Loss:2.094	translation_Loss:1.714	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.379                                                   	MRR:29.05	Hits@10:49.84	Best:29.05
2024-12-27 03:00:21,094: Snapshot:1	Epoch:57	Loss:2.089	translation_Loss:1.709	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.379                                                   	MRR:28.88	Hits@10:49.86	Best:29.05
2024-12-27 03:00:23,267: Snapshot:1	Epoch:58	Loss:2.083	translation_Loss:1.706	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:28.94	Hits@10:49.92	Best:29.05
2024-12-27 03:00:25,505: Early Stopping! Snapshot: 1 Epoch: 59 Best Results: 29.05
2024-12-27 03:00:25,505: Start to training tokens! Snapshot: 1 Epoch: 59 Loss:2.068 MRR:29.03 Best Results: 29.05
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:00:25,505: Snapshot:1	Epoch:59	Loss:2.068	translation_Loss:1.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.376                                                   	MRR:29.03	Hits@10:49.93	Best:29.05
2024-12-27 03:00:27,682: Snapshot:1	Epoch:60	Loss:57.8	translation_Loss:13.036	multi_layer_Loss:44.763	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.03	Hits@10:49.93	Best:29.05
2024-12-27 03:00:29,900: End of token training: 1 Epoch: 61 Loss:45.884 MRR:29.03 Best Results: 29.05
2024-12-27 03:00:29,900: Snapshot:1	Epoch:61	Loss:45.884	translation_Loss:13.025	multi_layer_Loss:32.859	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.03	Hits@10:49.93	Best:29.05
2024-12-27 03:00:30,189: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-27 03:00:33,877: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1523 | 0.3163 | 0.3797 |  0.4532 |
|     1      | 0.2857 | 0.1778 | 0.3378 | 0.4072 |  0.4975 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:01:06,120: Snapshot:2	Epoch:0	Loss:62.193	translation_Loss:61.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.84                                                   	MRR:5.23	Hits@10:11.07	Best:5.23
2024-12-27 03:01:16,013: Snapshot:2	Epoch:1	Loss:51.318	translation_Loss:49.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.498                                                   	MRR:8.45	Hits@10:18.31	Best:8.45
2024-12-27 03:01:25,884: Snapshot:2	Epoch:2	Loss:41.833	translation_Loss:40.003	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.829                                                   	MRR:11.4	Hits@10:23.97	Best:11.4
2024-12-27 03:01:35,710: Snapshot:2	Epoch:3	Loss:34.355	translation_Loss:32.391	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.964                                                   	MRR:13.54	Hits@10:28.13	Best:13.54
2024-12-27 03:01:45,650: Snapshot:2	Epoch:4	Loss:28.832	translation_Loss:26.874	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.958                                                   	MRR:15.03	Hits@10:30.52	Best:15.03
2024-12-27 03:01:55,532: Snapshot:2	Epoch:5	Loss:24.826	translation_Loss:22.943	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.882                                                   	MRR:16.26	Hits@10:32.21	Best:16.26
2024-12-27 03:02:05,358: Snapshot:2	Epoch:6	Loss:21.887	translation_Loss:20.108	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.779                                                   	MRR:17.05	Hits@10:33.46	Best:17.05
2024-12-27 03:02:15,091: Snapshot:2	Epoch:7	Loss:19.736	translation_Loss:18.063	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.673                                                   	MRR:17.77	Hits@10:34.37	Best:17.77
2024-12-27 03:02:24,863: Snapshot:2	Epoch:8	Loss:18.174	translation_Loss:16.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.585                                                   	MRR:18.32	Hits@10:34.93	Best:18.32
2024-12-27 03:02:34,634: Snapshot:2	Epoch:9	Loss:16.984	translation_Loss:15.473	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.511                                                   	MRR:18.71	Hits@10:35.34	Best:18.71
2024-12-27 03:02:44,476: Snapshot:2	Epoch:10	Loss:16.104	translation_Loss:14.652	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.453                                                   	MRR:19.0	Hits@10:35.72	Best:19.0
2024-12-27 03:02:54,235: Snapshot:2	Epoch:11	Loss:15.425	translation_Loss:14.011	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.414                                                   	MRR:19.33	Hits@10:35.9	Best:19.33
2024-12-27 03:03:04,008: Snapshot:2	Epoch:12	Loss:14.822	translation_Loss:13.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.376                                                   	MRR:19.56	Hits@10:36.12	Best:19.56
2024-12-27 03:03:13,959: Snapshot:2	Epoch:13	Loss:14.378	translation_Loss:13.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.343                                                   	MRR:19.7	Hits@10:36.28	Best:19.7
2024-12-27 03:03:23,816: Snapshot:2	Epoch:14	Loss:14.037	translation_Loss:12.723	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.314                                                   	MRR:19.83	Hits@10:36.32	Best:19.83
2024-12-27 03:03:33,604: Snapshot:2	Epoch:15	Loss:13.71	translation_Loss:12.408	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.301                                                   	MRR:20.04	Hits@10:36.46	Best:20.04
2024-12-27 03:03:43,471: Snapshot:2	Epoch:16	Loss:13.427	translation_Loss:12.145	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.282                                                   	MRR:20.12	Hits@10:36.45	Best:20.12
2024-12-27 03:03:53,247: Snapshot:2	Epoch:17	Loss:13.26	translation_Loss:11.991	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.269                                                   	MRR:20.1	Hits@10:36.47	Best:20.12
2024-12-27 03:04:03,562: Snapshot:2	Epoch:18	Loss:13.066	translation_Loss:11.815	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.251                                                   	MRR:20.29	Hits@10:36.53	Best:20.29
2024-12-27 03:04:13,381: Snapshot:2	Epoch:19	Loss:12.936	translation_Loss:11.692	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.244                                                   	MRR:20.3	Hits@10:36.61	Best:20.3
2024-12-27 03:04:23,093: Snapshot:2	Epoch:20	Loss:12.798	translation_Loss:11.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.238                                                   	MRR:20.29	Hits@10:36.66	Best:20.3
2024-12-27 03:04:32,819: Snapshot:2	Epoch:21	Loss:12.663	translation_Loss:11.436	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.227                                                   	MRR:20.35	Hits@10:36.55	Best:20.35
2024-12-27 03:04:42,502: Snapshot:2	Epoch:22	Loss:12.534	translation_Loss:11.316	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.218                                                   	MRR:20.3	Hits@10:36.59	Best:20.35
2024-12-27 03:04:52,210: Snapshot:2	Epoch:23	Loss:12.454	translation_Loss:11.245	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.21                                                   	MRR:20.31	Hits@10:36.56	Best:20.35
2024-12-27 03:05:02,427: Snapshot:2	Epoch:24	Loss:12.387	translation_Loss:11.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.206                                                   	MRR:20.4	Hits@10:36.65	Best:20.4
2024-12-27 03:05:12,345: Snapshot:2	Epoch:25	Loss:12.31	translation_Loss:11.104	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.206                                                   	MRR:20.45	Hits@10:36.73	Best:20.45
2024-12-27 03:05:22,103: Snapshot:2	Epoch:26	Loss:12.259	translation_Loss:11.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.199                                                   	MRR:20.37	Hits@10:36.63	Best:20.45
2024-12-27 03:05:31,851: Snapshot:2	Epoch:27	Loss:12.198	translation_Loss:10.999	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.199                                                   	MRR:20.51	Hits@10:36.63	Best:20.51
2024-12-27 03:05:41,586: Snapshot:2	Epoch:28	Loss:12.135	translation_Loss:10.942	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.193                                                   	MRR:20.47	Hits@10:36.7	Best:20.51
2024-12-27 03:05:51,386: Snapshot:2	Epoch:29	Loss:12.114	translation_Loss:10.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.187                                                   	MRR:20.52	Hits@10:36.73	Best:20.52
2024-12-27 03:06:01,164: Snapshot:2	Epoch:30	Loss:12.059	translation_Loss:10.88	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.179                                                   	MRR:20.47	Hits@10:36.82	Best:20.52
2024-12-27 03:06:10,929: Snapshot:2	Epoch:31	Loss:11.955	translation_Loss:10.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.18                                                   	MRR:20.46	Hits@10:36.92	Best:20.52
2024-12-27 03:06:20,671: Early Stopping! Snapshot: 2 Epoch: 32 Best Results: 20.52
2024-12-27 03:06:20,671: Start to training tokens! Snapshot: 2 Epoch: 32 Loss:11.961 MRR:20.4 Best Results: 20.52
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:06:20,671: Snapshot:2	Epoch:32	Loss:11.961	translation_Loss:10.78	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.181                                                   	MRR:20.4	Hits@10:36.79	Best:20.52
2024-12-27 03:06:30,462: Snapshot:2	Epoch:33	Loss:166.408	translation_Loss:57.573	multi_layer_Loss:108.834	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.4	Hits@10:36.79	Best:20.52
2024-12-27 03:06:40,123: End of token training: 2 Epoch: 34 Loss:66.967 MRR:20.4 Best Results: 20.52
2024-12-27 03:06:40,124: Snapshot:2	Epoch:34	Loss:66.967	translation_Loss:57.58	multi_layer_Loss:9.386	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.4	Hits@10:36.79	Best:20.52
2024-12-27 03:06:40,440: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-27 03:06:48,552: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2551 | 0.1453 | 0.3148 | 0.3795 |  0.4545 |
|     1      | 0.2783 | 0.1696 | 0.3244 | 0.399  |  0.4989 |
|     2      | 0.2048 | 0.1205 | 0.2355 | 0.2935 |  0.3706 |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:07:27,028: Snapshot:3	Epoch:0	Loss:65.46	translation_Loss:64.13	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.33                                                   	MRR:6.21	Hits@10:15.17	Best:6.21
2024-12-27 03:07:38,966: Snapshot:3	Epoch:1	Loss:50.996	translation_Loss:48.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.499                                                   	MRR:9.58	Hits@10:21.12	Best:9.58
2024-12-27 03:07:50,878: Snapshot:3	Epoch:2	Loss:41.139	translation_Loss:38.21	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.93                                                   	MRR:12.11	Hits@10:25.84	Best:12.11
2024-12-27 03:08:02,817: Snapshot:3	Epoch:3	Loss:34.77	translation_Loss:31.753	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.018                                                   	MRR:14.02	Hits@10:29.09	Best:14.02
2024-12-27 03:08:14,763: Snapshot:3	Epoch:4	Loss:30.467	translation_Loss:27.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.99                                                   	MRR:15.56	Hits@10:31.49	Best:15.56
2024-12-27 03:08:26,815: Snapshot:3	Epoch:5	Loss:27.276	translation_Loss:24.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.889                                                   	MRR:16.83	Hits@10:33.03	Best:16.83
2024-12-27 03:08:38,778: Snapshot:3	Epoch:6	Loss:24.859	translation_Loss:22.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.75                                                   	MRR:17.5	Hits@10:33.9	Best:17.5
2024-12-27 03:08:50,673: Snapshot:3	Epoch:7	Loss:23.115	translation_Loss:20.497	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.618                                                   	MRR:18.05	Hits@10:34.51	Best:18.05
2024-12-27 03:09:02,612: Snapshot:3	Epoch:8	Loss:21.782	translation_Loss:19.287	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.495                                                   	MRR:18.53	Hits@10:34.83	Best:18.53
2024-12-27 03:09:14,585: Snapshot:3	Epoch:9	Loss:20.825	translation_Loss:18.428	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.397                                                   	MRR:18.87	Hits@10:35.18	Best:18.87
2024-12-27 03:09:26,536: Snapshot:3	Epoch:10	Loss:20.064	translation_Loss:17.748	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.316                                                   	MRR:19.07	Hits@10:35.3	Best:19.07
2024-12-27 03:09:38,480: Snapshot:3	Epoch:11	Loss:19.432	translation_Loss:17.181	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.252                                                   	MRR:19.32	Hits@10:35.43	Best:19.32
2024-12-27 03:09:50,418: Snapshot:3	Epoch:12	Loss:18.932	translation_Loss:16.72	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.211                                                   	MRR:19.44	Hits@10:35.56	Best:19.44
2024-12-27 03:10:02,357: Snapshot:3	Epoch:13	Loss:18.574	translation_Loss:16.399	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.176                                                   	MRR:19.45	Hits@10:35.62	Best:19.45
2024-12-27 03:10:14,593: Snapshot:3	Epoch:14	Loss:18.27	translation_Loss:16.126	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.144                                                   	MRR:19.57	Hits@10:35.54	Best:19.57
2024-12-27 03:10:26,537: Snapshot:3	Epoch:15	Loss:18.084	translation_Loss:15.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.127                                                   	MRR:19.64	Hits@10:35.6	Best:19.64
2024-12-27 03:10:38,477: Snapshot:3	Epoch:16	Loss:17.83	translation_Loss:15.721	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.109                                                   	MRR:19.61	Hits@10:35.61	Best:19.64
2024-12-27 03:10:50,401: Snapshot:3	Epoch:17	Loss:17.668	translation_Loss:15.571	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.097                                                   	MRR:19.65	Hits@10:35.65	Best:19.65
2024-12-27 03:11:02,340: Snapshot:3	Epoch:18	Loss:17.553	translation_Loss:15.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.088                                                   	MRR:19.78	Hits@10:35.68	Best:19.78
2024-12-27 03:11:14,280: Snapshot:3	Epoch:19	Loss:17.446	translation_Loss:15.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.074                                                   	MRR:19.78	Hits@10:35.66	Best:19.78
2024-12-27 03:11:26,220: Snapshot:3	Epoch:20	Loss:17.337	translation_Loss:15.266	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.072                                                   	MRR:19.78	Hits@10:35.74	Best:19.78
2024-12-27 03:11:38,178: Snapshot:3	Epoch:21	Loss:17.228	translation_Loss:15.165	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.063                                                   	MRR:19.81	Hits@10:35.65	Best:19.81
2024-12-27 03:11:50,101: Snapshot:3	Epoch:22	Loss:17.118	translation_Loss:15.068	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.05                                                   	MRR:19.71	Hits@10:35.76	Best:19.81
2024-12-27 03:12:02,047: Snapshot:3	Epoch:23	Loss:17.021	translation_Loss:14.976	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.045                                                   	MRR:19.85	Hits@10:35.72	Best:19.85
2024-12-27 03:12:13,988: Snapshot:3	Epoch:24	Loss:16.99	translation_Loss:14.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.046                                                   	MRR:19.79	Hits@10:35.65	Best:19.85
2024-12-27 03:12:26,000: Snapshot:3	Epoch:25	Loss:16.917	translation_Loss:14.87	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.047                                                   	MRR:19.81	Hits@10:35.77	Best:19.85
2024-12-27 03:12:37,937: Snapshot:3	Epoch:26	Loss:16.917	translation_Loss:14.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.043                                                   	MRR:19.87	Hits@10:35.81	Best:19.87
2024-12-27 03:12:49,826: Snapshot:3	Epoch:27	Loss:16.844	translation_Loss:14.804	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.04                                                   	MRR:19.81	Hits@10:35.75	Best:19.87
2024-12-27 03:13:01,730: Snapshot:3	Epoch:28	Loss:16.808	translation_Loss:14.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.034                                                   	MRR:19.86	Hits@10:35.77	Best:19.87
2024-12-27 03:13:13,729: Early Stopping! Snapshot: 3 Epoch: 29 Best Results: 19.87
2024-12-27 03:13:13,729: Start to training tokens! Snapshot: 3 Epoch: 29 Loss:16.766 MRR:19.83 Best Results: 19.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:13:13,729: Snapshot:3	Epoch:29	Loss:16.766	translation_Loss:14.728	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.038                                                   	MRR:19.83	Hits@10:35.74	Best:19.87
2024-12-27 03:13:25,447: Snapshot:3	Epoch:30	Loss:183.274	translation_Loss:64.167	multi_layer_Loss:119.107	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.83	Hits@10:35.74	Best:19.87
2024-12-27 03:13:37,222: End of token training: 3 Epoch: 31 Loss:70.242 MRR:19.83 Best Results: 19.87
2024-12-27 03:13:37,223: Snapshot:3	Epoch:31	Loss:70.242	translation_Loss:64.195	multi_layer_Loss:6.047	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:19.83	Hits@10:35.74	Best:19.87
2024-12-27 03:13:37,538: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2024-12-27 03:13:51,025: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.149  | 0.3159 | 0.3795 |  0.4554 |
|     1      | 0.2793 | 0.1702 | 0.3284 | 0.3957 |  0.4956 |
|     2      | 0.2051 | 0.1197 | 0.2383 | 0.2949 |  0.3714 |
|     3      | 0.1971 | 0.1098 | 0.2348 | 0.2895 |  0.3594 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:14:09,041: Snapshot:4	Epoch:0	Loss:22.568	translation_Loss:22.252	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.316                                                   	MRR:6.11	Hits@10:13.14	Best:6.11
2024-12-27 03:14:14,009: Snapshot:4	Epoch:1	Loss:19.8	translation_Loss:19.057	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.743                                                   	MRR:7.64	Hits@10:16.54	Best:7.64
2024-12-27 03:14:18,982: Snapshot:4	Epoch:2	Loss:17.747	translation_Loss:16.653	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.094                                                   	MRR:11.12	Hits@10:24.67	Best:11.12
2024-12-27 03:14:23,940: Snapshot:4	Epoch:3	Loss:16.191	translation_Loss:14.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.353                                                   	MRR:13.12	Hits@10:27.05	Best:13.12
2024-12-27 03:14:28,864: Snapshot:4	Epoch:4	Loss:15.141	translation_Loss:13.607	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.534                                                   	MRR:14.58	Hits@10:28.66	Best:14.58
2024-12-27 03:14:33,819: Snapshot:4	Epoch:5	Loss:14.359	translation_Loss:12.705	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.655                                                   	MRR:15.62	Hits@10:29.77	Best:15.62
2024-12-27 03:14:38,739: Snapshot:4	Epoch:6	Loss:13.741	translation_Loss:12.013	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.728                                                   	MRR:16.35	Hits@10:30.96	Best:16.35
2024-12-27 03:14:43,606: Snapshot:4	Epoch:7	Loss:13.198	translation_Loss:11.427	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.772                                                   	MRR:17.12	Hits@10:31.99	Best:17.12
2024-12-27 03:14:48,549: Snapshot:4	Epoch:8	Loss:12.714	translation_Loss:10.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.796                                                   	MRR:17.66	Hits@10:32.59	Best:17.66
2024-12-27 03:14:53,411: Snapshot:4	Epoch:9	Loss:12.25	translation_Loss:10.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.805                                                   	MRR:18.17	Hits@10:33.05	Best:18.17
2024-12-27 03:14:58,355: Snapshot:4	Epoch:10	Loss:11.827	translation_Loss:10.023	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.804                                                   	MRR:18.69	Hits@10:33.51	Best:18.69
2024-12-27 03:15:03,312: Snapshot:4	Epoch:11	Loss:11.485	translation_Loss:9.691	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.795                                                   	MRR:18.82	Hits@10:33.83	Best:18.82
2024-12-27 03:15:08,293: Snapshot:4	Epoch:12	Loss:11.135	translation_Loss:9.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.784                                                   	MRR:19.32	Hits@10:33.81	Best:19.32
2024-12-27 03:15:13,175: Snapshot:4	Epoch:13	Loss:10.792	translation_Loss:9.021	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.771                                                   	MRR:19.45	Hits@10:33.92	Best:19.45
2024-12-27 03:15:18,088: Snapshot:4	Epoch:14	Loss:10.532	translation_Loss:8.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.755                                                   	MRR:19.87	Hits@10:34.11	Best:19.87
2024-12-27 03:15:23,473: Snapshot:4	Epoch:15	Loss:10.265	translation_Loss:8.525	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.74                                                   	MRR:20.17	Hits@10:34.08	Best:20.17
2024-12-27 03:15:28,364: Snapshot:4	Epoch:16	Loss:10.02	translation_Loss:8.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.724                                                   	MRR:20.33	Hits@10:34.09	Best:20.33
2024-12-27 03:15:33,242: Snapshot:4	Epoch:17	Loss:9.821	translation_Loss:8.115	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.706                                                   	MRR:20.51	Hits@10:34.38	Best:20.51
2024-12-27 03:15:38,166: Snapshot:4	Epoch:18	Loss:9.634	translation_Loss:7.944	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.691                                                   	MRR:20.6	Hits@10:34.25	Best:20.6
2024-12-27 03:15:43,053: Snapshot:4	Epoch:19	Loss:9.434	translation_Loss:7.762	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.672                                                   	MRR:20.82	Hits@10:34.22	Best:20.82
2024-12-27 03:15:47,930: Snapshot:4	Epoch:20	Loss:9.303	translation_Loss:7.644	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.659                                                   	MRR:20.66	Hits@10:34.17	Best:20.82
2024-12-27 03:15:52,842: Snapshot:4	Epoch:21	Loss:9.189	translation_Loss:7.546	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.644                                                   	MRR:20.84	Hits@10:34.11	Best:20.84
2024-12-27 03:15:57,824: Snapshot:4	Epoch:22	Loss:9.07	translation_Loss:7.439	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.631                                                   	MRR:21.0	Hits@10:34.16	Best:21.0
2024-12-27 03:16:02,735: Snapshot:4	Epoch:23	Loss:8.963	translation_Loss:7.35	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.613                                                   	MRR:21.04	Hits@10:34.25	Best:21.04
2024-12-27 03:16:07,586: Snapshot:4	Epoch:24	Loss:8.877	translation_Loss:7.278	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.599                                                   	MRR:20.98	Hits@10:34.16	Best:21.04
2024-12-27 03:16:12,444: Snapshot:4	Epoch:25	Loss:8.775	translation_Loss:7.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.585                                                   	MRR:21.02	Hits@10:34.37	Best:21.04
2024-12-27 03:16:17,298: Early Stopping! Snapshot: 4 Epoch: 26 Best Results: 21.04
2024-12-27 03:16:17,298: Start to training tokens! Snapshot: 4 Epoch: 26 Loss:8.714 MRR:21.03 Best Results: 21.04
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([5, 200]), requires_grad: True
 - torch.Size([5, 200]), requires_grad: True
2024-12-27 03:16:17,299: Snapshot:4	Epoch:26	Loss:8.714	translation_Loss:7.139	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.574                                                   	MRR:21.03	Hits@10:34.14	Best:21.04
2024-12-27 03:16:22,090: Snapshot:4	Epoch:27	Loss:108.49	translation_Loss:29.907	multi_layer_Loss:78.583	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.03	Hits@10:34.14	Best:21.04
2024-12-27 03:16:26,986: End of token training: 4 Epoch: 28 Loss:67.116 MRR:21.03 Best Results: 21.04
2024-12-27 03:16:26,986: Snapshot:4	Epoch:28	Loss:67.116	translation_Loss:29.934	multi_layer_Loss:37.182	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:21.03	Hits@10:34.14	Best:21.04
2024-12-27 03:16:27,291: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2024-12-27 03:16:43,392: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2301 | 0.1253 | 0.2814 | 0.3453 |  0.421  |
|     1      | 0.2679 | 0.1604 | 0.3152 | 0.3825 |  0.4832 |
|     2      | 0.1936 | 0.1098 | 0.2242 | 0.2808 |  0.357  |
|     3      | 0.1832 | 0.0973 | 0.218  | 0.2722 |  0.3434 |
|     4      |  0.21  | 0.1398 | 0.2403 | 0.2872 |  0.3419 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 03:16:43,393: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2577 | 0.1504 | 0.3154 | 0.379  |  0.4523 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2588 | 0.1523 | 0.3163 | 0.3797 |  0.4532 |
|     1      | 0.2857 | 0.1778 | 0.3378 | 0.4072 |  0.4975 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2551 | 0.1453 | 0.3148 | 0.3795 |  0.4545 |
|     1      | 0.2783 | 0.1696 | 0.3244 | 0.399  |  0.4989 |
|     2      | 0.2048 | 0.1205 | 0.2355 | 0.2935 |  0.3706 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2576 | 0.149  | 0.3159 | 0.3795 |  0.4554 |
|     1      | 0.2793 | 0.1702 | 0.3284 | 0.3957 |  0.4956 |
|     2      | 0.2051 | 0.1197 | 0.2383 | 0.2949 |  0.3714 |
|     3      | 0.1971 | 0.1098 | 0.2348 | 0.2895 |  0.3594 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2301 | 0.1253 | 0.2814 | 0.3453 |  0.421  |
|     1      | 0.2679 | 0.1604 | 0.3152 | 0.3825 |  0.4832 |
|     2      | 0.1936 | 0.1098 | 0.2242 | 0.2808 |  0.357  |
|     3      | 0.1832 | 0.0973 | 0.218  | 0.2722 |  0.3434 |
|     4      |  0.21  | 0.1398 | 0.2403 | 0.2872 |  0.3419 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 03:16:43,394: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 273.6489157676697  |   0.258   |     0.15     |    0.315     |     0.452     |
|    1     | 147.28138279914856 |   0.266   |    0.159     |    0.322     |     0.465     |
|    2     | 361.86640524864197 |   0.231   |    0.135     |    0.274     |     0.416     |
|    3     | 403.22833585739136 |   0.219   |    0.126     |     0.26     |     0.394     |
|    4     | 153.2453453540802  |   0.204   |    0.116     |    0.241     |     0.371     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 03:16:43,394: Sum_Training_Time:1339.2703850269318
2024-12-27 03:16:43,394: Every_Training_Time:[273.6489157676697, 147.28138279914856, 361.86640524864197, 403.22833585739136, 153.2453453540802]
2024-12-27 03:16:43,394: Forward transfer: 0.04245 Backward transfer: -0.017624999999999995
2024-12-27 03:17:17,282: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227031649/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=50000.0, token_num=2, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 03:17:26,548: Snapshot:0	Epoch:0	Loss:48.314	translation_Loss:48.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.3	Hits@10:2.38	Best:1.3
2024-12-27 03:17:32,272: Snapshot:0	Epoch:1	Loss:45.002	translation_Loss:45.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.79	Hits@10:7.92	Best:3.79
2024-12-27 03:17:38,033: Snapshot:0	Epoch:2	Loss:41.936	translation_Loss:41.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:12.67	Best:5.6
2024-12-27 03:17:43,755: Snapshot:0	Epoch:3	Loss:39.022	translation_Loss:39.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.85	Hits@10:16.11	Best:6.85
2024-12-27 03:17:49,500: Snapshot:0	Epoch:4	Loss:36.216	translation_Loss:36.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.83	Hits@10:18.97	Best:7.83
2024-12-27 03:17:55,227: Snapshot:0	Epoch:5	Loss:33.472	translation_Loss:33.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.68	Best:8.89
2024-12-27 03:18:01,369: Snapshot:0	Epoch:6	Loss:30.69	translation_Loss:30.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:25.15	Best:10.29
2024-12-27 03:18:07,211: Snapshot:0	Epoch:7	Loss:27.959	translation_Loss:27.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.01	Hits@10:28.75	Best:12.01
2024-12-27 03:18:13,004: Snapshot:0	Epoch:8	Loss:25.173	translation_Loss:25.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.67	Hits@10:31.86	Best:13.67
2024-12-27 03:18:18,748: Snapshot:0	Epoch:9	Loss:22.449	translation_Loss:22.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.24	Hits@10:34.29	Best:15.24
2024-12-27 03:18:24,472: Snapshot:0	Epoch:10	Loss:19.787	translation_Loss:19.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.98	Hits@10:36.32	Best:16.98
2024-12-27 03:18:30,205: Snapshot:0	Epoch:11	Loss:17.285	translation_Loss:17.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.5	Hits@10:38.17	Best:18.5
2024-12-27 03:18:35,958: Snapshot:0	Epoch:12	Loss:14.981	translation_Loss:14.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:39.64	Best:19.77
2024-12-27 03:18:41,766: Snapshot:0	Epoch:13	Loss:13.018	translation_Loss:13.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.86	Hits@10:40.78	Best:20.86
2024-12-27 03:18:47,551: Snapshot:0	Epoch:14	Loss:11.354	translation_Loss:11.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.79	Hits@10:41.75	Best:21.79
2024-12-27 03:18:53,311: Snapshot:0	Epoch:15	Loss:9.882	translation_Loss:9.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.56	Hits@10:42.53	Best:22.56
2024-12-27 03:18:59,177: Snapshot:0	Epoch:16	Loss:8.648	translation_Loss:8.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.08	Hits@10:43.08	Best:23.08
2024-12-27 03:19:05,409: Snapshot:0	Epoch:17	Loss:7.592	translation_Loss:7.592	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.49	Hits@10:43.69	Best:23.49
2024-12-27 03:19:11,203: Snapshot:0	Epoch:18	Loss:6.694	translation_Loss:6.694	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.85	Hits@10:44.12	Best:23.85
2024-12-27 03:19:16,996: Snapshot:0	Epoch:19	Loss:5.931	translation_Loss:5.931	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.16	Hits@10:44.39	Best:24.16
2024-12-27 03:19:22,765: Snapshot:0	Epoch:20	Loss:5.226	translation_Loss:5.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.38	Hits@10:44.65	Best:24.38
2024-12-27 03:19:28,507: Snapshot:0	Epoch:21	Loss:4.673	translation_Loss:4.673	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.5	Hits@10:44.88	Best:24.5
2024-12-27 03:19:34,259: Snapshot:0	Epoch:22	Loss:4.194	translation_Loss:4.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.62	Hits@10:45.15	Best:24.62
2024-12-27 03:19:40,059: Snapshot:0	Epoch:23	Loss:3.782	translation_Loss:3.782	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.71	Hits@10:45.33	Best:24.71
2024-12-27 03:19:45,865: Snapshot:0	Epoch:24	Loss:3.459	translation_Loss:3.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.88	Hits@10:45.47	Best:24.88
2024-12-27 03:19:51,603: Snapshot:0	Epoch:25	Loss:3.107	translation_Loss:3.107	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.99	Hits@10:45.49	Best:24.99
2024-12-27 03:19:57,358: Snapshot:0	Epoch:26	Loss:2.85	translation_Loss:2.85	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.08	Hits@10:45.67	Best:25.08
2024-12-27 03:20:03,131: Snapshot:0	Epoch:27	Loss:2.62	translation_Loss:2.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.18	Hits@10:45.66	Best:25.18
2024-12-27 03:20:09,071: Snapshot:0	Epoch:28	Loss:2.414	translation_Loss:2.414	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.21	Hits@10:45.83	Best:25.21
2024-12-27 03:20:15,300: Snapshot:0	Epoch:29	Loss:2.253	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.33	Hits@10:45.87	Best:25.33
2024-12-27 03:20:21,037: Snapshot:0	Epoch:30	Loss:2.102	translation_Loss:2.102	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:45.88	Best:25.41
2024-12-27 03:20:26,788: Snapshot:0	Epoch:31	Loss:1.939	translation_Loss:1.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.45	Hits@10:46.02	Best:25.45
2024-12-27 03:20:32,544: Snapshot:0	Epoch:32	Loss:1.818	translation_Loss:1.818	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.46	Hits@10:46.13	Best:25.46
2024-12-27 03:20:38,337: Snapshot:0	Epoch:33	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.59	Hits@10:46.29	Best:25.59
2024-12-27 03:20:44,067: Snapshot:0	Epoch:34	Loss:1.621	translation_Loss:1.621	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:46.08	Best:25.59
2024-12-27 03:20:49,813: Snapshot:0	Epoch:35	Loss:1.529	translation_Loss:1.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.62	Hits@10:46.18	Best:25.62
2024-12-27 03:20:55,598: Snapshot:0	Epoch:36	Loss:1.444	translation_Loss:1.444	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.64	Hits@10:46.19	Best:25.64
2024-12-27 03:21:01,401: Snapshot:0	Epoch:37	Loss:1.371	translation_Loss:1.371	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.66	Hits@10:46.16	Best:25.66
2024-12-27 03:21:07,263: Snapshot:0	Epoch:38	Loss:1.298	translation_Loss:1.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.7	Hits@10:46.29	Best:25.7
2024-12-27 03:21:13,114: Snapshot:0	Epoch:39	Loss:1.249	translation_Loss:1.249	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.75	Hits@10:46.31	Best:25.75
2024-12-27 03:21:18,958: Snapshot:0	Epoch:40	Loss:1.189	translation_Loss:1.189	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.84	Hits@10:46.3	Best:25.84
2024-12-27 03:21:24,759: Snapshot:0	Epoch:41	Loss:1.132	translation_Loss:1.132	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.29	Best:25.84
2024-12-27 03:21:30,904: Snapshot:0	Epoch:42	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.76	Hits@10:46.32	Best:25.84
2024-12-27 03:21:36,604: Early Stopping! Snapshot: 0 Epoch: 43 Best Results: 25.84
2024-12-27 03:21:36,604: Start to training tokens! Snapshot: 0 Epoch: 43 Loss:1.041 MRR:25.67 Best Results: 25.84
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 03:21:36,604: Snapshot:0	Epoch:43	Loss:1.041	translation_Loss:1.041	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.25	Best:25.84
2024-12-27 03:21:42,871: Snapshot:0	Epoch:44	Loss:107.589	translation_Loss:35.022	multi_layer_Loss:72.566	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.25	Best:25.84
2024-12-27 03:21:48,651: End of token training: 0 Epoch: 45 Loss:44.968 MRR:25.67 Best Results: 25.84
2024-12-27 03:21:48,651: Snapshot:0	Epoch:45	Loss:44.968	translation_Loss:35.029	multi_layer_Loss:9.938	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.67	Hits@10:46.25	Best:25.84
2024-12-27 03:21:48,898: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-27 03:21:51,417: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1493 | 0.3161 | 0.3794 |  0.4526 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:22:01,942: Snapshot:1	Epoch:0	Loss:15.055	translation_Loss:14.987	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.068                                                   	MRR:6.31	Hits@10:10.54	Best:6.31
2024-12-27 03:22:04,181: Snapshot:1	Epoch:1	Loss:13.584	translation_Loss:13.421	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.163                                                   	MRR:7.06	Hits@10:11.89	Best:7.06
2024-12-27 03:22:06,401: Snapshot:1	Epoch:2	Loss:12.33	translation_Loss:12.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.289                                                   	MRR:8.05	Hits@10:13.98	Best:8.05
2024-12-27 03:22:08,653: Snapshot:1	Epoch:3	Loss:11.191	translation_Loss:10.776	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.415                                                   	MRR:9.59	Hits@10:17.44	Best:9.59
2024-12-27 03:22:10,886: Snapshot:1	Epoch:4	Loss:10.098	translation_Loss:9.572	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.526                                                   	MRR:11.32	Hits@10:21.54	Best:11.32
2024-12-27 03:22:13,107: Snapshot:1	Epoch:5	Loss:9.045	translation_Loss:8.424	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.62                                                   	MRR:13.3	Hits@10:25.44	Best:13.3
2024-12-27 03:22:15,335: Snapshot:1	Epoch:6	Loss:8.084	translation_Loss:7.389	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.694                                                   	MRR:15.04	Hits@10:28.26	Best:15.04
2024-12-27 03:22:17,568: Snapshot:1	Epoch:7	Loss:7.248	translation_Loss:6.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.746                                                   	MRR:16.48	Hits@10:30.47	Best:16.48
2024-12-27 03:22:19,818: Snapshot:1	Epoch:8	Loss:6.553	translation_Loss:5.774	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.779                                                   	MRR:17.69	Hits@10:32.44	Best:17.69
2024-12-27 03:22:22,063: Snapshot:1	Epoch:9	Loss:5.957	translation_Loss:5.161	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.796                                                   	MRR:18.78	Hits@10:34.0	Best:18.78
2024-12-27 03:22:24,288: Snapshot:1	Epoch:10	Loss:5.46	translation_Loss:4.657	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.803                                                   	MRR:19.74	Hits@10:35.4	Best:19.74
2024-12-27 03:22:26,530: Snapshot:1	Epoch:11	Loss:5.048	translation_Loss:4.246	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.803                                                   	MRR:20.55	Hits@10:36.99	Best:20.55
2024-12-27 03:22:28,760: Snapshot:1	Epoch:12	Loss:4.661	translation_Loss:3.865	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.797                                                   	MRR:21.31	Hits@10:38.43	Best:21.31
2024-12-27 03:22:30,990: Snapshot:1	Epoch:13	Loss:4.375	translation_Loss:3.587	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.788                                                   	MRR:21.99	Hits@10:39.57	Best:21.99
2024-12-27 03:22:33,257: Snapshot:1	Epoch:14	Loss:4.111	translation_Loss:3.333	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.778                                                   	MRR:22.64	Hits@10:40.55	Best:22.64
2024-12-27 03:22:35,495: Snapshot:1	Epoch:15	Loss:3.888	translation_Loss:3.119	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.769                                                   	MRR:23.13	Hits@10:41.6	Best:23.13
2024-12-27 03:22:37,721: Snapshot:1	Epoch:16	Loss:3.685	translation_Loss:2.927	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.758                                                   	MRR:23.53	Hits@10:42.49	Best:23.53
2024-12-27 03:22:39,935: Snapshot:1	Epoch:17	Loss:3.512	translation_Loss:2.765	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.747                                                   	MRR:23.88	Hits@10:43.22	Best:23.88
2024-12-27 03:22:42,150: Snapshot:1	Epoch:18	Loss:3.364	translation_Loss:2.628	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.737                                                   	MRR:24.31	Hits@10:43.86	Best:24.31
2024-12-27 03:22:44,718: Snapshot:1	Epoch:19	Loss:3.23	translation_Loss:2.503	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.727                                                   	MRR:24.55	Hits@10:44.43	Best:24.55
2024-12-27 03:22:46,966: Snapshot:1	Epoch:20	Loss:3.105	translation_Loss:2.387	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.718                                                   	MRR:24.96	Hits@10:45.11	Best:24.96
2024-12-27 03:22:49,193: Snapshot:1	Epoch:21	Loss:3.006	translation_Loss:2.297	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.71                                                   	MRR:25.39	Hits@10:45.44	Best:25.39
2024-12-27 03:22:51,402: Snapshot:1	Epoch:22	Loss:2.907	translation_Loss:2.207	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.7                                                   	MRR:25.82	Hits@10:46.08	Best:25.82
2024-12-27 03:22:53,651: Snapshot:1	Epoch:23	Loss:2.813	translation_Loss:2.122	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.691                                                   	MRR:26.06	Hits@10:46.43	Best:26.06
2024-12-27 03:22:55,912: Snapshot:1	Epoch:24	Loss:2.736	translation_Loss:2.052	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.683                                                   	MRR:26.27	Hits@10:46.72	Best:26.27
2024-12-27 03:22:58,167: Snapshot:1	Epoch:25	Loss:2.669	translation_Loss:1.994	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.675                                                   	MRR:26.45	Hits@10:47.23	Best:26.45
2024-12-27 03:23:00,393: Snapshot:1	Epoch:26	Loss:2.604	translation_Loss:1.937	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.668                                                   	MRR:26.71	Hits@10:47.6	Best:26.71
2024-12-27 03:23:02,653: Snapshot:1	Epoch:27	Loss:2.536	translation_Loss:1.876	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.66                                                   	MRR:26.87	Hits@10:47.89	Best:26.87
2024-12-27 03:23:04,892: Snapshot:1	Epoch:28	Loss:2.485	translation_Loss:1.833	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.652                                                   	MRR:27.06	Hits@10:48.12	Best:27.06
2024-12-27 03:23:07,136: Snapshot:1	Epoch:29	Loss:2.443	translation_Loss:1.797	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.646                                                   	MRR:27.19	Hits@10:48.25	Best:27.19
2024-12-27 03:23:09,375: Snapshot:1	Epoch:30	Loss:2.395	translation_Loss:1.756	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.64                                                   	MRR:27.47	Hits@10:48.63	Best:27.47
2024-12-27 03:23:11,582: Snapshot:1	Epoch:31	Loss:2.34	translation_Loss:1.708	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.632                                                   	MRR:27.6	Hits@10:48.68	Best:27.6
2024-12-27 03:23:13,812: Snapshot:1	Epoch:32	Loss:2.303	translation_Loss:1.679	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.625                                                   	MRR:27.79	Hits@10:48.9	Best:27.79
2024-12-27 03:23:16,047: Snapshot:1	Epoch:33	Loss:2.266	translation_Loss:1.647	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.619                                                   	MRR:27.92	Hits@10:49.19	Best:27.92
2024-12-27 03:23:18,277: Snapshot:1	Epoch:34	Loss:2.241	translation_Loss:1.627	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.614                                                   	MRR:28.03	Hits@10:49.44	Best:28.03
2024-12-27 03:23:20,517: Snapshot:1	Epoch:35	Loss:2.203	translation_Loss:1.594	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.609                                                   	MRR:28.16	Hits@10:49.45	Best:28.16
2024-12-27 03:23:22,754: Snapshot:1	Epoch:36	Loss:2.168	translation_Loss:1.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.604                                                   	MRR:28.26	Hits@10:49.54	Best:28.26
2024-12-27 03:23:24,938: Snapshot:1	Epoch:37	Loss:2.144	translation_Loss:1.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.599                                                   	MRR:28.35	Hits@10:49.72	Best:28.35
2024-12-27 03:23:27,174: Snapshot:1	Epoch:38	Loss:2.108	translation_Loss:1.514	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.594                                                   	MRR:28.44	Hits@10:49.76	Best:28.44
2024-12-27 03:23:29,414: Snapshot:1	Epoch:39	Loss:2.098	translation_Loss:1.51	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.588                                                   	MRR:28.52	Hits@10:49.68	Best:28.52
2024-12-27 03:23:31,636: Snapshot:1	Epoch:40	Loss:2.062	translation_Loss:1.477	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.585                                                   	MRR:28.55	Hits@10:49.94	Best:28.55
2024-12-27 03:23:33,847: Snapshot:1	Epoch:41	Loss:2.045	translation_Loss:1.465	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.58                                                   	MRR:28.55	Hits@10:49.86	Best:28.55
2024-12-27 03:23:36,067: Snapshot:1	Epoch:42	Loss:2.022	translation_Loss:1.446	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.576                                                   	MRR:28.68	Hits@10:49.82	Best:28.68
2024-12-27 03:23:38,304: Snapshot:1	Epoch:43	Loss:1.99	translation_Loss:1.419	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.571                                                   	MRR:28.76	Hits@10:49.95	Best:28.76
2024-12-27 03:23:40,525: Snapshot:1	Epoch:44	Loss:1.971	translation_Loss:1.404	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.567                                                   	MRR:28.81	Hits@10:50.04	Best:28.81
2024-12-27 03:23:42,730: Snapshot:1	Epoch:45	Loss:1.955	translation_Loss:1.393	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.562                                                   	MRR:28.8	Hits@10:50.05	Best:28.81
2024-12-27 03:23:44,944: Snapshot:1	Epoch:46	Loss:1.934	translation_Loss:1.377	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.557                                                   	MRR:28.85	Hits@10:50.11	Best:28.85
2024-12-27 03:23:47,161: Snapshot:1	Epoch:47	Loss:1.915	translation_Loss:1.362	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.554                                                   	MRR:28.91	Hits@10:50.28	Best:28.91
2024-12-27 03:23:49,424: Snapshot:1	Epoch:48	Loss:1.901	translation_Loss:1.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.55                                                   	MRR:29.02	Hits@10:50.42	Best:29.02
2024-12-27 03:23:51,657: Snapshot:1	Epoch:49	Loss:1.895	translation_Loss:1.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.547                                                   	MRR:29.04	Hits@10:50.31	Best:29.04
2024-12-27 03:23:53,872: Snapshot:1	Epoch:50	Loss:1.863	translation_Loss:1.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.544                                                   	MRR:29.13	Hits@10:50.67	Best:29.13
2024-12-27 03:23:56,099: Snapshot:1	Epoch:51	Loss:1.857	translation_Loss:1.318	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.54                                                   	MRR:29.21	Hits@10:50.66	Best:29.21
2024-12-27 03:23:58,745: Snapshot:1	Epoch:52	Loss:1.844	translation_Loss:1.309	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.535                                                   	MRR:29.1	Hits@10:50.8	Best:29.21
2024-12-27 03:24:00,949: Snapshot:1	Epoch:53	Loss:1.822	translation_Loss:1.29	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.531                                                   	MRR:29.14	Hits@10:50.75	Best:29.21
2024-12-27 03:24:03,157: Early Stopping! Snapshot: 1 Epoch: 54 Best Results: 29.21
2024-12-27 03:24:03,157: Start to training tokens! Snapshot: 1 Epoch: 54 Loss:1.817 MRR:29.14 Best Results: 29.21
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 03:24:03,158: Snapshot:1	Epoch:54	Loss:1.817	translation_Loss:1.288	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.529                                                   	MRR:29.14	Hits@10:50.81	Best:29.21
2024-12-27 03:24:05,351: Snapshot:1	Epoch:55	Loss:54.226	translation_Loss:12.685	multi_layer_Loss:41.541	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:29.14	Hits@10:50.81	Best:29.21
2024-12-27 03:24:07,547: End of token training: 1 Epoch: 56 Loss:37.063 MRR:29.14 Best Results: 29.21
2024-12-27 03:24:07,547: Snapshot:1	Epoch:56	Loss:37.063	translation_Loss:12.713	multi_layer_Loss:24.35	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:29.14	Hits@10:50.81	Best:29.21
2024-12-27 03:24:07,850: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-27 03:24:11,551: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2563 | 0.1485 | 0.3144 | 0.3814 |  0.4527 |
|     1      | 0.2891 | 0.1804 | 0.3413 | 0.4097 |  0.5031 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:24:43,752: Snapshot:2	Epoch:0	Loss:61.718	translation_Loss:61.06	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.658                                                   	MRR:5.47	Hits@10:11.88	Best:5.47
2024-12-27 03:24:53,410: Snapshot:2	Epoch:1	Loss:49.857	translation_Loss:48.354	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.503                                                   	MRR:8.9	Hits@10:19.27	Best:8.9
2024-12-27 03:25:03,071: Snapshot:2	Epoch:2	Loss:40.037	translation_Loss:37.962	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.075                                                   	MRR:11.93	Hits@10:24.66	Best:11.93
2024-12-27 03:25:12,936: Snapshot:2	Epoch:3	Loss:32.528	translation_Loss:30.187	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.341                                                   	MRR:14.16	Hits@10:28.77	Best:14.16
2024-12-27 03:25:22,698: Snapshot:2	Epoch:4	Loss:27.204	translation_Loss:24.811	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.393                                                   	MRR:15.74	Hits@10:31.28	Best:15.74
2024-12-27 03:25:32,573: Snapshot:2	Epoch:5	Loss:23.344	translation_Loss:21.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.342                                                   	MRR:16.84	Hits@10:33.06	Best:16.84
2024-12-27 03:25:42,470: Snapshot:2	Epoch:6	Loss:20.501	translation_Loss:18.259	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.243                                                   	MRR:17.69	Hits@10:34.15	Best:17.69
2024-12-27 03:25:52,283: Snapshot:2	Epoch:7	Loss:18.433	translation_Loss:16.304	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.129                                                   	MRR:18.32	Hits@10:35.13	Best:18.32
2024-12-27 03:26:01,991: Snapshot:2	Epoch:8	Loss:16.938	translation_Loss:14.911	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.027                                                   	MRR:18.85	Hits@10:35.74	Best:18.85
2024-12-27 03:26:11,751: Snapshot:2	Epoch:9	Loss:15.796	translation_Loss:13.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.934                                                   	MRR:19.17	Hits@10:36.18	Best:19.17
2024-12-27 03:26:21,451: Snapshot:2	Epoch:10	Loss:14.904	translation_Loss:13.042	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.861                                                   	MRR:19.45	Hits@10:36.43	Best:19.45
2024-12-27 03:26:31,253: Snapshot:2	Epoch:11	Loss:14.27	translation_Loss:12.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.807                                                   	MRR:19.6	Hits@10:36.68	Best:19.6
2024-12-27 03:26:41,063: Snapshot:2	Epoch:12	Loss:13.718	translation_Loss:11.957	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.761                                                   	MRR:19.95	Hits@10:36.79	Best:19.95
2024-12-27 03:26:51,217: Snapshot:2	Epoch:13	Loss:13.285	translation_Loss:11.564	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.721                                                   	MRR:20.06	Hits@10:36.88	Best:20.06
2024-12-27 03:27:00,870: Snapshot:2	Epoch:14	Loss:12.925	translation_Loss:11.238	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.687                                                   	MRR:20.26	Hits@10:36.92	Best:20.26
2024-12-27 03:27:10,728: Snapshot:2	Epoch:15	Loss:12.616	translation_Loss:10.956	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.659                                                   	MRR:20.39	Hits@10:37.03	Best:20.39
2024-12-27 03:27:20,574: Snapshot:2	Epoch:16	Loss:12.411	translation_Loss:10.769	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.642                                                   	MRR:20.44	Hits@10:37.05	Best:20.44
2024-12-27 03:27:30,344: Snapshot:2	Epoch:17	Loss:12.207	translation_Loss:10.586	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.621                                                   	MRR:20.55	Hits@10:37.05	Best:20.55
2024-12-27 03:27:40,152: Snapshot:2	Epoch:18	Loss:12.041	translation_Loss:10.432	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.609                                                   	MRR:20.59	Hits@10:37.1	Best:20.59
2024-12-27 03:27:49,822: Snapshot:2	Epoch:19	Loss:11.88	translation_Loss:10.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.595                                                   	MRR:20.51	Hits@10:37.14	Best:20.59
2024-12-27 03:27:59,437: Snapshot:2	Epoch:20	Loss:11.721	translation_Loss:10.142	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.579                                                   	MRR:20.59	Hits@10:37.09	Best:20.59
2024-12-27 03:28:09,138: Snapshot:2	Epoch:21	Loss:11.625	translation_Loss:10.053	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.572                                                   	MRR:20.62	Hits@10:37.23	Best:20.62
2024-12-27 03:28:18,818: Snapshot:2	Epoch:22	Loss:11.513	translation_Loss:9.948	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.566                                                   	MRR:20.62	Hits@10:37.26	Best:20.62
2024-12-27 03:28:28,652: Snapshot:2	Epoch:23	Loss:11.445	translation_Loss:9.885	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.56                                                   	MRR:20.77	Hits@10:37.32	Best:20.77
2024-12-27 03:28:38,874: Snapshot:2	Epoch:24	Loss:11.369	translation_Loss:9.821	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.548                                                   	MRR:20.65	Hits@10:37.22	Best:20.77
2024-12-27 03:28:48,659: Snapshot:2	Epoch:25	Loss:11.296	translation_Loss:9.751	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.544                                                   	MRR:20.63	Hits@10:37.26	Best:20.77
2024-12-27 03:28:58,415: Early Stopping! Snapshot: 2 Epoch: 26 Best Results: 20.77
2024-12-27 03:28:58,415: Start to training tokens! Snapshot: 2 Epoch: 26 Loss:11.2 MRR:20.71 Best Results: 20.77
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 03:28:58,415: Snapshot:2	Epoch:26	Loss:11.2	translation_Loss:9.661	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.539                                                   	MRR:20.71	Hits@10:37.19	Best:20.77
2024-12-27 03:29:08,176: Snapshot:2	Epoch:27	Loss:136.59	translation_Loss:56.385	multi_layer_Loss:80.206	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.71	Hits@10:37.19	Best:20.77
2024-12-27 03:29:17,966: End of token training: 2 Epoch: 28 Loss:59.829 MRR:20.71 Best Results: 20.77
2024-12-27 03:29:17,966: Snapshot:2	Epoch:28	Loss:59.829	translation_Loss:56.419	multi_layer_Loss:3.409	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.71	Hits@10:37.19	Best:20.77
2024-12-27 03:29:18,284: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-27 03:29:26,516: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2508 | 0.1393 | 0.3106 | 0.3766 |  0.4539 |
|     1      | 0.2703 | 0.1595 | 0.3211 | 0.3926 |  0.4841 |
|     2      | 0.2084 | 0.1218 | 0.242  | 0.2997 |  0.374  |
+------------+--------+--------+--------+--------+---------+
Snapshot 3: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:30:04,490: Snapshot:3	Epoch:0	Loss:64.328	translation_Loss:63.3	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.029                                                   	MRR:6.49	Hits@10:15.86	Best:6.49
2024-12-27 03:30:16,614: Snapshot:3	Epoch:1	Loss:48.08	translation_Loss:45.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.529                                                   	MRR:10.23	Hits@10:22.64	Best:10.23
2024-12-27 03:30:28,595: Snapshot:3	Epoch:2	Loss:37.668	translation_Loss:34.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.453                                                   	MRR:12.87	Hits@10:27.32	Best:12.87
2024-12-27 03:30:40,961: Snapshot:3	Epoch:3	Loss:31.347	translation_Loss:27.516	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.832                                                   	MRR:14.7	Hits@10:30.67	Best:14.7
2024-12-27 03:30:52,947: Snapshot:3	Epoch:4	Loss:27.199	translation_Loss:23.28	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.919                                                   	MRR:16.28	Hits@10:33.13	Best:16.28
2024-12-27 03:31:04,909: Snapshot:3	Epoch:5	Loss:24.222	translation_Loss:20.347	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.875                                                   	MRR:17.54	Hits@10:34.64	Best:17.54
2024-12-27 03:31:16,926: Snapshot:3	Epoch:6	Loss:22.018	translation_Loss:18.247	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.771                                                   	MRR:18.22	Hits@10:35.44	Best:18.22
2024-12-27 03:31:28,894: Snapshot:3	Epoch:7	Loss:20.469	translation_Loss:16.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.639                                                   	MRR:18.73	Hits@10:36.08	Best:18.73
2024-12-27 03:31:40,919: Snapshot:3	Epoch:8	Loss:19.293	translation_Loss:15.785	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.508                                                   	MRR:19.2	Hits@10:36.51	Best:19.2
2024-12-27 03:31:52,941: Snapshot:3	Epoch:9	Loss:18.321	translation_Loss:14.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.382                                                   	MRR:19.51	Hits@10:36.68	Best:19.51
2024-12-27 03:32:04,881: Snapshot:3	Epoch:10	Loss:17.59	translation_Loss:14.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.278                                                   	MRR:19.68	Hits@10:36.85	Best:19.68
2024-12-27 03:32:16,692: Snapshot:3	Epoch:11	Loss:17.042	translation_Loss:13.857	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.186                                                   	MRR:19.86	Hits@10:36.97	Best:19.86
2024-12-27 03:32:28,967: Snapshot:3	Epoch:12	Loss:16.571	translation_Loss:13.463	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.108                                                   	MRR:19.96	Hits@10:36.97	Best:19.96
2024-12-27 03:32:40,814: Snapshot:3	Epoch:13	Loss:16.21	translation_Loss:13.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:3.042                                                   	MRR:20.04	Hits@10:36.94	Best:20.04
2024-12-27 03:32:52,672: Snapshot:3	Epoch:14	Loss:15.894	translation_Loss:12.904	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.99                                                   	MRR:20.1	Hits@10:37.01	Best:20.1
2024-12-27 03:33:04,583: Snapshot:3	Epoch:15	Loss:15.692	translation_Loss:12.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.952                                                   	MRR:20.16	Hits@10:37.11	Best:20.16
2024-12-27 03:33:16,429: Snapshot:3	Epoch:16	Loss:15.473	translation_Loss:12.56	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.913                                                   	MRR:20.18	Hits@10:37.09	Best:20.18
2024-12-27 03:33:28,768: Snapshot:3	Epoch:17	Loss:15.327	translation_Loss:12.44	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.888                                                   	MRR:20.21	Hits@10:37.12	Best:20.21
2024-12-27 03:33:40,609: Snapshot:3	Epoch:18	Loss:15.157	translation_Loss:12.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.865                                                   	MRR:20.24	Hits@10:37.08	Best:20.24
2024-12-27 03:33:52,439: Snapshot:3	Epoch:19	Loss:15.035	translation_Loss:12.191	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.843                                                   	MRR:20.29	Hits@10:37.01	Best:20.29
2024-12-27 03:34:04,240: Snapshot:3	Epoch:20	Loss:14.943	translation_Loss:12.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.832                                                   	MRR:20.36	Hits@10:37.05	Best:20.36
2024-12-27 03:34:16,281: Snapshot:3	Epoch:21	Loss:14.856	translation_Loss:12.034	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.822                                                   	MRR:20.41	Hits@10:37.06	Best:20.41
2024-12-27 03:34:28,108: Snapshot:3	Epoch:22	Loss:14.74	translation_Loss:11.932	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.808                                                   	MRR:20.3	Hits@10:37.09	Best:20.41
2024-12-27 03:34:39,849: Snapshot:3	Epoch:23	Loss:14.635	translation_Loss:11.839	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.796                                                   	MRR:20.35	Hits@10:37.05	Best:20.41
2024-12-27 03:34:51,628: Early Stopping! Snapshot: 3 Epoch: 24 Best Results: 20.41
2024-12-27 03:34:51,628: Start to training tokens! Snapshot: 3 Epoch: 24 Loss:14.599 MRR:20.38 Best Results: 20.41
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 03:34:51,628: Snapshot:3	Epoch:24	Loss:14.599	translation_Loss:11.808	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.79                                                   	MRR:20.38	Hits@10:37.14	Best:20.41
2024-12-27 03:35:03,330: Snapshot:3	Epoch:25	Loss:139.601	translation_Loss:62.029	multi_layer_Loss:77.573	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.38	Hits@10:37.14	Best:20.41
2024-12-27 03:35:15,178: End of token training: 3 Epoch: 26 Loss:64.946 MRR:20.38 Best Results: 20.41
2024-12-27 03:35:15,178: Snapshot:3	Epoch:26	Loss:64.946	translation_Loss:62.038	multi_layer_Loss:2.908	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.38	Hits@10:37.14	Best:20.41
2024-12-27 03:35:15,495: => loading checkpoint './checkpoint/HYBRID/3model_best.tar'
2024-12-27 03:35:29,837: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1463 | 0.3113 | 0.3754 |  0.4544 |
|     1      | 0.2715 | 0.1634 | 0.3193 | 0.3874 |  0.4817 |
|     2      | 0.2055 | 0.1168 | 0.2404 | 0.2987 |  0.3757 |
|     3      | 0.204  | 0.1152 | 0.2405 | 0.2975 |  0.3723 |
+------------+--------+--------+--------+--------+---------+
Snapshot 4: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:35:47,335: Snapshot:4	Epoch:0	Loss:21.903	translation_Loss:21.698	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.205                                                   	MRR:7.01	Hits@10:14.23	Best:7.01
2024-12-27 03:35:52,183: Snapshot:4	Epoch:1	Loss:18.534	translation_Loss:17.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.538                                                   	MRR:8.6	Hits@10:18.27	Best:8.6
2024-12-27 03:35:57,108: Snapshot:4	Epoch:2	Loss:15.975	translation_Loss:15.062	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.914                                                   	MRR:12.35	Hits@10:26.39	Best:12.35
2024-12-27 03:36:02,062: Snapshot:4	Epoch:3	Loss:14.104	translation_Loss:12.863	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.241                                                   	MRR:14.59	Hits@10:29.27	Best:14.59
2024-12-27 03:36:07,297: Snapshot:4	Epoch:4	Loss:12.885	translation_Loss:11.384	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.501                                                   	MRR:16.21	Hits@10:30.88	Best:16.21
2024-12-27 03:36:12,165: Snapshot:4	Epoch:5	Loss:12.017	translation_Loss:10.319	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.698                                                   	MRR:17.31	Hits@10:32.07	Best:17.31
2024-12-27 03:36:17,080: Snapshot:4	Epoch:6	Loss:11.38	translation_Loss:9.537	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.844                                                   	MRR:17.93	Hits@10:32.8	Best:17.93
2024-12-27 03:36:21,992: Snapshot:4	Epoch:7	Loss:10.824	translation_Loss:8.873	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.951                                                   	MRR:18.71	Hits@10:33.71	Best:18.71
2024-12-27 03:36:26,872: Snapshot:4	Epoch:8	Loss:10.377	translation_Loss:8.351	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.026                                                   	MRR:19.13	Hits@10:34.48	Best:19.13
2024-12-27 03:36:31,721: Snapshot:4	Epoch:9	Loss:9.979	translation_Loss:7.9	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.079                                                   	MRR:19.35	Hits@10:34.78	Best:19.35
2024-12-27 03:36:36,603: Snapshot:4	Epoch:10	Loss:9.618	translation_Loss:7.499	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.119                                                   	MRR:19.45	Hits@10:35.18	Best:19.45
2024-12-27 03:36:41,422: Snapshot:4	Epoch:11	Loss:9.254	translation_Loss:7.111	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.144                                                   	MRR:20.07	Hits@10:35.4	Best:20.07
2024-12-27 03:36:46,282: Snapshot:4	Epoch:12	Loss:8.938	translation_Loss:6.778	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.16                                                   	MRR:20.37	Hits@10:35.78	Best:20.37
2024-12-27 03:36:51,171: Snapshot:4	Epoch:13	Loss:8.674	translation_Loss:6.508	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.165                                                   	MRR:20.52	Hits@10:35.51	Best:20.52
2024-12-27 03:36:56,085: Snapshot:4	Epoch:14	Loss:8.45	translation_Loss:6.284	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.167                                                   	MRR:20.77	Hits@10:35.81	Best:20.77
2024-12-27 03:37:00,986: Snapshot:4	Epoch:15	Loss:8.163	translation_Loss:5.999	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.164                                                   	MRR:21.01	Hits@10:35.8	Best:21.01
2024-12-27 03:37:05,921: Snapshot:4	Epoch:16	Loss:7.995	translation_Loss:5.841	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.154                                                   	MRR:21.28	Hits@10:35.97	Best:21.28
2024-12-27 03:37:10,781: Snapshot:4	Epoch:17	Loss:7.792	translation_Loss:5.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.147                                                   	MRR:21.37	Hits@10:35.87	Best:21.37
2024-12-27 03:37:16,131: Snapshot:4	Epoch:18	Loss:7.641	translation_Loss:5.504	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.137                                                   	MRR:21.44	Hits@10:35.9	Best:21.44
2024-12-27 03:37:21,005: Snapshot:4	Epoch:19	Loss:7.504	translation_Loss:5.38	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.124                                                   	MRR:21.78	Hits@10:35.89	Best:21.78
2024-12-27 03:37:25,835: Snapshot:4	Epoch:20	Loss:7.355	translation_Loss:5.244	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.11                                                   	MRR:21.68	Hits@10:36.09	Best:21.78
2024-12-27 03:37:30,702: Snapshot:4	Epoch:21	Loss:7.221	translation_Loss:5.125	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.096                                                   	MRR:21.83	Hits@10:36.23	Best:21.83
2024-12-27 03:37:35,555: Snapshot:4	Epoch:22	Loss:7.111	translation_Loss:5.027	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.084                                                   	MRR:22.03	Hits@10:36.11	Best:22.03
2024-12-27 03:37:40,384: Snapshot:4	Epoch:23	Loss:7.019	translation_Loss:4.95	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.069                                                   	MRR:21.87	Hits@10:36.25	Best:22.03
2024-12-27 03:37:45,208: Snapshot:4	Epoch:24	Loss:6.946	translation_Loss:4.891	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.055                                                   	MRR:22.13	Hits@10:36.39	Best:22.13
2024-12-27 03:37:49,990: Snapshot:4	Epoch:25	Loss:6.866	translation_Loss:4.821	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.045                                                   	MRR:22.08	Hits@10:36.2	Best:22.13
2024-12-27 03:37:54,872: Snapshot:4	Epoch:26	Loss:6.778	translation_Loss:4.744	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.034                                                   	MRR:22.27	Hits@10:36.13	Best:22.27
2024-12-27 03:37:59,723: Snapshot:4	Epoch:27	Loss:6.764	translation_Loss:4.739	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.024                                                   	MRR:22.22	Hits@10:36.07	Best:22.27
2024-12-27 03:38:04,542: Snapshot:4	Epoch:28	Loss:6.705	translation_Loss:4.689	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.016                                                   	MRR:22.01	Hits@10:36.26	Best:22.27
2024-12-27 03:38:09,381: Snapshot:4	Epoch:29	Loss:6.66	translation_Loss:4.655	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:2.006                                                   	MRR:22.33	Hits@10:36.25	Best:22.33
2024-12-27 03:38:14,162: Snapshot:4	Epoch:30	Loss:6.584	translation_Loss:4.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.995                                                   	MRR:22.14	Hits@10:36.15	Best:22.33
2024-12-27 03:38:18,974: Snapshot:4	Epoch:31	Loss:6.565	translation_Loss:4.581	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.984                                                   	MRR:22.1	Hits@10:36.08	Best:22.33
2024-12-27 03:38:23,755: Early Stopping! Snapshot: 4 Epoch: 32 Best Results: 22.33
2024-12-27 03:38:23,756: Start to training tokens! Snapshot: 4 Epoch: 32 Loss:6.522 MRR:22.08 Best Results: 22.33
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([2, 200]), requires_grad: True
 - torch.Size([2, 200]), requires_grad: True
2024-12-27 03:38:23,756: Snapshot:4	Epoch:32	Loss:6.522	translation_Loss:4.545	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.977                                                   	MRR:22.08	Hits@10:36.21	Best:22.33
2024-12-27 03:38:28,559: Snapshot:4	Epoch:33	Loss:91.257	translation_Loss:28.537	multi_layer_Loss:62.72	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.08	Hits@10:36.21	Best:22.33
2024-12-27 03:38:33,364: End of token training: 4 Epoch: 34 Loss:45.95 MRR:22.08 Best Results: 22.33
2024-12-27 03:38:33,364: Snapshot:4	Epoch:34	Loss:45.95	translation_Loss:28.531	multi_layer_Loss:17.42	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:22.08	Hits@10:36.21	Best:22.33
2024-12-27 03:38:33,689: => loading checkpoint './checkpoint/HYBRID/4model_best.tar'
2024-12-27 03:38:49,621: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2274 | 0.1289 | 0.2727 | 0.3361 |  0.4075 |
|     1      | 0.2564 | 0.1511 | 0.3005 | 0.3672 |  0.4607 |
|     2      | 0.1917 | 0.1078 | 0.222  | 0.2752 |  0.354  |
|     3      | 0.1801 | 0.0933 | 0.2124 | 0.2687 |  0.3466 |
|     4      | 0.2211 | 0.1464 | 0.2544 |  0.3   |  0.3605 |
+------------+--------+--------+--------+--------+---------+
2024-12-27 03:38:49,623: Final Result:
[+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2571 | 0.1493 | 0.3161 | 0.3794 |  0.4526 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2563 | 0.1485 | 0.3144 | 0.3814 |  0.4527 |
|     1      | 0.2891 | 0.1804 | 0.3413 | 0.4097 |  0.5031 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2508 | 0.1393 | 0.3106 | 0.3766 |  0.4539 |
|     1      | 0.2703 | 0.1595 | 0.3211 | 0.3926 |  0.4841 |
|     2      | 0.2084 | 0.1218 | 0.242  | 0.2997 |  0.374  |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:3 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2549 | 0.1463 | 0.3113 | 0.3754 |  0.4544 |
|     1      | 0.2715 | 0.1634 | 0.3193 | 0.3874 |  0.4817 |
|     2      | 0.2055 | 0.1168 | 0.2404 | 0.2987 |  0.3757 |
|     3      | 0.204  | 0.1152 | 0.2405 | 0.2975 |  0.3723 |
+------------+--------+--------+--------+--------+---------+, +------------+--------+--------+--------+--------+---------+
| Snapshot:4 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2274 | 0.1289 | 0.2727 | 0.3361 |  0.4075 |
|     1      | 0.2564 | 0.1511 | 0.3005 | 0.3672 |  0.4607 |
|     2      | 0.1917 | 0.1078 | 0.222  | 0.2752 |  0.354  |
|     3      | 0.1801 | 0.0933 | 0.2124 | 0.2687 |  0.3466 |
|     4      | 0.2211 | 0.1464 | 0.2544 |  0.3   |  0.3605 |
+------------+--------+--------+--------+--------+---------+]
2024-12-27 03:38:49,624: Report Result:
+----------+--------------------+-----------+--------------+--------------+---------------+
| Snapshot |        Time        | Whole_MRR | Whole_Hits@1 | Whole_Hits@3 | Whole_Hits@10 |
+----------+--------------------+-----------+--------------+--------------+---------------+
|    0     | 271.36805152893066 |   0.257   |    0.149     |    0.316     |     0.453     |
|    1     | 134.9185140132904  |   0.265   |    0.157     |    0.322     |     0.466     |
|    2     |  302.295951128006  |   0.231   |    0.133     |    0.276     |     0.415     |
|    3     | 343.4453635215759  |   0.221   |    0.126     |    0.262     |     0.399     |
|    4     | 181.02928519248962 |   0.203   |    0.115     |    0.238     |     0.369     |
+----------+--------------------+-----------+--------------+--------------+---------------+
2024-12-27 03:38:49,624: Sum_Training_Time:1233.0571653842926
2024-12-27 03:38:49,624: Every_Training_Time:[271.36805152893066, 134.9185140132904, 302.295951128006, 343.4453635215759, 181.02928519248962]
2024-12-27 03:38:49,624: Forward transfer: 0.044149999999999995 Backward transfer: -0.025749999999999995
2024-12-27 03:39:21,720: Namespace(MAE_loss_weights=[1e-05, 1e-05, 1e-05, 1e-05], batch_size='1024', contrast_loss_weight=0.1, data_path='./data/HYBRID/', dataset='HYBRID', device=device(type='cuda', index=0), emb_dim=200, embedding_distill_weight=0.1, epoch_num=200, first_layer_epoch_num=10, first_training=True, gpu=0, l2=0.0, learning_rate=0.0001, lifelong_name='double_tokened', log_path='./logs/20241227033853/HYBRID', logger=<RootLogger root (INFO)>, margin=8.0, mask_ratio=0.2, multi_distill_num=3, multi_layer_distance_weight=40, multi_layer_weight=1.0, multi_layers_path='train_sorted_by_edges_betweenness.txt', muti_embedding_distill_weight=1, neg_ratio=10, note='', num_layer=1, num_old_triples=20000, patience=3, predict_result=False, random_seed=3407, record=False, reply_loss_weight=0.1, save_path='./checkpoint/HYBRID', score_distill_weight=1, second_layer_epoch_num=20, skip_previous='False', snapshot_num=5, structure_distill_weight=0.1, token_distillation_weight=50000.0, token_num=10, train_new=True, two_stage_epoch_num=20, use_multi_layers='False', use_two_stage='False', using_MAE_loss=False, using_all_data=False, using_contrast_distill=False, using_different_weights=True, using_embedding_distill=True, using_mask_weight=True, using_multi_embedding_distill=False, using_multi_layer_distance_loss=False, using_relation_distill=False, using_reply=False, using_score_distill=False, using_structure_distill=False, using_test=False, using_token_distillation_loss=True, valid_metrics='mrr', without_hier_distill=False, without_multi_layers='True', without_two_stage=False)
Snapshot 0: No changes made to optimizer or model parameters.
Start training =============================
2024-12-27 03:39:31,006: Snapshot:0	Epoch:0	Loss:48.314	translation_Loss:48.314	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:1.3	Hits@10:2.38	Best:1.3
2024-12-27 03:39:36,828: Snapshot:0	Epoch:1	Loss:45.002	translation_Loss:45.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:3.79	Hits@10:7.92	Best:3.79
2024-12-27 03:39:42,611: Snapshot:0	Epoch:2	Loss:41.936	translation_Loss:41.936	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:5.6	Hits@10:12.67	Best:5.6
2024-12-27 03:39:48,407: Snapshot:0	Epoch:3	Loss:39.022	translation_Loss:39.022	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:6.85	Hits@10:16.11	Best:6.85
2024-12-27 03:39:54,225: Snapshot:0	Epoch:4	Loss:36.216	translation_Loss:36.216	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:7.82	Hits@10:18.96	Best:7.82
2024-12-27 03:40:00,013: Snapshot:0	Epoch:5	Loss:33.472	translation_Loss:33.472	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:8.89	Hits@10:21.69	Best:8.89
2024-12-27 03:40:06,390: Snapshot:0	Epoch:6	Loss:30.69	translation_Loss:30.69	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:10.29	Hits@10:25.15	Best:10.29
2024-12-27 03:40:12,315: Snapshot:0	Epoch:7	Loss:27.959	translation_Loss:27.959	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:12.01	Hits@10:28.73	Best:12.01
2024-12-27 03:40:18,212: Snapshot:0	Epoch:8	Loss:25.173	translation_Loss:25.173	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:13.68	Hits@10:31.86	Best:13.68
2024-12-27 03:40:24,012: Snapshot:0	Epoch:9	Loss:22.449	translation_Loss:22.449	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:15.26	Hits@10:34.29	Best:15.26
2024-12-27 03:40:29,786: Snapshot:0	Epoch:10	Loss:19.787	translation_Loss:19.787	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:16.97	Hits@10:36.33	Best:16.97
2024-12-27 03:40:35,694: Snapshot:0	Epoch:11	Loss:17.285	translation_Loss:17.285	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:18.49	Hits@10:38.14	Best:18.49
2024-12-27 03:40:41,603: Snapshot:0	Epoch:12	Loss:14.981	translation_Loss:14.981	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:19.77	Hits@10:39.62	Best:19.77
2024-12-27 03:40:47,397: Snapshot:0	Epoch:13	Loss:13.018	translation_Loss:13.018	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.89	Hits@10:40.75	Best:20.89
2024-12-27 03:40:53,218: Snapshot:0	Epoch:14	Loss:11.355	translation_Loss:11.355	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:21.8	Hits@10:41.79	Best:21.8
2024-12-27 03:40:59,086: Snapshot:0	Epoch:15	Loss:9.882	translation_Loss:9.882	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:22.55	Hits@10:42.54	Best:22.55
2024-12-27 03:41:04,931: Snapshot:0	Epoch:16	Loss:8.648	translation_Loss:8.648	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.1	Hits@10:43.04	Best:23.1
2024-12-27 03:41:11,164: Snapshot:0	Epoch:17	Loss:7.591	translation_Loss:7.591	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.49	Hits@10:43.69	Best:23.49
2024-12-27 03:41:17,012: Snapshot:0	Epoch:18	Loss:6.695	translation_Loss:6.695	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:23.87	Hits@10:44.12	Best:23.87
2024-12-27 03:41:22,922: Snapshot:0	Epoch:19	Loss:5.93	translation_Loss:5.93	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.14	Hits@10:44.37	Best:24.14
2024-12-27 03:41:28,714: Snapshot:0	Epoch:20	Loss:5.225	translation_Loss:5.225	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.36	Hits@10:44.59	Best:24.36
2024-12-27 03:41:34,534: Snapshot:0	Epoch:21	Loss:4.674	translation_Loss:4.674	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.52	Hits@10:44.89	Best:24.52
2024-12-27 03:41:40,350: Snapshot:0	Epoch:22	Loss:4.194	translation_Loss:4.194	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.62	Hits@10:45.07	Best:24.62
2024-12-27 03:41:46,110: Snapshot:0	Epoch:23	Loss:3.781	translation_Loss:3.781	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.77	Hits@10:45.27	Best:24.77
2024-12-27 03:41:51,959: Snapshot:0	Epoch:24	Loss:3.458	translation_Loss:3.458	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:45.39	Best:24.98
2024-12-27 03:41:57,716: Snapshot:0	Epoch:25	Loss:3.106	translation_Loss:3.106	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:24.98	Hits@10:45.56	Best:24.98
2024-12-27 03:42:03,502: Snapshot:0	Epoch:26	Loss:2.851	translation_Loss:2.851	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.09	Hits@10:45.7	Best:25.09
2024-12-27 03:42:09,309: Snapshot:0	Epoch:27	Loss:2.619	translation_Loss:2.619	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.13	Hits@10:45.74	Best:25.13
2024-12-27 03:42:15,128: Snapshot:0	Epoch:28	Loss:2.413	translation_Loss:2.413	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.17	Hits@10:45.93	Best:25.17
2024-12-27 03:42:21,409: Snapshot:0	Epoch:29	Loss:2.253	translation_Loss:2.253	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.31	Hits@10:45.97	Best:25.31
2024-12-27 03:42:27,271: Snapshot:0	Epoch:30	Loss:2.101	translation_Loss:2.101	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.42	Hits@10:46.02	Best:25.42
2024-12-27 03:42:33,067: Snapshot:0	Epoch:31	Loss:1.939	translation_Loss:1.939	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.48	Hits@10:46.13	Best:25.48
2024-12-27 03:42:38,868: Snapshot:0	Epoch:32	Loss:1.819	translation_Loss:1.819	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.41	Hits@10:46.12	Best:25.48
2024-12-27 03:42:44,704: Snapshot:0	Epoch:33	Loss:1.719	translation_Loss:1.719	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.6	Hits@10:46.25	Best:25.6
2024-12-27 03:42:50,552: Snapshot:0	Epoch:34	Loss:1.62	translation_Loss:1.62	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.53	Hits@10:46.02	Best:25.6
2024-12-27 03:42:56,295: Snapshot:0	Epoch:35	Loss:1.528	translation_Loss:1.528	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.58	Hits@10:46.17	Best:25.6
2024-12-27 03:43:02,089: Snapshot:0	Epoch:36	Loss:1.445	translation_Loss:1.445	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.15	Best:25.67
2024-12-27 03:43:07,992: Snapshot:0	Epoch:37	Loss:1.372	translation_Loss:1.372	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.67	Hits@10:46.19	Best:25.67
2024-12-27 03:43:13,833: Snapshot:0	Epoch:38	Loss:1.298	translation_Loss:1.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.68	Hits@10:46.24	Best:25.68
2024-12-27 03:43:19,682: Snapshot:0	Epoch:39	Loss:1.248	translation_Loss:1.248	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.17	Best:25.77
2024-12-27 03:43:25,475: Snapshot:0	Epoch:40	Loss:1.188	translation_Loss:1.188	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.87	Hits@10:46.3	Best:25.87
2024-12-27 03:43:31,346: Snapshot:0	Epoch:41	Loss:1.131	translation_Loss:1.131	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.25	Best:25.87
2024-12-27 03:43:37,548: Snapshot:0	Epoch:42	Loss:1.082	translation_Loss:1.082	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.77	Hits@10:46.3	Best:25.87
2024-12-27 03:43:43,329: Early Stopping! Snapshot: 0 Epoch: 43 Best Results: 25.87
2024-12-27 03:43:43,329: Start to training tokens! Snapshot: 0 Epoch: 43 Loss:1.039 MRR:25.71 Best Results: 25.87
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:43:43,330: Snapshot:0	Epoch:43	Loss:1.039	translation_Loss:1.039	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.21	Best:25.87
2024-12-27 03:43:49,660: Snapshot:0	Epoch:44	Loss:137.245	translation_Loss:35.0	multi_layer_Loss:102.246	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:25.71	Hits@10:46.21	Best:25.87
2024-12-27 03:43:55,520: End of token training: 0 Epoch: 45 Loss:69.731 MRR:25.71 Best Results: 25.87
2024-12-27 03:43:55,520: Snapshot:0	Epoch:45	Loss:69.731	translation_Loss:35.006	multi_layer_Loss:34.725	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:25.71	Hits@10:46.21	Best:25.87
2024-12-27 03:43:55,765: => loading checkpoint './checkpoint/HYBRID/0model_best.tar'
2024-12-27 03:43:58,245: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:0 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2568 | 0.1488 | 0.3151 | 0.3784 |  0.4528 |
+------------+--------+--------+--------+--------+---------+
Snapshot 1: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:44:08,967: Snapshot:1	Epoch:0	Loss:15.13	translation_Loss:15.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.122                                                   	MRR:6.28	Hits@10:10.48	Best:6.28
2024-12-27 03:44:11,241: Snapshot:1	Epoch:1	Loss:13.92	translation_Loss:13.684	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.235                                                   	MRR:6.92	Hits@10:11.57	Best:6.92
2024-12-27 03:44:13,514: Snapshot:1	Epoch:2	Loss:12.897	translation_Loss:12.59	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.308                                                   	MRR:7.71	Hits@10:13.25	Best:7.71
2024-12-27 03:44:15,803: Snapshot:1	Epoch:3	Loss:11.907	translation_Loss:11.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.356                                                   	MRR:8.88	Hits@10:15.83	Best:8.88
2024-12-27 03:44:18,105: Snapshot:1	Epoch:4	Loss:10.914	translation_Loss:10.521	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.393                                                   	MRR:10.33	Hits@10:18.77	Best:10.33
2024-12-27 03:44:20,389: Snapshot:1	Epoch:5	Loss:9.928	translation_Loss:9.505	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.423                                                   	MRR:11.83	Hits@10:21.94	Best:11.83
2024-12-27 03:44:22,729: Snapshot:1	Epoch:6	Loss:8.998	translation_Loss:8.552	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.446                                                   	MRR:13.25	Hits@10:24.25	Best:13.25
2024-12-27 03:44:25,034: Snapshot:1	Epoch:7	Loss:8.17	translation_Loss:7.71	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.461                                                   	MRR:14.55	Hits@10:26.4	Best:14.55
2024-12-27 03:44:27,302: Snapshot:1	Epoch:8	Loss:7.466	translation_Loss:6.996	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.471                                                   	MRR:15.75	Hits@10:28.13	Best:15.75
2024-12-27 03:44:29,583: Snapshot:1	Epoch:9	Loss:6.853	translation_Loss:6.378	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.475                                                   	MRR:16.7	Hits@10:30.46	Best:16.7
2024-12-27 03:44:31,902: Snapshot:1	Epoch:10	Loss:6.338	translation_Loss:5.862	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.476                                                   	MRR:17.78	Hits@10:32.62	Best:17.78
2024-12-27 03:44:34,215: Snapshot:1	Epoch:11	Loss:5.909	translation_Loss:5.435	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.474                                                   	MRR:18.68	Hits@10:34.37	Best:18.68
2024-12-27 03:44:36,518: Snapshot:1	Epoch:12	Loss:5.504	translation_Loss:5.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.471                                                   	MRR:19.46	Hits@10:36.24	Best:19.46
2024-12-27 03:44:38,835: Snapshot:1	Epoch:13	Loss:5.202	translation_Loss:4.734	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.468                                                   	MRR:20.3	Hits@10:37.63	Best:20.3
2024-12-27 03:44:41,141: Snapshot:1	Epoch:14	Loss:4.923	translation_Loss:4.46	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.462                                                   	MRR:20.93	Hits@10:38.89	Best:20.93
2024-12-27 03:44:43,410: Snapshot:1	Epoch:15	Loss:4.684	translation_Loss:4.226	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.458                                                   	MRR:21.47	Hits@10:39.81	Best:21.47
2024-12-27 03:44:45,669: Snapshot:1	Epoch:16	Loss:4.468	translation_Loss:4.014	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.453                                                   	MRR:21.97	Hits@10:40.92	Best:21.97
2024-12-27 03:44:47,980: Snapshot:1	Epoch:17	Loss:4.278	translation_Loss:3.83	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.448                                                   	MRR:22.32	Hits@10:41.47	Best:22.32
2024-12-27 03:44:50,261: Snapshot:1	Epoch:18	Loss:4.118	translation_Loss:3.675	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.442                                                   	MRR:22.71	Hits@10:42.1	Best:22.71
2024-12-27 03:44:52,942: Snapshot:1	Epoch:19	Loss:3.971	translation_Loss:3.533	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.437                                                   	MRR:23.15	Hits@10:42.61	Best:23.15
2024-12-27 03:44:55,235: Snapshot:1	Epoch:20	Loss:3.831	translation_Loss:3.4	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.43                                                   	MRR:23.49	Hits@10:43.25	Best:23.49
2024-12-27 03:44:57,498: Snapshot:1	Epoch:21	Loss:3.72	translation_Loss:3.296	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.424                                                   	MRR:23.9	Hits@10:43.71	Best:23.9
2024-12-27 03:44:59,814: Snapshot:1	Epoch:22	Loss:3.604	translation_Loss:3.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.419                                                   	MRR:24.15	Hits@10:44.22	Best:24.15
2024-12-27 03:45:02,097: Snapshot:1	Epoch:23	Loss:3.499	translation_Loss:3.086	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.413                                                   	MRR:24.54	Hits@10:44.55	Best:24.54
2024-12-27 03:45:04,428: Snapshot:1	Epoch:24	Loss:3.41	translation_Loss:3.002	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.408                                                   	MRR:24.87	Hits@10:44.95	Best:24.87
2024-12-27 03:45:06,744: Snapshot:1	Epoch:25	Loss:3.327	translation_Loss:2.926	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.402                                                   	MRR:25.22	Hits@10:45.32	Best:25.22
2024-12-27 03:45:09,049: Snapshot:1	Epoch:26	Loss:3.249	translation_Loss:2.853	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.396                                                   	MRR:25.49	Hits@10:45.64	Best:25.49
2024-12-27 03:45:11,336: Snapshot:1	Epoch:27	Loss:3.166	translation_Loss:2.777	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.389                                                   	MRR:25.68	Hits@10:45.96	Best:25.68
2024-12-27 03:45:13,666: Snapshot:1	Epoch:28	Loss:3.105	translation_Loss:2.722	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.382                                                   	MRR:25.91	Hits@10:46.1	Best:25.91
2024-12-27 03:45:15,933: Snapshot:1	Epoch:29	Loss:3.054	translation_Loss:2.677	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.377                                                   	MRR:26.25	Hits@10:46.41	Best:26.25
2024-12-27 03:45:18,201: Snapshot:1	Epoch:30	Loss:2.995	translation_Loss:2.623	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.372                                                   	MRR:26.45	Hits@10:46.56	Best:26.45
2024-12-27 03:45:20,481: Snapshot:1	Epoch:31	Loss:2.932	translation_Loss:2.565	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.367                                                   	MRR:26.66	Hits@10:46.79	Best:26.66
2024-12-27 03:45:22,777: Snapshot:1	Epoch:32	Loss:2.885	translation_Loss:2.522	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.363                                                   	MRR:26.91	Hits@10:46.91	Best:26.91
2024-12-27 03:45:25,086: Snapshot:1	Epoch:33	Loss:2.84	translation_Loss:2.481	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.358                                                   	MRR:27.08	Hits@10:46.98	Best:27.08
2024-12-27 03:45:27,359: Snapshot:1	Epoch:34	Loss:2.809	translation_Loss:2.454	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.354                                                   	MRR:27.08	Hits@10:47.22	Best:27.08
2024-12-27 03:45:29,685: Snapshot:1	Epoch:35	Loss:2.762	translation_Loss:2.411	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.35                                                   	MRR:27.17	Hits@10:47.23	Best:27.17
2024-12-27 03:45:31,972: Snapshot:1	Epoch:36	Loss:2.722	translation_Loss:2.376	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.346                                                   	MRR:27.2	Hits@10:47.32	Best:27.2
2024-12-27 03:45:34,266: Snapshot:1	Epoch:37	Loss:2.688	translation_Loss:2.346	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.342                                                   	MRR:27.28	Hits@10:47.34	Best:27.28
2024-12-27 03:45:36,567: Snapshot:1	Epoch:38	Loss:2.651	translation_Loss:2.312	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.339                                                   	MRR:27.49	Hits@10:47.35	Best:27.49
2024-12-27 03:45:38,885: Snapshot:1	Epoch:39	Loss:2.633	translation_Loss:2.298	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.335                                                   	MRR:27.57	Hits@10:47.59	Best:27.57
2024-12-27 03:45:41,197: Snapshot:1	Epoch:40	Loss:2.593	translation_Loss:2.26	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.332                                                   	MRR:27.6	Hits@10:47.69	Best:27.6
2024-12-27 03:45:43,500: Snapshot:1	Epoch:41	Loss:2.57	translation_Loss:2.241	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.328                                                   	MRR:27.74	Hits@10:47.72	Best:27.74
2024-12-27 03:45:45,806: Snapshot:1	Epoch:42	Loss:2.543	translation_Loss:2.218	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.325                                                   	MRR:27.89	Hits@10:47.8	Best:27.89
2024-12-27 03:45:48,137: Snapshot:1	Epoch:43	Loss:2.506	translation_Loss:2.184	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.322                                                   	MRR:28.0	Hits@10:47.92	Best:28.0
2024-12-27 03:45:50,451: Snapshot:1	Epoch:44	Loss:2.488	translation_Loss:2.167	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.321                                                   	MRR:28.1	Hits@10:47.92	Best:28.1
2024-12-27 03:45:52,748: Snapshot:1	Epoch:45	Loss:2.467	translation_Loss:2.149	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.318                                                   	MRR:28.18	Hits@10:47.99	Best:28.18
2024-12-27 03:45:55,073: Snapshot:1	Epoch:46	Loss:2.443	translation_Loss:2.128	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.316                                                   	MRR:28.22	Hits@10:48.12	Best:28.22
2024-12-27 03:45:57,355: Snapshot:1	Epoch:47	Loss:2.423	translation_Loss:2.11	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.313                                                   	MRR:28.19	Hits@10:48.25	Best:28.22
2024-12-27 03:45:59,622: Snapshot:1	Epoch:48	Loss:2.404	translation_Loss:2.093	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.311                                                   	MRR:28.25	Hits@10:48.31	Best:28.25
2024-12-27 03:46:01,913: Snapshot:1	Epoch:49	Loss:2.392	translation_Loss:2.083	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.309                                                   	MRR:28.25	Hits@10:48.38	Best:28.25
2024-12-27 03:46:04,203: Snapshot:1	Epoch:50	Loss:2.362	translation_Loss:2.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.308                                                   	MRR:28.33	Hits@10:48.56	Best:28.33
2024-12-27 03:46:06,493: Snapshot:1	Epoch:51	Loss:2.353	translation_Loss:2.048	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.305                                                   	MRR:28.37	Hits@10:48.55	Best:28.37
2024-12-27 03:46:09,261: Snapshot:1	Epoch:52	Loss:2.337	translation_Loss:2.035	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.302                                                   	MRR:28.39	Hits@10:48.75	Best:28.39
2024-12-27 03:46:11,548: Snapshot:1	Epoch:53	Loss:2.313	translation_Loss:2.012	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.301                                                   	MRR:28.52	Hits@10:48.67	Best:28.52
2024-12-27 03:46:13,856: Snapshot:1	Epoch:54	Loss:2.308	translation_Loss:2.008	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.3                                                   	MRR:28.61	Hits@10:48.84	Best:28.61
2024-12-27 03:46:16,122: Snapshot:1	Epoch:55	Loss:2.289	translation_Loss:1.991	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.298                                                   	MRR:28.57	Hits@10:48.86	Best:28.61
2024-12-27 03:46:18,395: Snapshot:1	Epoch:56	Loss:2.269	translation_Loss:1.971	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.298                                                   	MRR:28.79	Hits@10:48.92	Best:28.79
2024-12-27 03:46:20,683: Snapshot:1	Epoch:57	Loss:2.262	translation_Loss:1.966	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.296                                                   	MRR:28.64	Hits@10:48.95	Best:28.79
2024-12-27 03:46:22,907: Snapshot:1	Epoch:58	Loss:2.256	translation_Loss:1.963	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.293                                                   	MRR:28.65	Hits@10:48.95	Best:28.79
2024-12-27 03:46:25,191: Early Stopping! Snapshot: 1 Epoch: 59 Best Results: 28.79
2024-12-27 03:46:25,191: Start to training tokens! Snapshot: 1 Epoch: 59 Loss:2.241 MRR:28.62 Best Results: 28.79
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:46:25,192: Snapshot:1	Epoch:59	Loss:2.241	translation_Loss:1.949	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.292                                                   	MRR:28.62	Hits@10:49.03	Best:28.79
2024-12-27 03:46:27,410: Snapshot:1	Epoch:60	Loss:59.698	translation_Loss:13.172	multi_layer_Loss:46.527	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:28.62	Hits@10:49.03	Best:28.79
2024-12-27 03:46:29,671: End of token training: 1 Epoch: 61 Loss:49.72 MRR:28.62 Best Results: 28.79
2024-12-27 03:46:29,671: Snapshot:1	Epoch:61	Loss:49.72	translation_Loss:13.16	multi_layer_Loss:36.56	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:28.62	Hits@10:49.03	Best:28.79
2024-12-27 03:46:29,960: => loading checkpoint './checkpoint/HYBRID/1model_best.tar'
2024-12-27 03:46:33,629: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:1 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2587 | 0.152  | 0.3164 | 0.3795 |  0.4521 |
|     1      | 0.2829 | 0.1775 | 0.3304 | 0.4011 |  0.4913 |
+------------+--------+--------+--------+--------+---------+
Snapshot 2: Resetting for new snapshot...
Token frozen for new snapshot.
Embeddings are now trainable.
Reinitializing token...
Optimizer reset: Training embeddings and reinitialized token.
Start training =============================
2024-12-27 03:47:06,196: Snapshot:2	Epoch:0	Loss:62.576	translation_Loss:61.663	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:0.913                                                   	MRR:5.01	Hits@10:10.55	Best:5.01
2024-12-27 03:47:16,336: Snapshot:2	Epoch:1	Loss:52.165	translation_Loss:50.76	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.405                                                   	MRR:7.98	Hits@10:17.3	Best:7.98
2024-12-27 03:47:26,296: Snapshot:2	Epoch:2	Loss:42.864	translation_Loss:41.25	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.614                                                   	MRR:10.78	Hits@10:23.15	Best:10.78
2024-12-27 03:47:36,257: Snapshot:2	Epoch:3	Loss:35.424	translation_Loss:33.708	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.716                                                   	MRR:12.88	Hits@10:27.33	Best:12.88
2024-12-27 03:47:46,237: Snapshot:2	Epoch:4	Loss:29.827	translation_Loss:28.087	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.741                                                   	MRR:14.51	Hits@10:29.76	Best:14.51
2024-12-27 03:47:56,154: Snapshot:2	Epoch:5	Loss:25.71	translation_Loss:24.01	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.7                                                   	MRR:15.72	Hits@10:31.47	Best:15.72
2024-12-27 03:48:06,306: Snapshot:2	Epoch:6	Loss:22.671	translation_Loss:21.054	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.617                                                   	MRR:16.64	Hits@10:32.77	Best:16.64
2024-12-27 03:48:16,511: Snapshot:2	Epoch:7	Loss:20.453	translation_Loss:18.918	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.535                                                   	MRR:17.31	Hits@10:33.82	Best:17.31
2024-12-27 03:48:26,473: Snapshot:2	Epoch:8	Loss:18.845	translation_Loss:17.382	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.463                                                   	MRR:17.94	Hits@10:34.37	Best:17.94
2024-12-27 03:48:36,551: Snapshot:2	Epoch:9	Loss:17.616	translation_Loss:16.215	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.401                                                   	MRR:18.37	Hits@10:34.82	Best:18.37
2024-12-27 03:48:46,575: Snapshot:2	Epoch:10	Loss:16.709	translation_Loss:15.358	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.351                                                   	MRR:18.7	Hits@10:35.14	Best:18.7
2024-12-27 03:48:56,650: Snapshot:2	Epoch:11	Loss:16.018	translation_Loss:14.697	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.321                                                   	MRR:19.1	Hits@10:35.44	Best:19.1
2024-12-27 03:49:06,718: Snapshot:2	Epoch:12	Loss:15.393	translation_Loss:14.109	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.284                                                   	MRR:19.31	Hits@10:35.66	Best:19.31
2024-12-27 03:49:16,840: Snapshot:2	Epoch:13	Loss:14.936	translation_Loss:13.68	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.256                                                   	MRR:19.47	Hits@10:35.77	Best:19.47
2024-12-27 03:49:26,868: Snapshot:2	Epoch:14	Loss:14.583	translation_Loss:13.352	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.231                                                   	MRR:19.61	Hits@10:35.75	Best:19.61
2024-12-27 03:49:36,809: Snapshot:2	Epoch:15	Loss:14.249	translation_Loss:13.032	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.218                                                   	MRR:19.81	Hits@10:35.83	Best:19.81
2024-12-27 03:49:46,879: Snapshot:2	Epoch:16	Loss:13.963	translation_Loss:12.764	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.199                                                   	MRR:19.92	Hits@10:35.92	Best:19.92
2024-12-27 03:49:56,845: Snapshot:2	Epoch:17	Loss:13.789	translation_Loss:12.602	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.187                                                   	MRR:19.88	Hits@10:35.92	Best:19.92
2024-12-27 03:50:07,398: Snapshot:2	Epoch:18	Loss:13.586	translation_Loss:12.416	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.17                                                   	MRR:20.03	Hits@10:35.95	Best:20.03
2024-12-27 03:50:17,463: Snapshot:2	Epoch:19	Loss:13.455	translation_Loss:12.292	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.163                                                   	MRR:20.09	Hits@10:36.08	Best:20.09
2024-12-27 03:50:27,458: Snapshot:2	Epoch:20	Loss:13.31	translation_Loss:12.153	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.157                                                   	MRR:20.1	Hits@10:36.06	Best:20.1
2024-12-27 03:50:37,501: Snapshot:2	Epoch:21	Loss:13.18	translation_Loss:12.03	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.15                                                   	MRR:20.11	Hits@10:36.16	Best:20.11
2024-12-27 03:50:47,511: Snapshot:2	Epoch:22	Loss:13.045	translation_Loss:11.905	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.14                                                   	MRR:20.1	Hits@10:36.08	Best:20.11
2024-12-27 03:50:57,492: Snapshot:2	Epoch:23	Loss:12.967	translation_Loss:11.834	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.133                                                   	MRR:20.08	Hits@10:36.12	Best:20.11
2024-12-27 03:51:07,880: Snapshot:2	Epoch:24	Loss:12.897	translation_Loss:11.768	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.128                                                   	MRR:20.18	Hits@10:36.16	Best:20.18
2024-12-27 03:51:17,949: Snapshot:2	Epoch:25	Loss:12.82	translation_Loss:11.693	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.126                                                   	MRR:20.19	Hits@10:36.18	Best:20.19
2024-12-27 03:51:27,906: Snapshot:2	Epoch:26	Loss:12.764	translation_Loss:11.645	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.119                                                   	MRR:20.11	Hits@10:36.08	Best:20.19
2024-12-27 03:51:37,881: Snapshot:2	Epoch:27	Loss:12.709	translation_Loss:11.589	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.12                                                   	MRR:20.27	Hits@10:36.09	Best:20.27
2024-12-27 03:51:47,863: Snapshot:2	Epoch:28	Loss:12.643	translation_Loss:11.529	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.113                                                   	MRR:20.25	Hits@10:36.16	Best:20.27
2024-12-27 03:51:57,851: Snapshot:2	Epoch:29	Loss:12.619	translation_Loss:11.509	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.11                                                   	MRR:20.18	Hits@10:36.21	Best:20.27
2024-12-27 03:52:07,808: Early Stopping! Snapshot: 2 Epoch: 30 Best Results: 20.27
2024-12-27 03:52:07,808: Start to training tokens! Snapshot: 2 Epoch: 30 Loss:12.561 MRR:20.15 Best Results: 20.27
Token added to optimizer, embeddings excluded successfully.
Optimizer parameter groups:
Group 0:
 - torch.Size([10, 200]), requires_grad: True
 - torch.Size([10, 200]), requires_grad: True
2024-12-27 03:52:07,808: Snapshot:2	Epoch:30	Loss:12.561	translation_Loss:11.459	multi_layer_Loss:0.0	MAE_Loss:0.0	decompose_Loss:1.102                                                   	MRR:20.15	Hits@10:36.19	Best:20.27
2024-12-27 03:52:17,523: Snapshot:2	Epoch:31	Loss:178.775	translation_Loss:58.056	multi_layer_Loss:120.72	MAE_Loss:0.0	decompose_Loss:0.0                                                   	MRR:20.15	Hits@10:36.19	Best:20.27
2024-12-27 03:52:27,308: End of token training: 2 Epoch: 32 Loss:73.538 MRR:20.15 Best Results: 20.27
2024-12-27 03:52:27,308: Snapshot:2	Epoch:32	Loss:73.538	translation_Loss:58.052	multi_layer_Loss:15.486	MAE_Loss:0.0	decompose_Loss:0.0                                                           	MRR:20.15	Hits@10:36.19	Best:20.27
2024-12-27 03:52:27,625: => loading checkpoint './checkpoint/HYBRID/2model_best.tar'
2024-12-27 03:52:35,843: 
+------------+--------+--------+--------+--------+---------+
| Snapshot:2 |  MRR   | Hits@1 | Hits@3 | Hits@5 | Hits@10 |
+------------+--------+--------+--------+--------+---------+
|     0      | 0.2575 | 0.1492 | 0.3161 | 0.3784 |  0.4534 |
|     1      | 0.2783 | 0.1721 | 0.3241 | 0.395  |  0.4902 |
|     2      | 0.2013 | 0.1177 | 0.2322 | 0.2885 |  0.3634 |
+------------+--------+--------+--------+--------+---------+
